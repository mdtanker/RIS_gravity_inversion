{"traceEvents": [{"ph": "M", "pid": 573092, "tid": 573092, "name": "process_name", "args": {"name": "MainProcess"}}, {"ph": "M", "pid": 573092, "tid": 573092, "name": "thread_name", "args": {"name": "MainThread"}}, {"pid": 573092, "tid": 573092, "ts": 7628140201183.685, "dur": 0.544, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201191.729, "dur": 0.399, "name": "builtins.getattr", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201191.302, "dur": 1.426, "name": "_check (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/generic.py:43)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201189.022, "dur": 4.17, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201194.687, "dur": 0.227, "name": "builtins.hash", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201194.155, "dur": 1.052, "name": "is_hashable (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/inference.py:321)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201187.862, "dur": 7.572, "name": "maybe_extract_name (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/indexes/base.py:7082)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201198.202, "dur": 4.633, "name": "pandas._libs.lib.is_list_like", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201203.161, "dur": 0.321, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201197.166, "dur": 6.825, "name": "is_empty_data (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/construction.py:802)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201204.961, "dur": 0.14, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201205.357, "dur": 0.078, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201206.546, "dur": 0.067, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201206.845, "dur": 0.07, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201210.399, "dur": 0.297, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201209.867, "dur": 0.959, "name": "<genexpr> (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/inference.py:288)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201211.328, "dur": 0.142, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201211.085, "dur": 0.437, "name": "<genexpr> (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/inference.py:288)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201209.574, "dur": 2.058, "name": "builtins.all", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201212.236, "dur": 0.274, "name": "<genexpr> (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/inference.py:288)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201207.775, "dur": 5.559, "name": "is_dict_like (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/inference.py:262)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201213.901, "dur": 0.186, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201214.88, "dur": 0.107, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201219.088, "dur": 1.872, "name": "_abc._abc_instancecheck", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201218.552, "dur": 2.542, "name": "__instancecheck__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/abc.py:117)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201217.161, "dur": 4.148, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201226.126, "dur": 1.785, "name": "_check_methods (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/_collections_abc.py:78)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201225.177, "dur": 2.993, "name": "__subclasshook__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/_collections_abc.py:381)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201224.045, "dur": 4.927, "name": "_abc._abc_subclasscheck", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201223.69, "dur": 5.373, "name": "__subclasscheck__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/abc.py:121)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201222.227, "dur": 7.044, "name": "_abc._abc_instancecheck", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201222.097, "dur": 7.244, "name": "__instancecheck__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/abc.py:117)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201221.835, "dur": 7.703, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201230.093, "dur": 0.205, "name": "cast (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/typing.py:1736)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201216.216, "dur": 14.296, "name": "maybe_iterable_to_list (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/common.py:287)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201230.924, "dur": 0.205, "name": "pandas._libs.lib.is_list_like", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201231.915, "dur": 0.134, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201237.721, "dur": 0.36, "name": "type.__new__", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201238.438, "dur": 0.089, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201242.471, "dur": 3.137, "name": "_reset_identity (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/indexes/base.py:834)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201237.137, "dur": 8.83, "name": "_simple_new (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/indexes/range.py:167)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201232.86, "dur": 13.388, "name": "default_index (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/indexes/api.py:322)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201246.769, "dur": 0.151, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201249.565, "dur": 0.09, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201250.181, "dur": 0.108, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201253.371, "dur": 0.155, "name": "builtins.getattr", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201253.241, "dur": 0.89, "name": "_check (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/generic.py:43)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201256.357, "dur": 0.068, "name": "builtins.getattr", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201256.245, "dur": 0.312, "name": "_check (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/generic.py:43)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201252.047, "dur": 4.652, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201258.374, "dur": 0.068, "name": "builtins.getattr", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201258.294, "dur": 0.384, "name": "_check (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/generic.py:43)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201257.159, "dur": 1.636, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201251.074, "dur": 7.835, "name": "extract_array (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/construction.py:379)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201259.337, "dur": 0.091, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201259.768, "dur": 0.07, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201260.006, "dur": 0.138, "name": "pandas._libs.lib.is_list_like", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201260.422, "dur": 0.072, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201262.271, "dur": 0.058, "name": "builtins.getattr", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201262.185, "dur": 0.487, "name": "_check (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/generic.py:43)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201261.113, "dur": 1.679, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201263.179, "dur": 0.176, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201263.667, "dur": 0.365, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201264.63, "dur": 0.077, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201267.178, "dur": 0.084, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201269.248, "dur": 0.068, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201269.488, "dur": 12.621, "name": "numpy.empty", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201268.061, "dur": 24.322, "name": "construct_1d_object_array_from_listlike (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/cast.py:1960)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201294.589, "dur": 0.123, "name": "cast (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/typing.py:1736)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201333.826, "dur": 3.491, "name": "numpy.asarray", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201338.019, "dur": 1.49, "name": "numpy.empty", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201342.952, "dur": 0.44, "name": "copyto (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/numpy/core/multiarray.py:1071)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201344.087, "dur": 7.106, "name": "numpy.core._multiarray_umath.implement_array_function", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201341.208, "dur": 10.123, "name": "copyto (<__array_function__ internals>:2)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201332.615, "dur": 19.027, "name": "full (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/numpy/core/numeric.py:289)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201296.412, "dur": 64.351, "name": "pandas._libs.lib.maybe_convert_objects", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201266.248, "dur": 94.726, "name": "maybe_convert_platform (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/cast.py:115)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201362.086, "dur": 0.099, "name": "cast (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/typing.py:1736)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201363.507, "dur": 0.119, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201364.434, "dur": 0.723, "name": "numpy.array", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201366.722, "dur": 0.14, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201368.944, "dur": 6.385, "name": "pandas._libs.algos.ensure_object", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201375.589, "dur": 10.807, "name": "pandas._libs.lib.infer_datetimelike_array", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201362.943, "dur": 26.425, "name": "maybe_infer_to_datetimelike (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/cast.py:1466)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201391.275, "dur": 0.241, "name": "builtins.getattr", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201392.951, "dur": 0.107, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201392.611, "dur": 0.761, "name": "_maybe_repeat (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/construction.py:684)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201390.898, "dur": 2.645, "name": "_sanitize_ndim (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/construction.py:627)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201393.904, "dur": 0.082, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201394.557, "dur": 0.082, "name": "cast (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/typing.py:1736)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201396.256, "dur": 0.328, "name": "builtins.issubclass", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201395.274, "dur": 1.468, "name": "_sanitize_str_dtypes (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/construction.py:664)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201247.921, "dur": 149.043, "name": "sanitize_array (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/construction.py:470)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201403.21, "dur": 1.303, "name": "_select_options (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:571)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201407.233, "dur": 0.096, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201407.55, "dur": 0.053, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201409.633, "dur": 1.99, "name": "_get_deprecated_option (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:603)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201408.853, "dur": 3.18, "name": "_warn_if_deprecated (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:642)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201412.746, "dur": 0.387, "name": "_get_deprecated_option (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:603)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201412.407, "dur": 0.92, "name": "_translate_key (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:630)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201401.988, "dur": 11.56, "name": "_get_single_key (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:109)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201415.004, "dur": 0.457, "name": "str.split", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201414.605, "dur": 2.645, "name": "_get_root (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:589)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201401.04, "dur": 17.157, "name": "_get_option (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:127)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201399.975, "dur": 18.45, "name": "__call__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:255)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201424.096, "dur": 0.201, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201423.611, "dur": 0.818, "name": "__len__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/indexes/range.py:909)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201422.643, "dur": 1.995, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201427.209, "dur": 0.48, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201439.272, "dur": 0.354, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201438.448, "dur": 1.327, "name": "is_1d_only_ea_dtype (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/common.py:1416)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201440.221, "dur": 3.174, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201443.586, "dur": 0.109, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201436.862, "dur": 7.073, "name": "check_ndim (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/internals/blocks.py:2055)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201445.948, "dur": 0.169, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201446.452, "dur": 0.092, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201447.217, "dur": 0.085, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201447.539, "dur": 0.092, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201444.778, "dur": 3.86, "name": "get_block_type (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/internals/blocks.py:1989)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201449.752, "dur": 0.064, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201451.03, "dur": 0.05, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201450.677, "dur": 0.915, "name": "ensure_wrapped_if_datetimelike (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/construction.py:438)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201452.114, "dur": 0.135, "name": "builtins.issubclass", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201452.84, "dur": 0.305, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201449.275, "dur": 4.011, "name": "maybe_coerce_values (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/internals/blocks.py:1960)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201426.261, "dur": 29.282, "name": "new_block (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/internals/blocks.py:2041)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201456.68, "dur": 2.556, "name": "__init__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/internals/managers.py:1700)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201421.626, "dur": 37.944, "name": "from_array (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/internals/managers.py:1731)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201469.899, "dur": 2.121, "name": "__init__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/flags.py:47)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201461.928, "dur": 11.348, "name": "__init__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/generic.py:239)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201483.501, "dur": 2.441, "name": "__getattr__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/generic.py:5561)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201480.734, "dur": 6.637, "name": "name (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/series.py:590)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201495.727, "dur": 0.169, "name": "builtins.hash", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201495.501, "dur": 0.584, "name": "is_hashable (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/inference.py:321)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201494.983, "dur": 1.263, "name": "<genexpr> (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/common.py:1740)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201496.467, "dur": 0.187, "name": "<genexpr> (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/common.py:1740)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201494.719, "dur": 2.189, "name": "builtins.all", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201493.627, "dur": 3.481, "name": "validate_all_hashable (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/common.py:1721)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201491.313, "dur": 7.001, "name": "name (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/series.py:640)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201479.047, "dur": 19.696, "name": "__setattr__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/generic.py:5577)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201503.364, "dur": 0.393, "name": "_is_all_dates (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/indexes/numeric.py:331)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201502.175, "dur": 2.218, "name": "_set_axis (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/series.py:542)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201182.463, "dur": 322.286, "name": "__init__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/series.py:323)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201506.52, "dur": 0.243, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201508.613, "dur": 0.112, "name": "builtins.getattr", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201508.449, "dur": 0.456, "name": "_check (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/generic.py:43)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201508.077, "dur": 0.959, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201509.504, "dur": 0.064, "name": "builtins.hash", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201509.348, "dur": 0.288, "name": "is_hashable (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/inference.py:321)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201507.653, "dur": 2.134, "name": "maybe_extract_name (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/indexes/base.py:7082)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201510.402, "dur": 0.211, "name": "pandas._libs.lib.is_list_like", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201510.86, "dur": 0.129, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201510.159, "dur": 1.202, "name": "is_empty_data (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/construction.py:802)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201511.949, "dur": 0.11, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201512.257, "dur": 0.07, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201512.53, "dur": 0.07, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201512.741, "dur": 0.072, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201514.3, "dur": 0.316, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201514.035, "dur": 0.659, "name": "<genexpr> (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/inference.py:288)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201515.121, "dur": 0.101, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201514.903, "dur": 0.391, "name": "<genexpr> (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/inference.py:288)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201513.823, "dur": 1.597, "name": "builtins.all", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201515.848, "dur": 0.115, "name": "<genexpr> (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/inference.py:288)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201513.107, "dur": 3.265, "name": "is_dict_like (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/inference.py:262)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201516.807, "dur": 0.182, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201517.251, "dur": 0.07, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201518.611, "dur": 0.575, "name": "_abc._abc_instancecheck", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201518.465, "dur": 0.797, "name": "__instancecheck__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/abc.py:117)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201518.15, "dur": 1.261, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201520.109, "dur": 0.235, "name": "_abc._abc_instancecheck", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201520.027, "dur": 0.39, "name": "__instancecheck__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/abc.py:117)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201519.779, "dur": 0.739, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201520.858, "dur": 0.103, "name": "cast (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/typing.py:1736)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201517.79, "dur": 3.326, "name": "maybe_iterable_to_list (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/common.py:287)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201521.423, "dur": 0.12, "name": "pandas._libs.lib.is_list_like", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201521.772, "dur": 0.137, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201523.702, "dur": 0.281, "name": "type.__new__", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201524.247, "dur": 0.07, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201525.313, "dur": 0.512, "name": "_reset_identity (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/indexes/base.py:834)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201523.379, "dur": 2.674, "name": "_simple_new (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/indexes/range.py:167)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201522.127, "dur": 4.114, "name": "default_index (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/indexes/api.py:322)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201526.597, "dur": 0.11, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201527.456, "dur": 0.119, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201527.768, "dur": 0.082, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201528.955, "dur": 0.101, "name": "builtins.getattr", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201528.851, "dur": 0.397, "name": "_check (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/generic.py:43)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201529.576, "dur": 0.059, "name": "builtins.getattr", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201529.48, "dur": 1.778, "name": "_check (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/generic.py:43)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201528.493, "dur": 2.948, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201531.995, "dur": 0.065, "name": "builtins.getattr", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201531.906, "dur": 0.269, "name": "_check (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/generic.py:43)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201531.689, "dur": 0.598, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201528.28, "dur": 4.148, "name": "extract_array (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/construction.py:379)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201532.788, "dur": 0.094, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201533.073, "dur": 0.086, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201533.298, "dur": 0.125, "name": "pandas._libs.lib.is_list_like", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201533.639, "dur": 0.072, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201534.364, "dur": 0.058, "name": "builtins.getattr", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201534.282, "dur": 0.28, "name": "_check (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/generic.py:43)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201533.945, "dur": 0.701, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201534.98, "dur": 0.147, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201535.302, "dur": 0.127, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201535.902, "dur": 0.089, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201536.695, "dur": 0.075, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201537.633, "dur": 0.067, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201537.8, "dur": 1.928, "name": "numpy.empty", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201537.27, "dur": 5.216, "name": "construct_1d_object_array_from_listlike (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/cast.py:1960)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201543.309, "dur": 0.123, "name": "cast (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/typing.py:1736)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201552.734, "dur": 0.944, "name": "numpy.asarray", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201553.958, "dur": 0.832, "name": "numpy.empty", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201556.155, "dur": 0.252, "name": "copyto (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/numpy/core/multiarray.py:1071)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201556.797, "dur": 1.774, "name": "numpy.core._multiarray_umath.implement_array_function", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201555.433, "dur": 3.24, "name": "copyto (<__array_function__ internals>:2)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201552.408, "dur": 6.55, "name": "full (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/numpy/core/numeric.py:289)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201543.789, "dur": 18.232, "name": "pandas._libs.lib.maybe_convert_objects", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201536.34, "dur": 26.25, "name": "maybe_convert_platform (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/cast.py:115)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201566.499, "dur": 0.229, "name": "builtins.getattr", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201567.454, "dur": 0.131, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201567.247, "dur": 0.578, "name": "_maybe_repeat (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/construction.py:684)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201566.274, "dur": 1.701, "name": "_sanitize_ndim (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/construction.py:627)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201568.303, "dur": 0.08, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201568.73, "dur": 0.082, "name": "cast (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/typing.py:1736)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201569.765, "dur": 0.379, "name": "builtins.issubclass", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201569.128, "dur": 1.154, "name": "_sanitize_str_dtypes (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/construction.py:664)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201527.233, "dur": 43.194, "name": "sanitize_array (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/construction.py:470)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201572.059, "dur": 0.451, "name": "_select_options (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:571)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201572.768, "dur": 0.078, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201573.032, "dur": 0.065, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201573.78, "dur": 0.684, "name": "_get_deprecated_option (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:603)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201573.528, "dur": 1.194, "name": "_warn_if_deprecated (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:642)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201575.19, "dur": 0.32, "name": "_get_deprecated_option (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:603)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201575.005, "dur": 0.648, "name": "_translate_key (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:630)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201571.817, "dur": 3.996, "name": "_get_single_key (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:109)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201578.018, "dur": 0.291, "name": "str.split", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201577.78, "dur": 1.623, "name": "_get_root (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:589)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201571.578, "dur": 8.275, "name": "_get_option (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:127)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201571.055, "dur": 9.055, "name": "__call__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:255)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201581.95, "dur": 0.098, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201581.723, "dur": 0.43, "name": "__len__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/indexes/range.py:909)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201581.386, "dur": 0.905, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201583.053, "dur": 0.154, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201584.75, "dur": 0.229, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201584.48, "dur": 0.624, "name": "is_1d_only_ea_dtype (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/common.py:1416)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201585.482, "dur": 0.261, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201585.884, "dur": 0.107, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201584.066, "dur": 2.073, "name": "check_ndim (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/internals/blocks.py:2055)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201587.03, "dur": 0.173, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201587.375, "dur": 0.099, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201587.736, "dur": 0.103, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201587.999, "dur": 0.073, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201586.475, "dur": 2.21, "name": "get_block_type (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/internals/blocks.py:1989)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201589.185, "dur": 0.051, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201589.676, "dur": 0.049, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201589.485, "dur": 0.67, "name": "ensure_wrapped_if_datetimelike (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/construction.py:438)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201590.626, "dur": 0.084, "name": "builtins.issubclass", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201590.96, "dur": 0.144, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201588.963, "dur": 2.26, "name": "maybe_coerce_values (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/internals/blocks.py:1960)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201582.828, "dur": 9.833, "name": "new_block (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/internals/blocks.py:2041)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201593.174, "dur": 0.822, "name": "__init__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/internals/managers.py:1700)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201581.087, "dur": 13.11, "name": "from_array (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/internals/managers.py:1731)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201597.072, "dur": 0.803, "name": "__init__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/flags.py:47)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201594.811, "dur": 3.47, "name": "__init__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/generic.py:239)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201599.94, "dur": 1.001, "name": "__getattr__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/generic.py:5561)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201599.13, "dur": 2.553, "name": "name (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/series.py:590)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201605.191, "dur": 0.117, "name": "builtins.hash", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201605.016, "dur": 0.418, "name": "is_hashable (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/inference.py:321)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201604.751, "dur": 0.811, "name": "<genexpr> (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/common.py:1740)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201605.714, "dur": 0.168, "name": "<genexpr> (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/common.py:1740)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201604.545, "dur": 1.513, "name": "builtins.all", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201604.024, "dur": 2.196, "name": "validate_all_hashable (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/common.py:1721)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201603.2, "dur": 3.611, "name": "name (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/series.py:640)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201598.66, "dur": 8.512, "name": "__setattr__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/generic.py:5577)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201608.172, "dur": 0.115, "name": "_is_all_dates (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/indexes/numeric.py:331)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201607.888, "dur": 0.712, "name": "_set_axis (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/series.py:542)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201506.184, "dur": 102.678, "name": "__init__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/series.py:323)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201610.175, "dur": 0.156, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201611.781, "dur": 0.089, "name": "builtins.getattr", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201611.643, "dur": 0.369, "name": "_check (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/generic.py:43)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201611.248, "dur": 0.879, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201612.481, "dur": 0.062, "name": "builtins.hash", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201612.354, "dur": 4.438, "name": "is_hashable (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/inference.py:321)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201610.894, "dur": 6.072, "name": "maybe_extract_name (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/indexes/base.py:7082)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201617.532, "dur": 0.182, "name": "pandas._libs.lib.is_list_like", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201617.901, "dur": 0.104, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201617.341, "dur": 0.991, "name": "is_empty_data (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/construction.py:802)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201618.811, "dur": 0.114, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201619.083, "dur": 0.082, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201619.376, "dur": 0.067, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201619.585, "dur": 0.066, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201620.919, "dur": 0.207, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201620.719, "dur": 0.498, "name": "<genexpr> (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/inference.py:288)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201621.509, "dur": 0.082, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201621.335, "dur": 0.329, "name": "<genexpr> (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/inference.py:288)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201620.531, "dur": 1.206, "name": "builtins.all", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201622.018, "dur": 0.13, "name": "<genexpr> (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/inference.py:288)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201619.923, "dur": 2.548, "name": "is_dict_like (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/inference.py:262)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201622.88, "dur": 0.124, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201623.214, "dur": 0.069, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201624.213, "dur": 0.37, "name": "_abc._abc_instancecheck", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201624.087, "dur": 0.574, "name": "__instancecheck__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/abc.py:117)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201623.871, "dur": 0.898, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201625.317, "dur": 0.171, "name": "_abc._abc_instancecheck", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201625.247, "dur": 0.31, "name": "__instancecheck__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/abc.py:117)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201625.015, "dur": 0.64, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201625.925, "dur": 0.106, "name": "cast (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/typing.py:1736)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201623.616, "dur": 2.508, "name": "maybe_iterable_to_list (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/common.py:287)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201626.406, "dur": 0.119, "name": "pandas._libs.lib.is_list_like", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201626.706, "dur": 0.119, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201628.021, "dur": 0.192, "name": "type.__new__", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201628.427, "dur": 0.07, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201629.272, "dur": 0.352, "name": "_reset_identity (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/indexes/base.py:834)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201627.828, "dur": 1.962, "name": "_simple_new (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/indexes/range.py:167)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201627.017, "dur": 2.915, "name": "default_index (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/indexes/api.py:322)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201630.215, "dur": 0.131, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201630.961, "dur": 0.092, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201631.211, "dur": 0.07, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201632.139, "dur": 0.062, "name": "builtins.getattr", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201632.003, "dur": 0.394, "name": "_check (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/generic.py:43)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201632.765, "dur": 0.065, "name": "builtins.getattr", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201632.636, "dur": 0.322, "name": "_check (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/generic.py:43)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201631.825, "dur": 1.224, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201633.556, "dur": 0.059, "name": "builtins.getattr", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201633.463, "dur": 0.227, "name": "_check (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/generic.py:43)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201633.291, "dur": 0.489, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201631.606, "dur": 2.285, "name": "extract_array (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/construction.py:379)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201634.197, "dur": 0.078, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201635.913, "dur": 0.078, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201636.123, "dur": 0.111, "name": "pandas._libs.lib.is_list_like", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201636.434, "dur": 0.078, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201636.979, "dur": 0.065, "name": "builtins.getattr", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201636.889, "dur": 0.293, "name": "_check (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/generic.py:43)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201636.699, "dur": 0.571, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201637.522, "dur": 0.097, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201637.766, "dur": 0.098, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201638.285, "dur": 0.076, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201638.942, "dur": 0.066, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201639.582, "dur": 0.066, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201639.723, "dur": 1.364, "name": "numpy.empty", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201639.254, "dur": 3.545, "name": "construct_1d_object_array_from_listlike (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/cast.py:1960)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201643.55, "dur": 0.111, "name": "cast (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/typing.py:1736)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201651.316, "dur": 0.626, "name": "numpy.asarray", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201652.22, "dur": 0.76, "name": "numpy.empty", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201653.933, "dur": 0.19, "name": "copyto (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/numpy/core/multiarray.py:1071)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201654.444, "dur": 1.177, "name": "numpy.core._multiarray_umath.implement_array_function", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201653.351, "dur": 2.367, "name": "copyto (<__array_function__ internals>:2)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201651.028, "dur": 4.955, "name": "full (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/numpy/core/numeric.py:289)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201643.972, "dur": 16.403, "name": "pandas._libs.lib.maybe_convert_objects", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201638.624, "dur": 22.037, "name": "maybe_convert_platform (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/cast.py:115)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201663.431, "dur": 0.174, "name": "builtins.getattr", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201664.218, "dur": 0.088, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201663.991, "dur": 0.491, "name": "_maybe_repeat (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/construction.py:684)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201663.21, "dur": 1.383, "name": "_sanitize_ndim (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/construction.py:627)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201664.849, "dur": 0.065, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201665.183, "dur": 0.077, "name": "cast (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/typing.py:1736)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201666.157, "dur": 0.299, "name": "builtins.issubclass", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201665.51, "dur": 1.082, "name": "_sanitize_str_dtypes (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/construction.py:664)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201630.777, "dur": 35.969, "name": "sanitize_array (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/construction.py:470)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201668.217, "dur": 0.38, "name": "_select_options (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:571)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201668.818, "dur": 0.088, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201669.098, "dur": 0.079, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201669.711, "dur": 0.601, "name": "_get_deprecated_option (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:603)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201669.526, "dur": 0.988, "name": "_warn_if_deprecated (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:642)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201670.99, "dur": 0.327, "name": "_get_deprecated_option (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:603)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201670.793, "dur": 0.675, "name": "_translate_key (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:630)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201667.876, "dur": 3.746, "name": "_get_single_key (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:109)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201672.101, "dur": 0.234, "name": "str.split", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201671.91, "dur": 1.309, "name": "_get_root (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:589)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201667.672, "dur": 5.887, "name": "_get_option (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:127)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201667.247, "dur": 6.467, "name": "__call__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:255)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201675.237, "dur": 0.093, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201675.051, "dur": 0.367, "name": "__len__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/indexes/range.py:909)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201674.767, "dur": 0.756, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201676.113, "dur": 0.141, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201679.139, "dur": 0.249, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201678.984, "dur": 0.515, "name": "is_1d_only_ea_dtype (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/common.py:1416)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201679.848, "dur": 0.22, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201680.205, "dur": 0.088, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201678.579, "dur": 1.877, "name": "check_ndim (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/internals/blocks.py:2055)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201681.241, "dur": 0.124, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201681.519, "dur": 0.117, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201681.86, "dur": 0.08, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201682.107, "dur": 0.094, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201680.739, "dur": 1.888, "name": "get_block_type (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/internals/blocks.py:1989)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201683.107, "dur": 0.052, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201683.534, "dur": 0.065, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201683.382, "dur": 0.603, "name": "ensure_wrapped_if_datetimelike (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/construction.py:438)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201684.316, "dur": 0.098, "name": "builtins.issubclass", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201684.639, "dur": 0.133, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201682.902, "dur": 1.961, "name": "maybe_coerce_values (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/internals/blocks.py:1960)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201675.965, "dur": 9.661, "name": "new_block (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/internals/blocks.py:2041)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201686.097, "dur": 0.682, "name": "__init__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/internals/managers.py:1700)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201674.469, "dur": 12.452, "name": "from_array (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/internals/managers.py:1731)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201689.237, "dur": 0.7, "name": "__init__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/flags.py:47)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201687.412, "dur": 2.922, "name": "__init__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/generic.py:239)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201691.689, "dur": 0.782, "name": "__getattr__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/generic.py:5561)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201691.095, "dur": 1.87, "name": "name (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/series.py:590)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201696.007, "dur": 0.1, "name": "builtins.hash", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201695.851, "dur": 0.368, "name": "is_hashable (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/inference.py:321)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201695.595, "dur": 0.75, "name": "<genexpr> (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/common.py:1740)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201696.468, "dur": 0.143, "name": "<genexpr> (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/common.py:1740)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201695.395, "dur": 1.348, "name": "builtins.all", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201694.946, "dur": 1.974, "name": "validate_all_hashable (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/common.py:1721)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201694.248, "dur": 3.225, "name": "name (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/series.py:640)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201690.669, "dur": 7.151, "name": "__setattr__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/generic.py:5577)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201698.644, "dur": 0.102, "name": "_is_all_dates (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/indexes/numeric.py:331)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201698.395, "dur": 0.554, "name": "_set_axis (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/series.py:542)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201609.882, "dur": 89.355, "name": "__init__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/series.py:323)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201700.485, "dur": 0.108, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201701.812, "dur": 0.069, "name": "builtins.getattr", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201701.685, "dur": 0.321, "name": "_check (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/generic.py:43)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201701.395, "dur": 0.71, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201702.452, "dur": 0.066, "name": "builtins.hash", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201702.34, "dur": 0.286, "name": "is_hashable (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/inference.py:321)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201701.106, "dur": 1.658, "name": "maybe_extract_name (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/indexes/base.py:7082)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201703.221, "dur": 0.109, "name": "pandas._libs.lib.is_list_like", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201703.504, "dur": 0.09, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201703.048, "dur": 0.81, "name": "is_empty_data (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/construction.py:802)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201704.208, "dur": 0.073, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201704.416, "dur": 0.068, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201706.182, "dur": 0.074, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201706.457, "dur": 0.071, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201707.687, "dur": 0.176, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201707.528, "dur": 0.41, "name": "<genexpr> (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/inference.py:288)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201708.199, "dur": 0.056, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201708.055, "dur": 0.27, "name": "<genexpr> (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/inference.py:288)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201707.378, "dur": 1.025, "name": "builtins.all", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201708.683, "dur": 0.119, "name": "<genexpr> (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/inference.py:288)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201706.822, "dur": 2.325, "name": "is_dict_like (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/inference.py:262)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201709.493, "dur": 0.123, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201709.814, "dur": 0.07, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201710.733, "dur": 0.321, "name": "_abc._abc_instancecheck", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201710.637, "dur": 0.495, "name": "__instancecheck__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/abc.py:117)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201710.437, "dur": 0.805, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201711.759, "dur": 0.237, "name": "_abc._abc_instancecheck", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201711.671, "dur": 0.396, "name": "__instancecheck__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/abc.py:117)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201711.447, "dur": 0.722, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201712.435, "dur": 0.1, "name": "cast (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/typing.py:1736)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201710.204, "dur": 2.439, "name": "maybe_iterable_to_list (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/common.py:287)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201712.901, "dur": 0.074, "name": "pandas._libs.lib.is_list_like", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201713.17, "dur": 0.132, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201714.329, "dur": 0.169, "name": "type.__new__", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201714.675, "dur": 0.054, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201715.318, "dur": 0.274, "name": "_reset_identity (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/indexes/base.py:834)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201714.124, "dur": 1.61, "name": "_simple_new (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/indexes/range.py:167)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201713.463, "dur": 2.398, "name": "default_index (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/indexes/api.py:322)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201716.109, "dur": 0.121, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201716.738, "dur": 0.093, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201716.986, "dur": 0.071, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201717.827, "dur": 0.056, "name": "builtins.getattr", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201717.705, "dur": 0.393, "name": "_check (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/generic.py:43)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201718.373, "dur": 0.056, "name": "builtins.getattr", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201718.277, "dur": 0.254, "name": "_check (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/generic.py:43)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201717.504, "dur": 1.112, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201719.135, "dur": 0.056, "name": "builtins.getattr", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201719.037, "dur": 0.229, "name": "_check (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/generic.py:43)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201718.809, "dur": 0.54, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201717.311, "dur": 2.149, "name": "extract_array (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/construction.py:379)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201719.722, "dur": 0.078, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201719.965, "dur": 0.069, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201720.126, "dur": 0.106, "name": "pandas._libs.lib.is_list_like", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201720.443, "dur": 0.076, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201720.944, "dur": 0.061, "name": "builtins.getattr", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201720.854, "dur": 0.271, "name": "_check (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/generic.py:43)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201720.693, "dur": 0.522, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201721.421, "dur": 0.101, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201721.672, "dur": 0.081, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201723.518, "dur": 0.091, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201724.107, "dur": 0.054, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201724.574, "dur": 0.076, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201724.728, "dur": 1.16, "name": "numpy.empty", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201724.359, "dur": 2.945, "name": "construct_1d_object_array_from_listlike (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/cast.py:1960)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201727.929, "dur": 0.093, "name": "cast (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/typing.py:1736)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201736.796, "dur": 0.588, "name": "numpy.asarray", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201737.645, "dur": 0.685, "name": "numpy.empty", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201739.263, "dur": 0.176, "name": "copyto (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/numpy/core/multiarray.py:1071)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201739.682, "dur": 1.058, "name": "numpy.core._multiarray_umath.implement_array_function", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201738.774, "dur": 2.064, "name": "copyto (<__array_function__ internals>:2)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201736.482, "dur": 4.597, "name": "full (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/numpy/core/numeric.py:289)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201728.311, "dur": 14.779, "name": "pandas._libs.lib.maybe_convert_objects", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201723.827, "dur": 19.424, "name": "maybe_convert_platform (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/cast.py:115)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201743.989, "dur": 0.087, "name": "cast (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/typing.py:1736)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201744.694, "dur": 0.084, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201745.261, "dur": 0.388, "name": "numpy.array", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201746.214, "dur": 0.121, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201747.177, "dur": 0.194, "name": "pandas._libs.algos.ensure_object", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201747.477, "dur": 2.691, "name": "pandas._libs.lib.infer_datetimelike_array", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201744.427, "dur": 6.523, "name": "maybe_infer_to_datetimelike (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/cast.py:1466)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201751.831, "dur": 0.193, "name": "builtins.getattr", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201752.61, "dur": 0.073, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201752.411, "dur": 0.456, "name": "_maybe_repeat (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/construction.py:684)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201751.649, "dur": 1.34, "name": "_sanitize_ndim (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/construction.py:627)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201753.254, "dur": 0.082, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201753.587, "dur": 0.072, "name": "cast (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/typing.py:1736)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201754.209, "dur": 0.13, "name": "builtins.issubclass", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201753.927, "dur": 0.536, "name": "_sanitize_str_dtypes (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/construction.py:664)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201716.565, "dur": 38.015, "name": "sanitize_array (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/construction.py:470)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201755.732, "dur": 0.344, "name": "_select_options (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:571)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201756.262, "dur": 0.077, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201756.502, "dur": 0.048, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201757.146, "dur": 0.558, "name": "_get_deprecated_option (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:603)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201756.925, "dur": 0.97, "name": "_warn_if_deprecated (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:642)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201758.297, "dur": 0.313, "name": "_get_deprecated_option (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:603)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201758.119, "dur": 0.62, "name": "_translate_key (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:630)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201755.487, "dur": 3.385, "name": "_get_single_key (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:109)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201759.342, "dur": 0.236, "name": "str.split", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201759.187, "dur": 1.238, "name": "_get_root (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:589)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201755.326, "dur": 5.474, "name": "_get_option (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:127)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201754.991, "dur": 5.979, "name": "__call__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:255)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201762.241, "dur": 0.105, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201762.087, "dur": 0.347, "name": "__len__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/indexes/range.py:909)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201761.851, "dur": 0.687, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201763.015, "dur": 0.125, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201765.778, "dur": 0.105, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201765.607, "dur": 0.379, "name": "is_1d_only_ea_dtype (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/common.py:1416)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201766.294, "dur": 0.196, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201766.607, "dur": 0.056, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201765.16, "dur": 1.631, "name": "check_ndim (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/internals/blocks.py:2055)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201767.506, "dur": 0.131, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201767.801, "dur": 0.085, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201768.111, "dur": 0.072, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201768.306, "dur": 0.072, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201767.086, "dur": 1.795, "name": "get_block_type (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/internals/blocks.py:1989)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201769.298, "dur": 0.052, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201769.699, "dur": 0.063, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201769.547, "dur": 0.598, "name": "ensure_wrapped_if_datetimelike (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/construction.py:438)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201770.464, "dur": 0.102, "name": "builtins.issubclass", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201770.773, "dur": 0.102, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201769.14, "dur": 1.844, "name": "maybe_coerce_values (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/internals/blocks.py:1960)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201762.885, "dur": 8.725, "name": "new_block (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/internals/blocks.py:2041)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201771.981, "dur": 0.596, "name": "__init__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/internals/managers.py:1700)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201761.64, "dur": 11.101, "name": "from_array (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/internals/managers.py:1731)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201775.068, "dur": 0.607, "name": "__init__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/flags.py:47)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201773.193, "dur": 2.822, "name": "__init__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/generic.py:239)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201777.289, "dur": 0.789, "name": "__getattr__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/generic.py:5561)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201776.73, "dur": 1.856, "name": "name (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/series.py:590)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201781.548, "dur": 0.104, "name": "builtins.hash", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201781.391, "dur": 0.388, "name": "is_hashable (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/inference.py:321)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201781.127, "dur": 0.767, "name": "<genexpr> (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/common.py:1740)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201782.012, "dur": 0.174, "name": "<genexpr> (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/common.py:1740)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201780.912, "dur": 1.416, "name": "builtins.all", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201780.439, "dur": 2.065, "name": "validate_all_hashable (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/common.py:1721)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201779.816, "dur": 3.129, "name": "name (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/series.py:640)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201776.317, "dur": 6.926, "name": "__setattr__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/generic.py:5577)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201783.979, "dur": 0.098, "name": "_is_all_dates (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/indexes/numeric.py:331)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201783.729, "dur": 0.57, "name": "_set_axis (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/series.py:542)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201700.263, "dur": 84.324, "name": "__init__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/series.py:323)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140201164.751, "dur": 622.007, "name": "<module> (<string>:1)", "ph": "X", "cat": "FEE"}, {"pid": 573092, "tid": 573092, "ts": 7628140200857.229, "dur": 937.125, "name": "builtins.exec", "ph": "X", "cat": "FEE"}], "viztracer_metadata": {"version": "0.15.3", "overflow": false}, "file_info": {"files": {"/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/generic.py": ["\"\"\" define generic base classes for pandas objects \"\"\"\nfrom __future__ import annotations\n\nfrom typing import (\n    TYPE_CHECKING,\n    Type,\n    cast,\n)\n\nif TYPE_CHECKING:\n    from pandas import (\n        Categorical,\n        CategoricalIndex,\n        DataFrame,\n        DatetimeIndex,\n        Float64Index,\n        Index,\n        Int64Index,\n        IntervalIndex,\n        MultiIndex,\n        PeriodIndex,\n        RangeIndex,\n        Series,\n        TimedeltaIndex,\n        UInt64Index,\n    )\n    from pandas.core.arrays import (\n        DatetimeArray,\n        ExtensionArray,\n        PandasArray,\n        PeriodArray,\n        TimedeltaArray,\n    )\n    from pandas.core.generic import NDFrame\n\n\n# define abstract base classes to enable isinstance type checking on our\n# objects\ndef create_pandas_abc_type(name, attr, comp):\n\n    # https://github.com/python/mypy/issues/1006\n    # error: 'classmethod' used with a non-method\n    @classmethod  # type: ignore[misc]\n    def _check(cls, inst) -> bool:\n        return getattr(inst, attr, \"_typ\") in comp\n\n    dct = {\"__instancecheck__\": _check, \"__subclasscheck__\": _check}\n    meta = type(\"ABCBase\", (type,), dct)\n    return meta(name, (), dct)\n\n\nABCInt64Index = cast(\n    \"Type[Int64Index]\",\n    create_pandas_abc_type(\"ABCInt64Index\", \"_typ\", (\"int64index\",)),\n)\nABCUInt64Index = cast(\n    \"Type[UInt64Index]\",\n    create_pandas_abc_type(\"ABCUInt64Index\", \"_typ\", (\"uint64index\",)),\n)\nABCRangeIndex = cast(\n    \"Type[RangeIndex]\",\n    create_pandas_abc_type(\"ABCRangeIndex\", \"_typ\", (\"rangeindex\",)),\n)\nABCFloat64Index = cast(\n    \"Type[Float64Index]\",\n    create_pandas_abc_type(\"ABCFloat64Index\", \"_typ\", (\"float64index\",)),\n)\nABCMultiIndex = cast(\n    \"Type[MultiIndex]\",\n    create_pandas_abc_type(\"ABCMultiIndex\", \"_typ\", (\"multiindex\",)),\n)\nABCDatetimeIndex = cast(\n    \"Type[DatetimeIndex]\",\n    create_pandas_abc_type(\"ABCDatetimeIndex\", \"_typ\", (\"datetimeindex\",)),\n)\nABCTimedeltaIndex = cast(\n    \"Type[TimedeltaIndex]\",\n    create_pandas_abc_type(\"ABCTimedeltaIndex\", \"_typ\", (\"timedeltaindex\",)),\n)\nABCPeriodIndex = cast(\n    \"Type[PeriodIndex]\",\n    create_pandas_abc_type(\"ABCPeriodIndex\", \"_typ\", (\"periodindex\",)),\n)\nABCCategoricalIndex = cast(\n    \"Type[CategoricalIndex]\",\n    create_pandas_abc_type(\"ABCCategoricalIndex\", \"_typ\", (\"categoricalindex\",)),\n)\nABCIntervalIndex = cast(\n    \"Type[IntervalIndex]\",\n    create_pandas_abc_type(\"ABCIntervalIndex\", \"_typ\", (\"intervalindex\",)),\n)\nABCIndex = cast(\n    \"Type[Index]\",\n    create_pandas_abc_type(\n        \"ABCIndex\",\n        \"_typ\",\n        {\n            \"index\",\n            \"int64index\",\n            \"rangeindex\",\n            \"float64index\",\n            \"uint64index\",\n            \"numericindex\",\n            \"multiindex\",\n            \"datetimeindex\",\n            \"timedeltaindex\",\n            \"periodindex\",\n            \"categoricalindex\",\n            \"intervalindex\",\n        },\n    ),\n)\n\n\nABCNDFrame = cast(\n    \"Type[NDFrame]\",\n    create_pandas_abc_type(\"ABCNDFrame\", \"_typ\", (\"series\", \"dataframe\")),\n)\nABCSeries = cast(\n    \"Type[Series]\",\n    create_pandas_abc_type(\"ABCSeries\", \"_typ\", (\"series\",)),\n)\nABCDataFrame = cast(\n    \"Type[DataFrame]\", create_pandas_abc_type(\"ABCDataFrame\", \"_typ\", (\"dataframe\",))\n)\n\nABCCategorical = cast(\n    \"Type[Categorical]\",\n    create_pandas_abc_type(\"ABCCategorical\", \"_typ\", (\"categorical\")),\n)\nABCDatetimeArray = cast(\n    \"Type[DatetimeArray]\",\n    create_pandas_abc_type(\"ABCDatetimeArray\", \"_typ\", (\"datetimearray\")),\n)\nABCTimedeltaArray = cast(\n    \"Type[TimedeltaArray]\",\n    create_pandas_abc_type(\"ABCTimedeltaArray\", \"_typ\", (\"timedeltaarray\")),\n)\nABCPeriodArray = cast(\n    \"Type[PeriodArray]\",\n    create_pandas_abc_type(\"ABCPeriodArray\", \"_typ\", (\"periodarray\",)),\n)\nABCExtensionArray = cast(\n    \"Type[ExtensionArray]\",\n    create_pandas_abc_type(\n        \"ABCExtensionArray\",\n        \"_typ\",\n        # Note: IntervalArray and SparseArray are included bc they have _typ=\"extension\"\n        {\"extension\", \"categorical\", \"periodarray\", \"datetimearray\", \"timedeltaarray\"},\n    ),\n)\nABCPandasArray = cast(\n    \"Type[PandasArray]\",\n    create_pandas_abc_type(\"ABCPandasArray\", \"_typ\", (\"npy_extension\",)),\n)\n", 155], "/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/inference.py": ["\"\"\" basic inference routines \"\"\"\n\nfrom collections import abc\nfrom numbers import Number\nimport re\nfrom typing import Pattern\n\nimport numpy as np\n\nfrom pandas._libs import lib\nfrom pandas._typing import ArrayLike\n\nis_bool = lib.is_bool\n\nis_integer = lib.is_integer\n\nis_float = lib.is_float\n\nis_complex = lib.is_complex\n\nis_scalar = lib.is_scalar\n\nis_decimal = lib.is_decimal\n\nis_interval = lib.is_interval\n\nis_list_like = lib.is_list_like\n\nis_iterator = lib.is_iterator\n\n\ndef is_number(obj) -> bool:\n    \"\"\"\n    Check if the object is a number.\n\n    Returns True when the object is a number, and False if is not.\n\n    Parameters\n    ----------\n    obj : any type\n        The object to check if is a number.\n\n    Returns\n    -------\n    is_number : bool\n        Whether `obj` is a number or not.\n\n    See Also\n    --------\n    api.types.is_integer: Checks a subgroup of numbers.\n\n    Examples\n    --------\n    >>> from pandas.api.types import is_number\n    >>> is_number(1)\n    True\n    >>> is_number(7.15)\n    True\n\n    Booleans are valid because they are int subclass.\n\n    >>> is_number(False)\n    True\n\n    >>> is_number(\"foo\")\n    False\n    >>> is_number(\"5\")\n    False\n    \"\"\"\n    return isinstance(obj, (Number, np.number))\n\n\ndef iterable_not_string(obj) -> bool:\n    \"\"\"\n    Check if the object is an iterable but not a string.\n\n    Parameters\n    ----------\n    obj : The object to check.\n\n    Returns\n    -------\n    is_iter_not_string : bool\n        Whether `obj` is a non-string iterable.\n\n    Examples\n    --------\n    >>> iterable_not_string([1, 2, 3])\n    True\n    >>> iterable_not_string(\"foo\")\n    False\n    >>> iterable_not_string(1)\n    False\n    \"\"\"\n    return isinstance(obj, abc.Iterable) and not isinstance(obj, str)\n\n\ndef is_file_like(obj) -> bool:\n    \"\"\"\n    Check if the object is a file-like object.\n\n    For objects to be considered file-like, they must\n    be an iterator AND have either a `read` and/or `write`\n    method as an attribute.\n\n    Note: file-like objects must be iterable, but\n    iterable objects need not be file-like.\n\n    Parameters\n    ----------\n    obj : The object to check\n\n    Returns\n    -------\n    is_file_like : bool\n        Whether `obj` has file-like properties.\n\n    Examples\n    --------\n    >>> import io\n    >>> buffer = io.StringIO(\"data\")\n    >>> is_file_like(buffer)\n    True\n    >>> is_file_like([1, 2, 3])\n    False\n    \"\"\"\n    if not (hasattr(obj, \"read\") or hasattr(obj, \"write\")):\n        return False\n\n    return bool(hasattr(obj, \"__iter__\"))\n\n\ndef is_re(obj) -> bool:\n    \"\"\"\n    Check if the object is a regex pattern instance.\n\n    Parameters\n    ----------\n    obj : The object to check\n\n    Returns\n    -------\n    is_regex : bool\n        Whether `obj` is a regex pattern.\n\n    Examples\n    --------\n    >>> is_re(re.compile(\".*\"))\n    True\n    >>> is_re(\"foo\")\n    False\n    \"\"\"\n    return isinstance(obj, Pattern)\n\n\ndef is_re_compilable(obj) -> bool:\n    \"\"\"\n    Check if the object can be compiled into a regex pattern instance.\n\n    Parameters\n    ----------\n    obj : The object to check\n\n    Returns\n    -------\n    is_regex_compilable : bool\n        Whether `obj` can be compiled as a regex pattern.\n\n    Examples\n    --------\n    >>> is_re_compilable(\".*\")\n    True\n    >>> is_re_compilable(1)\n    False\n    \"\"\"\n    try:\n        re.compile(obj)\n    except TypeError:\n        return False\n    else:\n        return True\n\n\ndef is_array_like(obj) -> bool:\n    \"\"\"\n    Check if the object is array-like.\n\n    For an object to be considered array-like, it must be list-like and\n    have a `dtype` attribute.\n\n    Parameters\n    ----------\n    obj : The object to check\n\n    Returns\n    -------\n    is_array_like : bool\n        Whether `obj` has array-like properties.\n\n    Examples\n    --------\n    >>> is_array_like(np.array([1, 2, 3]))\n    True\n    >>> is_array_like(pd.Series([\"a\", \"b\"]))\n    True\n    >>> is_array_like(pd.Index([\"2016-01-01\"]))\n    True\n    >>> is_array_like([1, 2, 3])\n    False\n    >>> is_array_like((\"a\", \"b\"))\n    False\n    \"\"\"\n    return is_list_like(obj) and hasattr(obj, \"dtype\")\n\n\ndef is_nested_list_like(obj) -> bool:\n    \"\"\"\n    Check if the object is list-like, and that all of its elements\n    are also list-like.\n\n    Parameters\n    ----------\n    obj : The object to check\n\n    Returns\n    -------\n    is_list_like : bool\n        Whether `obj` has list-like properties.\n\n    Examples\n    --------\n    >>> is_nested_list_like([[1, 2, 3]])\n    True\n    >>> is_nested_list_like([{1, 2, 3}, {1, 2, 3}])\n    True\n    >>> is_nested_list_like([\"foo\"])\n    False\n    >>> is_nested_list_like([])\n    False\n    >>> is_nested_list_like([[1, 2, 3], 1])\n    False\n\n    Notes\n    -----\n    This won't reliably detect whether a consumable iterator (e. g.\n    a generator) is a nested-list-like without consuming the iterator.\n    To avoid consuming it, we always return False if the outer container\n    doesn't define `__len__`.\n\n    See Also\n    --------\n    is_list_like\n    \"\"\"\n    return (\n        is_list_like(obj)\n        and hasattr(obj, \"__len__\")\n        and len(obj) > 0\n        and all(is_list_like(item) for item in obj)\n    )\n\n\ndef is_dict_like(obj) -> bool:\n    \"\"\"\n    Check if the object is dict-like.\n\n    Parameters\n    ----------\n    obj : The object to check\n\n    Returns\n    -------\n    is_dict_like : bool\n        Whether `obj` has dict-like properties.\n\n    Examples\n    --------\n    >>> is_dict_like({1: 2})\n    True\n    >>> is_dict_like([1, 2, 3])\n    False\n    >>> is_dict_like(dict)\n    False\n    >>> is_dict_like(dict())\n    True\n    \"\"\"\n    dict_like_attrs = (\"__getitem__\", \"keys\", \"__contains__\")\n    return (\n        all(hasattr(obj, attr) for attr in dict_like_attrs)\n        # [GH 25196] exclude classes\n        and not isinstance(obj, type)\n    )\n\n\ndef is_named_tuple(obj) -> bool:\n    \"\"\"\n    Check if the object is a named tuple.\n\n    Parameters\n    ----------\n    obj : The object to check\n\n    Returns\n    -------\n    is_named_tuple : bool\n        Whether `obj` is a named tuple.\n\n    Examples\n    --------\n    >>> from collections import namedtuple\n    >>> Point = namedtuple(\"Point\", [\"x\", \"y\"])\n    >>> p = Point(1, 2)\n    >>>\n    >>> is_named_tuple(p)\n    True\n    >>> is_named_tuple((1, 2))\n    False\n    \"\"\"\n    return isinstance(obj, abc.Sequence) and hasattr(obj, \"_fields\")\n\n\ndef is_hashable(obj) -> bool:\n    \"\"\"\n    Return True if hash(obj) will succeed, False otherwise.\n\n    Some types will pass a test against collections.abc.Hashable but fail when\n    they are actually hashed with hash().\n\n    Distinguish between these and other types by trying the call to hash() and\n    seeing if they raise TypeError.\n\n    Returns\n    -------\n    bool\n\n    Examples\n    --------\n    >>> import collections\n    >>> a = ([],)\n    >>> isinstance(a, collections.abc.Hashable)\n    True\n    >>> is_hashable(a)\n    False\n    \"\"\"\n    # Unfortunately, we can't use isinstance(obj, collections.abc.Hashable),\n    # which can be faster than calling hash. That is because numpy scalars\n    # fail this test.\n\n    # Reconsider this decision once this numpy bug is fixed:\n    # https://github.com/numpy/numpy/issues/5562\n\n    try:\n        hash(obj)\n    except TypeError:\n        return False\n    else:\n        return True\n\n\ndef is_sequence(obj) -> bool:\n    \"\"\"\n    Check if the object is a sequence of objects.\n    String types are not included as sequences here.\n\n    Parameters\n    ----------\n    obj : The object to check\n\n    Returns\n    -------\n    is_sequence : bool\n        Whether `obj` is a sequence of objects.\n\n    Examples\n    --------\n    >>> l = [1, 2, 3]\n    >>>\n    >>> is_sequence(l)\n    True\n    >>> is_sequence(iter(l))\n    False\n    \"\"\"\n    try:\n        iter(obj)  # Can iterate over it.\n        len(obj)  # Has a length associated with it.\n        return not isinstance(obj, (str, bytes))\n    except (TypeError, AttributeError):\n        return False\n\n\ndef is_dataclass(item):\n    \"\"\"\n    Checks if the object is a data-class instance\n\n    Parameters\n    ----------\n    item : object\n\n    Returns\n    --------\n    is_dataclass : bool\n        True if the item is an instance of a data-class,\n        will return false if you pass the data class itself\n\n    Examples\n    --------\n    >>> from dataclasses import dataclass\n    >>> @dataclass\n    ... class Point:\n    ...     x: int\n    ...     y: int\n\n    >>> is_dataclass(Point)\n    False\n    >>> is_dataclass(Point(0,2))\n    True\n\n    \"\"\"\n    try:\n        from dataclasses import is_dataclass\n\n        return is_dataclass(item) and not isinstance(item, type)\n    except ImportError:\n        return False\n\n\ndef is_inferred_bool_dtype(arr: ArrayLike) -> bool:\n    \"\"\"\n    Check if this is a ndarray[bool] or an ndarray[object] of bool objects.\n\n    Parameters\n    ----------\n    arr : np.ndarray or ExtensionArray\n\n    Returns\n    -------\n    bool\n\n    Notes\n    -----\n    This does not include the special treatment is_bool_dtype uses for\n    Categorical.\n    \"\"\"\n    if not isinstance(arr, np.ndarray):\n        return False\n\n    dtype = arr.dtype\n    if dtype == np.dtype(bool):\n        return True\n    elif dtype == np.dtype(\"object\"):\n        return lib.is_bool_array(arr)\n    return False\n", 451], "/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/indexes/base.py": ["from __future__ import annotations\n\nfrom datetime import datetime\nimport functools\nfrom itertools import zip_longest\nimport operator\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Hashable,\n    Literal,\n    Sequence,\n    TypeVar,\n    cast,\n    final,\n    overload,\n)\nimport warnings\n\nimport numpy as np\n\nfrom pandas._config import get_option\n\nfrom pandas._libs import (\n    NaT,\n    algos as libalgos,\n    index as libindex,\n    lib,\n)\nimport pandas._libs.join as libjoin\nfrom pandas._libs.lib import (\n    is_datetime_array,\n    no_default,\n)\nfrom pandas._libs.missing import is_float_nan\nfrom pandas._libs.tslibs import (\n    IncompatibleFrequency,\n    OutOfBoundsDatetime,\n    Timestamp,\n    tz_compare,\n)\nfrom pandas._typing import (\n    AnyArrayLike,\n    ArrayLike,\n    Dtype,\n    DtypeObj,\n    F,\n    Shape,\n    npt,\n)\nfrom pandas.compat.numpy import function as nv\nfrom pandas.errors import (\n    DuplicateLabelError,\n    InvalidIndexError,\n)\nfrom pandas.util._decorators import (\n    Appender,\n    cache_readonly,\n    deprecate_nonkeyword_arguments,\n    doc,\n)\nfrom pandas.util._exceptions import (\n    find_stack_level,\n    rewrite_exception,\n)\n\nfrom pandas.core.dtypes.cast import (\n    can_hold_element,\n    find_common_type,\n    infer_dtype_from,\n    maybe_cast_pointwise_result,\n)\nfrom pandas.core.dtypes.common import (\n    ensure_int64,\n    ensure_object,\n    ensure_platform_int,\n    is_bool_dtype,\n    is_categorical_dtype,\n    is_dtype_equal,\n    is_ea_or_datetimelike_dtype,\n    is_extension_array_dtype,\n    is_float,\n    is_float_dtype,\n    is_hashable,\n    is_integer,\n    is_interval_dtype,\n    is_iterator,\n    is_list_like,\n    is_numeric_dtype,\n    is_object_dtype,\n    is_scalar,\n    is_signed_integer_dtype,\n    is_unsigned_integer_dtype,\n    needs_i8_conversion,\n    pandas_dtype,\n    validate_all_hashable,\n)\nfrom pandas.core.dtypes.concat import concat_compat\nfrom pandas.core.dtypes.dtypes import (\n    CategoricalDtype,\n    DatetimeTZDtype,\n    ExtensionDtype,\n    IntervalDtype,\n    PandasDtype,\n    PeriodDtype,\n)\nfrom pandas.core.dtypes.generic import (\n    ABCDataFrame,\n    ABCDatetimeIndex,\n    ABCMultiIndex,\n    ABCPeriodIndex,\n    ABCSeries,\n    ABCTimedeltaIndex,\n)\nfrom pandas.core.dtypes.inference import is_dict_like\nfrom pandas.core.dtypes.missing import (\n    array_equivalent,\n    is_valid_na_for_dtype,\n    isna,\n)\n\nfrom pandas.core import (\n    arraylike,\n    missing,\n    ops,\n)\nfrom pandas.core.accessor import CachedAccessor\nimport pandas.core.algorithms as algos\nfrom pandas.core.array_algos.putmask import (\n    setitem_datetimelike_compat,\n    validate_putmask,\n)\nfrom pandas.core.arrays import (\n    Categorical,\n    ExtensionArray,\n)\nfrom pandas.core.arrays.datetimes import (\n    tz_to_dtype,\n    validate_tz_from_dtype,\n)\nfrom pandas.core.arrays.masked import BaseMaskedArray\nfrom pandas.core.arrays.sparse import SparseDtype\nfrom pandas.core.base import (\n    IndexOpsMixin,\n    PandasObject,\n)\nimport pandas.core.common as com\nfrom pandas.core.construction import (\n    ensure_wrapped_if_datetimelike,\n    extract_array,\n    sanitize_array,\n)\nfrom pandas.core.indexers import deprecate_ndim_indexing\nfrom pandas.core.indexes.frozen import FrozenList\nfrom pandas.core.ops import get_op_result_name\nfrom pandas.core.ops.invalid import make_invalid_op\nfrom pandas.core.sorting import (\n    ensure_key_mapped,\n    get_group_index_sorter,\n    nargsort,\n)\nfrom pandas.core.strings import StringMethods\n\nfrom pandas.io.formats.printing import (\n    PrettyDict,\n    default_pprint,\n    format_object_summary,\n    pprint_thing,\n)\n\nif TYPE_CHECKING:\n    from pandas import (\n        CategoricalIndex,\n        DataFrame,\n        IntervalIndex,\n        MultiIndex,\n        Series,\n    )\n    from pandas.core.arrays import PeriodArray\n\n\n__all__ = [\"Index\"]\n\n_unsortable_types = frozenset((\"mixed\", \"mixed-integer\"))\n\n_index_doc_kwargs: dict[str, str] = {\n    \"klass\": \"Index\",\n    \"inplace\": \"\",\n    \"target_klass\": \"Index\",\n    \"raises_section\": \"\",\n    \"unique\": \"Index\",\n    \"duplicated\": \"np.ndarray\",\n}\n_index_shared_docs: dict[str, str] = {}\nstr_t = str\n\n\n_dtype_obj = np.dtype(\"object\")\n\n\ndef _maybe_return_indexers(meth: F) -> F:\n    \"\"\"\n    Decorator to simplify 'return_indexers' checks in Index.join.\n    \"\"\"\n\n    @functools.wraps(meth)\n    def join(\n        self,\n        other,\n        how: str_t = \"left\",\n        level=None,\n        return_indexers: bool = False,\n        sort: bool = False,\n    ):\n        join_index, lidx, ridx = meth(self, other, how=how, level=level, sort=sort)\n        if not return_indexers:\n            return join_index\n\n        if lidx is not None:\n            lidx = ensure_platform_int(lidx)\n        if ridx is not None:\n            ridx = ensure_platform_int(ridx)\n        return join_index, lidx, ridx\n\n    return cast(F, join)\n\n\ndef disallow_kwargs(kwargs: dict[str, Any]) -> None:\n    if kwargs:\n        raise TypeError(f\"Unexpected keyword arguments {repr(set(kwargs))}\")\n\n\ndef _new_Index(cls, d):\n    \"\"\"\n    This is called upon unpickling, rather than the default which doesn't\n    have arguments and breaks __new__.\n    \"\"\"\n    # required for backward compat, because PI can't be instantiated with\n    # ordinals through __new__ GH #13277\n    if issubclass(cls, ABCPeriodIndex):\n        from pandas.core.indexes.period import _new_PeriodIndex\n\n        return _new_PeriodIndex(cls, **d)\n\n    if issubclass(cls, ABCMultiIndex):\n        if \"labels\" in d and \"codes\" not in d:\n            # GH#23752 \"labels\" kwarg has been replaced with \"codes\"\n            d[\"codes\"] = d.pop(\"labels\")\n\n    elif \"dtype\" not in d and \"data\" in d:\n        # Prevent Index.__new__ from conducting inference;\n        #  \"data\" key not in RangeIndex\n        d[\"dtype\"] = d[\"data\"].dtype\n    return cls.__new__(cls, **d)\n\n\n_IndexT = TypeVar(\"_IndexT\", bound=\"Index\")\n\n\nclass Index(IndexOpsMixin, PandasObject):\n    \"\"\"\n    Immutable sequence used for indexing and alignment. The basic object\n    storing axis labels for all pandas objects.\n\n    Parameters\n    ----------\n    data : array-like (1-dimensional)\n    dtype : NumPy dtype (default: object)\n        If dtype is None, we find the dtype that best fits the data.\n        If an actual dtype is provided, we coerce to that dtype if it's safe.\n        Otherwise, an error will be raised.\n    copy : bool\n        Make a copy of input ndarray.\n    name : object\n        Name to be stored in the index.\n    tupleize_cols : bool (default: True)\n        When True, attempt to create a MultiIndex if possible.\n\n    See Also\n    --------\n    RangeIndex : Index implementing a monotonic integer range.\n    CategoricalIndex : Index of :class:`Categorical` s.\n    MultiIndex : A multi-level, or hierarchical Index.\n    IntervalIndex : An Index of :class:`Interval` s.\n    DatetimeIndex : Index of datetime64 data.\n    TimedeltaIndex : Index of timedelta64 data.\n    PeriodIndex : Index of Period data.\n    NumericIndex : Index of numpy int/uint/float data.\n    Int64Index : Index of purely int64 labels (deprecated).\n    UInt64Index : Index of purely uint64 labels (deprecated).\n    Float64Index : Index of  purely float64 labels (deprecated).\n\n    Notes\n    -----\n    An Index instance can **only** contain hashable objects\n\n    Examples\n    --------\n    >>> pd.Index([1, 2, 3])\n    Int64Index([1, 2, 3], dtype='int64')\n\n    >>> pd.Index(list('abc'))\n    Index(['a', 'b', 'c'], dtype='object')\n    \"\"\"\n\n    # tolist is not actually deprecated, just suppressed in the __dir__\n    _hidden_attrs: frozenset[str] = (\n        PandasObject._hidden_attrs\n        | IndexOpsMixin._hidden_attrs\n        | frozenset([\"contains\", \"set_value\"])\n    )\n\n    # To hand over control to subclasses\n    _join_precedence = 1\n\n    # Cython methods; see github.com/cython/cython/issues/2647\n    #  for why we need to wrap these instead of making them class attributes\n    # Moreover, cython will choose the appropriate-dtyped sub-function\n    #  given the dtypes of the passed arguments\n\n    @final\n    def _left_indexer_unique(self: _IndexT, other: _IndexT) -> npt.NDArray[np.intp]:\n        # Caller is responsible for ensuring other.dtype == self.dtype\n        sv = self._get_engine_target()\n        ov = other._get_engine_target()\n        return libjoin.left_join_indexer_unique(sv, ov)\n\n    @final\n    def _left_indexer(\n        self: _IndexT, other: _IndexT\n    ) -> tuple[ArrayLike, npt.NDArray[np.intp], npt.NDArray[np.intp]]:\n        # Caller is responsible for ensuring other.dtype == self.dtype\n        sv = self._get_engine_target()\n        ov = other._get_engine_target()\n        joined_ndarray, lidx, ridx = libjoin.left_join_indexer(sv, ov)\n        joined = self._from_join_target(joined_ndarray)\n        return joined, lidx, ridx\n\n    @final\n    def _inner_indexer(\n        self: _IndexT, other: _IndexT\n    ) -> tuple[ArrayLike, npt.NDArray[np.intp], npt.NDArray[np.intp]]:\n        # Caller is responsible for ensuring other.dtype == self.dtype\n        sv = self._get_engine_target()\n        ov = other._get_engine_target()\n        joined_ndarray, lidx, ridx = libjoin.inner_join_indexer(sv, ov)\n        joined = self._from_join_target(joined_ndarray)\n        return joined, lidx, ridx\n\n    @final\n    def _outer_indexer(\n        self: _IndexT, other: _IndexT\n    ) -> tuple[ArrayLike, npt.NDArray[np.intp], npt.NDArray[np.intp]]:\n        # Caller is responsible for ensuring other.dtype == self.dtype\n        sv = self._get_engine_target()\n        ov = other._get_engine_target()\n        joined_ndarray, lidx, ridx = libjoin.outer_join_indexer(sv, ov)\n        joined = self._from_join_target(joined_ndarray)\n        return joined, lidx, ridx\n\n    _typ: str = \"index\"\n    _data: ExtensionArray | np.ndarray\n    _data_cls: type[ExtensionArray] | tuple[type[np.ndarray], type[ExtensionArray]] = (\n        np.ndarray,\n        ExtensionArray,\n    )\n    _id: object | None = None\n    _name: Hashable = None\n    # MultiIndex.levels previously allowed setting the index name. We\n    # don't allow this anymore, and raise if it happens rather than\n    # failing silently.\n    _no_setting_name: bool = False\n    _comparables: list[str] = [\"name\"]\n    _attributes: list[str] = [\"name\"]\n    _is_numeric_dtype: bool = False\n    _can_hold_na: bool = True\n    _can_hold_strings: bool = True\n\n    # Whether this index is a NumericIndex, but not a Int64Index, Float64Index,\n    # UInt64Index or RangeIndex. Needed for backwards compat. Remove this attribute and\n    # associated code in pandas 2.0.\n    _is_backward_compat_public_numeric_index: bool = False\n\n    _engine_type: type[libindex.IndexEngine] = libindex.ObjectEngine\n    # whether we support partial string indexing. Overridden\n    # in DatetimeIndex and PeriodIndex\n    _supports_partial_string_indexing = False\n\n    _accessors = {\"str\"}\n\n    str = CachedAccessor(\"str\", StringMethods)\n\n    # --------------------------------------------------------------------\n    # Constructors\n\n    def __new__(\n        cls, data=None, dtype=None, copy=False, name=None, tupleize_cols=True, **kwargs\n    ) -> Index:\n\n        if kwargs:\n            warnings.warn(\n                \"Passing keywords other than 'data', 'dtype', 'copy', 'name', \"\n                \"'tupleize_cols' is deprecated and will raise TypeError in a \"\n                \"future version.  Use the specific Index subclass directly instead.\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n\n        from pandas.core.arrays import PandasArray\n        from pandas.core.indexes.range import RangeIndex\n\n        name = maybe_extract_name(name, data, cls)\n\n        if dtype is not None:\n            dtype = pandas_dtype(dtype)\n        if \"tz\" in kwargs:\n            tz = kwargs.pop(\"tz\")\n            validate_tz_from_dtype(dtype, tz)\n            dtype = tz_to_dtype(tz)\n\n        if type(data) is PandasArray:\n            # ensure users don't accidentally put a PandasArray in an index,\n            #  but don't unpack StringArray\n            data = data.to_numpy()\n        if isinstance(dtype, PandasDtype):\n            dtype = dtype.numpy_dtype\n\n        data_dtype = getattr(data, \"dtype\", None)\n\n        # range\n        if isinstance(data, (range, RangeIndex)):\n            result = RangeIndex(start=data, copy=copy, name=name)\n            if dtype is not None:\n                return result.astype(dtype, copy=False)\n            return result\n\n        elif is_ea_or_datetimelike_dtype(dtype):\n            # non-EA dtype indexes have special casting logic, so we punt here\n            klass = cls._dtype_to_subclass(dtype)\n            if klass is not Index:\n                return klass(data, dtype=dtype, copy=copy, name=name, **kwargs)\n\n            ea_cls = dtype.construct_array_type()\n            data = ea_cls._from_sequence(data, dtype=dtype, copy=copy)\n            disallow_kwargs(kwargs)\n            return Index._simple_new(data, name=name)\n\n        elif is_ea_or_datetimelike_dtype(data_dtype):\n            data_dtype = cast(DtypeObj, data_dtype)\n            klass = cls._dtype_to_subclass(data_dtype)\n            if klass is not Index:\n                result = klass(data, copy=copy, name=name, **kwargs)\n                if dtype is not None:\n                    return result.astype(dtype, copy=False)\n                return result\n            elif dtype is not None:\n                # GH#45206\n                data = data.astype(dtype, copy=False)\n\n            disallow_kwargs(kwargs)\n            data = extract_array(data, extract_numpy=True)\n            return Index._simple_new(data, name=name)\n\n        # index-like\n        elif (\n            isinstance(data, Index)\n            and data._is_backward_compat_public_numeric_index\n            and dtype is None\n        ):\n            return data._constructor(data, name=name, copy=copy)\n        elif isinstance(data, (np.ndarray, Index, ABCSeries)):\n\n            if isinstance(data, ABCMultiIndex):\n                data = data._values\n\n            if dtype is not None:\n                # we need to avoid having numpy coerce\n                # things that look like ints/floats to ints unless\n                # they are actually ints, e.g. '0' and 0.0\n                # should not be coerced\n                # GH 11836\n                data = sanitize_array(data, None, dtype=dtype, copy=copy)\n\n                dtype = data.dtype\n\n            if data.dtype.kind in [\"i\", \"u\", \"f\"]:\n                # maybe coerce to a sub-class\n                arr = data\n            else:\n                arr = com.asarray_tuplesafe(data, dtype=_dtype_obj)\n\n                if dtype is None:\n                    arr = _maybe_cast_data_without_dtype(\n                        arr, cast_numeric_deprecated=True\n                    )\n                    dtype = arr.dtype\n\n                    if kwargs:\n                        return cls(arr, dtype, copy=copy, name=name, **kwargs)\n\n            klass = cls._dtype_to_subclass(arr.dtype)\n            arr = klass._ensure_array(arr, dtype, copy)\n            disallow_kwargs(kwargs)\n            return klass._simple_new(arr, name)\n\n        elif is_scalar(data):\n            raise cls._scalar_data_error(data)\n        elif hasattr(data, \"__array__\"):\n            return Index(np.asarray(data), dtype=dtype, copy=copy, name=name, **kwargs)\n        else:\n\n            if tupleize_cols and is_list_like(data):\n                # GH21470: convert iterable to list before determining if empty\n                if is_iterator(data):\n                    data = list(data)\n\n                if data and all(isinstance(e, tuple) for e in data):\n                    # we must be all tuples, otherwise don't construct\n                    # 10697\n                    from pandas.core.indexes.multi import MultiIndex\n\n                    return MultiIndex.from_tuples(\n                        data, names=name or kwargs.get(\"names\")\n                    )\n            # other iterable of some kind\n\n            subarr = com.asarray_tuplesafe(data, dtype=_dtype_obj)\n            if dtype is None:\n                # with e.g. a list [1, 2, 3] casting to numeric is _not_ deprecated\n                # error: Incompatible types in assignment (expression has type\n                # \"Union[ExtensionArray, ndarray[Any, Any]]\", variable has type\n                # \"ndarray[Any, Any]\")\n                subarr = _maybe_cast_data_without_dtype(  # type: ignore[assignment]\n                    subarr, cast_numeric_deprecated=False\n                )\n                dtype = subarr.dtype\n            return Index(subarr, dtype=dtype, copy=copy, name=name, **kwargs)\n\n    @classmethod\n    def _ensure_array(cls, data, dtype, copy: bool):\n        \"\"\"\n        Ensure we have a valid array to pass to _simple_new.\n        \"\"\"\n        if data.ndim > 1:\n            # GH#13601, GH#20285, GH#27125\n            raise ValueError(\"Index data must be 1-dimensional\")\n        if copy:\n            # asarray_tuplesafe does not always copy underlying data,\n            #  so need to make sure that this happens\n            data = data.copy()\n        return data\n\n    @final\n    @classmethod\n    def _dtype_to_subclass(cls, dtype: DtypeObj):\n        # Delay import for perf. https://github.com/pandas-dev/pandas/pull/31423\n\n        if isinstance(dtype, ExtensionDtype):\n            if isinstance(dtype, DatetimeTZDtype):\n                from pandas import DatetimeIndex\n\n                return DatetimeIndex\n            elif isinstance(dtype, CategoricalDtype):\n                from pandas import CategoricalIndex\n\n                return CategoricalIndex\n            elif isinstance(dtype, IntervalDtype):\n                from pandas import IntervalIndex\n\n                return IntervalIndex\n            elif isinstance(dtype, PeriodDtype):\n                from pandas import PeriodIndex\n\n                return PeriodIndex\n\n            elif isinstance(dtype, SparseDtype):\n                warnings.warn(\n                    \"In a future version, passing a SparseArray to pd.Index \"\n                    \"will store that array directly instead of converting to a \"\n                    \"dense numpy ndarray. To retain the old behavior, use \"\n                    \"pd.Index(arr.to_numpy()) instead\",\n                    FutureWarning,\n                    stacklevel=find_stack_level(),\n                )\n                return cls._dtype_to_subclass(dtype.subtype)\n\n            return Index\n\n        if dtype.kind == \"M\":\n            from pandas import DatetimeIndex\n\n            return DatetimeIndex\n\n        elif dtype.kind == \"m\":\n            from pandas import TimedeltaIndex\n\n            return TimedeltaIndex\n\n        elif is_float_dtype(dtype):\n            from pandas.core.api import Float64Index\n\n            return Float64Index\n        elif is_unsigned_integer_dtype(dtype):\n            from pandas.core.api import UInt64Index\n\n            return UInt64Index\n        elif is_signed_integer_dtype(dtype):\n            from pandas.core.api import Int64Index\n\n            return Int64Index\n\n        elif dtype == _dtype_obj:\n            # NB: assuming away MultiIndex\n            return Index\n\n        elif issubclass(dtype.type, (str, bool, np.bool_)):\n            return Index\n\n        raise NotImplementedError(dtype)\n\n    \"\"\"\n    NOTE for new Index creation:\n\n    - _simple_new: It returns new Index with the same type as the caller.\n      All metadata (such as name) must be provided by caller's responsibility.\n      Using _shallow_copy is recommended because it fills these metadata\n      otherwise specified.\n\n    - _shallow_copy: It returns new Index with the same type (using\n      _simple_new), but fills caller's metadata otherwise specified. Passed\n      kwargs will overwrite corresponding metadata.\n\n    See each method's docstring.\n    \"\"\"\n\n    @property\n    def asi8(self):\n        \"\"\"\n        Integer representation of the values.\n\n        Returns\n        -------\n        ndarray\n            An ndarray with int64 dtype.\n        \"\"\"\n        warnings.warn(\n            \"Index.asi8 is deprecated and will be removed in a future version.\",\n            FutureWarning,\n            stacklevel=find_stack_level(),\n        )\n        return None\n\n    @classmethod\n    def _simple_new(cls: type[_IndexT], values, name: Hashable = None) -> _IndexT:\n        \"\"\"\n        We require that we have a dtype compat for the values. If we are passed\n        a non-dtype compat, then coerce using the constructor.\n\n        Must be careful not to recurse.\n        \"\"\"\n        assert isinstance(values, cls._data_cls), type(values)\n\n        result = object.__new__(cls)\n        result._data = values\n        result._name = name\n        result._cache = {}\n        result._reset_identity()\n\n        return result\n\n    @classmethod\n    def _with_infer(cls, *args, **kwargs):\n        \"\"\"\n        Constructor that uses the 1.0.x behavior inferring numeric dtypes\n        for ndarray[object] inputs.\n        \"\"\"\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", \".*the Index constructor\", FutureWarning)\n            result = cls(*args, **kwargs)\n\n        if result.dtype == _dtype_obj and not result._is_multi:\n            # error: Argument 1 to \"maybe_convert_objects\" has incompatible type\n            # \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected\n            # \"ndarray[Any, Any]\"\n            values = lib.maybe_convert_objects(result._values)  # type: ignore[arg-type]\n            if values.dtype.kind in [\"i\", \"u\", \"f\"]:\n                return Index(values, name=result.name)\n\n        return result\n\n    @cache_readonly\n    def _constructor(self: _IndexT) -> type[_IndexT]:\n        return type(self)\n\n    @final\n    def _maybe_check_unique(self) -> None:\n        \"\"\"\n        Check that an Index has no duplicates.\n\n        This is typically only called via\n        `NDFrame.flags.allows_duplicate_labels.setter` when it's set to\n        True (duplicates aren't allowed).\n\n        Raises\n        ------\n        DuplicateLabelError\n            When the index is not unique.\n        \"\"\"\n        if not self.is_unique:\n            msg = \"\"\"Index has duplicates.\"\"\"\n            duplicates = self._format_duplicate_message()\n            msg += f\"\\n{duplicates}\"\n\n            raise DuplicateLabelError(msg)\n\n    @final\n    def _format_duplicate_message(self) -> DataFrame:\n        \"\"\"\n        Construct the DataFrame for a DuplicateLabelError.\n\n        This returns a DataFrame indicating the labels and positions\n        of duplicates in an index. This should only be called when it's\n        already known that duplicates are present.\n\n        Examples\n        --------\n        >>> idx = pd.Index(['a', 'b', 'a'])\n        >>> idx._format_duplicate_message()\n            positions\n        label\n        a        [0, 2]\n        \"\"\"\n        from pandas import Series\n\n        duplicates = self[self.duplicated(keep=\"first\")].unique()\n        assert len(duplicates)\n\n        out = Series(np.arange(len(self))).groupby(self).agg(list)[duplicates]\n        if self._is_multi:\n            # test_format_duplicate_labels_message_multi\n            # error: \"Type[Index]\" has no attribute \"from_tuples\"  [attr-defined]\n            out.index = type(self).from_tuples(out.index)  # type: ignore[attr-defined]\n\n        if self.nlevels == 1:\n            out = out.rename_axis(\"label\")\n        return out.to_frame(name=\"positions\")\n\n    # --------------------------------------------------------------------\n    # Index Internals Methods\n\n    @final\n    def _get_attributes_dict(self) -> dict[str_t, Any]:\n        \"\"\"\n        Return an attributes dict for my class.\n\n        Temporarily added back for compatibility issue in dask, see\n        https://github.com/pandas-dev/pandas/pull/43895\n        \"\"\"\n        warnings.warn(\n            \"The Index._get_attributes_dict method is deprecated, and will be \"\n            \"removed in a future version\",\n            DeprecationWarning,\n            stacklevel=find_stack_level(),\n        )\n        return {k: getattr(self, k, None) for k in self._attributes}\n\n    def _shallow_copy(self: _IndexT, values, name: Hashable = no_default) -> _IndexT:\n        \"\"\"\n        Create a new Index with the same class as the caller, don't copy the\n        data, use the same object attributes with passed in attributes taking\n        precedence.\n\n        *this is an internal non-public method*\n\n        Parameters\n        ----------\n        values : the values to create the new Index, optional\n        name : Label, defaults to self.name\n        \"\"\"\n        name = self._name if name is no_default else name\n\n        return self._simple_new(values, name=name)\n\n    def _view(self: _IndexT) -> _IndexT:\n        \"\"\"\n        fastpath to make a shallow copy, i.e. new object with same data.\n        \"\"\"\n        result = self._simple_new(self._values, name=self._name)\n\n        result._cache = self._cache\n        return result\n\n    @final\n    def _rename(self: _IndexT, name: Hashable) -> _IndexT:\n        \"\"\"\n        fastpath for rename if new name is already validated.\n        \"\"\"\n        result = self._view()\n        result._name = name\n        return result\n\n    @final\n    def is_(self, other) -> bool:\n        \"\"\"\n        More flexible, faster check like ``is`` but that works through views.\n\n        Note: this is *not* the same as ``Index.identical()``, which checks\n        that metadata is also the same.\n\n        Parameters\n        ----------\n        other : object\n            Other object to compare against.\n\n        Returns\n        -------\n        bool\n            True if both have same underlying data, False otherwise.\n\n        See Also\n        --------\n        Index.identical : Works like ``Index.is_`` but also checks metadata.\n        \"\"\"\n        if self is other:\n            return True\n        elif not hasattr(other, \"_id\"):\n            return False\n        elif self._id is None or other._id is None:\n            return False\n        else:\n            return self._id is other._id\n\n    @final\n    def _reset_identity(self) -> None:\n        \"\"\"\n        Initializes or resets ``_id`` attribute with new object.\n        \"\"\"\n        self._id = object()\n\n    @final\n    def _cleanup(self) -> None:\n        self._engine.clear_mapping()\n\n    @cache_readonly\n    def _engine(\n        self,\n    ) -> libindex.IndexEngine:\n        # For base class (object dtype) we get ObjectEngine\n\n        if isinstance(self._values, BaseMaskedArray):\n            # TODO(ExtensionIndex): use libindex.NullableEngine(self._values)\n            return libindex.ObjectEngine(self._get_engine_target())\n        elif (\n            isinstance(self._values, ExtensionArray)\n            and self._engine_type is libindex.ObjectEngine\n        ):\n            # TODO(ExtensionIndex): use libindex.ExtensionEngine(self._values)\n            return libindex.ObjectEngine(self._get_engine_target())\n\n        # to avoid a reference cycle, bind `target_values` to a local variable, so\n        # `self` is not passed into the lambda.\n        target_values = self._get_engine_target()\n        return self._engine_type(target_values)\n\n    @final\n    @cache_readonly\n    def _dir_additions_for_owner(self) -> set[str_t]:\n        \"\"\"\n        Add the string-like labels to the owner dataframe/series dir output.\n\n        If this is a MultiIndex, it's first level values are used.\n        \"\"\"\n        return {\n            c\n            for c in self.unique(level=0)[: get_option(\"display.max_dir_items\")]\n            if isinstance(c, str) and c.isidentifier()\n        }\n\n    # --------------------------------------------------------------------\n    # Array-Like Methods\n\n    # ndarray compat\n    def __len__(self) -> int:\n        \"\"\"\n        Return the length of the Index.\n        \"\"\"\n        return len(self._data)\n\n    def __array__(self, dtype=None) -> np.ndarray:\n        \"\"\"\n        The array interface, return my values.\n        \"\"\"\n        return np.asarray(self._data, dtype=dtype)\n\n    def __array_ufunc__(self, ufunc: np.ufunc, method: str_t, *inputs, **kwargs):\n        if any(isinstance(other, (ABCSeries, ABCDataFrame)) for other in inputs):\n            return NotImplemented\n\n        result = arraylike.maybe_dispatch_ufunc_to_dunder_op(\n            self, ufunc, method, *inputs, **kwargs\n        )\n        if result is not NotImplemented:\n            return result\n\n        if \"out\" in kwargs:\n            # e.g. test_dti_isub_tdi\n            return arraylike.dispatch_ufunc_with_out(\n                self, ufunc, method, *inputs, **kwargs\n            )\n\n        if method == \"reduce\":\n            result = arraylike.dispatch_reduction_ufunc(\n                self, ufunc, method, *inputs, **kwargs\n            )\n            if result is not NotImplemented:\n                return result\n\n        new_inputs = [x if x is not self else x._values for x in inputs]\n        result = getattr(ufunc, method)(*new_inputs, **kwargs)\n        if ufunc.nout == 2:\n            # i.e. np.divmod, np.modf, np.frexp\n            return tuple(self.__array_wrap__(x) for x in result)\n\n        return self.__array_wrap__(result)\n\n    def __array_wrap__(self, result, context=None):\n        \"\"\"\n        Gets called after a ufunc and other functions e.g. np.split.\n        \"\"\"\n        result = lib.item_from_zerodim(result)\n        if is_bool_dtype(result) or lib.is_scalar(result) or np.ndim(result) > 1:\n            return result\n\n        return Index(result, name=self.name)\n\n    @cache_readonly\n    def dtype(self) -> DtypeObj:\n        \"\"\"\n        Return the dtype object of the underlying data.\n        \"\"\"\n        return self._data.dtype\n\n    @final\n    def ravel(self, order=\"C\"):\n        \"\"\"\n        Return an ndarray of the flattened values of the underlying data.\n\n        Returns\n        -------\n        numpy.ndarray\n            Flattened array.\n\n        See Also\n        --------\n        numpy.ndarray.ravel : Return a flattened array.\n        \"\"\"\n        warnings.warn(\n            \"Index.ravel returning ndarray is deprecated; in a future version \"\n            \"this will return a view on self.\",\n            FutureWarning,\n            stacklevel=find_stack_level(),\n        )\n        if needs_i8_conversion(self.dtype):\n            # Item \"ndarray[Any, Any]\" of \"Union[ExtensionArray, ndarray[Any, Any]]\"\n            # has no attribute \"_ndarray\"\n            values = self._data._ndarray  # type: ignore[union-attr]\n        elif is_interval_dtype(self.dtype):\n            values = np.asarray(self._data)\n        else:\n            values = self._get_engine_target()\n        return values.ravel(order=order)\n\n    def view(self, cls=None):\n\n        # we need to see if we are subclassing an\n        # index type here\n        if cls is not None and not hasattr(cls, \"_typ\"):\n            dtype = cls\n            if isinstance(cls, str):\n                dtype = pandas_dtype(cls)\n\n            if isinstance(dtype, (np.dtype, ExtensionDtype)) and needs_i8_conversion(\n                dtype\n            ):\n                if dtype.kind == \"m\" and dtype != \"m8[ns]\":\n                    # e.g. m8[s]\n                    return self._data.view(cls)\n\n                idx_cls = self._dtype_to_subclass(dtype)\n                # NB: we only get here for subclasses that override\n                #  _data_cls such that it is a type and not a tuple\n                #  of types.\n                arr_cls = idx_cls._data_cls\n                arr = arr_cls(self._data.view(\"i8\"), dtype=dtype)\n                return idx_cls._simple_new(arr, name=self.name)\n\n            result = self._data.view(cls)\n        else:\n            result = self._view()\n        if isinstance(result, Index):\n            result._id = self._id\n        return result\n\n    def astype(self, dtype, copy: bool = True):\n        \"\"\"\n        Create an Index with values cast to dtypes.\n\n        The class of a new Index is determined by dtype. When conversion is\n        impossible, a TypeError exception is raised.\n\n        Parameters\n        ----------\n        dtype : numpy dtype or pandas type\n            Note that any signed integer `dtype` is treated as ``'int64'``,\n            and any unsigned integer `dtype` is treated as ``'uint64'``,\n            regardless of the size.\n        copy : bool, default True\n            By default, astype always returns a newly allocated object.\n            If copy is set to False and internal requirements on dtype are\n            satisfied, the original data is used to create a new Index\n            or the original Index is returned.\n\n        Returns\n        -------\n        Index\n            Index with values cast to specified dtype.\n        \"\"\"\n        if dtype is not None:\n            dtype = pandas_dtype(dtype)\n\n        if is_dtype_equal(self.dtype, dtype):\n            # Ensure that self.astype(self.dtype) is self\n            return self.copy() if copy else self\n\n        if (\n            self.dtype == np.dtype(\"M8[ns]\")\n            and isinstance(dtype, np.dtype)\n            and dtype.kind == \"M\"\n            and dtype != np.dtype(\"M8[ns]\")\n        ):\n            # For now DatetimeArray supports this by unwrapping ndarray,\n            #  but DatetimeIndex doesn't\n            raise TypeError(f\"Cannot cast {type(self).__name__} to dtype\")\n\n        values = self._data\n        if isinstance(values, ExtensionArray):\n            with rewrite_exception(type(values).__name__, type(self).__name__):\n                new_values = values.astype(dtype, copy=copy)\n\n        elif isinstance(dtype, ExtensionDtype):\n            cls = dtype.construct_array_type()\n            # Note: for RangeIndex and CategoricalDtype self vs self._values\n            #  behaves differently here.\n            new_values = cls._from_sequence(self, dtype=dtype, copy=copy)\n\n        else:\n            try:\n                new_values = values.astype(dtype, copy=copy)\n            except (TypeError, ValueError) as err:\n                raise TypeError(\n                    f\"Cannot cast {type(self).__name__} to dtype {dtype}\"\n                ) from err\n\n        # pass copy=False because any copying will be done in the astype above\n        return Index(new_values, name=self.name, dtype=new_values.dtype, copy=False)\n\n    _index_shared_docs[\n        \"take\"\n    ] = \"\"\"\n        Return a new %(klass)s of the values selected by the indices.\n\n        For internal compatibility with numpy arrays.\n\n        Parameters\n        ----------\n        indices : array-like\n            Indices to be taken.\n        axis : int, optional\n            The axis over which to select values, always 0.\n        allow_fill : bool, default True\n        fill_value : scalar, default None\n            If allow_fill=True and fill_value is not None, indices specified by\n            -1 are regarded as NA. If Index doesn't hold NA, raise ValueError.\n\n        Returns\n        -------\n        Index\n            An index formed of elements at the given indices. Will be the same\n            type as self, except for RangeIndex.\n\n        See Also\n        --------\n        numpy.ndarray.take: Return an array formed from the\n            elements of a at the given indices.\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"take\"] % _index_doc_kwargs)\n    def take(\n        self, indices, axis: int = 0, allow_fill: bool = True, fill_value=None, **kwargs\n    ):\n        if kwargs:\n            nv.validate_take((), kwargs)\n        if is_scalar(indices):\n            raise TypeError(\"Expected indices to be array-like\")\n        indices = ensure_platform_int(indices)\n        allow_fill = self._maybe_disallow_fill(allow_fill, fill_value, indices)\n\n        # Note: we discard fill_value and use self._na_value, only relevant\n        #  in the case where allow_fill is True and fill_value is not None\n        values = self._values\n        if isinstance(values, np.ndarray):\n            taken = algos.take(\n                values, indices, allow_fill=allow_fill, fill_value=self._na_value\n            )\n        else:\n            # algos.take passes 'axis' keyword which not all EAs accept\n            taken = values.take(\n                indices, allow_fill=allow_fill, fill_value=self._na_value\n            )\n        # _constructor so RangeIndex->Int64Index\n        return self._constructor._simple_new(taken, name=self.name)\n\n    @final\n    def _maybe_disallow_fill(self, allow_fill: bool, fill_value, indices) -> bool:\n        \"\"\"\n        We only use pandas-style take when allow_fill is True _and_\n        fill_value is not None.\n        \"\"\"\n        if allow_fill and fill_value is not None:\n            # only fill if we are passing a non-None fill_value\n            if self._can_hold_na:\n                if (indices < -1).any():\n                    raise ValueError(\n                        \"When allow_fill=True and fill_value is not None, \"\n                        \"all indices must be >= -1\"\n                    )\n            else:\n                cls_name = type(self).__name__\n                raise ValueError(\n                    f\"Unable to fill values because {cls_name} cannot contain NA\"\n                )\n        else:\n            allow_fill = False\n        return allow_fill\n\n    _index_shared_docs[\n        \"repeat\"\n    ] = \"\"\"\n        Repeat elements of a %(klass)s.\n\n        Returns a new %(klass)s where each element of the current %(klass)s\n        is repeated consecutively a given number of times.\n\n        Parameters\n        ----------\n        repeats : int or array of ints\n            The number of repetitions for each element. This should be a\n            non-negative integer. Repeating 0 times will return an empty\n            %(klass)s.\n        axis : None\n            Must be ``None``. Has no effect but is accepted for compatibility\n            with numpy.\n\n        Returns\n        -------\n        repeated_index : %(klass)s\n            Newly created %(klass)s with repeated elements.\n\n        See Also\n        --------\n        Series.repeat : Equivalent function for Series.\n        numpy.repeat : Similar method for :class:`numpy.ndarray`.\n\n        Examples\n        --------\n        >>> idx = pd.Index(['a', 'b', 'c'])\n        >>> idx\n        Index(['a', 'b', 'c'], dtype='object')\n        >>> idx.repeat(2)\n        Index(['a', 'a', 'b', 'b', 'c', 'c'], dtype='object')\n        >>> idx.repeat([1, 2, 3])\n        Index(['a', 'b', 'b', 'c', 'c', 'c'], dtype='object')\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"repeat\"] % _index_doc_kwargs)\n    def repeat(self, repeats, axis=None):\n        repeats = ensure_platform_int(repeats)\n        nv.validate_repeat((), {\"axis\": axis})\n        res_values = self._values.repeat(repeats)\n\n        # _constructor so RangeIndex->Int64Index\n        return self._constructor._simple_new(res_values, name=self.name)\n\n    # --------------------------------------------------------------------\n    # Copying Methods\n\n    def copy(\n        self: _IndexT,\n        name: Hashable | None = None,\n        deep: bool = False,\n        dtype: Dtype | None = None,\n        names: Sequence[Hashable] | None = None,\n    ) -> _IndexT:\n        \"\"\"\n        Make a copy of this object.\n\n        Name and dtype sets those attributes on the new object.\n\n        Parameters\n        ----------\n        name : Label, optional\n            Set name for new object.\n        deep : bool, default False\n        dtype : numpy dtype or pandas type, optional\n            Set dtype for new object.\n\n            .. deprecated:: 1.2.0\n                use ``astype`` method instead.\n        names : list-like, optional\n            Kept for compatibility with MultiIndex. Should not be used.\n\n            .. deprecated:: 1.4.0\n                use ``name`` instead.\n\n        Returns\n        -------\n        Index\n            Index refer to new object which is a copy of this object.\n\n        Notes\n        -----\n        In most cases, there should be no functional difference from using\n        ``deep``, but if ``deep`` is passed it will attempt to deepcopy.\n        \"\"\"\n        if names is not None:\n            warnings.warn(\n                \"parameter names is deprecated and will be removed in a future \"\n                \"version. Use the name parameter instead.\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n\n        name = self._validate_names(name=name, names=names, deep=deep)[0]\n        if deep:\n            new_data = self._data.copy()\n            new_index = type(self)._simple_new(new_data, name=name)\n        else:\n            new_index = self._rename(name=name)\n\n        if dtype:\n            warnings.warn(\n                \"parameter dtype is deprecated and will be removed in a future \"\n                \"version. Use the astype method instead.\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n            new_index = new_index.astype(dtype)\n        return new_index\n\n    @final\n    def __copy__(self: _IndexT, **kwargs) -> _IndexT:\n        return self.copy(**kwargs)\n\n    @final\n    def __deepcopy__(self: _IndexT, memo=None) -> _IndexT:\n        \"\"\"\n        Parameters\n        ----------\n        memo, default None\n            Standard signature. Unused\n        \"\"\"\n        return self.copy(deep=True)\n\n    # --------------------------------------------------------------------\n    # Rendering Methods\n\n    @final\n    def __repr__(self) -> str_t:\n        \"\"\"\n        Return a string representation for this object.\n        \"\"\"\n        klass_name = type(self).__name__\n        data = self._format_data()\n        attrs = self._format_attrs()\n        space = self._format_space()\n        attrs_str = [f\"{k}={v}\" for k, v in attrs]\n        prepr = f\",{space}\".join(attrs_str)\n\n        # no data provided, just attributes\n        if data is None:\n            data = \"\"\n\n        return f\"{klass_name}({data}{prepr})\"\n\n    def _format_space(self) -> str_t:\n\n        # using space here controls if the attributes\n        # are line separated or not (the default)\n\n        # max_seq_items = get_option('display.max_seq_items')\n        # if len(self) > max_seq_items:\n        #    space = \"\\n%s\" % (' ' * (len(klass) + 1))\n        return \" \"\n\n    @property\n    def _formatter_func(self):\n        \"\"\"\n        Return the formatter function.\n        \"\"\"\n        return default_pprint\n\n    def _format_data(self, name=None) -> str_t:\n        \"\"\"\n        Return the formatted data as a unicode string.\n        \"\"\"\n        # do we want to justify (only do so for non-objects)\n        is_justify = True\n\n        if self.inferred_type == \"string\":\n            is_justify = False\n        elif self.inferred_type == \"categorical\":\n            self = cast(\"CategoricalIndex\", self)\n            if is_object_dtype(self.categories):\n                is_justify = False\n\n        return format_object_summary(\n            self,\n            self._formatter_func,\n            is_justify=is_justify,\n            name=name,\n            line_break_each_value=self._is_multi,\n        )\n\n    def _format_attrs(self) -> list[tuple[str_t, str_t | int | bool | None]]:\n        \"\"\"\n        Return a list of tuples of the (attr,formatted_value).\n        \"\"\"\n        attrs: list[tuple[str_t, str_t | int | bool | None]] = []\n\n        if not self._is_multi:\n            attrs.append((\"dtype\", f\"'{self.dtype}'\"))\n\n        if self.name is not None:\n            attrs.append((\"name\", default_pprint(self.name)))\n        elif self._is_multi and any(x is not None for x in self.names):\n            attrs.append((\"names\", default_pprint(self.names)))\n\n        max_seq_items = get_option(\"display.max_seq_items\") or len(self)\n        if len(self) > max_seq_items:\n            attrs.append((\"length\", len(self)))\n        return attrs\n\n    @final\n    def _mpl_repr(self) -> np.ndarray:\n        # how to represent ourselves to matplotlib\n        if isinstance(self.dtype, np.dtype) and self.dtype.kind != \"M\":\n            return cast(np.ndarray, self.values)\n        return self.astype(object, copy=False)._values\n\n    def format(\n        self,\n        name: bool = False,\n        formatter: Callable | None = None,\n        na_rep: str_t = \"NaN\",\n    ) -> list[str_t]:\n        \"\"\"\n        Render a string representation of the Index.\n        \"\"\"\n        header = []\n        if name:\n            header.append(\n                pprint_thing(self.name, escape_chars=(\"\\t\", \"\\r\", \"\\n\"))\n                if self.name is not None\n                else \"\"\n            )\n\n        if formatter is not None:\n            return header + list(self.map(formatter))\n\n        return self._format_with_header(header, na_rep=na_rep)\n\n    def _format_with_header(self, header: list[str_t], na_rep: str_t) -> list[str_t]:\n        from pandas.io.formats.format import format_array\n\n        values = self._values\n\n        if is_object_dtype(values.dtype):\n            values = cast(np.ndarray, values)\n            values = lib.maybe_convert_objects(values, safe=True)\n\n            result = [pprint_thing(x, escape_chars=(\"\\t\", \"\\r\", \"\\n\")) for x in values]\n\n            # could have nans\n            mask = is_float_nan(values)\n            if mask.any():\n                result_arr = np.array(result)\n                result_arr[mask] = na_rep\n                result = result_arr.tolist()\n        else:\n            result = trim_front(format_array(values, None, justify=\"left\"))\n        return header + result\n\n    @final\n    def to_native_types(self, slicer=None, **kwargs) -> np.ndarray:\n        \"\"\"\n        Format specified values of `self` and return them.\n\n        .. deprecated:: 1.2.0\n\n        Parameters\n        ----------\n        slicer : int, array-like\n            An indexer into `self` that specifies which values\n            are used in the formatting process.\n        kwargs : dict\n            Options for specifying how the values should be formatted.\n            These options include the following:\n\n            1) na_rep : str\n                The value that serves as a placeholder for NULL values\n            2) quoting : bool or None\n                Whether or not there are quoted values in `self`\n            3) date_format : str\n                The format used to represent date-like values.\n\n        Returns\n        -------\n        numpy.ndarray\n            Formatted values.\n        \"\"\"\n        warnings.warn(\n            \"The 'to_native_types' method is deprecated and will be removed in \"\n            \"a future version. Use 'astype(str)' instead.\",\n            FutureWarning,\n            stacklevel=find_stack_level(),\n        )\n        values = self\n        if slicer is not None:\n            values = values[slicer]\n        return values._format_native_types(**kwargs)\n\n    def _format_native_types(self, *, na_rep=\"\", quoting=None, **kwargs):\n        \"\"\"\n        Actually format specific types of the index.\n        \"\"\"\n        mask = isna(self)\n        if not self.is_object() and not quoting:\n            values = np.asarray(self).astype(str)\n        else:\n            values = np.array(self, dtype=object, copy=True)\n\n        values[mask] = na_rep\n        return values\n\n    def _summary(self, name=None) -> str_t:\n        \"\"\"\n        Return a summarized representation.\n\n        Parameters\n        ----------\n        name : str\n            name to use in the summary representation\n\n        Returns\n        -------\n        String with a summarized representation of the index\n        \"\"\"\n        if len(self) > 0:\n            head = self[0]\n            if hasattr(head, \"format\") and not isinstance(head, str):\n                head = head.format()\n            elif needs_i8_conversion(self.dtype):\n                # e.g. Timedelta, display as values, not quoted\n                head = self._formatter_func(head).replace(\"'\", \"\")\n            tail = self[-1]\n            if hasattr(tail, \"format\") and not isinstance(tail, str):\n                tail = tail.format()\n            elif needs_i8_conversion(self.dtype):\n                # e.g. Timedelta, display as values, not quoted\n                tail = self._formatter_func(tail).replace(\"'\", \"\")\n\n            index_summary = f\", {head} to {tail}\"\n        else:\n            index_summary = \"\"\n\n        if name is None:\n            name = type(self).__name__\n        return f\"{name}: {len(self)} entries{index_summary}\"\n\n    # --------------------------------------------------------------------\n    # Conversion Methods\n\n    def to_flat_index(self):\n        \"\"\"\n        Identity method.\n\n        This is implemented for compatibility with subclass implementations\n        when chaining.\n\n        Returns\n        -------\n        pd.Index\n            Caller.\n\n        See Also\n        --------\n        MultiIndex.to_flat_index : Subclass implementation.\n        \"\"\"\n        return self\n\n    def to_series(self, index=None, name: Hashable = None) -> Series:\n        \"\"\"\n        Create a Series with both index and values equal to the index keys.\n\n        Useful with map for returning an indexer based on an index.\n\n        Parameters\n        ----------\n        index : Index, optional\n            Index of resulting Series. If None, defaults to original index.\n        name : str, optional\n            Name of resulting Series. If None, defaults to name of original\n            index.\n\n        Returns\n        -------\n        Series\n            The dtype will be based on the type of the Index values.\n\n        See Also\n        --------\n        Index.to_frame : Convert an Index to a DataFrame.\n        Series.to_frame : Convert Series to DataFrame.\n\n        Examples\n        --------\n        >>> idx = pd.Index(['Ant', 'Bear', 'Cow'], name='animal')\n\n        By default, the original Index and original name is reused.\n\n        >>> idx.to_series()\n        animal\n        Ant      Ant\n        Bear    Bear\n        Cow      Cow\n        Name: animal, dtype: object\n\n        To enforce a new Index, specify new labels to ``index``:\n\n        >>> idx.to_series(index=[0, 1, 2])\n        0     Ant\n        1    Bear\n        2     Cow\n        Name: animal, dtype: object\n\n        To override the name of the resulting column, specify `name`:\n\n        >>> idx.to_series(name='zoo')\n        animal\n        Ant      Ant\n        Bear    Bear\n        Cow      Cow\n        Name: zoo, dtype: object\n        \"\"\"\n        from pandas import Series\n\n        if index is None:\n            index = self._view()\n        if name is None:\n            name = self.name\n\n        return Series(self._values.copy(), index=index, name=name)\n\n    def to_frame(\n        self, index: bool = True, name: Hashable = lib.no_default\n    ) -> DataFrame:\n        \"\"\"\n        Create a DataFrame with a column containing the Index.\n\n        Parameters\n        ----------\n        index : bool, default True\n            Set the index of the returned DataFrame as the original Index.\n\n        name : object, default None\n            The passed name should substitute for the index name (if it has\n            one).\n\n        Returns\n        -------\n        DataFrame\n            DataFrame containing the original Index data.\n\n        See Also\n        --------\n        Index.to_series : Convert an Index to a Series.\n        Series.to_frame : Convert Series to DataFrame.\n\n        Examples\n        --------\n        >>> idx = pd.Index(['Ant', 'Bear', 'Cow'], name='animal')\n        >>> idx.to_frame()\n               animal\n        animal\n        Ant       Ant\n        Bear     Bear\n        Cow       Cow\n\n        By default, the original Index is reused. To enforce a new Index:\n\n        >>> idx.to_frame(index=False)\n            animal\n        0   Ant\n        1  Bear\n        2   Cow\n\n        To override the name of the resulting column, specify `name`:\n\n        >>> idx.to_frame(index=False, name='zoo')\n            zoo\n        0   Ant\n        1  Bear\n        2   Cow\n        \"\"\"\n        from pandas import DataFrame\n\n        if name is None:\n            warnings.warn(\n                \"Explicitly passing `name=None` currently preserves the Index's name \"\n                \"or uses a default name of 0. This behaviour is deprecated, and in \"\n                \"the future `None` will be used as the name of the resulting \"\n                \"DataFrame column.\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n            name = lib.no_default\n\n        if name is lib.no_default:\n            name = self.name or 0\n        result = DataFrame({name: self._values.copy()})\n\n        if index:\n            result.index = self\n        return result\n\n    # --------------------------------------------------------------------\n    # Name-Centric Methods\n\n    @property\n    def name(self):\n        \"\"\"\n        Return Index or MultiIndex name.\n        \"\"\"\n        return self._name\n\n    @name.setter\n    def name(self, value: Hashable):\n        if self._no_setting_name:\n            # Used in MultiIndex.levels to avoid silently ignoring name updates.\n            raise RuntimeError(\n                \"Cannot set name on a level of a MultiIndex. Use \"\n                \"'MultiIndex.set_names' instead.\"\n            )\n        maybe_extract_name(value, None, type(self))\n        self._name = value\n\n    @final\n    def _validate_names(\n        self, name=None, names=None, deep: bool = False\n    ) -> list[Hashable]:\n        \"\"\"\n        Handles the quirks of having a singular 'name' parameter for general\n        Index and plural 'names' parameter for MultiIndex.\n        \"\"\"\n        from copy import deepcopy\n\n        if names is not None and name is not None:\n            raise TypeError(\"Can only provide one of `names` and `name`\")\n        elif names is None and name is None:\n            new_names = deepcopy(self.names) if deep else self.names\n        elif names is not None:\n            if not is_list_like(names):\n                raise TypeError(\"Must pass list-like as `names`.\")\n            new_names = names\n        elif not is_list_like(name):\n            new_names = [name]\n        else:\n            new_names = name\n\n        if len(new_names) != len(self.names):\n            raise ValueError(\n                f\"Length of new names must be {len(self.names)}, got {len(new_names)}\"\n            )\n\n        # All items in 'new_names' need to be hashable\n        validate_all_hashable(*new_names, error_name=f\"{type(self).__name__}.name\")\n\n        return new_names\n\n    def _get_names(self) -> FrozenList:\n        return FrozenList((self.name,))\n\n    def _set_names(self, values, *, level=None) -> None:\n        \"\"\"\n        Set new names on index. Each name has to be a hashable type.\n\n        Parameters\n        ----------\n        values : str or sequence\n            name(s) to set\n        level : int, level name, or sequence of int/level names (default None)\n            If the index is a MultiIndex (hierarchical), level(s) to set (None\n            for all levels).  Otherwise level must be None\n\n        Raises\n        ------\n        TypeError if each name is not hashable.\n        \"\"\"\n        if not is_list_like(values):\n            raise ValueError(\"Names must be a list-like\")\n        if len(values) != 1:\n            raise ValueError(f\"Length of new names must be 1, got {len(values)}\")\n\n        # GH 20527\n        # All items in 'name' need to be hashable:\n        validate_all_hashable(*values, error_name=f\"{type(self).__name__}.name\")\n\n        self._name = values[0]\n\n    names = property(fset=_set_names, fget=_get_names)\n\n    @deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\", \"names\"])\n    def set_names(self, names, level=None, inplace: bool = False):\n        \"\"\"\n        Set Index or MultiIndex name.\n\n        Able to set new names partially and by level.\n\n        Parameters\n        ----------\n\n        names : label or list of label or dict-like for MultiIndex\n            Name(s) to set.\n\n            .. versionchanged:: 1.3.0\n\n        level : int, label or list of int or label, optional\n            If the index is a MultiIndex and names is not dict-like, level(s) to set\n            (None for all levels). Otherwise level must be None.\n\n            .. versionchanged:: 1.3.0\n\n        inplace : bool, default False\n            Modifies the object directly, instead of creating a new Index or\n            MultiIndex.\n\n        Returns\n        -------\n        Index or None\n            The same type as the caller or None if ``inplace=True``.\n\n        See Also\n        --------\n        Index.rename : Able to set new names without level.\n\n        Examples\n        --------\n        >>> idx = pd.Index([1, 2, 3, 4])\n        >>> idx\n        Int64Index([1, 2, 3, 4], dtype='int64')\n        >>> idx.set_names('quarter')\n        Int64Index([1, 2, 3, 4], dtype='int64', name='quarter')\n\n        >>> idx = pd.MultiIndex.from_product([['python', 'cobra'],\n        ...                                   [2018, 2019]])\n        >>> idx\n        MultiIndex([('python', 2018),\n                    ('python', 2019),\n                    ( 'cobra', 2018),\n                    ( 'cobra', 2019)],\n                   )\n        >>> idx.set_names(['kind', 'year'], inplace=True)\n        >>> idx\n        MultiIndex([('python', 2018),\n                    ('python', 2019),\n                    ( 'cobra', 2018),\n                    ( 'cobra', 2019)],\n                   names=['kind', 'year'])\n        >>> idx.set_names('species', level=0)\n        MultiIndex([('python', 2018),\n                    ('python', 2019),\n                    ( 'cobra', 2018),\n                    ( 'cobra', 2019)],\n                   names=['species', 'year'])\n\n        When renaming levels with a dict, levels can not be passed.\n\n        >>> idx.set_names({'kind': 'snake'})\n        MultiIndex([('python', 2018),\n                    ('python', 2019),\n                    ( 'cobra', 2018),\n                    ( 'cobra', 2019)],\n                   names=['snake', 'year'])\n        \"\"\"\n        if level is not None and not isinstance(self, ABCMultiIndex):\n            raise ValueError(\"Level must be None for non-MultiIndex\")\n\n        elif level is not None and not is_list_like(level) and is_list_like(names):\n            raise TypeError(\"Names must be a string when a single level is provided.\")\n\n        elif not is_list_like(names) and level is None and self.nlevels > 1:\n            raise TypeError(\"Must pass list-like as `names`.\")\n\n        elif is_dict_like(names) and not isinstance(self, ABCMultiIndex):\n            raise TypeError(\"Can only pass dict-like as `names` for MultiIndex.\")\n\n        elif is_dict_like(names) and level is not None:\n            raise TypeError(\"Can not pass level for dictlike `names`.\")\n\n        if isinstance(self, ABCMultiIndex) and is_dict_like(names) and level is None:\n            # Transform dict to list of new names and corresponding levels\n            level, names_adjusted = [], []\n            for i, name in enumerate(self.names):\n                if name in names.keys():\n                    level.append(i)\n                    names_adjusted.append(names[name])\n            names = names_adjusted\n\n        if not is_list_like(names):\n            names = [names]\n        if level is not None and not is_list_like(level):\n            level = [level]\n\n        if inplace:\n            idx = self\n        else:\n            idx = self._view()\n\n        idx._set_names(names, level=level)\n        if not inplace:\n            return idx\n\n    def rename(self, name, inplace=False):\n        \"\"\"\n        Alter Index or MultiIndex name.\n\n        Able to set new names without level. Defaults to returning new index.\n        Length of names must match number of levels in MultiIndex.\n\n        Parameters\n        ----------\n        name : label or list of labels\n            Name(s) to set.\n        inplace : bool, default False\n            Modifies the object directly, instead of creating a new Index or\n            MultiIndex.\n\n        Returns\n        -------\n        Index or None\n            The same type as the caller or None if ``inplace=True``.\n\n        See Also\n        --------\n        Index.set_names : Able to set new names partially and by level.\n\n        Examples\n        --------\n        >>> idx = pd.Index(['A', 'C', 'A', 'B'], name='score')\n        >>> idx.rename('grade')\n        Index(['A', 'C', 'A', 'B'], dtype='object', name='grade')\n\n        >>> idx = pd.MultiIndex.from_product([['python', 'cobra'],\n        ...                                   [2018, 2019]],\n        ...                                   names=['kind', 'year'])\n        >>> idx\n        MultiIndex([('python', 2018),\n                    ('python', 2019),\n                    ( 'cobra', 2018),\n                    ( 'cobra', 2019)],\n                   names=['kind', 'year'])\n        >>> idx.rename(['species', 'year'])\n        MultiIndex([('python', 2018),\n                    ('python', 2019),\n                    ( 'cobra', 2018),\n                    ( 'cobra', 2019)],\n                   names=['species', 'year'])\n        >>> idx.rename('species')\n        Traceback (most recent call last):\n        TypeError: Must pass list-like as `names`.\n        \"\"\"\n        return self.set_names([name], inplace=inplace)\n\n    # --------------------------------------------------------------------\n    # Level-Centric Methods\n\n    @property\n    def nlevels(self) -> int:\n        \"\"\"\n        Number of levels.\n        \"\"\"\n        return 1\n\n    def _sort_levels_monotonic(self: _IndexT) -> _IndexT:\n        \"\"\"\n        Compat with MultiIndex.\n        \"\"\"\n        return self\n\n    @final\n    def _validate_index_level(self, level) -> None:\n        \"\"\"\n        Validate index level.\n\n        For single-level Index getting level number is a no-op, but some\n        verification must be done like in MultiIndex.\n\n        \"\"\"\n        if isinstance(level, int):\n            if level < 0 and level != -1:\n                raise IndexError(\n                    \"Too many levels: Index has only 1 level, \"\n                    f\"{level} is not a valid level number\"\n                )\n            elif level > 0:\n                raise IndexError(\n                    f\"Too many levels: Index has only 1 level, not {level + 1}\"\n                )\n        elif level != self.name:\n            raise KeyError(\n                f\"Requested level ({level}) does not match index name ({self.name})\"\n            )\n\n    def _get_level_number(self, level) -> int:\n        self._validate_index_level(level)\n        return 0\n\n    def sortlevel(self, level=None, ascending=True, sort_remaining=None):\n        \"\"\"\n        For internal compatibility with the Index API.\n\n        Sort the Index. This is for compat with MultiIndex\n\n        Parameters\n        ----------\n        ascending : bool, default True\n            False to sort in descending order\n\n        level, sort_remaining are compat parameters\n\n        Returns\n        -------\n        Index\n        \"\"\"\n        if not isinstance(ascending, (list, bool)):\n            raise TypeError(\n                \"ascending must be a single bool value or\"\n                \"a list of bool values of length 1\"\n            )\n\n        if isinstance(ascending, list):\n            if len(ascending) != 1:\n                raise TypeError(\"ascending must be a list of bool values of length 1\")\n            ascending = ascending[0]\n\n        if not isinstance(ascending, bool):\n            raise TypeError(\"ascending must be a bool value\")\n\n        return self.sort_values(return_indexer=True, ascending=ascending)\n\n    def _get_level_values(self, level) -> Index:\n        \"\"\"\n        Return an Index of values for requested level.\n\n        This is primarily useful to get an individual level of values from a\n        MultiIndex, but is provided on Index as well for compatibility.\n\n        Parameters\n        ----------\n        level : int or str\n            It is either the integer position or the name of the level.\n\n        Returns\n        -------\n        Index\n            Calling object, as there is only one level in the Index.\n\n        See Also\n        --------\n        MultiIndex.get_level_values : Get values for a level of a MultiIndex.\n\n        Notes\n        -----\n        For Index, level should be 0, since there are no multiple levels.\n\n        Examples\n        --------\n        >>> idx = pd.Index(list('abc'))\n        >>> idx\n        Index(['a', 'b', 'c'], dtype='object')\n\n        Get level values by supplying `level` as integer:\n\n        >>> idx.get_level_values(0)\n        Index(['a', 'b', 'c'], dtype='object')\n        \"\"\"\n        self._validate_index_level(level)\n        return self\n\n    get_level_values = _get_level_values\n\n    @final\n    def droplevel(self, level=0):\n        \"\"\"\n        Return index with requested level(s) removed.\n\n        If resulting index has only 1 level left, the result will be\n        of Index type, not MultiIndex.\n\n        Parameters\n        ----------\n        level : int, str, or list-like, default 0\n            If a string is given, must be the name of a level\n            If list-like, elements must be names or indexes of levels.\n\n        Returns\n        -------\n        Index or MultiIndex\n\n        Examples\n        --------\n        >>> mi = pd.MultiIndex.from_arrays(\n        ... [[1, 2], [3, 4], [5, 6]], names=['x', 'y', 'z'])\n        >>> mi\n        MultiIndex([(1, 3, 5),\n                    (2, 4, 6)],\n                   names=['x', 'y', 'z'])\n\n        >>> mi.droplevel()\n        MultiIndex([(3, 5),\n                    (4, 6)],\n                   names=['y', 'z'])\n\n        >>> mi.droplevel(2)\n        MultiIndex([(1, 3),\n                    (2, 4)],\n                   names=['x', 'y'])\n\n        >>> mi.droplevel('z')\n        MultiIndex([(1, 3),\n                    (2, 4)],\n                   names=['x', 'y'])\n\n        >>> mi.droplevel(['x', 'y'])\n        Int64Index([5, 6], dtype='int64', name='z')\n        \"\"\"\n        if not isinstance(level, (tuple, list)):\n            level = [level]\n\n        levnums = sorted(self._get_level_number(lev) for lev in level)[::-1]\n\n        return self._drop_level_numbers(levnums)\n\n    @final\n    def _drop_level_numbers(self, levnums: list[int]):\n        \"\"\"\n        Drop MultiIndex levels by level _number_, not name.\n        \"\"\"\n\n        if not levnums and not isinstance(self, ABCMultiIndex):\n            return self\n        if len(levnums) >= self.nlevels:\n            raise ValueError(\n                f\"Cannot remove {len(levnums)} levels from an index with \"\n                f\"{self.nlevels} levels: at least one level must be left.\"\n            )\n        # The two checks above guarantee that here self is a MultiIndex\n        self = cast(\"MultiIndex\", self)\n\n        new_levels = list(self.levels)\n        new_codes = list(self.codes)\n        new_names = list(self.names)\n\n        for i in levnums:\n            new_levels.pop(i)\n            new_codes.pop(i)\n            new_names.pop(i)\n\n        if len(new_levels) == 1:\n            lev = new_levels[0]\n\n            if len(lev) == 0:\n                # If lev is empty, lev.take will fail GH#42055\n                if len(new_codes[0]) == 0:\n                    # GH#45230 preserve RangeIndex here\n                    #  see test_reset_index_empty_rangeindex\n                    result = lev[:0]\n                else:\n                    res_values = algos.take(lev._values, new_codes[0], allow_fill=True)\n                    # _constructor instead of type(lev) for RangeIndex compat GH#35230\n                    result = lev._constructor._simple_new(res_values, name=new_names[0])\n            else:\n                # set nan if needed\n                mask = new_codes[0] == -1\n                result = new_levels[0].take(new_codes[0])\n                if mask.any():\n                    result = result.putmask(mask, np.nan)\n\n                result._name = new_names[0]\n\n            return result\n        else:\n            from pandas.core.indexes.multi import MultiIndex\n\n            return MultiIndex(\n                levels=new_levels,\n                codes=new_codes,\n                names=new_names,\n                verify_integrity=False,\n            )\n\n    def _get_grouper_for_level(self, mapper, *, level=None):\n        \"\"\"\n        Get index grouper corresponding to an index level\n\n        Parameters\n        ----------\n        mapper: Group mapping function or None\n            Function mapping index values to groups\n        level : int or None\n            Index level, positional\n\n        Returns\n        -------\n        grouper : Index\n            Index of values to group on.\n        labels : ndarray of int or None\n            Array of locations in level_index.\n        uniques : Index or None\n            Index of unique values for level.\n        \"\"\"\n        assert level is None or level == 0\n        if mapper is None:\n            grouper = self\n        else:\n            grouper = self.map(mapper)\n\n        return grouper, None, None\n\n    # --------------------------------------------------------------------\n    # Introspection Methods\n\n    @final\n    @property\n    def is_monotonic(self) -> bool:\n        \"\"\"\n        Alias for is_monotonic_increasing.\n        \"\"\"\n        return self.is_monotonic_increasing\n\n    @property\n    def is_monotonic_increasing(self) -> bool:\n        \"\"\"\n        Return if the index is monotonic increasing (only equal or\n        increasing) values.\n\n        Examples\n        --------\n        >>> Index([1, 2, 3]).is_monotonic_increasing\n        True\n        >>> Index([1, 2, 2]).is_monotonic_increasing\n        True\n        >>> Index([1, 3, 2]).is_monotonic_increasing\n        False\n        \"\"\"\n        return self._engine.is_monotonic_increasing\n\n    @property\n    def is_monotonic_decreasing(self) -> bool:\n        \"\"\"\n        Return if the index is monotonic decreasing (only equal or\n        decreasing) values.\n\n        Examples\n        --------\n        >>> Index([3, 2, 1]).is_monotonic_decreasing\n        True\n        >>> Index([3, 2, 2]).is_monotonic_decreasing\n        True\n        >>> Index([3, 1, 2]).is_monotonic_decreasing\n        False\n        \"\"\"\n        return self._engine.is_monotonic_decreasing\n\n    @final\n    @property\n    def _is_strictly_monotonic_increasing(self) -> bool:\n        \"\"\"\n        Return if the index is strictly monotonic increasing\n        (only increasing) values.\n\n        Examples\n        --------\n        >>> Index([1, 2, 3])._is_strictly_monotonic_increasing\n        True\n        >>> Index([1, 2, 2])._is_strictly_monotonic_increasing\n        False\n        >>> Index([1, 3, 2])._is_strictly_monotonic_increasing\n        False\n        \"\"\"\n        return self.is_unique and self.is_monotonic_increasing\n\n    @final\n    @property\n    def _is_strictly_monotonic_decreasing(self) -> bool:\n        \"\"\"\n        Return if the index is strictly monotonic decreasing\n        (only decreasing) values.\n\n        Examples\n        --------\n        >>> Index([3, 2, 1])._is_strictly_monotonic_decreasing\n        True\n        >>> Index([3, 2, 2])._is_strictly_monotonic_decreasing\n        False\n        >>> Index([3, 1, 2])._is_strictly_monotonic_decreasing\n        False\n        \"\"\"\n        return self.is_unique and self.is_monotonic_decreasing\n\n    @cache_readonly\n    def is_unique(self) -> bool:\n        \"\"\"\n        Return if the index has unique values.\n        \"\"\"\n        return self._engine.is_unique\n\n    @final\n    @property\n    def has_duplicates(self) -> bool:\n        \"\"\"\n        Check if the Index has duplicate values.\n\n        Returns\n        -------\n        bool\n            Whether or not the Index has duplicate values.\n\n        Examples\n        --------\n        >>> idx = pd.Index([1, 5, 7, 7])\n        >>> idx.has_duplicates\n        True\n\n        >>> idx = pd.Index([1, 5, 7])\n        >>> idx.has_duplicates\n        False\n\n        >>> idx = pd.Index([\"Watermelon\", \"Orange\", \"Apple\",\n        ...                 \"Watermelon\"]).astype(\"category\")\n        >>> idx.has_duplicates\n        True\n\n        >>> idx = pd.Index([\"Orange\", \"Apple\",\n        ...                 \"Watermelon\"]).astype(\"category\")\n        >>> idx.has_duplicates\n        False\n        \"\"\"\n        return not self.is_unique\n\n    @final\n    def is_boolean(self) -> bool:\n        \"\"\"\n        Check if the Index only consists of booleans.\n\n        Returns\n        -------\n        bool\n            Whether or not the Index only consists of booleans.\n\n        See Also\n        --------\n        is_integer : Check if the Index only consists of integers.\n        is_floating : Check if the Index is a floating type.\n        is_numeric : Check if the Index only consists of numeric data.\n        is_object : Check if the Index is of the object dtype.\n        is_categorical : Check if the Index holds categorical data.\n        is_interval : Check if the Index holds Interval objects.\n        is_mixed : Check if the Index holds data with mixed data types.\n\n        Examples\n        --------\n        >>> idx = pd.Index([True, False, True])\n        >>> idx.is_boolean()\n        True\n\n        >>> idx = pd.Index([\"True\", \"False\", \"True\"])\n        >>> idx.is_boolean()\n        False\n\n        >>> idx = pd.Index([True, False, \"True\"])\n        >>> idx.is_boolean()\n        False\n        \"\"\"\n        return self.inferred_type in [\"boolean\"]\n\n    @final\n    def is_integer(self) -> bool:\n        \"\"\"\n        Check if the Index only consists of integers.\n\n        Returns\n        -------\n        bool\n            Whether or not the Index only consists of integers.\n\n        See Also\n        --------\n        is_boolean : Check if the Index only consists of booleans.\n        is_floating : Check if the Index is a floating type.\n        is_numeric : Check if the Index only consists of numeric data.\n        is_object : Check if the Index is of the object dtype.\n        is_categorical : Check if the Index holds categorical data.\n        is_interval : Check if the Index holds Interval objects.\n        is_mixed : Check if the Index holds data with mixed data types.\n\n        Examples\n        --------\n        >>> idx = pd.Index([1, 2, 3, 4])\n        >>> idx.is_integer()\n        True\n\n        >>> idx = pd.Index([1.0, 2.0, 3.0, 4.0])\n        >>> idx.is_integer()\n        False\n\n        >>> idx = pd.Index([\"Apple\", \"Mango\", \"Watermelon\"])\n        >>> idx.is_integer()\n        False\n        \"\"\"\n        return self.inferred_type in [\"integer\"]\n\n    @final\n    def is_floating(self) -> bool:\n        \"\"\"\n        Check if the Index is a floating type.\n\n        The Index may consist of only floats, NaNs, or a mix of floats,\n        integers, or NaNs.\n\n        Returns\n        -------\n        bool\n            Whether or not the Index only consists of only consists of floats, NaNs, or\n            a mix of floats, integers, or NaNs.\n\n        See Also\n        --------\n        is_boolean : Check if the Index only consists of booleans.\n        is_integer : Check if the Index only consists of integers.\n        is_numeric : Check if the Index only consists of numeric data.\n        is_object : Check if the Index is of the object dtype.\n        is_categorical : Check if the Index holds categorical data.\n        is_interval : Check if the Index holds Interval objects.\n        is_mixed : Check if the Index holds data with mixed data types.\n\n        Examples\n        --------\n        >>> idx = pd.Index([1.0, 2.0, 3.0, 4.0])\n        >>> idx.is_floating()\n        True\n\n        >>> idx = pd.Index([1.0, 2.0, np.nan, 4.0])\n        >>> idx.is_floating()\n        True\n\n        >>> idx = pd.Index([1, 2, 3, 4, np.nan])\n        >>> idx.is_floating()\n        True\n\n        >>> idx = pd.Index([1, 2, 3, 4])\n        >>> idx.is_floating()\n        False\n        \"\"\"\n        return self.inferred_type in [\"floating\", \"mixed-integer-float\", \"integer-na\"]\n\n    @final\n    def is_numeric(self) -> bool:\n        \"\"\"\n        Check if the Index only consists of numeric data.\n\n        Returns\n        -------\n        bool\n            Whether or not the Index only consists of numeric data.\n\n        See Also\n        --------\n        is_boolean : Check if the Index only consists of booleans.\n        is_integer : Check if the Index only consists of integers.\n        is_floating : Check if the Index is a floating type.\n        is_object : Check if the Index is of the object dtype.\n        is_categorical : Check if the Index holds categorical data.\n        is_interval : Check if the Index holds Interval objects.\n        is_mixed : Check if the Index holds data with mixed data types.\n\n        Examples\n        --------\n        >>> idx = pd.Index([1.0, 2.0, 3.0, 4.0])\n        >>> idx.is_numeric()\n        True\n\n        >>> idx = pd.Index([1, 2, 3, 4.0])\n        >>> idx.is_numeric()\n        True\n\n        >>> idx = pd.Index([1, 2, 3, 4])\n        >>> idx.is_numeric()\n        True\n\n        >>> idx = pd.Index([1, 2, 3, 4.0, np.nan])\n        >>> idx.is_numeric()\n        True\n\n        >>> idx = pd.Index([1, 2, 3, 4.0, np.nan, \"Apple\"])\n        >>> idx.is_numeric()\n        False\n        \"\"\"\n        return self.inferred_type in [\"integer\", \"floating\"]\n\n    @final\n    def is_object(self) -> bool:\n        \"\"\"\n        Check if the Index is of the object dtype.\n\n        Returns\n        -------\n        bool\n            Whether or not the Index is of the object dtype.\n\n        See Also\n        --------\n        is_boolean : Check if the Index only consists of booleans.\n        is_integer : Check if the Index only consists of integers.\n        is_floating : Check if the Index is a floating type.\n        is_numeric : Check if the Index only consists of numeric data.\n        is_categorical : Check if the Index holds categorical data.\n        is_interval : Check if the Index holds Interval objects.\n        is_mixed : Check if the Index holds data with mixed data types.\n\n        Examples\n        --------\n        >>> idx = pd.Index([\"Apple\", \"Mango\", \"Watermelon\"])\n        >>> idx.is_object()\n        True\n\n        >>> idx = pd.Index([\"Apple\", \"Mango\", 2.0])\n        >>> idx.is_object()\n        True\n\n        >>> idx = pd.Index([\"Watermelon\", \"Orange\", \"Apple\",\n        ...                 \"Watermelon\"]).astype(\"category\")\n        >>> idx.is_object()\n        False\n\n        >>> idx = pd.Index([1.0, 2.0, 3.0, 4.0])\n        >>> idx.is_object()\n        False\n        \"\"\"\n        return is_object_dtype(self.dtype)\n\n    @final\n    def is_categorical(self) -> bool:\n        \"\"\"\n        Check if the Index holds categorical data.\n\n        Returns\n        -------\n        bool\n            True if the Index is categorical.\n\n        See Also\n        --------\n        CategoricalIndex : Index for categorical data.\n        is_boolean : Check if the Index only consists of booleans.\n        is_integer : Check if the Index only consists of integers.\n        is_floating : Check if the Index is a floating type.\n        is_numeric : Check if the Index only consists of numeric data.\n        is_object : Check if the Index is of the object dtype.\n        is_interval : Check if the Index holds Interval objects.\n        is_mixed : Check if the Index holds data with mixed data types.\n\n        Examples\n        --------\n        >>> idx = pd.Index([\"Watermelon\", \"Orange\", \"Apple\",\n        ...                 \"Watermelon\"]).astype(\"category\")\n        >>> idx.is_categorical()\n        True\n\n        >>> idx = pd.Index([1, 3, 5, 7])\n        >>> idx.is_categorical()\n        False\n\n        >>> s = pd.Series([\"Peter\", \"Victor\", \"Elisabeth\", \"Mar\"])\n        >>> s\n        0        Peter\n        1       Victor\n        2    Elisabeth\n        3          Mar\n        dtype: object\n        >>> s.index.is_categorical()\n        False\n        \"\"\"\n        return self.inferred_type in [\"categorical\"]\n\n    @final\n    def is_interval(self) -> bool:\n        \"\"\"\n        Check if the Index holds Interval objects.\n\n        Returns\n        -------\n        bool\n            Whether or not the Index holds Interval objects.\n\n        See Also\n        --------\n        IntervalIndex : Index for Interval objects.\n        is_boolean : Check if the Index only consists of booleans.\n        is_integer : Check if the Index only consists of integers.\n        is_floating : Check if the Index is a floating type.\n        is_numeric : Check if the Index only consists of numeric data.\n        is_object : Check if the Index is of the object dtype.\n        is_categorical : Check if the Index holds categorical data.\n        is_mixed : Check if the Index holds data with mixed data types.\n\n        Examples\n        --------\n        >>> idx = pd.Index([pd.Interval(left=0, right=5),\n        ...                 pd.Interval(left=5, right=10)])\n        >>> idx.is_interval()\n        True\n\n        >>> idx = pd.Index([1, 3, 5, 7])\n        >>> idx.is_interval()\n        False\n        \"\"\"\n        return self.inferred_type in [\"interval\"]\n\n    @final\n    def is_mixed(self) -> bool:\n        \"\"\"\n        Check if the Index holds data with mixed data types.\n\n        Returns\n        -------\n        bool\n            Whether or not the Index holds data with mixed data types.\n\n        See Also\n        --------\n        is_boolean : Check if the Index only consists of booleans.\n        is_integer : Check if the Index only consists of integers.\n        is_floating : Check if the Index is a floating type.\n        is_numeric : Check if the Index only consists of numeric data.\n        is_object : Check if the Index is of the object dtype.\n        is_categorical : Check if the Index holds categorical data.\n        is_interval : Check if the Index holds Interval objects.\n\n        Examples\n        --------\n        >>> idx = pd.Index(['a', np.nan, 'b'])\n        >>> idx.is_mixed()\n        True\n\n        >>> idx = pd.Index([1.0, 2.0, 3.0, 5.0])\n        >>> idx.is_mixed()\n        False\n        \"\"\"\n        warnings.warn(\n            \"Index.is_mixed is deprecated and will be removed in a future version. \"\n            \"Check index.inferred_type directly instead.\",\n            FutureWarning,\n            stacklevel=find_stack_level(),\n        )\n        return self.inferred_type in [\"mixed\"]\n\n    @final\n    def holds_integer(self) -> bool:\n        \"\"\"\n        Whether the type is an integer type.\n        \"\"\"\n        return self.inferred_type in [\"integer\", \"mixed-integer\"]\n\n    @cache_readonly\n    def inferred_type(self) -> str_t:\n        \"\"\"\n        Return a string of the type inferred from the values.\n        \"\"\"\n        return lib.infer_dtype(self._values, skipna=False)\n\n    @cache_readonly\n    def _is_all_dates(self) -> bool:\n        \"\"\"\n        Whether or not the index values only consist of dates.\n        \"\"\"\n        return is_datetime_array(ensure_object(self._values))\n\n    @cache_readonly\n    @final\n    def is_all_dates(self) -> bool:\n        \"\"\"\n        Whether or not the index values only consist of dates.\n        \"\"\"\n        warnings.warn(\n            \"Index.is_all_dates is deprecated, will be removed in a future version. \"\n            \"check index.inferred_type instead.\",\n            FutureWarning,\n            stacklevel=find_stack_level(),\n        )\n        return self._is_all_dates\n\n    @final\n    @cache_readonly\n    def _is_multi(self) -> bool:\n        \"\"\"\n        Cached check equivalent to isinstance(self, MultiIndex)\n        \"\"\"\n        return isinstance(self, ABCMultiIndex)\n\n    # --------------------------------------------------------------------\n    # Pickle Methods\n\n    def __reduce__(self):\n        d = {\"data\": self._data, \"name\": self.name}\n        return _new_Index, (type(self), d), None\n\n    # --------------------------------------------------------------------\n    # Null Handling Methods\n\n    @cache_readonly\n    def _na_value(self):\n        \"\"\"The expected NA value to use with this index.\"\"\"\n        dtype = self.dtype\n        if isinstance(dtype, np.dtype):\n            if dtype.kind in [\"m\", \"M\"]:\n                return NaT\n            return np.nan\n        return dtype.na_value\n\n    @cache_readonly\n    def _isnan(self) -> npt.NDArray[np.bool_]:\n        \"\"\"\n        Return if each value is NaN.\n        \"\"\"\n        if self._can_hold_na:\n            return isna(self)\n        else:\n            # shouldn't reach to this condition by checking hasnans beforehand\n            values = np.empty(len(self), dtype=np.bool_)\n            values.fill(False)\n            return values\n\n    @cache_readonly\n    def hasnans(self) -> bool:\n        \"\"\"\n        Return True if there are any NaNs.\n\n        Enables various performance speedups.\n        \"\"\"\n        if self._can_hold_na:\n            return bool(self._isnan.any())\n        else:\n            return False\n\n    @final\n    def isna(self) -> npt.NDArray[np.bool_]:\n        \"\"\"\n        Detect missing values.\n\n        Return a boolean same-sized object indicating if the values are NA.\n        NA values, such as ``None``, :attr:`numpy.NaN` or :attr:`pd.NaT`, get\n        mapped to ``True`` values.\n        Everything else get mapped to ``False`` values. Characters such as\n        empty strings `''` or :attr:`numpy.inf` are not considered NA values\n        (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n\n        Returns\n        -------\n        numpy.ndarray[bool]\n            A boolean array of whether my values are NA.\n\n        See Also\n        --------\n        Index.notna : Boolean inverse of isna.\n        Index.dropna : Omit entries with missing values.\n        isna : Top-level isna.\n        Series.isna : Detect missing values in Series object.\n\n        Examples\n        --------\n        Show which entries in a pandas.Index are NA. The result is an\n        array.\n\n        >>> idx = pd.Index([5.2, 6.0, np.NaN])\n        >>> idx\n        Float64Index([5.2, 6.0, nan], dtype='float64')\n        >>> idx.isna()\n        array([False, False,  True])\n\n        Empty strings are not considered NA values. None is considered an NA\n        value.\n\n        >>> idx = pd.Index(['black', '', 'red', None])\n        >>> idx\n        Index(['black', '', 'red', None], dtype='object')\n        >>> idx.isna()\n        array([False, False, False,  True])\n\n        For datetimes, `NaT` (Not a Time) is considered as an NA value.\n\n        >>> idx = pd.DatetimeIndex([pd.Timestamp('1940-04-25'),\n        ...                         pd.Timestamp(''), None, pd.NaT])\n        >>> idx\n        DatetimeIndex(['1940-04-25', 'NaT', 'NaT', 'NaT'],\n                      dtype='datetime64[ns]', freq=None)\n        >>> idx.isna()\n        array([False,  True,  True,  True])\n        \"\"\"\n        return self._isnan\n\n    isnull = isna\n\n    @final\n    def notna(self) -> npt.NDArray[np.bool_]:\n        \"\"\"\n        Detect existing (non-missing) values.\n\n        Return a boolean same-sized object indicating if the values are not NA.\n        Non-missing values get mapped to ``True``. Characters such as empty\n        strings ``''`` or :attr:`numpy.inf` are not considered NA values\n        (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n        NA values, such as None or :attr:`numpy.NaN`, get mapped to ``False``\n        values.\n\n        Returns\n        -------\n        numpy.ndarray[bool]\n            Boolean array to indicate which entries are not NA.\n\n        See Also\n        --------\n        Index.notnull : Alias of notna.\n        Index.isna: Inverse of notna.\n        notna : Top-level notna.\n\n        Examples\n        --------\n        Show which entries in an Index are not NA. The result is an\n        array.\n\n        >>> idx = pd.Index([5.2, 6.0, np.NaN])\n        >>> idx\n        Float64Index([5.2, 6.0, nan], dtype='float64')\n        >>> idx.notna()\n        array([ True,  True, False])\n\n        Empty strings are not considered NA values. None is considered a NA\n        value.\n\n        >>> idx = pd.Index(['black', '', 'red', None])\n        >>> idx\n        Index(['black', '', 'red', None], dtype='object')\n        >>> idx.notna()\n        array([ True,  True,  True, False])\n        \"\"\"\n        return ~self.isna()\n\n    notnull = notna\n\n    def fillna(self, value=None, downcast=None):\n        \"\"\"\n        Fill NA/NaN values with the specified value.\n\n        Parameters\n        ----------\n        value : scalar\n            Scalar value to use to fill holes (e.g. 0).\n            This value cannot be a list-likes.\n        downcast : dict, default is None\n            A dict of item->dtype of what to downcast if possible,\n            or the string 'infer' which will try to downcast to an appropriate\n            equal type (e.g. float64 to int64 if possible).\n\n        Returns\n        -------\n        Index\n\n        See Also\n        --------\n        DataFrame.fillna : Fill NaN values of a DataFrame.\n        Series.fillna : Fill NaN Values of a Series.\n        \"\"\"\n\n        value = self._require_scalar(value)\n        if self.hasnans:\n            result = self.putmask(self._isnan, value)\n            if downcast is None:\n                # no need to care metadata other than name\n                # because it can't have freq if it has NaTs\n                return Index._with_infer(result, name=self.name)\n            raise NotImplementedError(\n                f\"{type(self).__name__}.fillna does not support 'downcast' \"\n                \"argument values other than 'None'.\"\n            )\n        return self._view()\n\n    def dropna(self: _IndexT, how: str_t = \"any\") -> _IndexT:\n        \"\"\"\n        Return Index without NA/NaN values.\n\n        Parameters\n        ----------\n        how : {'any', 'all'}, default 'any'\n            If the Index is a MultiIndex, drop the value when any or all levels\n            are NaN.\n\n        Returns\n        -------\n        Index\n        \"\"\"\n        if how not in (\"any\", \"all\"):\n            raise ValueError(f\"invalid how option: {how}\")\n\n        if self.hasnans:\n            res_values = self._values[~self._isnan]\n            return type(self)._simple_new(res_values, name=self.name)\n        return self._view()\n\n    # --------------------------------------------------------------------\n    # Uniqueness Methods\n\n    def unique(self: _IndexT, level: Hashable | None = None) -> _IndexT:\n        \"\"\"\n        Return unique values in the index.\n\n        Unique values are returned in order of appearance, this does NOT sort.\n\n        Parameters\n        ----------\n        level : int or hashable, optional\n            Only return values from specified level (for MultiIndex).\n            If int, gets the level by integer position, else by level name.\n\n        Returns\n        -------\n        Index\n\n        See Also\n        --------\n        unique : Numpy array of unique values in that column.\n        Series.unique : Return unique values of Series object.\n        \"\"\"\n        if level is not None:\n            self._validate_index_level(level)\n\n        if self.is_unique:\n            return self._view()\n\n        result = super().unique()\n        return self._shallow_copy(result)\n\n    @deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\"])\n    def drop_duplicates(self: _IndexT, keep: str_t | bool = \"first\") -> _IndexT:\n        \"\"\"\n        Return Index with duplicate values removed.\n\n        Parameters\n        ----------\n        keep : {'first', 'last', ``False``}, default 'first'\n            - 'first' : Drop duplicates except for the first occurrence.\n            - 'last' : Drop duplicates except for the last occurrence.\n            - ``False`` : Drop all duplicates.\n\n        Returns\n        -------\n        deduplicated : Index\n\n        See Also\n        --------\n        Series.drop_duplicates : Equivalent method on Series.\n        DataFrame.drop_duplicates : Equivalent method on DataFrame.\n        Index.duplicated : Related method on Index, indicating duplicate\n            Index values.\n\n        Examples\n        --------\n        Generate an pandas.Index with duplicate values.\n\n        >>> idx = pd.Index(['lama', 'cow', 'lama', 'beetle', 'lama', 'hippo'])\n\n        The `keep` parameter controls  which duplicate values are removed.\n        The value 'first' keeps the first occurrence for each\n        set of duplicated entries. The default value of keep is 'first'.\n\n        >>> idx.drop_duplicates(keep='first')\n        Index(['lama', 'cow', 'beetle', 'hippo'], dtype='object')\n\n        The value 'last' keeps the last occurrence for each set of duplicated\n        entries.\n\n        >>> idx.drop_duplicates(keep='last')\n        Index(['cow', 'beetle', 'lama', 'hippo'], dtype='object')\n\n        The value ``False`` discards all sets of duplicated entries.\n\n        >>> idx.drop_duplicates(keep=False)\n        Index(['cow', 'beetle', 'hippo'], dtype='object')\n        \"\"\"\n        if self.is_unique:\n            return self._view()\n\n        return super().drop_duplicates(keep=keep)\n\n    def duplicated(\n        self, keep: Literal[\"first\", \"last\", False] = \"first\"\n    ) -> npt.NDArray[np.bool_]:\n        \"\"\"\n        Indicate duplicate index values.\n\n        Duplicated values are indicated as ``True`` values in the resulting\n        array. Either all duplicates, all except the first, or all except the\n        last occurrence of duplicates can be indicated.\n\n        Parameters\n        ----------\n        keep : {'first', 'last', False}, default 'first'\n            The value or values in a set of duplicates to mark as missing.\n\n            - 'first' : Mark duplicates as ``True`` except for the first\n              occurrence.\n            - 'last' : Mark duplicates as ``True`` except for the last\n              occurrence.\n            - ``False`` : Mark all duplicates as ``True``.\n\n        Returns\n        -------\n        np.ndarray[bool]\n\n        See Also\n        --------\n        Series.duplicated : Equivalent method on pandas.Series.\n        DataFrame.duplicated : Equivalent method on pandas.DataFrame.\n        Index.drop_duplicates : Remove duplicate values from Index.\n\n        Examples\n        --------\n        By default, for each set of duplicated values, the first occurrence is\n        set to False and all others to True:\n\n        >>> idx = pd.Index(['lama', 'cow', 'lama', 'beetle', 'lama'])\n        >>> idx.duplicated()\n        array([False, False,  True, False,  True])\n\n        which is equivalent to\n\n        >>> idx.duplicated(keep='first')\n        array([False, False,  True, False,  True])\n\n        By using 'last', the last occurrence of each set of duplicated values\n        is set on False and all others on True:\n\n        >>> idx.duplicated(keep='last')\n        array([ True, False,  True, False, False])\n\n        By setting keep on ``False``, all duplicates are True:\n\n        >>> idx.duplicated(keep=False)\n        array([ True, False,  True, False,  True])\n        \"\"\"\n        if self.is_unique:\n            # fastpath available bc we are immutable\n            return np.zeros(len(self), dtype=bool)\n        return self._duplicated(keep=keep)\n\n    # --------------------------------------------------------------------\n    # Arithmetic & Logical Methods\n\n    def __iadd__(self, other):\n        # alias for __add__\n        return self + other\n\n    @final\n    def __and__(self, other):\n        warnings.warn(\n            \"Index.__and__ operating as a set operation is deprecated, \"\n            \"in the future this will be a logical operation matching \"\n            \"Series.__and__.  Use index.intersection(other) instead.\",\n            FutureWarning,\n            stacklevel=find_stack_level(),\n        )\n        return self.intersection(other)\n\n    @final\n    def __or__(self, other):\n        warnings.warn(\n            \"Index.__or__ operating as a set operation is deprecated, \"\n            \"in the future this will be a logical operation matching \"\n            \"Series.__or__.  Use index.union(other) instead.\",\n            FutureWarning,\n            stacklevel=find_stack_level(),\n        )\n        return self.union(other)\n\n    @final\n    def __xor__(self, other):\n        warnings.warn(\n            \"Index.__xor__ operating as a set operation is deprecated, \"\n            \"in the future this will be a logical operation matching \"\n            \"Series.__xor__.  Use index.symmetric_difference(other) instead.\",\n            FutureWarning,\n            stacklevel=find_stack_level(),\n        )\n        return self.symmetric_difference(other)\n\n    @final\n    def __nonzero__(self):\n        raise ValueError(\n            f\"The truth value of a {type(self).__name__} is ambiguous. \"\n            \"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\n        )\n\n    __bool__ = __nonzero__\n\n    # --------------------------------------------------------------------\n    # Set Operation Methods\n\n    def _get_reconciled_name_object(self, other):\n        \"\"\"\n        If the result of a set operation will be self,\n        return self, unless the name changes, in which\n        case make a shallow copy of self.\n        \"\"\"\n        name = get_op_result_name(self, other)\n        if self.name is not name:\n            return self.rename(name)\n        return self\n\n    @final\n    def _validate_sort_keyword(self, sort):\n        if sort not in [None, False]:\n            raise ValueError(\n                \"The 'sort' keyword only takes the values of \"\n                f\"None or False; {sort} was passed.\"\n            )\n\n    @final\n    def union(self, other, sort=None):\n        \"\"\"\n        Form the union of two Index objects.\n\n        If the Index objects are incompatible, both Index objects will be\n        cast to dtype('object') first.\n\n            .. versionchanged:: 0.25.0\n\n        Parameters\n        ----------\n        other : Index or array-like\n        sort : bool or None, default None\n            Whether to sort the resulting Index.\n\n            * None : Sort the result, except when\n\n              1. `self` and `other` are equal.\n              2. `self` or `other` has length 0.\n              3. Some values in `self` or `other` cannot be compared.\n                 A RuntimeWarning is issued in this case.\n\n            * False : do not sort the result.\n\n        Returns\n        -------\n        union : Index\n\n        Examples\n        --------\n        Union matching dtypes\n\n        >>> idx1 = pd.Index([1, 2, 3, 4])\n        >>> idx2 = pd.Index([3, 4, 5, 6])\n        >>> idx1.union(idx2)\n        Int64Index([1, 2, 3, 4, 5, 6], dtype='int64')\n\n        Union mismatched dtypes\n\n        >>> idx1 = pd.Index(['a', 'b', 'c', 'd'])\n        >>> idx2 = pd.Index([1, 2, 3, 4])\n        >>> idx1.union(idx2)\n        Index(['a', 'b', 'c', 'd', 1, 2, 3, 4], dtype='object')\n\n        MultiIndex case\n\n        >>> idx1 = pd.MultiIndex.from_arrays(\n        ...     [[1, 1, 2, 2], [\"Red\", \"Blue\", \"Red\", \"Blue\"]]\n        ... )\n        >>> idx1\n        MultiIndex([(1,  'Red'),\n            (1, 'Blue'),\n            (2,  'Red'),\n            (2, 'Blue')],\n           )\n        >>> idx2 = pd.MultiIndex.from_arrays(\n        ...     [[3, 3, 2, 2], [\"Red\", \"Green\", \"Red\", \"Green\"]]\n        ... )\n        >>> idx2\n        MultiIndex([(3,   'Red'),\n            (3, 'Green'),\n            (2,   'Red'),\n            (2, 'Green')],\n           )\n        >>> idx1.union(idx2)\n        MultiIndex([(1,  'Blue'),\n            (1,   'Red'),\n            (2,  'Blue'),\n            (2, 'Green'),\n            (2,   'Red'),\n            (3, 'Green'),\n            (3,   'Red')],\n           )\n        >>> idx1.union(idx2, sort=False)\n        MultiIndex([(1,   'Red'),\n            (1,  'Blue'),\n            (2,   'Red'),\n            (2,  'Blue'),\n            (3,   'Red'),\n            (3, 'Green'),\n            (2, 'Green')],\n           )\n        \"\"\"\n        self._validate_sort_keyword(sort)\n        self._assert_can_do_setop(other)\n        other, result_name = self._convert_can_do_setop(other)\n\n        if not is_dtype_equal(self.dtype, other.dtype):\n            if (\n                isinstance(self, ABCMultiIndex)\n                and not is_object_dtype(unpack_nested_dtype(other))\n                and len(other) > 0\n            ):\n                raise NotImplementedError(\n                    \"Can only union MultiIndex with MultiIndex or Index of tuples, \"\n                    \"try mi.to_flat_index().union(other) instead.\"\n                )\n            if (\n                isinstance(self, ABCDatetimeIndex)\n                and isinstance(other, ABCDatetimeIndex)\n                and self.tz is not None\n                and other.tz is not None\n            ):\n                # GH#39328\n                warnings.warn(\n                    \"In a future version, the union of DatetimeIndex objects \"\n                    \"with mismatched timezones will cast both to UTC instead of \"\n                    \"object dtype. To retain the old behavior, \"\n                    \"use `index.astype(object).union(other)`\",\n                    FutureWarning,\n                    stacklevel=find_stack_level(),\n                )\n\n            dtype = self._find_common_type_compat(other)\n            left = self.astype(dtype, copy=False)\n            right = other.astype(dtype, copy=False)\n            return left.union(right, sort=sort)\n\n        elif not len(other) or self.equals(other):\n            # NB: whether this (and the `if not len(self)` check below) come before\n            #  or after the is_dtype_equal check above affects the returned dtype\n            return self._get_reconciled_name_object(other)\n\n        elif not len(self):\n            return other._get_reconciled_name_object(self)\n\n        result = self._union(other, sort=sort)\n\n        return self._wrap_setop_result(other, result)\n\n    def _union(self, other: Index, sort):\n        \"\"\"\n        Specific union logic should go here. In subclasses, union behavior\n        should be overwritten here rather than in `self.union`.\n\n        Parameters\n        ----------\n        other : Index or array-like\n        sort : False or None, default False\n            Whether to sort the resulting index.\n\n            * False : do not sort the result.\n            * None : sort the result, except when `self` and `other` are equal\n              or when the values cannot be compared.\n\n        Returns\n        -------\n        Index\n        \"\"\"\n        lvals = self._values\n        rvals = other._values\n\n        if (\n            sort is None\n            and self.is_monotonic\n            and other.is_monotonic\n            and not (self.has_duplicates and other.has_duplicates)\n            and self._can_use_libjoin\n        ):\n            # Both are monotonic and at least one is unique, so can use outer join\n            #  (actually don't need either unique, but without this restriction\n            #  test_union_same_value_duplicated_in_both fails)\n            try:\n                return self._outer_indexer(other)[0]\n            except (TypeError, IncompatibleFrequency):\n                # incomparable objects; should only be for object dtype\n                value_list = list(lvals)\n\n                # worth making this faster? a very unusual case\n                value_set = set(lvals)\n                value_list.extend([x for x in rvals if x not in value_set])\n                # If objects are unorderable, we must have object dtype.\n                return np.array(value_list, dtype=object)\n\n        elif not other.is_unique:\n            # other has duplicates\n            result = algos.union_with_duplicates(lvals, rvals)\n            return _maybe_try_sort(result, sort)\n\n        # Self may have duplicates; other already checked as unique\n        # find indexes of things in \"other\" that are not in \"self\"\n        if self._index_as_unique:\n            indexer = self.get_indexer(other)\n            missing = (indexer == -1).nonzero()[0]\n        else:\n            missing = algos.unique1d(self.get_indexer_non_unique(other)[1])\n\n        if len(missing) > 0:\n            other_diff = rvals.take(missing)\n            result = concat_compat((lvals, other_diff))\n        else:\n            result = lvals\n\n        if not self.is_monotonic or not other.is_monotonic:\n            # if both are monotonic then result should already be sorted\n            result = _maybe_try_sort(result, sort)\n\n        return result\n\n    @final\n    def _wrap_setop_result(self, other: Index, result) -> Index:\n        name = get_op_result_name(self, other)\n        if isinstance(result, Index):\n            if result.name != name:\n                result = result.rename(name)\n        else:\n            result = self._shallow_copy(result, name=name)\n\n        if type(self) is Index and self.dtype != _dtype_obj:\n            # i.e. ExtensionArray-backed\n            # TODO(ExtensionIndex): revert this astype; it is a kludge to make\n            #  it possible to split ExtensionEngine from ExtensionIndex PR.\n            return result.astype(self.dtype, copy=False)\n        return result\n\n    @final\n    def intersection(self, other, sort=False):\n        \"\"\"\n        Form the intersection of two Index objects.\n\n        This returns a new Index with elements common to the index and `other`.\n\n        Parameters\n        ----------\n        other : Index or array-like\n        sort : False or None, default False\n            Whether to sort the resulting index.\n\n            * False : do not sort the result.\n            * None : sort the result, except when `self` and `other` are equal\n              or when the values cannot be compared.\n\n        Returns\n        -------\n        intersection : Index\n\n        Examples\n        --------\n        >>> idx1 = pd.Index([1, 2, 3, 4])\n        >>> idx2 = pd.Index([3, 4, 5, 6])\n        >>> idx1.intersection(idx2)\n        Int64Index([3, 4], dtype='int64')\n        \"\"\"\n        self._validate_sort_keyword(sort)\n        self._assert_can_do_setop(other)\n        other, result_name = self._convert_can_do_setop(other)\n\n        if self.equals(other):\n            if self.has_duplicates:\n                return self.unique()._get_reconciled_name_object(other)\n            return self._get_reconciled_name_object(other)\n\n        if len(self) == 0 or len(other) == 0:\n            # fastpath; we need to be careful about having commutativity\n\n            if self._is_multi or other._is_multi:\n                # _convert_can_do_setop ensures that we have both or neither\n                # We retain self.levels\n                return self[:0].rename(result_name)\n\n            dtype = self._find_common_type_compat(other)\n            if is_dtype_equal(self.dtype, dtype):\n                # Slicing allows us to retain DTI/TDI.freq, RangeIndex\n\n                # Note: self[:0] vs other[:0] affects\n                #  1) which index's `freq` we get in DTI/TDI cases\n                #     This may be a historical artifact, i.e. no documented\n                #     reason for this choice.\n                #  2) The `step` we get in RangeIndex cases\n                if len(self) == 0:\n                    return self[:0].rename(result_name)\n                else:\n                    return other[:0].rename(result_name)\n\n            return Index([], dtype=dtype, name=result_name)\n\n        elif not self._should_compare(other):\n            # We can infer that the intersection is empty.\n            if isinstance(self, ABCMultiIndex):\n                return self[:0].rename(result_name)\n            return Index([], name=result_name)\n\n        elif not is_dtype_equal(self.dtype, other.dtype):\n            dtype = self._find_common_type_compat(other)\n            this = self.astype(dtype, copy=False)\n            other = other.astype(dtype, copy=False)\n            return this.intersection(other, sort=sort)\n\n        result = self._intersection(other, sort=sort)\n        return self._wrap_intersection_result(other, result)\n\n    def _intersection(self, other: Index, sort=False):\n        \"\"\"\n        intersection specialized to the case with matching dtypes.\n        \"\"\"\n        if self.is_monotonic and other.is_monotonic and self._can_use_libjoin:\n            try:\n                result = self._inner_indexer(other)[0]\n            except TypeError:\n                # non-comparable; should only be for object dtype\n                pass\n            else:\n                # TODO: algos.unique1d should preserve DTA/TDA\n                res = algos.unique1d(result)\n                return ensure_wrapped_if_datetimelike(res)\n\n        res_values = self._intersection_via_get_indexer(other, sort=sort)\n        res_values = _maybe_try_sort(res_values, sort)\n        return res_values\n\n    def _wrap_intersection_result(self, other, result):\n        # We will override for MultiIndex to handle empty results\n        return self._wrap_setop_result(other, result)\n\n    @final\n    def _intersection_via_get_indexer(self, other: Index, sort) -> ArrayLike:\n        \"\"\"\n        Find the intersection of two Indexes using get_indexer.\n\n        Returns\n        -------\n        np.ndarray or ExtensionArray\n            The returned array will be unique.\n        \"\"\"\n        left_unique = self.unique()\n        right_unique = other.unique()\n\n        # even though we are unique, we need get_indexer_for for IntervalIndex\n        indexer = left_unique.get_indexer_for(right_unique)\n\n        mask = indexer != -1\n\n        taker = indexer.take(mask.nonzero()[0])\n        if sort is False:\n            # sort bc we want the elements in the same order they are in self\n            # unnecessary in the case with sort=None bc we will sort later\n            taker = np.sort(taker)\n\n        result = left_unique.take(taker)._values\n        return result\n\n    @final\n    def difference(self, other, sort=None):\n        \"\"\"\n        Return a new Index with elements of index not in `other`.\n\n        This is the set difference of two Index objects.\n\n        Parameters\n        ----------\n        other : Index or array-like\n        sort : False or None, default None\n            Whether to sort the resulting index. By default, the\n            values are attempted to be sorted, but any TypeError from\n            incomparable elements is caught by pandas.\n\n            * None : Attempt to sort the result, but catch any TypeErrors\n              from comparing incomparable elements.\n            * False : Do not sort the result.\n\n        Returns\n        -------\n        difference : Index\n\n        Examples\n        --------\n        >>> idx1 = pd.Index([2, 1, 3, 4])\n        >>> idx2 = pd.Index([3, 4, 5, 6])\n        >>> idx1.difference(idx2)\n        Int64Index([1, 2], dtype='int64')\n        >>> idx1.difference(idx2, sort=False)\n        Int64Index([2, 1], dtype='int64')\n        \"\"\"\n        self._validate_sort_keyword(sort)\n        self._assert_can_do_setop(other)\n        other, result_name = self._convert_can_do_setop(other)\n\n        if self.equals(other):\n            # Note: we do not (yet) sort even if sort=None GH#24959\n            return self[:0].rename(result_name)\n\n        if len(other) == 0:\n            # Note: we do not (yet) sort even if sort=None GH#24959\n            return self.rename(result_name)\n\n        if not self._should_compare(other):\n            # Nothing matches -> difference is everything\n            return self.rename(result_name)\n\n        result = self._difference(other, sort=sort)\n        return self._wrap_difference_result(other, result)\n\n    def _difference(self, other, sort):\n        # overridden by RangeIndex\n\n        this = self.unique()\n\n        indexer = this.get_indexer_for(other)\n        indexer = indexer.take((indexer != -1).nonzero()[0])\n\n        label_diff = np.setdiff1d(np.arange(this.size), indexer, assume_unique=True)\n        the_diff = this._values.take(label_diff)\n        the_diff = _maybe_try_sort(the_diff, sort)\n\n        return the_diff\n\n    def _wrap_difference_result(self, other, result):\n        # We will override for MultiIndex to handle empty results\n        return self._wrap_setop_result(other, result)\n\n    def symmetric_difference(self, other, result_name=None, sort=None):\n        \"\"\"\n        Compute the symmetric difference of two Index objects.\n\n        Parameters\n        ----------\n        other : Index or array-like\n        result_name : str\n        sort : False or None, default None\n            Whether to sort the resulting index. By default, the\n            values are attempted to be sorted, but any TypeError from\n            incomparable elements is caught by pandas.\n\n            * None : Attempt to sort the result, but catch any TypeErrors\n              from comparing incomparable elements.\n            * False : Do not sort the result.\n\n        Returns\n        -------\n        symmetric_difference : Index\n\n        Notes\n        -----\n        ``symmetric_difference`` contains elements that appear in either\n        ``idx1`` or ``idx2`` but not both. Equivalent to the Index created by\n        ``idx1.difference(idx2) | idx2.difference(idx1)`` with duplicates\n        dropped.\n\n        Examples\n        --------\n        >>> idx1 = pd.Index([1, 2, 3, 4])\n        >>> idx2 = pd.Index([2, 3, 4, 5])\n        >>> idx1.symmetric_difference(idx2)\n        Int64Index([1, 5], dtype='int64')\n        \"\"\"\n        self._validate_sort_keyword(sort)\n        self._assert_can_do_setop(other)\n        other, result_name_update = self._convert_can_do_setop(other)\n        if result_name is None:\n            result_name = result_name_update\n\n        if not self._should_compare(other):\n            return self.union(other, sort=sort).rename(result_name)\n\n        elif not is_dtype_equal(self.dtype, other.dtype):\n            dtype = self._find_common_type_compat(other)\n            this = self.astype(dtype, copy=False)\n            that = other.astype(dtype, copy=False)\n            return this.symmetric_difference(that, sort=sort).rename(result_name)\n\n        this = self.unique()\n        other = other.unique()\n        indexer = this.get_indexer_for(other)\n\n        # {this} minus {other}\n        common_indexer = indexer.take((indexer != -1).nonzero()[0])\n        left_indexer = np.setdiff1d(\n            np.arange(this.size), common_indexer, assume_unique=True\n        )\n        left_diff = this._values.take(left_indexer)\n\n        # {other} minus {this}\n        right_indexer = (indexer == -1).nonzero()[0]\n        right_diff = other._values.take(right_indexer)\n\n        res_values = concat_compat([left_diff, right_diff])\n        res_values = _maybe_try_sort(res_values, sort)\n\n        # pass dtype so we retain object dtype\n        result = Index(res_values, name=result_name, dtype=res_values.dtype)\n\n        if self._is_multi:\n            self = cast(\"MultiIndex\", self)\n            if len(result) == 0:\n                # On equal symmetric_difference MultiIndexes the difference is empty.\n                # Therefore, an empty MultiIndex is returned GH#13490\n                return type(self)(\n                    levels=[[] for _ in range(self.nlevels)],\n                    codes=[[] for _ in range(self.nlevels)],\n                    names=result.name,\n                )\n            return type(self).from_tuples(result, names=result.name)\n\n        return result\n\n    @final\n    def _assert_can_do_setop(self, other) -> bool:\n        if not is_list_like(other):\n            raise TypeError(\"Input must be Index or array-like\")\n        return True\n\n    def _convert_can_do_setop(self, other) -> tuple[Index, Hashable]:\n        if not isinstance(other, Index):\n            # TODO(2.0): no need to special-case here once _with_infer\n            #  deprecation is enforced\n            if hasattr(other, \"dtype\"):\n                other = Index(other, name=self.name, dtype=other.dtype)\n            else:\n                # e.g. list\n                other = Index(other, name=self.name)\n            result_name = self.name\n        else:\n            result_name = get_op_result_name(self, other)\n        return other, result_name\n\n    # --------------------------------------------------------------------\n    # Indexing Methods\n\n    def get_loc(self, key, method=None, tolerance=None):\n        \"\"\"\n        Get integer location, slice or boolean mask for requested label.\n\n        Parameters\n        ----------\n        key : label\n        method : {None, 'pad'/'ffill', 'backfill'/'bfill', 'nearest'}, optional\n            * default: exact matches only.\n            * pad / ffill: find the PREVIOUS index value if no exact match.\n            * backfill / bfill: use NEXT index value if no exact match\n            * nearest: use the NEAREST index value if no exact match. Tied\n              distances are broken by preferring the larger index value.\n        tolerance : int or float, optional\n            Maximum distance from index value for inexact matches. The value of\n            the index at the matching location must satisfy the equation\n            ``abs(index[loc] - key) <= tolerance``.\n\n        Returns\n        -------\n        loc : int if unique index, slice if monotonic index, else mask\n\n        Examples\n        --------\n        >>> unique_index = pd.Index(list('abc'))\n        >>> unique_index.get_loc('b')\n        1\n\n        >>> monotonic_index = pd.Index(list('abbc'))\n        >>> monotonic_index.get_loc('b')\n        slice(1, 3, None)\n\n        >>> non_monotonic_index = pd.Index(list('abcb'))\n        >>> non_monotonic_index.get_loc('b')\n        array([False,  True, False,  True])\n        \"\"\"\n        if method is None:\n            if tolerance is not None:\n                raise ValueError(\n                    \"tolerance argument only valid if using pad, \"\n                    \"backfill or nearest lookups\"\n                )\n            casted_key = self._maybe_cast_indexer(key)\n            try:\n                return self._engine.get_loc(casted_key)\n            except KeyError as err:\n                raise KeyError(key) from err\n            except TypeError:\n                # If we have a listlike key, _check_indexing_error will raise\n                #  InvalidIndexError. Otherwise we fall through and re-raise\n                #  the TypeError.\n                self._check_indexing_error(key)\n                raise\n\n        # GH#42269\n        warnings.warn(\n            f\"Passing method to {type(self).__name__}.get_loc is deprecated \"\n            \"and will raise in a future version. Use \"\n            \"index.get_indexer([item], method=...) instead.\",\n            FutureWarning,\n            stacklevel=find_stack_level(),\n        )\n\n        if is_scalar(key) and isna(key) and not self.hasnans:\n            raise KeyError(key)\n\n        if tolerance is not None:\n            tolerance = self._convert_tolerance(tolerance, np.asarray(key))\n\n        indexer = self.get_indexer([key], method=method, tolerance=tolerance)\n        if indexer.ndim > 1 or indexer.size > 1:\n            raise TypeError(\"get_loc requires scalar valued input\")\n        loc = indexer.item()\n        if loc == -1:\n            raise KeyError(key)\n        return loc\n\n    _index_shared_docs[\n        \"get_indexer\"\n    ] = \"\"\"\n        Compute indexer and mask for new index given the current index. The\n        indexer should be then used as an input to ndarray.take to align the\n        current data to the new index.\n\n        Parameters\n        ----------\n        target : %(target_klass)s\n        method : {None, 'pad'/'ffill', 'backfill'/'bfill', 'nearest'}, optional\n            * default: exact matches only.\n            * pad / ffill: find the PREVIOUS index value if no exact match.\n            * backfill / bfill: use NEXT index value if no exact match\n            * nearest: use the NEAREST index value if no exact match. Tied\n              distances are broken by preferring the larger index value.\n        limit : int, optional\n            Maximum number of consecutive labels in ``target`` to match for\n            inexact matches.\n        tolerance : optional\n            Maximum distance between original and new labels for inexact\n            matches. The values of the index at the matching locations must\n            satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n\n            Tolerance may be a scalar value, which applies the same tolerance\n            to all values, or list-like, which applies variable tolerance per\n            element. List-like includes list, tuple, array, Series, and must be\n            the same size as the index and its dtype must exactly match the\n            index's type.\n\n        Returns\n        -------\n        indexer : np.ndarray[np.intp]\n            Integers from 0 to n - 1 indicating that the index at these\n            positions matches the corresponding target values. Missing values\n            in the target are marked by -1.\n        %(raises_section)s\n        Notes\n        -----\n        Returns -1 for unmatched values, for further explanation see the\n        example below.\n\n        Examples\n        --------\n        >>> index = pd.Index(['c', 'a', 'b'])\n        >>> index.get_indexer(['a', 'b', 'x'])\n        array([ 1,  2, -1])\n\n        Notice that the return value is an array of locations in ``index``\n        and ``x`` is marked by -1, as it is not in ``index``.\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"get_indexer\"] % _index_doc_kwargs)\n    @final\n    def get_indexer(\n        self,\n        target,\n        method: str_t | None = None,\n        limit: int | None = None,\n        tolerance=None,\n    ) -> npt.NDArray[np.intp]:\n        method = missing.clean_reindex_fill_method(method)\n        target = self._maybe_cast_listlike_indexer(target)\n\n        self._check_indexing_method(method, limit, tolerance)\n\n        if not self._index_as_unique:\n            raise InvalidIndexError(self._requires_unique_msg)\n\n        if len(target) == 0:\n            return np.array([], dtype=np.intp)\n\n        if not self._should_compare(target) and not self._should_partial_index(target):\n            # IntervalIndex get special treatment bc numeric scalars can be\n            #  matched to Interval scalars\n            return self._get_indexer_non_comparable(target, method=method, unique=True)\n\n        if is_categorical_dtype(self.dtype):\n            # _maybe_cast_listlike_indexer ensures target has our dtype\n            #  (could improve perf by doing _should_compare check earlier?)\n            assert is_dtype_equal(self.dtype, target.dtype)\n\n            indexer = self._engine.get_indexer(target.codes)\n            if self.hasnans and target.hasnans:\n                loc = self.get_loc(np.nan)\n                mask = target.isna()\n                indexer[mask] = loc\n            return indexer\n\n        if is_categorical_dtype(target.dtype):\n            # potential fastpath\n            # get an indexer for unique categories then propagate to codes via take_nd\n            # get_indexer instead of _get_indexer needed for MultiIndex cases\n            #  e.g. test_append_different_columns_types\n            categories_indexer = self.get_indexer(target.categories)\n\n            indexer = algos.take_nd(categories_indexer, target.codes, fill_value=-1)\n\n            if (not self._is_multi and self.hasnans) and target.hasnans:\n                # Exclude MultiIndex because hasnans raises NotImplementedError\n                # we should only get here if we are unique, so loc is an integer\n                # GH#41934\n                loc = self.get_loc(np.nan)\n                mask = target.isna()\n                indexer[mask] = loc\n\n            return ensure_platform_int(indexer)\n\n        pself, ptarget = self._maybe_promote(target)\n        if pself is not self or ptarget is not target:\n            return pself.get_indexer(\n                ptarget, method=method, limit=limit, tolerance=tolerance\n            )\n\n        if is_dtype_equal(self.dtype, target.dtype) and self.equals(target):\n            # Only call equals if we have same dtype to avoid inference/casting\n            return np.arange(len(target), dtype=np.intp)\n\n        if not is_dtype_equal(self.dtype, target.dtype) and not is_interval_dtype(\n            self.dtype\n        ):\n            # IntervalIndex gets special treatment for partial-indexing\n            dtype = self._find_common_type_compat(target)\n\n            this = self.astype(dtype, copy=False)\n            target = target.astype(dtype, copy=False)\n            return this._get_indexer(\n                target, method=method, limit=limit, tolerance=tolerance\n            )\n\n        return self._get_indexer(target, method, limit, tolerance)\n\n    def _get_indexer(\n        self,\n        target: Index,\n        method: str_t | None = None,\n        limit: int | None = None,\n        tolerance=None,\n    ) -> npt.NDArray[np.intp]:\n        if tolerance is not None:\n            tolerance = self._convert_tolerance(tolerance, target)\n\n        if method in [\"pad\", \"backfill\"]:\n            indexer = self._get_fill_indexer(target, method, limit, tolerance)\n        elif method == \"nearest\":\n            indexer = self._get_nearest_indexer(target, limit, tolerance)\n        else:\n            tgt_values = target._get_engine_target()\n            if target._is_multi and self._is_multi:\n                engine = self._engine\n                # error: \"IndexEngine\" has no attribute \"_extract_level_codes\"\n                tgt_values = engine._extract_level_codes(  # type: ignore[attr-defined]\n                    target\n                )\n\n            indexer = self._engine.get_indexer(tgt_values)\n\n        return ensure_platform_int(indexer)\n\n    @final\n    def _should_partial_index(self, target: Index) -> bool:\n        \"\"\"\n        Should we attempt partial-matching indexing?\n        \"\"\"\n        if is_interval_dtype(self.dtype):\n            # \"Index\" has no attribute \"left\"\n            return self.left._should_compare(target)  # type: ignore[attr-defined]\n        return False\n\n    @final\n    def _check_indexing_method(\n        self,\n        method: str_t | None,\n        limit: int | None = None,\n        tolerance=None,\n    ) -> None:\n        \"\"\"\n        Raise if we have a get_indexer `method` that is not supported or valid.\n        \"\"\"\n        if method not in [None, \"bfill\", \"backfill\", \"pad\", \"ffill\", \"nearest\"]:\n            # in practice the clean_reindex_fill_method call would raise\n            #  before we get here\n            raise ValueError(\"Invalid fill method\")  # pragma: no cover\n\n        if self._is_multi:\n            if method == \"nearest\":\n                raise NotImplementedError(\n                    \"method='nearest' not implemented yet \"\n                    \"for MultiIndex; see GitHub issue 9365\"\n                )\n            elif method == \"pad\" or method == \"backfill\":\n                if tolerance is not None:\n                    raise NotImplementedError(\n                        \"tolerance not implemented yet for MultiIndex\"\n                    )\n\n        if is_interval_dtype(self.dtype) or is_categorical_dtype(self.dtype):\n            # GH#37871 for now this is only for IntervalIndex and CategoricalIndex\n            if method is not None:\n                raise NotImplementedError(\n                    f\"method {method} not yet implemented for {type(self).__name__}\"\n                )\n\n        if method is None:\n            if tolerance is not None:\n                raise ValueError(\n                    \"tolerance argument only valid if doing pad, \"\n                    \"backfill or nearest reindexing\"\n                )\n            if limit is not None:\n                raise ValueError(\n                    \"limit argument only valid if doing pad, \"\n                    \"backfill or nearest reindexing\"\n                )\n\n    def _convert_tolerance(self, tolerance, target: np.ndarray | Index) -> np.ndarray:\n        # override this method on subclasses\n        tolerance = np.asarray(tolerance)\n        if target.size != tolerance.size and tolerance.size > 1:\n            raise ValueError(\"list-like tolerance size must match target index size\")\n        return tolerance\n\n    @final\n    def _get_fill_indexer(\n        self, target: Index, method: str_t, limit: int | None = None, tolerance=None\n    ) -> npt.NDArray[np.intp]:\n\n        if self._is_multi:\n            # TODO: get_indexer_with_fill docstring says values must be _sorted_\n            #  but that doesn't appear to be enforced\n            # error: \"IndexEngine\" has no attribute \"get_indexer_with_fill\"\n            return self._engine.get_indexer_with_fill(  # type: ignore[attr-defined]\n                target=target._values, values=self._values, method=method, limit=limit\n            )\n\n        if self.is_monotonic_increasing and target.is_monotonic_increasing:\n            target_values = target._get_engine_target()\n            own_values = self._get_engine_target()\n\n            if method == \"pad\":\n                indexer = libalgos.pad(own_values, target_values, limit=limit)\n            else:\n                # i.e. \"backfill\"\n                indexer = libalgos.backfill(own_values, target_values, limit=limit)\n        else:\n            indexer = self._get_fill_indexer_searchsorted(target, method, limit)\n        if tolerance is not None and len(self):\n            indexer = self._filter_indexer_tolerance(target, indexer, tolerance)\n        return indexer\n\n    @final\n    def _get_fill_indexer_searchsorted(\n        self, target: Index, method: str_t, limit: int | None = None\n    ) -> npt.NDArray[np.intp]:\n        \"\"\"\n        Fallback pad/backfill get_indexer that works for monotonic decreasing\n        indexes and non-monotonic targets.\n        \"\"\"\n        if limit is not None:\n            raise ValueError(\n                f\"limit argument for {repr(method)} method only well-defined \"\n                \"if index and target are monotonic\"\n            )\n\n        side: Literal[\"left\", \"right\"] = \"left\" if method == \"pad\" else \"right\"\n\n        # find exact matches first (this simplifies the algorithm)\n        indexer = self.get_indexer(target)\n        nonexact = indexer == -1\n        indexer[nonexact] = self._searchsorted_monotonic(target[nonexact], side)\n        if side == \"left\":\n            # searchsorted returns \"indices into a sorted array such that,\n            # if the corresponding elements in v were inserted before the\n            # indices, the order of a would be preserved\".\n            # Thus, we need to subtract 1 to find values to the left.\n            indexer[nonexact] -= 1\n            # This also mapped not found values (values of 0 from\n            # np.searchsorted) to -1, which conveniently is also our\n            # sentinel for missing values\n        else:\n            # Mark indices to the right of the largest value as not found\n            indexer[indexer == len(self)] = -1\n        return indexer\n\n    @final\n    def _get_nearest_indexer(\n        self, target: Index, limit: int | None, tolerance\n    ) -> npt.NDArray[np.intp]:\n        \"\"\"\n        Get the indexer for the nearest index labels; requires an index with\n        values that can be subtracted from each other (e.g., not strings or\n        tuples).\n        \"\"\"\n        if not len(self):\n            return self._get_fill_indexer(target, \"pad\")\n\n        left_indexer = self.get_indexer(target, \"pad\", limit=limit)\n        right_indexer = self.get_indexer(target, \"backfill\", limit=limit)\n\n        left_distances = self._difference_compat(target, left_indexer)\n        right_distances = self._difference_compat(target, right_indexer)\n\n        op = operator.lt if self.is_monotonic_increasing else operator.le\n        indexer = np.where(\n            op(left_distances, right_distances) | (right_indexer == -1),\n            left_indexer,\n            right_indexer,\n        )\n        if tolerance is not None:\n            indexer = self._filter_indexer_tolerance(target, indexer, tolerance)\n        return indexer\n\n    @final\n    def _filter_indexer_tolerance(\n        self,\n        target: Index,\n        indexer: npt.NDArray[np.intp],\n        tolerance,\n    ) -> npt.NDArray[np.intp]:\n\n        distance = self._difference_compat(target, indexer)\n\n        return np.where(distance <= tolerance, indexer, -1)\n\n    @final\n    def _difference_compat(\n        self, target: Index, indexer: npt.NDArray[np.intp]\n    ) -> ArrayLike:\n        # Compatibility for PeriodArray, for which __sub__ returns an ndarray[object]\n        #  of DateOffset objects, which do not support __abs__ (and would be slow\n        #  if they did)\n\n        if isinstance(self.dtype, PeriodDtype):\n            # Note: we only get here with matching dtypes\n            own_values = cast(\"PeriodArray\", self._data)._ndarray\n            target_values = cast(\"PeriodArray\", target._data)._ndarray\n            diff = own_values[indexer] - target_values\n        else:\n            # error: Unsupported left operand type for - (\"ExtensionArray\")\n            diff = self._values[indexer] - target._values  # type: ignore[operator]\n        return abs(diff)\n\n    # --------------------------------------------------------------------\n    # Indexer Conversion Methods\n\n    @final\n    def _validate_positional_slice(self, key: slice) -> None:\n        \"\"\"\n        For positional indexing, a slice must have either int or None\n        for each of start, stop, and step.\n        \"\"\"\n        self._validate_indexer(\"positional\", key.start, \"iloc\")\n        self._validate_indexer(\"positional\", key.stop, \"iloc\")\n        self._validate_indexer(\"positional\", key.step, \"iloc\")\n\n    def _convert_slice_indexer(self, key: slice, kind: str_t):\n        \"\"\"\n        Convert a slice indexer.\n\n        By definition, these are labels unless 'iloc' is passed in.\n        Floats are not allowed as the start, step, or stop of the slice.\n\n        Parameters\n        ----------\n        key : label of the slice bound\n        kind : {'loc', 'getitem'}\n        \"\"\"\n        assert kind in [\"loc\", \"getitem\"], kind\n\n        # potentially cast the bounds to integers\n        start, stop, step = key.start, key.stop, key.step\n\n        # figure out if this is a positional indexer\n        def is_int(v):\n            return v is None or is_integer(v)\n\n        is_index_slice = is_int(start) and is_int(stop) and is_int(step)\n        is_positional = is_index_slice and not (\n            self.is_integer() or self.is_categorical()\n        )\n\n        if kind == \"getitem\":\n            \"\"\"\n            called from the getitem slicers, validate that we are in fact\n            integers\n            \"\"\"\n            if self.is_integer() or is_index_slice:\n                self._validate_indexer(\"slice\", key.start, \"getitem\")\n                self._validate_indexer(\"slice\", key.stop, \"getitem\")\n                self._validate_indexer(\"slice\", key.step, \"getitem\")\n                return key\n\n        # convert the slice to an indexer here\n\n        # if we are mixed and have integers\n        if is_positional:\n            try:\n                # Validate start & stop\n                if start is not None:\n                    self.get_loc(start)\n                if stop is not None:\n                    self.get_loc(stop)\n                is_positional = False\n            except KeyError:\n                pass\n\n        if com.is_null_slice(key):\n            # It doesn't matter if we are positional or label based\n            indexer = key\n        elif is_positional:\n            if kind == \"loc\":\n                # GH#16121, GH#24612, GH#31810\n                warnings.warn(\n                    \"Slicing a positional slice with .loc is not supported, \"\n                    \"and will raise TypeError in a future version.  \"\n                    \"Use .loc with labels or .iloc with positions instead.\",\n                    FutureWarning,\n                    stacklevel=find_stack_level(),\n                )\n            indexer = key\n        else:\n            indexer = self.slice_indexer(start, stop, step)\n\n        return indexer\n\n    @final\n    def _invalid_indexer(self, form: str_t, key) -> TypeError:\n        \"\"\"\n        Consistent invalid indexer message.\n        \"\"\"\n        return TypeError(\n            f\"cannot do {form} indexing on {type(self).__name__} with these \"\n            f\"indexers [{key}] of type {type(key).__name__}\"\n        )\n\n    # --------------------------------------------------------------------\n    # Reindex Methods\n\n    @final\n    def _validate_can_reindex(self, indexer: np.ndarray) -> None:\n        \"\"\"\n        Check if we are allowing reindexing with this particular indexer.\n\n        Parameters\n        ----------\n        indexer : an integer ndarray\n\n        Raises\n        ------\n        ValueError if its a duplicate axis\n        \"\"\"\n        # trying to reindex on an axis with duplicates\n        if not self._index_as_unique and len(indexer):\n            raise ValueError(\"cannot reindex on an axis with duplicate labels\")\n\n    def reindex(\n        self, target, method=None, level=None, limit=None, tolerance=None\n    ) -> tuple[Index, npt.NDArray[np.intp] | None]:\n        \"\"\"\n        Create index with target's values.\n\n        Parameters\n        ----------\n        target : an iterable\n        method : {None, 'pad'/'ffill', 'backfill'/'bfill', 'nearest'}, optional\n            * default: exact matches only.\n            * pad / ffill: find the PREVIOUS index value if no exact match.\n            * backfill / bfill: use NEXT index value if no exact match\n            * nearest: use the NEAREST index value if no exact match. Tied\n              distances are broken by preferring the larger index value.\n        level : int, optional\n            Level of multiindex.\n        limit : int, optional\n            Maximum number of consecutive labels in ``target`` to match for\n            inexact matches.\n        tolerance : int or float, optional\n            Maximum distance between original and new labels for inexact\n            matches. The values of the index at the matching locations must\n            satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n\n            Tolerance may be a scalar value, which applies the same tolerance\n            to all values, or list-like, which applies variable tolerance per\n            element. List-like includes list, tuple, array, Series, and must be\n            the same size as the index and its dtype must exactly match the\n            index's type.\n\n        Returns\n        -------\n        new_index : pd.Index\n            Resulting index.\n        indexer : np.ndarray[np.intp] or None\n            Indices of output values in original index.\n\n        Raises\n        ------\n        TypeError\n            If ``method`` passed along with ``level``.\n        ValueError\n            If non-unique multi-index\n        ValueError\n            If non-unique index and ``method`` or ``limit`` passed.\n\n        See Also\n        --------\n        Series.reindex\n        DataFrame.reindex\n\n        Examples\n        --------\n        >>> idx = pd.Index(['car', 'bike', 'train', 'tractor'])\n        >>> idx\n        Index(['car', 'bike', 'train', 'tractor'], dtype='object')\n        >>> idx.reindex(['car', 'bike'])\n        (Index(['car', 'bike'], dtype='object'), array([0, 1]))\n        \"\"\"\n        # GH6552: preserve names when reindexing to non-named target\n        # (i.e. neither Index nor Series).\n        preserve_names = not hasattr(target, \"name\")\n\n        # GH7774: preserve dtype/tz if target is empty and not an Index.\n        target = ensure_has_len(target)  # target may be an iterator\n\n        if not isinstance(target, Index) and len(target) == 0:\n            if level is not None and self._is_multi:\n                # \"Index\" has no attribute \"levels\"; maybe \"nlevels\"?\n                idx = self.levels[level]  # type: ignore[attr-defined]\n            else:\n                idx = self\n            target = idx[:0]\n        else:\n            target = ensure_index(target)\n\n        if level is not None:\n            if method is not None:\n                raise TypeError(\"Fill method not supported if level passed\")\n\n            # TODO: tests where passing `keep_order=not self._is_multi`\n            #  makes a difference for non-MultiIndex case\n            target, indexer, _ = self._join_level(\n                target, level, how=\"right\", keep_order=not self._is_multi\n            )\n\n        else:\n            if self.equals(target):\n                indexer = None\n            else:\n                if self._index_as_unique:\n                    indexer = self.get_indexer(\n                        target, method=method, limit=limit, tolerance=tolerance\n                    )\n                elif self._is_multi:\n                    raise ValueError(\"cannot handle a non-unique multi-index!\")\n                else:\n                    if method is not None or limit is not None:\n                        raise ValueError(\n                            \"cannot reindex a non-unique index \"\n                            \"with a method or limit\"\n                        )\n                    indexer, _ = self.get_indexer_non_unique(target)\n\n                if not self.is_unique:\n                    # GH#42568\n                    warnings.warn(\n                        \"reindexing with a non-unique Index is deprecated and \"\n                        \"will raise in a future version.\",\n                        FutureWarning,\n                        stacklevel=find_stack_level(),\n                    )\n\n        target = self._wrap_reindex_result(target, indexer, preserve_names)\n        return target, indexer\n\n    def _wrap_reindex_result(self, target, indexer, preserve_names: bool):\n        target = self._maybe_preserve_names(target, preserve_names)\n        return target\n\n    def _maybe_preserve_names(self, target: Index, preserve_names: bool):\n        if preserve_names and target.nlevels == 1 and target.name != self.name:\n            target = target.copy(deep=False)\n            target.name = self.name\n        return target\n\n    @final\n    def _reindex_non_unique(\n        self, target: Index\n    ) -> tuple[Index, npt.NDArray[np.intp], npt.NDArray[np.intp] | None]:\n        \"\"\"\n        Create a new index with target's values (move/add/delete values as\n        necessary) use with non-unique Index and a possibly non-unique target.\n\n        Parameters\n        ----------\n        target : an iterable\n\n        Returns\n        -------\n        new_index : pd.Index\n            Resulting index.\n        indexer : np.ndarray[np.intp]\n            Indices of output values in original index.\n        new_indexer : np.ndarray[np.intp] or None\n\n        \"\"\"\n        target = ensure_index(target)\n        if len(target) == 0:\n            # GH#13691\n            return self[:0], np.array([], dtype=np.intp), None\n\n        indexer, missing = self.get_indexer_non_unique(target)\n        check = indexer != -1\n        new_labels = self.take(indexer[check])\n        new_indexer = None\n\n        if len(missing):\n            length = np.arange(len(indexer), dtype=np.intp)\n\n            missing = ensure_platform_int(missing)\n            missing_labels = target.take(missing)\n            missing_indexer = length[~check]\n            cur_labels = self.take(indexer[check]).values\n            cur_indexer = length[check]\n\n            # Index constructor below will do inference\n            new_labels = np.empty((len(indexer),), dtype=object)\n            new_labels[cur_indexer] = cur_labels\n            new_labels[missing_indexer] = missing_labels\n\n            # GH#38906\n            if not len(self):\n\n                new_indexer = np.arange(0, dtype=np.intp)\n\n            # a unique indexer\n            elif target.is_unique:\n\n                # see GH5553, make sure we use the right indexer\n                new_indexer = np.arange(len(indexer), dtype=np.intp)\n                new_indexer[cur_indexer] = np.arange(len(cur_labels))\n                new_indexer[missing_indexer] = -1\n\n            # we have a non_unique selector, need to use the original\n            # indexer here\n            else:\n\n                # need to retake to have the same size as the indexer\n                indexer[~check] = -1\n\n                # reset the new indexer to account for the new size\n                new_indexer = np.arange(len(self.take(indexer)), dtype=np.intp)\n                new_indexer[~check] = -1\n\n        if isinstance(self, ABCMultiIndex):\n            new_index = type(self).from_tuples(new_labels, names=self.names)\n        else:\n            new_index = Index._with_infer(new_labels, name=self.name)\n        return new_index, indexer, new_indexer\n\n    # --------------------------------------------------------------------\n    # Join Methods\n\n    @final\n    @_maybe_return_indexers\n    def join(\n        self,\n        other,\n        how: str_t = \"left\",\n        level=None,\n        return_indexers: bool = False,\n        sort: bool = False,\n    ):\n        \"\"\"\n        Compute join_index and indexers to conform data\n        structures to the new index.\n\n        Parameters\n        ----------\n        other : Index\n        how : {'left', 'right', 'inner', 'outer'}\n        level : int or level name, default None\n        return_indexers : bool, default False\n        sort : bool, default False\n            Sort the join keys lexicographically in the result Index. If False,\n            the order of the join keys depends on the join type (how keyword).\n\n        Returns\n        -------\n        join_index, (left_indexer, right_indexer)\n        \"\"\"\n        other = ensure_index(other)\n\n        if isinstance(self, ABCDatetimeIndex) and isinstance(other, ABCDatetimeIndex):\n            if (self.tz is None) ^ (other.tz is None):\n                # Raise instead of casting to object below.\n                raise TypeError(\"Cannot join tz-naive with tz-aware DatetimeIndex\")\n\n        if not self._is_multi and not other._is_multi:\n            # We have specific handling for MultiIndex below\n            pself, pother = self._maybe_promote(other)\n            if pself is not self or pother is not other:\n                return pself.join(\n                    pother, how=how, level=level, return_indexers=True, sort=sort\n                )\n\n        lindexer: np.ndarray | None\n        rindexer: np.ndarray | None\n\n        # try to figure out the join level\n        # GH3662\n        if level is None and (self._is_multi or other._is_multi):\n\n            # have the same levels/names so a simple join\n            if self.names == other.names:\n                pass\n            else:\n                return self._join_multi(other, how=how)\n\n        # join on the level\n        if level is not None and (self._is_multi or other._is_multi):\n            return self._join_level(other, level, how=how)\n\n        if len(other) == 0 and how in (\"left\", \"outer\"):\n            join_index = self._view()\n            rindexer = np.repeat(np.intp(-1), len(join_index))\n            return join_index, None, rindexer\n\n        if len(self) == 0 and how in (\"right\", \"outer\"):\n            join_index = other._view()\n            lindexer = np.repeat(np.intp(-1), len(join_index))\n            return join_index, lindexer, None\n\n        if self._join_precedence < other._join_precedence:\n            how = {\"right\": \"left\", \"left\": \"right\"}.get(how, how)\n            join_index, lidx, ridx = other.join(\n                self, how=how, level=level, return_indexers=True\n            )\n            lidx, ridx = ridx, lidx\n            return join_index, lidx, ridx\n\n        if not is_dtype_equal(self.dtype, other.dtype):\n            dtype = self._find_common_type_compat(other)\n            this = self.astype(dtype, copy=False)\n            other = other.astype(dtype, copy=False)\n            return this.join(other, how=how, return_indexers=True)\n\n        _validate_join_method(how)\n\n        if not self.is_unique and not other.is_unique:\n            return self._join_non_unique(other, how=how)\n        elif not self.is_unique or not other.is_unique:\n            if self.is_monotonic and other.is_monotonic:\n                if self._can_use_libjoin:\n                    # otherwise we will fall through to _join_via_get_indexer\n                    return self._join_monotonic(other, how=how)\n            else:\n                return self._join_non_unique(other, how=how)\n        elif (\n            self.is_monotonic\n            and other.is_monotonic\n            and self._can_use_libjoin\n            and (\n                not isinstance(self, ABCMultiIndex)\n                or not any(is_categorical_dtype(dtype) for dtype in self.dtypes)\n            )\n        ):\n            # Categorical is monotonic if data are ordered as categories, but join can\n            #  not handle this in case of not lexicographically monotonic GH#38502\n            try:\n                return self._join_monotonic(other, how=how)\n            except TypeError:\n                # object dtype; non-comparable objects\n                pass\n\n        return self._join_via_get_indexer(other, how, sort)\n\n    @final\n    def _join_via_get_indexer(\n        self, other: Index, how: str_t, sort: bool\n    ) -> tuple[Index, npt.NDArray[np.intp] | None, npt.NDArray[np.intp] | None]:\n        # Fallback if we do not have any fastpaths available based on\n        #  uniqueness/monotonicity\n\n        # Note: at this point we have checked matching dtypes\n\n        if how == \"left\":\n            join_index = self\n        elif how == \"right\":\n            join_index = other\n        elif how == \"inner\":\n            # TODO: sort=False here for backwards compat. It may\n            # be better to use the sort parameter passed into join\n            join_index = self.intersection(other, sort=False)\n        elif how == \"outer\":\n            # TODO: sort=True here for backwards compat. It may\n            # be better to use the sort parameter passed into join\n            join_index = self.union(other)\n\n        if sort:\n            join_index = join_index.sort_values()\n\n        if join_index is self:\n            lindexer = None\n        else:\n            lindexer = self.get_indexer_for(join_index)\n        if join_index is other:\n            rindexer = None\n        else:\n            rindexer = other.get_indexer_for(join_index)\n        return join_index, lindexer, rindexer\n\n    @final\n    def _join_multi(self, other: Index, how: str_t):\n        from pandas.core.indexes.multi import MultiIndex\n        from pandas.core.reshape.merge import restore_dropped_levels_multijoin\n\n        # figure out join names\n        self_names_list = list(com.not_none(*self.names))\n        other_names_list = list(com.not_none(*other.names))\n        self_names_order = self_names_list.index\n        other_names_order = other_names_list.index\n        self_names = set(self_names_list)\n        other_names = set(other_names_list)\n        overlap = self_names & other_names\n\n        # need at least 1 in common\n        if not overlap:\n            raise ValueError(\"cannot join with no overlapping index names\")\n\n        if isinstance(self, MultiIndex) and isinstance(other, MultiIndex):\n\n            # Drop the non-matching levels from left and right respectively\n            ldrop_names = sorted(self_names - overlap, key=self_names_order)\n            rdrop_names = sorted(other_names - overlap, key=other_names_order)\n\n            # if only the order differs\n            if not len(ldrop_names + rdrop_names):\n                self_jnlevels = self\n                other_jnlevels = other.reorder_levels(self.names)\n            else:\n                self_jnlevels = self.droplevel(ldrop_names)\n                other_jnlevels = other.droplevel(rdrop_names)\n\n            # Join left and right\n            # Join on same leveled multi-index frames is supported\n            join_idx, lidx, ridx = self_jnlevels.join(\n                other_jnlevels, how, return_indexers=True\n            )\n\n            # Restore the dropped levels\n            # Returned index level order is\n            # common levels, ldrop_names, rdrop_names\n            dropped_names = ldrop_names + rdrop_names\n\n            levels, codes, names = restore_dropped_levels_multijoin(\n                self, other, dropped_names, join_idx, lidx, ridx\n            )\n\n            # Re-create the multi-index\n            multi_join_idx = MultiIndex(\n                levels=levels, codes=codes, names=names, verify_integrity=False\n            )\n\n            multi_join_idx = multi_join_idx.remove_unused_levels()\n\n            return multi_join_idx, lidx, ridx\n\n        jl = list(overlap)[0]\n\n        # Case where only one index is multi\n        # make the indices into mi's that match\n        flip_order = False\n        if isinstance(self, MultiIndex):\n            self, other = other, self\n            flip_order = True\n            # flip if join method is right or left\n            how = {\"right\": \"left\", \"left\": \"right\"}.get(how, how)\n\n        level = other.names.index(jl)\n        result = self._join_level(other, level, how=how)\n\n        if flip_order:\n            return result[0], result[2], result[1]\n        return result\n\n    @final\n    def _join_non_unique(\n        self, other: Index, how: str_t = \"left\"\n    ) -> tuple[Index, npt.NDArray[np.intp], npt.NDArray[np.intp]]:\n        from pandas.core.reshape.merge import get_join_indexers\n\n        # We only get here if dtypes match\n        assert self.dtype == other.dtype\n\n        left_idx, right_idx = get_join_indexers(\n            [self._values], [other._values], how=how, sort=True\n        )\n        mask = left_idx == -1\n\n        join_array = self._values.take(left_idx)\n        right = other._values.take(right_idx)\n\n        if isinstance(join_array, np.ndarray):\n            # Argument 3 to \"putmask\" has incompatible type \"Union[ExtensionArray,\n            # ndarray[Any, Any]]\"; expected \"Union[_SupportsArray[dtype[Any]],\n            # _NestedSequence[_SupportsArray[dtype[Any]]], bool, int, f\n            # loat, complex, str, bytes, _NestedSequence[Union[bool, int, float,\n            # complex, str, bytes]]]\"  [arg-type]\n            np.putmask(join_array, mask, right)  # type: ignore[arg-type]\n        else:\n            join_array._putmask(mask, right)\n\n        join_index = self._wrap_joined_index(join_array, other)\n\n        return join_index, left_idx, right_idx\n\n    @final\n    def _join_level(\n        self, other: Index, level, how: str_t = \"left\", keep_order: bool = True\n    ) -> tuple[MultiIndex, npt.NDArray[np.intp] | None, npt.NDArray[np.intp] | None]:\n        \"\"\"\n        The join method *only* affects the level of the resulting\n        MultiIndex. Otherwise it just exactly aligns the Index data to the\n        labels of the level in the MultiIndex.\n\n        If ```keep_order == True```, the order of the data indexed by the\n        MultiIndex will not be changed; otherwise, it will tie out\n        with `other`.\n        \"\"\"\n        from pandas.core.indexes.multi import MultiIndex\n\n        def _get_leaf_sorter(labels: list[np.ndarray]) -> npt.NDArray[np.intp]:\n            \"\"\"\n            Returns sorter for the inner most level while preserving the\n            order of higher levels.\n\n            Parameters\n            ----------\n            labels : list[np.ndarray]\n                Each ndarray has signed integer dtype, not necessarily identical.\n\n            Returns\n            -------\n            np.ndarray[np.intp]\n            \"\"\"\n            if labels[0].size == 0:\n                return np.empty(0, dtype=np.intp)\n\n            if len(labels) == 1:\n                return get_group_index_sorter(ensure_platform_int(labels[0]))\n\n            # find indexers of beginning of each set of\n            # same-key labels w.r.t all but last level\n            tic = labels[0][:-1] != labels[0][1:]\n            for lab in labels[1:-1]:\n                tic |= lab[:-1] != lab[1:]\n\n            starts = np.hstack(([True], tic, [True])).nonzero()[0]\n            lab = ensure_int64(labels[-1])\n            return lib.get_level_sorter(lab, ensure_platform_int(starts))\n\n        if isinstance(self, MultiIndex) and isinstance(other, MultiIndex):\n            raise TypeError(\"Join on level between two MultiIndex objects is ambiguous\")\n\n        left, right = self, other\n\n        flip_order = not isinstance(self, MultiIndex)\n        if flip_order:\n            left, right = right, left\n            how = {\"right\": \"left\", \"left\": \"right\"}.get(how, how)\n\n        assert isinstance(left, MultiIndex)\n\n        level = left._get_level_number(level)\n        old_level = left.levels[level]\n\n        if not right.is_unique:\n            raise NotImplementedError(\n                \"Index._join_level on non-unique index is not implemented\"\n            )\n\n        new_level, left_lev_indexer, right_lev_indexer = old_level.join(\n            right, how=how, return_indexers=True\n        )\n\n        if left_lev_indexer is None:\n            if keep_order or len(left) == 0:\n                left_indexer = None\n                join_index = left\n            else:  # sort the leaves\n                left_indexer = _get_leaf_sorter(left.codes[: level + 1])\n                join_index = left[left_indexer]\n\n        else:\n            left_lev_indexer = ensure_platform_int(left_lev_indexer)\n            rev_indexer = lib.get_reverse_indexer(left_lev_indexer, len(old_level))\n            old_codes = left.codes[level]\n\n            taker = old_codes[old_codes != -1]\n            new_lev_codes = rev_indexer.take(taker)\n\n            new_codes = list(left.codes)\n            new_codes[level] = new_lev_codes\n\n            new_levels = list(left.levels)\n            new_levels[level] = new_level\n\n            if keep_order:  # just drop missing values. o.w. keep order\n                left_indexer = np.arange(len(left), dtype=np.intp)\n                left_indexer = cast(np.ndarray, left_indexer)\n                mask = new_lev_codes != -1\n                if not mask.all():\n                    new_codes = [lab[mask] for lab in new_codes]\n                    left_indexer = left_indexer[mask]\n\n            else:  # tie out the order with other\n                if level == 0:  # outer most level, take the fast route\n                    max_new_lev = 0 if len(new_lev_codes) == 0 else new_lev_codes.max()\n                    ngroups = 1 + max_new_lev\n                    left_indexer, counts = libalgos.groupsort_indexer(\n                        new_lev_codes, ngroups\n                    )\n\n                    # missing values are placed first; drop them!\n                    left_indexer = left_indexer[counts[0] :]\n                    new_codes = [lab[left_indexer] for lab in new_codes]\n\n                else:  # sort the leaves\n                    mask = new_lev_codes != -1\n                    mask_all = mask.all()\n                    if not mask_all:\n                        new_codes = [lab[mask] for lab in new_codes]\n\n                    left_indexer = _get_leaf_sorter(new_codes[: level + 1])\n                    new_codes = [lab[left_indexer] for lab in new_codes]\n\n                    # left_indexers are w.r.t masked frame.\n                    # reverse to original frame!\n                    if not mask_all:\n                        left_indexer = mask.nonzero()[0][left_indexer]\n\n            join_index = MultiIndex(\n                levels=new_levels,\n                codes=new_codes,\n                names=left.names,\n                verify_integrity=False,\n            )\n\n        if right_lev_indexer is not None:\n            right_indexer = right_lev_indexer.take(join_index.codes[level])\n        else:\n            right_indexer = join_index.codes[level]\n\n        if flip_order:\n            left_indexer, right_indexer = right_indexer, left_indexer\n\n        left_indexer = (\n            None if left_indexer is None else ensure_platform_int(left_indexer)\n        )\n        right_indexer = (\n            None if right_indexer is None else ensure_platform_int(right_indexer)\n        )\n        return join_index, left_indexer, right_indexer\n\n    @final\n    def _join_monotonic(\n        self, other: Index, how: str_t = \"left\"\n    ) -> tuple[Index, npt.NDArray[np.intp] | None, npt.NDArray[np.intp] | None]:\n        # We only get here with matching dtypes and both monotonic increasing\n        assert other.dtype == self.dtype\n\n        if self.equals(other):\n            ret_index = other if how == \"right\" else self\n            return ret_index, None, None\n\n        ridx: np.ndarray | None\n        lidx: np.ndarray | None\n\n        if self.is_unique and other.is_unique:\n            # We can perform much better than the general case\n            if how == \"left\":\n                join_index = self\n                lidx = None\n                ridx = self._left_indexer_unique(other)\n            elif how == \"right\":\n                join_index = other\n                lidx = other._left_indexer_unique(self)\n                ridx = None\n            elif how == \"inner\":\n                join_array, lidx, ridx = self._inner_indexer(other)\n                join_index = self._wrap_joined_index(join_array, other)\n            elif how == \"outer\":\n                join_array, lidx, ridx = self._outer_indexer(other)\n                join_index = self._wrap_joined_index(join_array, other)\n        else:\n            if how == \"left\":\n                join_array, lidx, ridx = self._left_indexer(other)\n            elif how == \"right\":\n                join_array, ridx, lidx = other._left_indexer(self)\n            elif how == \"inner\":\n                join_array, lidx, ridx = self._inner_indexer(other)\n            elif how == \"outer\":\n                join_array, lidx, ridx = self._outer_indexer(other)\n\n            join_index = self._wrap_joined_index(join_array, other)\n\n        lidx = None if lidx is None else ensure_platform_int(lidx)\n        ridx = None if ridx is None else ensure_platform_int(ridx)\n        return join_index, lidx, ridx\n\n    def _wrap_joined_index(self: _IndexT, joined: ArrayLike, other: _IndexT) -> _IndexT:\n        assert other.dtype == self.dtype\n\n        if isinstance(self, ABCMultiIndex):\n            name = self.names if self.names == other.names else None\n            # error: Incompatible return value type (got \"MultiIndex\",\n            # expected \"_IndexT\")\n            return self._constructor(joined, name=name)  # type: ignore[return-value]\n        else:\n            name = get_op_result_name(self, other)\n            return self._constructor._with_infer(joined, name=name)\n\n    @cache_readonly\n    def _can_use_libjoin(self) -> bool:\n        \"\"\"\n        Whether we can use the fastpaths implement in _libs.join\n        \"\"\"\n        # Note: this will need to be updated when e.g. Nullable dtypes\n        #  are supported in Indexes.\n        return not is_interval_dtype(self.dtype)\n\n    # --------------------------------------------------------------------\n    # Uncategorized Methods\n\n    @property\n    def values(self) -> ArrayLike:\n        \"\"\"\n        Return an array representing the data in the Index.\n\n        .. warning::\n\n           We recommend using :attr:`Index.array` or\n           :meth:`Index.to_numpy`, depending on whether you need\n           a reference to the underlying data or a NumPy array.\n\n        Returns\n        -------\n        array: numpy.ndarray or ExtensionArray\n\n        See Also\n        --------\n        Index.array : Reference to the underlying data.\n        Index.to_numpy : A NumPy array representing the underlying data.\n        \"\"\"\n        return self._data\n\n    # error: Decorated property not supported\n    # https://github.com/python/mypy/issues/1362\n    @cache_readonly  # type: ignore[misc]\n    @doc(IndexOpsMixin.array)\n    def array(self) -> ExtensionArray:\n        array = self._data\n        if isinstance(array, np.ndarray):\n            from pandas.core.arrays.numpy_ import PandasArray\n\n            array = PandasArray(array)\n        return array\n\n    @property\n    def _values(self) -> ExtensionArray | np.ndarray:\n        \"\"\"\n        The best array representation.\n\n        This is an ndarray or ExtensionArray.\n\n        ``_values`` are consistent between ``Series`` and ``Index``.\n\n        It may differ from the public '.values' method.\n\n        index             | values          | _values       |\n        ----------------- | --------------- | ------------- |\n        Index             | ndarray         | ndarray       |\n        CategoricalIndex  | Categorical     | Categorical   |\n        DatetimeIndex     | ndarray[M8ns]   | DatetimeArray |\n        DatetimeIndex[tz] | ndarray[M8ns]   | DatetimeArray |\n        PeriodIndex       | ndarray[object] | PeriodArray   |\n        IntervalIndex     | IntervalArray   | IntervalArray |\n\n        See Also\n        --------\n        values : Values\n        \"\"\"\n        return self._data\n\n    def _get_engine_target(self) -> np.ndarray:\n        \"\"\"\n        Get the ndarray that we can pass to the IndexEngine constructor.\n        \"\"\"\n        # error: Incompatible return value type (got \"Union[ExtensionArray,\n        # ndarray]\", expected \"ndarray\")\n        if type(self) is Index and isinstance(self._values, ExtensionArray):\n            # TODO(ExtensionIndex): remove special-case, just use self._values\n            return self._values.astype(object)\n        return self._values  # type: ignore[return-value]\n\n    def _from_join_target(self, result: np.ndarray) -> ArrayLike:\n        \"\"\"\n        Cast the ndarray returned from one of the libjoin.foo_indexer functions\n        back to type(self)._data.\n        \"\"\"\n        return result\n\n    @doc(IndexOpsMixin._memory_usage)\n    def memory_usage(self, deep: bool = False) -> int:\n        result = self._memory_usage(deep=deep)\n\n        # include our engine hashtable\n        result += self._engine.sizeof(deep=deep)\n        return result\n\n    @final\n    def where(self, cond, other=None) -> Index:\n        \"\"\"\n        Replace values where the condition is False.\n\n        The replacement is taken from other.\n\n        Parameters\n        ----------\n        cond : bool array-like with the same length as self\n            Condition to select the values on.\n        other : scalar, or array-like, default None\n            Replacement if the condition is False.\n\n        Returns\n        -------\n        pandas.Index\n            A copy of self with values replaced from other\n            where the condition is False.\n\n        See Also\n        --------\n        Series.where : Same method for Series.\n        DataFrame.where : Same method for DataFrame.\n\n        Examples\n        --------\n        >>> idx = pd.Index(['car', 'bike', 'train', 'tractor'])\n        >>> idx\n        Index(['car', 'bike', 'train', 'tractor'], dtype='object')\n        >>> idx.where(idx.isin(['car', 'train']), 'other')\n        Index(['car', 'other', 'train', 'other'], dtype='object')\n        \"\"\"\n        if isinstance(self, ABCMultiIndex):\n            raise NotImplementedError(\n                \".where is not supported for MultiIndex operations\"\n            )\n        cond = np.asarray(cond, dtype=bool)\n        return self.putmask(~cond, other)\n\n    # construction helpers\n    @final\n    @classmethod\n    def _scalar_data_error(cls, data):\n        # We return the TypeError so that we can raise it from the constructor\n        #  in order to keep mypy happy\n        return TypeError(\n            f\"{cls.__name__}(...) must be called with a collection of some \"\n            f\"kind, {repr(data)} was passed\"\n        )\n\n    @final\n    @classmethod\n    def _string_data_error(cls, data):\n        raise TypeError(\n            \"String dtype not supported, you may need \"\n            \"to explicitly cast to a numeric type\"\n        )\n\n    def _validate_fill_value(self, value):\n        \"\"\"\n        Check if the value can be inserted into our array without casting,\n        and convert it to an appropriate native type if necessary.\n\n        Raises\n        ------\n        TypeError\n            If the value cannot be inserted into an array of this dtype.\n        \"\"\"\n        if not can_hold_element(self._values, value):\n            raise TypeError\n        return value\n\n    @final\n    def _require_scalar(self, value):\n        \"\"\"\n        Check that this is a scalar value that we can use for setitem-like\n        operations without changing dtype.\n        \"\"\"\n        if not is_scalar(value):\n            raise TypeError(f\"'value' must be a scalar, passed: {type(value).__name__}\")\n        return value\n\n    def _is_memory_usage_qualified(self) -> bool:\n        \"\"\"\n        Return a boolean if we need a qualified .info display.\n        \"\"\"\n        return self.is_object()\n\n    def is_type_compatible(self, kind: str_t) -> bool:\n        \"\"\"\n        Whether the index type is compatible with the provided type.\n        \"\"\"\n        warnings.warn(\n            \"Index.is_type_compatible is deprecated and will be removed in a \"\n            \"future version.\",\n            FutureWarning,\n            stacklevel=find_stack_level(),\n        )\n        return kind == self.inferred_type\n\n    def __contains__(self, key: Any) -> bool:\n        \"\"\"\n        Return a boolean indicating whether the provided key is in the index.\n\n        Parameters\n        ----------\n        key : label\n            The key to check if it is present in the index.\n\n        Returns\n        -------\n        bool\n            Whether the key search is in the index.\n\n        Raises\n        ------\n        TypeError\n            If the key is not hashable.\n\n        See Also\n        --------\n        Index.isin : Returns an ndarray of boolean dtype indicating whether the\n            list-like key is in the index.\n\n        Examples\n        --------\n        >>> idx = pd.Index([1, 2, 3, 4])\n        >>> idx\n        Int64Index([1, 2, 3, 4], dtype='int64')\n\n        >>> 2 in idx\n        True\n        >>> 6 in idx\n        False\n        \"\"\"\n        hash(key)\n        try:\n            return key in self._engine\n        except (OverflowError, TypeError, ValueError):\n            return False\n\n    # https://github.com/python/typeshed/issues/2148#issuecomment-520783318\n    # Incompatible types in assignment (expression has type \"None\", base class\n    # \"object\" defined the type as \"Callable[[object], int]\")\n    __hash__: None  # type: ignore[assignment]\n\n    @final\n    def __setitem__(self, key, value):\n        raise TypeError(\"Index does not support mutable operations\")\n\n    def __getitem__(self, key):\n        \"\"\"\n        Override numpy.ndarray's __getitem__ method to work as desired.\n\n        This function adds lists and Series as valid boolean indexers\n        (ndarrays only supports ndarray with dtype=bool).\n\n        If resulting ndim != 1, plain ndarray is returned instead of\n        corresponding `Index` subclass.\n\n        \"\"\"\n        getitem = self._data.__getitem__\n\n        if is_integer(key) or is_float(key):\n            # GH#44051 exclude bool, which would return a 2d ndarray\n            key = com.cast_scalar_indexer(key, warn_float=True)\n            return getitem(key)\n\n        if isinstance(key, slice):\n            # This case is separated from the conditional above to avoid\n            # pessimization com.is_bool_indexer and ndim checks.\n            result = getitem(key)\n            # Going through simple_new for performance.\n            return type(self)._simple_new(result, name=self._name)\n\n        if com.is_bool_indexer(key):\n            # if we have list[bools, length=1e5] then doing this check+convert\n            #  takes 166 \u00b5s + 2.1 ms and cuts the ndarray.__getitem__\n            #  time below from 3.8 ms to 496 \u00b5s\n            # if we already have ndarray[bool], the overhead is 1.4 \u00b5s or .25%\n            key = np.asarray(key, dtype=bool)\n\n        result = getitem(key)\n        # Because we ruled out integer above, we always get an arraylike here\n        if result.ndim > 1:\n            deprecate_ndim_indexing(result)\n            if hasattr(result, \"_ndarray\"):\n                # error: Item \"ndarray[Any, Any]\" of \"Union[ExtensionArray,\n                # ndarray[Any, Any]]\" has no attribute \"_ndarray\"  [union-attr]\n                # i.e. NDArrayBackedExtensionArray\n                # Unpack to ndarray for MPL compat\n                return result._ndarray  # type: ignore[union-attr]\n            return result\n\n        # NB: Using _constructor._simple_new would break if MultiIndex\n        #  didn't override __getitem__\n        return self._constructor._simple_new(result, name=self._name)\n\n    def _getitem_slice(self: _IndexT, slobj: slice) -> _IndexT:\n        \"\"\"\n        Fastpath for __getitem__ when we know we have a slice.\n        \"\"\"\n        res = self._data[slobj]\n        return type(self)._simple_new(res, name=self._name)\n\n    @final\n    def _can_hold_identifiers_and_holds_name(self, name) -> bool:\n        \"\"\"\n        Faster check for ``name in self`` when we know `name` is a Python\n        identifier (e.g. in NDFrame.__getattr__, which hits this to support\n        . key lookup). For indexes that can't hold identifiers (everything\n        but object & categorical) we just return False.\n\n        https://github.com/pandas-dev/pandas/issues/19764\n        \"\"\"\n        if self.is_object() or self.is_categorical():\n            return name in self\n        return False\n\n    def append(self, other: Index | Sequence[Index]) -> Index:\n        \"\"\"\n        Append a collection of Index options together.\n\n        Parameters\n        ----------\n        other : Index or list/tuple of indices\n\n        Returns\n        -------\n        Index\n        \"\"\"\n        to_concat = [self]\n\n        if isinstance(other, (list, tuple)):\n            to_concat += list(other)\n        else:\n            # error: Argument 1 to \"append\" of \"list\" has incompatible type\n            # \"Union[Index, Sequence[Index]]\"; expected \"Index\"\n            to_concat.append(other)  # type: ignore[arg-type]\n\n        for obj in to_concat:\n            if not isinstance(obj, Index):\n                raise TypeError(\"all inputs must be Index\")\n\n        names = {obj.name for obj in to_concat}\n        name = None if len(names) > 1 else self.name\n\n        return self._concat(to_concat, name)\n\n    def _concat(self, to_concat: list[Index], name: Hashable) -> Index:\n        \"\"\"\n        Concatenate multiple Index objects.\n        \"\"\"\n        to_concat_vals = [x._values for x in to_concat]\n\n        result = concat_compat(to_concat_vals)\n\n        is_numeric = result.dtype.kind in [\"i\", \"u\", \"f\"]\n        if self._is_backward_compat_public_numeric_index and is_numeric:\n            return type(self)._simple_new(result, name=name)\n\n        return Index._with_infer(result, name=name)\n\n    @final\n    def putmask(self, mask, value) -> Index:\n        \"\"\"\n        Return a new Index of the values set with the mask.\n\n        Returns\n        -------\n        Index\n\n        See Also\n        --------\n        numpy.ndarray.putmask : Changes elements of an array\n            based on conditional and input values.\n        \"\"\"\n        mask, noop = validate_putmask(self._values, mask)\n        if noop:\n            return self.copy()\n\n        if value is None and (self._is_numeric_dtype or self.dtype == object):\n            value = self._na_value\n        try:\n            converted = self._validate_fill_value(value)\n        except (ValueError, TypeError) as err:\n            if is_object_dtype(self):  # pragma: no cover\n                raise err\n\n            dtype = self._find_common_type_compat(value)\n            return self.astype(dtype).putmask(mask, value)\n\n        values = self._values.copy()\n\n        if isinstance(values, np.ndarray):\n            converted = setitem_datetimelike_compat(values, mask.sum(), converted)\n            np.putmask(values, mask, converted)\n\n        else:\n            # Note: we use the original value here, not converted, as\n            #  _validate_fill_value is not idempotent\n            values._putmask(mask, value)\n\n        return self._shallow_copy(values)\n\n    def equals(self, other: Any) -> bool:\n        \"\"\"\n        Determine if two Index object are equal.\n\n        The things that are being compared are:\n\n        * The elements inside the Index object.\n        * The order of the elements inside the Index object.\n\n        Parameters\n        ----------\n        other : Any\n            The other object to compare against.\n\n        Returns\n        -------\n        bool\n            True if \"other\" is an Index and it has the same elements and order\n            as the calling index; False otherwise.\n\n        Examples\n        --------\n        >>> idx1 = pd.Index([1, 2, 3])\n        >>> idx1\n        Int64Index([1, 2, 3], dtype='int64')\n        >>> idx1.equals(pd.Index([1, 2, 3]))\n        True\n\n        The elements inside are compared\n\n        >>> idx2 = pd.Index([\"1\", \"2\", \"3\"])\n        >>> idx2\n        Index(['1', '2', '3'], dtype='object')\n\n        >>> idx1.equals(idx2)\n        False\n\n        The order is compared\n\n        >>> ascending_idx = pd.Index([1, 2, 3])\n        >>> ascending_idx\n        Int64Index([1, 2, 3], dtype='int64')\n        >>> descending_idx = pd.Index([3, 2, 1])\n        >>> descending_idx\n        Int64Index([3, 2, 1], dtype='int64')\n        >>> ascending_idx.equals(descending_idx)\n        False\n\n        The dtype is *not* compared\n\n        >>> int64_idx = pd.Int64Index([1, 2, 3])\n        >>> int64_idx\n        Int64Index([1, 2, 3], dtype='int64')\n        >>> uint64_idx = pd.UInt64Index([1, 2, 3])\n        >>> uint64_idx\n        UInt64Index([1, 2, 3], dtype='uint64')\n        >>> int64_idx.equals(uint64_idx)\n        True\n        \"\"\"\n        if self.is_(other):\n            return True\n\n        if not isinstance(other, Index):\n            return False\n\n        if is_object_dtype(self.dtype) and not is_object_dtype(other.dtype):\n            # if other is not object, use other's logic for coercion\n            return other.equals(self)\n\n        if isinstance(other, ABCMultiIndex):\n            # d-level MultiIndex can equal d-tuple Index\n            return other.equals(self)\n\n        if isinstance(self._values, ExtensionArray):\n            # Dispatch to the ExtensionArray's .equals method.\n            if not isinstance(other, type(self)):\n                return False\n\n            earr = cast(ExtensionArray, self._data)\n            return earr.equals(other._data)\n\n        if is_extension_array_dtype(other.dtype):\n            # All EA-backed Index subclasses override equals\n            return other.equals(self)\n\n        return array_equivalent(self._values, other._values)\n\n    @final\n    def identical(self, other) -> bool:\n        \"\"\"\n        Similar to equals, but checks that object attributes and types are also equal.\n\n        Returns\n        -------\n        bool\n            If two Index objects have equal elements and same type True,\n            otherwise False.\n        \"\"\"\n        return (\n            self.equals(other)\n            and all(\n                getattr(self, c, None) == getattr(other, c, None)\n                for c in self._comparables\n            )\n            and type(self) == type(other)\n        )\n\n    @final\n    def asof(self, label):\n        \"\"\"\n        Return the label from the index, or, if not present, the previous one.\n\n        Assuming that the index is sorted, return the passed index label if it\n        is in the index, or return the previous index label if the passed one\n        is not in the index.\n\n        Parameters\n        ----------\n        label : object\n            The label up to which the method returns the latest index label.\n\n        Returns\n        -------\n        object\n            The passed label if it is in the index. The previous label if the\n            passed label is not in the sorted index or `NaN` if there is no\n            such label.\n\n        See Also\n        --------\n        Series.asof : Return the latest value in a Series up to the\n            passed index.\n        merge_asof : Perform an asof merge (similar to left join but it\n            matches on nearest key rather than equal key).\n        Index.get_loc : An `asof` is a thin wrapper around `get_loc`\n            with method='pad'.\n\n        Examples\n        --------\n        `Index.asof` returns the latest index label up to the passed label.\n\n        >>> idx = pd.Index(['2013-12-31', '2014-01-02', '2014-01-03'])\n        >>> idx.asof('2014-01-01')\n        '2013-12-31'\n\n        If the label is in the index, the method returns the passed label.\n\n        >>> idx.asof('2014-01-02')\n        '2014-01-02'\n\n        If all of the labels in the index are later than the passed label,\n        NaN is returned.\n\n        >>> idx.asof('1999-01-02')\n        nan\n\n        If the index is not sorted, an error is raised.\n\n        >>> idx_not_sorted = pd.Index(['2013-12-31', '2015-01-02',\n        ...                            '2014-01-03'])\n        >>> idx_not_sorted.asof('2013-12-31')\n        Traceback (most recent call last):\n        ValueError: index must be monotonic increasing or decreasing\n        \"\"\"\n        self._searchsorted_monotonic(label)  # validate sortedness\n        try:\n            loc = self.get_loc(label)\n        except (KeyError, TypeError):\n            # KeyError -> No exact match, try for padded\n            # TypeError -> passed e.g. non-hashable, fall through to get\n            #  the tested exception message\n            indexer = self.get_indexer([label], method=\"pad\")\n            if indexer.ndim > 1 or indexer.size > 1:\n                raise TypeError(\"asof requires scalar valued input\")\n            loc = indexer.item()\n            if loc == -1:\n                return self._na_value\n        else:\n            if isinstance(loc, slice):\n                loc = loc.indices(len(self))[-1]\n\n        return self[loc]\n\n    def asof_locs(self, where: Index, mask: np.ndarray) -> npt.NDArray[np.intp]:\n        \"\"\"\n        Return the locations (indices) of labels in the index.\n\n        As in the `asof` function, if the label (a particular entry in\n        `where`) is not in the index, the latest index label up to the\n        passed label is chosen and its index returned.\n\n        If all of the labels in the index are later than a label in `where`,\n        -1 is returned.\n\n        `mask` is used to ignore NA values in the index during calculation.\n\n        Parameters\n        ----------\n        where : Index\n            An Index consisting of an array of timestamps.\n        mask : np.ndarray[bool]\n            Array of booleans denoting where values in the original\n            data are not NA.\n\n        Returns\n        -------\n        np.ndarray[np.intp]\n            An array of locations (indices) of the labels from the Index\n            which correspond to the return values of the `asof` function\n            for every element in `where`.\n        \"\"\"\n        # error: No overload variant of \"searchsorted\" of \"ndarray\" matches argument\n        # types \"Union[ExtensionArray, ndarray[Any, Any]]\", \"str\"\n        # TODO: will be fixed when ExtensionArray.searchsorted() is fixed\n        locs = self._values[mask].searchsorted(\n            where._values, side=\"right\"  # type: ignore[call-overload]\n        )\n        locs = np.where(locs > 0, locs - 1, 0)\n\n        result = np.arange(len(self), dtype=np.intp)[mask].take(locs)\n\n        first_value = self._values[mask.argmax()]\n        result[(locs == 0) & (where._values < first_value)] = -1\n\n        return result\n\n    def sort_values(\n        self,\n        return_indexer: bool = False,\n        ascending: bool = True,\n        na_position: str_t = \"last\",\n        key: Callable | None = None,\n    ):\n        \"\"\"\n        Return a sorted copy of the index.\n\n        Return a sorted copy of the index, and optionally return the indices\n        that sorted the index itself.\n\n        Parameters\n        ----------\n        return_indexer : bool, default False\n            Should the indices that would sort the index be returned.\n        ascending : bool, default True\n            Should the index values be sorted in an ascending order.\n        na_position : {'first' or 'last'}, default 'last'\n            Argument 'first' puts NaNs at the beginning, 'last' puts NaNs at\n            the end.\n\n            .. versionadded:: 1.2.0\n\n        key : callable, optional\n            If not None, apply the key function to the index values\n            before sorting. This is similar to the `key` argument in the\n            builtin :meth:`sorted` function, with the notable difference that\n            this `key` function should be *vectorized*. It should expect an\n            ``Index`` and return an ``Index`` of the same shape.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        sorted_index : pandas.Index\n            Sorted copy of the index.\n        indexer : numpy.ndarray, optional\n            The indices that the index itself was sorted by.\n\n        See Also\n        --------\n        Series.sort_values : Sort values of a Series.\n        DataFrame.sort_values : Sort values in a DataFrame.\n\n        Examples\n        --------\n        >>> idx = pd.Index([10, 100, 1, 1000])\n        >>> idx\n        Int64Index([10, 100, 1, 1000], dtype='int64')\n\n        Sort values in ascending order (default behavior).\n\n        >>> idx.sort_values()\n        Int64Index([1, 10, 100, 1000], dtype='int64')\n\n        Sort values in descending order, and also get the indices `idx` was\n        sorted by.\n\n        >>> idx.sort_values(ascending=False, return_indexer=True)\n        (Int64Index([1000, 100, 10, 1], dtype='int64'), array([3, 1, 0, 2]))\n        \"\"\"\n        idx = ensure_key_mapped(self, key)\n\n        # GH 35584. Sort missing values according to na_position kwarg\n        # ignore na_position for MultiIndex\n        if not isinstance(self, ABCMultiIndex):\n            _as = nargsort(\n                items=idx, ascending=ascending, na_position=na_position, key=key\n            )\n        else:\n            _as = idx.argsort()\n            if not ascending:\n                _as = _as[::-1]\n\n        sorted_index = self.take(_as)\n\n        if return_indexer:\n            return sorted_index, _as\n        else:\n            return sorted_index\n\n    @final\n    def sort(self, *args, **kwargs):\n        \"\"\"\n        Use sort_values instead.\n        \"\"\"\n        raise TypeError(\"cannot sort an Index object in-place, use sort_values instead\")\n\n    def shift(self, periods=1, freq=None):\n        \"\"\"\n        Shift index by desired number of time frequency increments.\n\n        This method is for shifting the values of datetime-like indexes\n        by a specified time increment a given number of times.\n\n        Parameters\n        ----------\n        periods : int, default 1\n            Number of periods (or increments) to shift by,\n            can be positive or negative.\n        freq : pandas.DateOffset, pandas.Timedelta or str, optional\n            Frequency increment to shift by.\n            If None, the index is shifted by its own `freq` attribute.\n            Offset aliases are valid strings, e.g., 'D', 'W', 'M' etc.\n\n        Returns\n        -------\n        pandas.Index\n            Shifted index.\n\n        See Also\n        --------\n        Series.shift : Shift values of Series.\n\n        Notes\n        -----\n        This method is only implemented for datetime-like index classes,\n        i.e., DatetimeIndex, PeriodIndex and TimedeltaIndex.\n\n        Examples\n        --------\n        Put the first 5 month starts of 2011 into an index.\n\n        >>> month_starts = pd.date_range('1/1/2011', periods=5, freq='MS')\n        >>> month_starts\n        DatetimeIndex(['2011-01-01', '2011-02-01', '2011-03-01', '2011-04-01',\n                       '2011-05-01'],\n                      dtype='datetime64[ns]', freq='MS')\n\n        Shift the index by 10 days.\n\n        >>> month_starts.shift(10, freq='D')\n        DatetimeIndex(['2011-01-11', '2011-02-11', '2011-03-11', '2011-04-11',\n                       '2011-05-11'],\n                      dtype='datetime64[ns]', freq=None)\n\n        The default value of `freq` is the `freq` attribute of the index,\n        which is 'MS' (month start) in this example.\n\n        >>> month_starts.shift(10)\n        DatetimeIndex(['2011-11-01', '2011-12-01', '2012-01-01', '2012-02-01',\n                       '2012-03-01'],\n                      dtype='datetime64[ns]', freq='MS')\n        \"\"\"\n        raise NotImplementedError(\n            f\"This method is only implemented for DatetimeIndex, PeriodIndex and \"\n            f\"TimedeltaIndex; Got type {type(self).__name__}\"\n        )\n\n    def argsort(self, *args, **kwargs) -> npt.NDArray[np.intp]:\n        \"\"\"\n        Return the integer indices that would sort the index.\n\n        Parameters\n        ----------\n        *args\n            Passed to `numpy.ndarray.argsort`.\n        **kwargs\n            Passed to `numpy.ndarray.argsort`.\n\n        Returns\n        -------\n        np.ndarray[np.intp]\n            Integer indices that would sort the index if used as\n            an indexer.\n\n        See Also\n        --------\n        numpy.argsort : Similar method for NumPy arrays.\n        Index.sort_values : Return sorted copy of Index.\n\n        Examples\n        --------\n        >>> idx = pd.Index(['b', 'a', 'd', 'c'])\n        >>> idx\n        Index(['b', 'a', 'd', 'c'], dtype='object')\n\n        >>> order = idx.argsort()\n        >>> order\n        array([1, 0, 3, 2])\n\n        >>> idx[order]\n        Index(['a', 'b', 'c', 'd'], dtype='object')\n        \"\"\"\n        # This works for either ndarray or EA, is overridden\n        #  by RangeIndex, MultIIndex\n        return self._data.argsort(*args, **kwargs)\n\n    @final\n    def get_value(self, series: Series, key):\n        \"\"\"\n        Fast lookup of value from 1-dimensional ndarray.\n\n        Only use this if you know what you're doing.\n\n        Returns\n        -------\n        scalar or Series\n        \"\"\"\n        warnings.warn(\n            \"get_value is deprecated and will be removed in a future version. \"\n            \"Use Series[key] instead.\",\n            FutureWarning,\n            stacklevel=find_stack_level(),\n        )\n\n        self._check_indexing_error(key)\n\n        try:\n            # GH 20882, 21257\n            # First try to convert the key to a location\n            # If that fails, raise a KeyError if an integer\n            # index, otherwise, see if key is an integer, and\n            # try that\n            loc = self.get_loc(key)\n        except KeyError:\n            if not self._should_fallback_to_positional:\n                raise\n            elif is_integer(key):\n                # If the Index cannot hold integer, then this is unambiguously\n                #  a locational lookup.\n                loc = key\n            else:\n                raise\n\n        return self._get_values_for_loc(series, loc, key)\n\n    def _check_indexing_error(self, key):\n        if not is_scalar(key):\n            # if key is not a scalar, directly raise an error (the code below\n            # would convert to numpy arrays and raise later any way) - GH29926\n            raise InvalidIndexError(key)\n\n    @cache_readonly\n    def _should_fallback_to_positional(self) -> bool:\n        \"\"\"\n        Should an integer key be treated as positional?\n        \"\"\"\n        return not self.holds_integer() and not self.is_boolean()\n\n    def _get_values_for_loc(self, series: Series, loc, key):\n        \"\"\"\n        Do a positional lookup on the given Series, returning either a scalar\n        or a Series.\n\n        Assumes that `series.index is self`\n\n        key is included for MultiIndex compat.\n        \"\"\"\n        if is_integer(loc):\n            return series._values[loc]\n\n        return series.iloc[loc]\n\n    @final\n    def set_value(self, arr, key, value):\n        \"\"\"\n        Fast lookup of value from 1-dimensional ndarray.\n\n        .. deprecated:: 1.0\n\n        Notes\n        -----\n        Only use this if you know what you're doing.\n        \"\"\"\n        warnings.warn(\n            (\n                \"The 'set_value' method is deprecated, and \"\n                \"will be removed in a future version.\"\n            ),\n            FutureWarning,\n            stacklevel=find_stack_level(),\n        )\n        loc = self._engine.get_loc(key)\n        if not can_hold_element(arr, value):\n            raise ValueError\n        arr[loc] = value\n\n    _index_shared_docs[\n        \"get_indexer_non_unique\"\n    ] = \"\"\"\n        Compute indexer and mask for new index given the current index. The\n        indexer should be then used as an input to ndarray.take to align the\n        current data to the new index.\n\n        Parameters\n        ----------\n        target : %(target_klass)s\n\n        Returns\n        -------\n        indexer : np.ndarray[np.intp]\n            Integers from 0 to n - 1 indicating that the index at these\n            positions matches the corresponding target values. Missing values\n            in the target are marked by -1.\n        missing : np.ndarray[np.intp]\n            An indexer into the target of the values not found.\n            These correspond to the -1 in the indexer array.\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"get_indexer_non_unique\"] % _index_doc_kwargs)\n    def get_indexer_non_unique(\n        self, target\n    ) -> tuple[npt.NDArray[np.intp], npt.NDArray[np.intp]]:\n        target = ensure_index(target)\n        target = self._maybe_cast_listlike_indexer(target)\n\n        if not self._should_compare(target) and not is_interval_dtype(self.dtype):\n            # IntervalIndex get special treatment bc numeric scalars can be\n            #  matched to Interval scalars\n            return self._get_indexer_non_comparable(target, method=None, unique=False)\n\n        pself, ptarget = self._maybe_promote(target)\n        if pself is not self or ptarget is not target:\n            return pself.get_indexer_non_unique(ptarget)\n\n        if not is_dtype_equal(self.dtype, target.dtype):\n            # TODO: if object, could use infer_dtype to preempt costly\n            #  conversion if still non-comparable?\n            dtype = self._find_common_type_compat(target)\n\n            this = self.astype(dtype, copy=False)\n            that = target.astype(dtype, copy=False)\n            return this.get_indexer_non_unique(that)\n\n        # Note: _maybe_promote ensures we never get here with MultiIndex\n        #  self and non-Multi target\n        tgt_values = target._get_engine_target()\n        if self._is_multi and target._is_multi:\n            engine = self._engine\n            # error: \"IndexEngine\" has no attribute \"_extract_level_codes\"\n            tgt_values = engine._extract_level_codes(  # type: ignore[attr-defined]\n                target\n            )\n\n        indexer, missing = self._engine.get_indexer_non_unique(tgt_values)\n        return ensure_platform_int(indexer), ensure_platform_int(missing)\n\n    @final\n    def get_indexer_for(self, target) -> npt.NDArray[np.intp]:\n        \"\"\"\n        Guaranteed return of an indexer even when non-unique.\n\n        This dispatches to get_indexer or get_indexer_non_unique\n        as appropriate.\n\n        Returns\n        -------\n        np.ndarray[np.intp]\n            List of indices.\n\n        Examples\n        --------\n        >>> idx = pd.Index([np.nan, 'var1', np.nan])\n        >>> idx.get_indexer_for([np.nan])\n        array([0, 2])\n        \"\"\"\n        if self._index_as_unique:\n            return self.get_indexer(target)\n        indexer, _ = self.get_indexer_non_unique(target)\n        return indexer\n\n    def _get_indexer_strict(self, key, axis_name: str_t) -> tuple[Index, np.ndarray]:\n        \"\"\"\n        Analogue to get_indexer that raises if any elements are missing.\n        \"\"\"\n        keyarr = key\n        if not isinstance(keyarr, Index):\n            keyarr = com.asarray_tuplesafe(keyarr)\n\n        if self._index_as_unique:\n            indexer = self.get_indexer_for(keyarr)\n            keyarr = self.reindex(keyarr)[0]\n        else:\n            keyarr, indexer, new_indexer = self._reindex_non_unique(keyarr)\n\n        self._raise_if_missing(keyarr, indexer, axis_name)\n\n        keyarr = self.take(indexer)\n        if isinstance(key, Index):\n            # GH 42790 - Preserve name from an Index\n            keyarr.name = key.name\n        if keyarr.dtype.kind in [\"m\", \"M\"]:\n            # DTI/TDI.take can infer a freq in some cases when we dont want one\n            if isinstance(key, list) or (\n                isinstance(key, type(self))\n                # \"Index\" has no attribute \"freq\"\n                and key.freq is None  # type: ignore[attr-defined]\n            ):\n                keyarr = keyarr._with_freq(None)\n\n        return keyarr, indexer\n\n    def _raise_if_missing(self, key, indexer, axis_name: str_t) -> None:\n        \"\"\"\n        Check that indexer can be used to return a result.\n\n        e.g. at least one element was found,\n        unless the list of keys was actually empty.\n\n        Parameters\n        ----------\n        key : list-like\n            Targeted labels (only used to show correct error message).\n        indexer: array-like of booleans\n            Indices corresponding to the key,\n            (with -1 indicating not found).\n        axis_name : str\n\n        Raises\n        ------\n        KeyError\n            If at least one key was requested but none was found.\n        \"\"\"\n        if len(key) == 0:\n            return\n\n        # Count missing values\n        missing_mask = indexer < 0\n        nmissing = missing_mask.sum()\n\n        if nmissing:\n\n            # TODO: remove special-case; this is just to keep exception\n            #  message tests from raising while debugging\n            use_interval_msg = is_interval_dtype(self.dtype) or (\n                is_categorical_dtype(self.dtype)\n                # \"Index\" has no attribute \"categories\"  [attr-defined]\n                and is_interval_dtype(\n                    self.categories.dtype  # type: ignore[attr-defined]\n                )\n            )\n\n            if nmissing == len(indexer):\n                if use_interval_msg:\n                    key = list(key)\n                raise KeyError(f\"None of [{key}] are in the [{axis_name}]\")\n\n            not_found = list(ensure_index(key)[missing_mask.nonzero()[0]].unique())\n            raise KeyError(f\"{not_found} not in index\")\n\n    @overload\n    def _get_indexer_non_comparable(\n        self, target: Index, method, unique: Literal[True] = ...\n    ) -> npt.NDArray[np.intp]:\n        ...\n\n    @overload\n    def _get_indexer_non_comparable(\n        self, target: Index, method, unique: Literal[False]\n    ) -> tuple[npt.NDArray[np.intp], npt.NDArray[np.intp]]:\n        ...\n\n    @overload\n    def _get_indexer_non_comparable(\n        self, target: Index, method, unique: bool = True\n    ) -> npt.NDArray[np.intp] | tuple[npt.NDArray[np.intp], npt.NDArray[np.intp]]:\n        ...\n\n    @final\n    def _get_indexer_non_comparable(\n        self, target: Index, method, unique: bool = True\n    ) -> npt.NDArray[np.intp] | tuple[npt.NDArray[np.intp], npt.NDArray[np.intp]]:\n        \"\"\"\n        Called from get_indexer or get_indexer_non_unique when the target\n        is of a non-comparable dtype.\n\n        For get_indexer lookups with method=None, get_indexer is an _equality_\n        check, so non-comparable dtypes mean we will always have no matches.\n\n        For get_indexer lookups with a method, get_indexer is an _inequality_\n        check, so non-comparable dtypes mean we will always raise TypeError.\n\n        Parameters\n        ----------\n        target : Index\n        method : str or None\n        unique : bool, default True\n            * True if called from get_indexer.\n            * False if called from get_indexer_non_unique.\n\n        Raises\n        ------\n        TypeError\n            If doing an inequality check, i.e. method is not None.\n        \"\"\"\n        if method is not None:\n            other = unpack_nested_dtype(target)\n            raise TypeError(f\"Cannot compare dtypes {self.dtype} and {other.dtype}\")\n\n        no_matches = -1 * np.ones(target.shape, dtype=np.intp)\n        if unique:\n            # This is for get_indexer\n            return no_matches\n        else:\n            # This is for get_indexer_non_unique\n            missing = np.arange(len(target), dtype=np.intp)\n            return no_matches, missing\n\n    @property\n    def _index_as_unique(self) -> bool:\n        \"\"\"\n        Whether we should treat this as unique for the sake of\n        get_indexer vs get_indexer_non_unique.\n\n        For IntervalIndex compat.\n        \"\"\"\n        return self.is_unique\n\n    _requires_unique_msg = \"Reindexing only valid with uniquely valued Index objects\"\n\n    @final\n    def _maybe_promote(self, other: Index) -> tuple[Index, Index]:\n        \"\"\"\n        When dealing with an object-dtype Index and a non-object Index, see\n        if we can upcast the object-dtype one to improve performance.\n        \"\"\"\n\n        if isinstance(self, ABCDatetimeIndex) and isinstance(other, ABCDatetimeIndex):\n            if (\n                self.tz is not None\n                and other.tz is not None\n                and not tz_compare(self.tz, other.tz)\n            ):\n                # standardize on UTC\n                return self.tz_convert(\"UTC\"), other.tz_convert(\"UTC\")\n\n        elif self.inferred_type == \"date\" and isinstance(other, ABCDatetimeIndex):\n            try:\n                return type(other)(self), other\n            except OutOfBoundsDatetime:\n                return self, other\n        elif self.inferred_type == \"timedelta\" and isinstance(other, ABCTimedeltaIndex):\n            # TODO: we dont have tests that get here\n            return type(other)(self), other\n\n        elif self.dtype.kind == \"u\" and other.dtype.kind == \"i\":\n            # GH#41873\n            if other.min() >= 0:\n                # lookup min as it may be cached\n                # TODO: may need itemsize check if we have non-64-bit Indexes\n                return self, other.astype(self.dtype)\n\n        elif self._is_multi and not other._is_multi:\n            try:\n                # \"Type[Index]\" has no attribute \"from_tuples\"\n                other = type(self).from_tuples(other)  # type: ignore[attr-defined]\n            except (TypeError, ValueError):\n                # let's instead try with a straight Index\n                self = Index(self._values)\n\n        if not is_object_dtype(self.dtype) and is_object_dtype(other.dtype):\n            # Reverse op so we dont need to re-implement on the subclasses\n            other, self = other._maybe_promote(self)\n\n        return self, other\n\n    @final\n    def _find_common_type_compat(self, target) -> DtypeObj:\n        \"\"\"\n        Implementation of find_common_type that adjusts for Index-specific\n        special cases.\n        \"\"\"\n        if is_interval_dtype(self.dtype) and is_valid_na_for_dtype(target, self.dtype):\n            # e.g. setting NA value into IntervalArray[int64]\n            self = cast(\"IntervalIndex\", self)\n            return IntervalDtype(np.float64, closed=self.closed)\n\n        target_dtype, _ = infer_dtype_from(target, pandas_dtype=True)\n\n        # special case: if one dtype is uint64 and the other a signed int, return object\n        # See https://github.com/pandas-dev/pandas/issues/26778 for discussion\n        # Now it's:\n        # * float | [u]int -> float\n        # * uint64 | signed int  -> object\n        # We may change union(float | [u]int) to go to object.\n        if self.dtype == \"uint64\" or target_dtype == \"uint64\":\n            if is_signed_integer_dtype(self.dtype) or is_signed_integer_dtype(\n                target_dtype\n            ):\n                return _dtype_obj\n\n        dtype = find_common_type([self.dtype, target_dtype])\n\n        if dtype.kind in [\"i\", \"u\"]:\n            # TODO: what about reversed with self being categorical?\n            if (\n                isinstance(target, Index)\n                and is_categorical_dtype(target.dtype)\n                and target.hasnans\n            ):\n                # FIXME: find_common_type incorrect with Categorical GH#38240\n                # FIXME: some cases where float64 cast can be lossy?\n                dtype = np.dtype(np.float64)\n        if dtype.kind == \"c\":\n            dtype = _dtype_obj\n        return dtype\n\n    @final\n    def _should_compare(self, other: Index) -> bool:\n        \"\"\"\n        Check if `self == other` can ever have non-False entries.\n        \"\"\"\n\n        if (other.is_boolean() and self.is_numeric()) or (\n            self.is_boolean() and other.is_numeric()\n        ):\n            # GH#16877 Treat boolean labels passed to a numeric index as not\n            #  found. Without this fix False and True would be treated as 0 and 1\n            #  respectively.\n            return False\n\n        other = unpack_nested_dtype(other)\n        dtype = other.dtype\n        return self._is_comparable_dtype(dtype) or is_object_dtype(dtype)\n\n    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:\n        \"\"\"\n        Can we compare values of the given dtype to our own?\n        \"\"\"\n        return True\n\n    @final\n    def groupby(self, values) -> PrettyDict[Hashable, np.ndarray]:\n        \"\"\"\n        Group the index labels by a given array of values.\n\n        Parameters\n        ----------\n        values : array\n            Values used to determine the groups.\n\n        Returns\n        -------\n        dict\n            {group name -> group labels}\n        \"\"\"\n        # TODO: if we are a MultiIndex, we can do better\n        # that converting to tuples\n        if isinstance(values, ABCMultiIndex):\n            values = values._values\n        values = Categorical(values)\n        result = values._reverse_indexer()\n\n        # map to the label\n        result = {k: self.take(v) for k, v in result.items()}\n\n        return PrettyDict(result)\n\n    def map(self, mapper, na_action=None):\n        \"\"\"\n        Map values using an input mapping or function.\n\n        Parameters\n        ----------\n        mapper : function, dict, or Series\n            Mapping correspondence.\n        na_action : {None, 'ignore'}\n            If 'ignore', propagate NA values, without passing them to the\n            mapping correspondence.\n\n        Returns\n        -------\n        applied : Union[Index, MultiIndex], inferred\n            The output of the mapping function applied to the index.\n            If the function returns a tuple with more than one element\n            a MultiIndex will be returned.\n        \"\"\"\n        from pandas.core.indexes.multi import MultiIndex\n\n        new_values = self._map_values(mapper, na_action=na_action)\n\n        # we can return a MultiIndex\n        if new_values.size and isinstance(new_values[0], tuple):\n            if isinstance(self, MultiIndex):\n                names = self.names\n            elif self.name:\n                names = [self.name] * len(new_values[0])\n            else:\n                names = None\n            return MultiIndex.from_tuples(new_values, names=names)\n\n        dtype = None\n        if not new_values.size:\n            # empty\n            dtype = self.dtype\n\n        # e.g. if we are floating and new_values is all ints, then we\n        #  don't want to cast back to floating.  But if we are UInt64\n        #  and new_values is all ints, we want to try.\n        same_dtype = lib.infer_dtype(new_values, skipna=False) == self.inferred_type\n        if same_dtype:\n            new_values = maybe_cast_pointwise_result(\n                new_values, self.dtype, same_dtype=same_dtype\n            )\n\n        if self._is_backward_compat_public_numeric_index and is_numeric_dtype(\n            new_values.dtype\n        ):\n            return self._constructor(\n                new_values, dtype=dtype, copy=False, name=self.name\n            )\n\n        return Index._with_infer(new_values, dtype=dtype, copy=False, name=self.name)\n\n    # TODO: De-duplicate with map, xref GH#32349\n    @final\n    def _transform_index(self, func, *, level=None) -> Index:\n        \"\"\"\n        Apply function to all values found in index.\n\n        This includes transforming multiindex entries separately.\n        Only apply function to one level of the MultiIndex if level is specified.\n        \"\"\"\n        if isinstance(self, ABCMultiIndex):\n            if level is not None:\n                # Caller is responsible for ensuring level is positional.\n                items = [\n                    tuple(func(y) if i == level else y for i, y in enumerate(x))\n                    for x in self\n                ]\n            else:\n                items = [tuple(func(y) for y in x) for x in self]\n            return type(self).from_tuples(items, names=self.names)\n        else:\n            items = [func(x) for x in self]\n            return Index(items, name=self.name, tupleize_cols=False)\n\n    def isin(self, values, level=None) -> np.ndarray:\n        \"\"\"\n        Return a boolean array where the index values are in `values`.\n\n        Compute boolean array of whether each index value is found in the\n        passed set of values. The length of the returned boolean array matches\n        the length of the index.\n\n        Parameters\n        ----------\n        values : set or list-like\n            Sought values.\n        level : str or int, optional\n            Name or position of the index level to use (if the index is a\n            `MultiIndex`).\n\n        Returns\n        -------\n        np.ndarray[bool]\n            NumPy array of boolean values.\n\n        See Also\n        --------\n        Series.isin : Same for Series.\n        DataFrame.isin : Same method for DataFrames.\n\n        Notes\n        -----\n        In the case of `MultiIndex` you must either specify `values` as a\n        list-like object containing tuples that are the same length as the\n        number of levels, or specify `level`. Otherwise it will raise a\n        ``ValueError``.\n\n        If `level` is specified:\n\n        - if it is the name of one *and only one* index level, use that level;\n        - otherwise it should be a number indicating level position.\n\n        Examples\n        --------\n        >>> idx = pd.Index([1,2,3])\n        >>> idx\n        Int64Index([1, 2, 3], dtype='int64')\n\n        Check whether each index value in a list of values.\n\n        >>> idx.isin([1, 4])\n        array([ True, False, False])\n\n        >>> midx = pd.MultiIndex.from_arrays([[1,2,3],\n        ...                                  ['red', 'blue', 'green']],\n        ...                                  names=('number', 'color'))\n        >>> midx\n        MultiIndex([(1,   'red'),\n                    (2,  'blue'),\n                    (3, 'green')],\n                   names=['number', 'color'])\n\n        Check whether the strings in the 'color' level of the MultiIndex\n        are in a list of colors.\n\n        >>> midx.isin(['red', 'orange', 'yellow'], level='color')\n        array([ True, False, False])\n\n        To check across the levels of a MultiIndex, pass a list of tuples:\n\n        >>> midx.isin([(1, 'red'), (3, 'red')])\n        array([ True, False, False])\n\n        For a DatetimeIndex, string values in `values` are converted to\n        Timestamps.\n\n        >>> dates = ['2000-03-11', '2000-03-12', '2000-03-13']\n        >>> dti = pd.to_datetime(dates)\n        >>> dti\n        DatetimeIndex(['2000-03-11', '2000-03-12', '2000-03-13'],\n        dtype='datetime64[ns]', freq=None)\n\n        >>> dti.isin(['2000-03-11'])\n        array([ True, False, False])\n        \"\"\"\n        if level is not None:\n            self._validate_index_level(level)\n        return algos.isin(self._values, values)\n\n    def _get_string_slice(self, key: str_t):\n        # this is for partial string indexing,\n        # overridden in DatetimeIndex, TimedeltaIndex and PeriodIndex\n        raise NotImplementedError\n\n    def slice_indexer(\n        self,\n        start: Hashable | None = None,\n        end: Hashable | None = None,\n        step: int | None = None,\n        kind=no_default,\n    ) -> slice:\n        \"\"\"\n        Compute the slice indexer for input labels and step.\n\n        Index needs to be ordered and unique.\n\n        Parameters\n        ----------\n        start : label, default None\n            If None, defaults to the beginning.\n        end : label, default None\n            If None, defaults to the end.\n        step : int, default None\n        kind : str, default None\n\n            .. deprecated:: 1.4.0\n\n        Returns\n        -------\n        indexer : slice\n\n        Raises\n        ------\n        KeyError : If key does not exist, or key is not unique and index is\n            not ordered.\n\n        Notes\n        -----\n        This function assumes that the data is sorted, so use at your own peril\n\n        Examples\n        --------\n        This is a method on all index types. For example you can do:\n\n        >>> idx = pd.Index(list('abcd'))\n        >>> idx.slice_indexer(start='b', end='c')\n        slice(1, 3, None)\n\n        >>> idx = pd.MultiIndex.from_arrays([list('abcd'), list('efgh')])\n        >>> idx.slice_indexer(start='b', end=('c', 'g'))\n        slice(1, 3, None)\n        \"\"\"\n        self._deprecated_arg(kind, \"kind\", \"slice_indexer\")\n\n        start_slice, end_slice = self.slice_locs(start, end, step=step)\n\n        # return a slice\n        if not is_scalar(start_slice):\n            raise AssertionError(\"Start slice bound is non-scalar\")\n        if not is_scalar(end_slice):\n            raise AssertionError(\"End slice bound is non-scalar\")\n\n        return slice(start_slice, end_slice, step)\n\n    def _maybe_cast_indexer(self, key):\n        \"\"\"\n        If we have a float key and are not a floating index, then try to cast\n        to an int if equivalent.\n        \"\"\"\n        if not self.is_floating():\n            return com.cast_scalar_indexer(key)\n        return key\n\n    def _maybe_cast_listlike_indexer(self, target) -> Index:\n        \"\"\"\n        Analogue to maybe_cast_indexer for get_indexer instead of get_loc.\n        \"\"\"\n        return ensure_index(target)\n\n    @final\n    def _validate_indexer(self, form: str_t, key, kind: str_t):\n        \"\"\"\n        If we are positional indexer, validate that we have appropriate\n        typed bounds must be an integer.\n        \"\"\"\n        assert kind in [\"getitem\", \"iloc\"]\n\n        if key is not None and not is_integer(key):\n            raise self._invalid_indexer(form, key)\n\n    def _maybe_cast_slice_bound(self, label, side: str_t, kind=no_default):\n        \"\"\"\n        This function should be overloaded in subclasses that allow non-trivial\n        casting on label-slice bounds, e.g. datetime-like indices allowing\n        strings containing formatted datetimes.\n\n        Parameters\n        ----------\n        label : object\n        side : {'left', 'right'}\n        kind : {'loc', 'getitem'} or None\n\n            .. deprecated:: 1.3.0\n\n        Returns\n        -------\n        label : object\n\n        Notes\n        -----\n        Value of `side` parameter should be validated in caller.\n        \"\"\"\n        assert kind in [\"loc\", \"getitem\", None, no_default]\n        self._deprecated_arg(kind, \"kind\", \"_maybe_cast_slice_bound\")\n\n        # We are a plain index here (sub-class override this method if they\n        # wish to have special treatment for floats/ints, e.g. Float64Index and\n        # datetimelike Indexes\n        # reject them, if index does not contain label\n        if (is_float(label) or is_integer(label)) and label not in self:\n            raise self._invalid_indexer(\"slice\", label)\n\n        return label\n\n    def _searchsorted_monotonic(self, label, side: Literal[\"left\", \"right\"] = \"left\"):\n        if self.is_monotonic_increasing:\n            return self.searchsorted(label, side=side)\n        elif self.is_monotonic_decreasing:\n            # np.searchsorted expects ascending sort order, have to reverse\n            # everything for it to work (element ordering, search side and\n            # resulting value).\n            pos = self[::-1].searchsorted(\n                label, side=\"right\" if side == \"left\" else \"left\"\n            )\n            return len(self) - pos\n\n        raise ValueError(\"index must be monotonic increasing or decreasing\")\n\n    def get_slice_bound(\n        self, label, side: Literal[\"left\", \"right\"], kind=no_default\n    ) -> int:\n        \"\"\"\n        Calculate slice bound that corresponds to given label.\n\n        Returns leftmost (one-past-the-rightmost if ``side=='right'``) position\n        of given label.\n\n        Parameters\n        ----------\n        label : object\n        side : {'left', 'right'}\n        kind : {'loc', 'getitem'} or None\n\n            .. deprecated:: 1.4.0\n\n        Returns\n        -------\n        int\n            Index of label.\n        \"\"\"\n        assert kind in [\"loc\", \"getitem\", None, no_default]\n        self._deprecated_arg(kind, \"kind\", \"get_slice_bound\")\n\n        if side not in (\"left\", \"right\"):\n            raise ValueError(\n                \"Invalid value for side kwarg, must be either \"\n                f\"'left' or 'right': {side}\"\n            )\n\n        original_label = label\n\n        # For datetime indices label may be a string that has to be converted\n        # to datetime boundary according to its resolution.\n        label = self._maybe_cast_slice_bound(label, side)\n\n        # we need to look up the label\n        try:\n            slc = self.get_loc(label)\n        except KeyError as err:\n            try:\n                return self._searchsorted_monotonic(label, side)\n            except ValueError:\n                # raise the original KeyError\n                raise err\n\n        if isinstance(slc, np.ndarray):\n            # get_loc may return a boolean array, which\n            # is OK as long as they are representable by a slice.\n            assert is_bool_dtype(slc.dtype)\n            slc = lib.maybe_booleans_to_slice(slc.view(\"u1\"))\n            if isinstance(slc, np.ndarray):\n                raise KeyError(\n                    f\"Cannot get {side} slice bound for non-unique \"\n                    f\"label: {repr(original_label)}\"\n                )\n\n        if isinstance(slc, slice):\n            if side == \"left\":\n                return slc.start\n            else:\n                return slc.stop\n        else:\n            if side == \"right\":\n                return slc + 1\n            else:\n                return slc\n\n    def slice_locs(\n        self, start=None, end=None, step=None, kind=no_default\n    ) -> tuple[int, int]:\n        \"\"\"\n        Compute slice locations for input labels.\n\n        Parameters\n        ----------\n        start : label, default None\n            If None, defaults to the beginning.\n        end : label, default None\n            If None, defaults to the end.\n        step : int, defaults None\n            If None, defaults to 1.\n        kind : {'loc', 'getitem'} or None\n\n            .. deprecated:: 1.4.0\n\n        Returns\n        -------\n        start, end : int\n\n        See Also\n        --------\n        Index.get_loc : Get location for a single label.\n\n        Notes\n        -----\n        This method only works if the index is monotonic or unique.\n\n        Examples\n        --------\n        >>> idx = pd.Index(list('abcd'))\n        >>> idx.slice_locs(start='b', end='c')\n        (1, 3)\n        \"\"\"\n        self._deprecated_arg(kind, \"kind\", \"slice_locs\")\n        inc = step is None or step >= 0\n\n        if not inc:\n            # If it's a reverse slice, temporarily swap bounds.\n            start, end = end, start\n\n        # GH 16785: If start and end happen to be date strings with UTC offsets\n        # attempt to parse and check that the offsets are the same\n        if isinstance(start, (str, datetime)) and isinstance(end, (str, datetime)):\n            try:\n                ts_start = Timestamp(start)\n                ts_end = Timestamp(end)\n            except (ValueError, TypeError):\n                pass\n            else:\n                if not tz_compare(ts_start.tzinfo, ts_end.tzinfo):\n                    raise ValueError(\"Both dates must have the same UTC offset\")\n\n        start_slice = None\n        if start is not None:\n            start_slice = self.get_slice_bound(start, \"left\")\n        if start_slice is None:\n            start_slice = 0\n\n        end_slice = None\n        if end is not None:\n            end_slice = self.get_slice_bound(end, \"right\")\n        if end_slice is None:\n            end_slice = len(self)\n\n        if not inc:\n            # Bounds at this moment are swapped, swap them back and shift by 1.\n            #\n            # slice_locs('B', 'A', step=-1): s='B', e='A'\n            #\n            #              s='A'                 e='B'\n            # AFTER SWAP:    |                     |\n            #                v ------------------> V\n            #           -----------------------------------\n            #           | | |A|A|A|A| | | | | |B|B| | | | |\n            #           -----------------------------------\n            #              ^ <------------------ ^\n            # SHOULD BE:   |                     |\n            #           end=s-1              start=e-1\n            #\n            end_slice, start_slice = start_slice - 1, end_slice - 1\n\n            # i == -1 triggers ``len(self) + i`` selection that points to the\n            # last element, not before-the-first one, subtracting len(self)\n            # compensates that.\n            if end_slice == -1:\n                end_slice -= len(self)\n            if start_slice == -1:\n                start_slice -= len(self)\n\n        return start_slice, end_slice\n\n    def delete(self: _IndexT, loc) -> _IndexT:\n        \"\"\"\n        Make new Index with passed location(-s) deleted.\n\n        Parameters\n        ----------\n        loc : int or list of int\n            Location of item(-s) which will be deleted.\n            Use a list of locations to delete more than one value at the same time.\n\n        Returns\n        -------\n        Index\n            Will be same type as self, except for RangeIndex.\n\n        See Also\n        --------\n        numpy.delete : Delete any rows and column from NumPy array (ndarray).\n\n        Examples\n        --------\n        >>> idx = pd.Index(['a', 'b', 'c'])\n        >>> idx.delete(1)\n        Index(['a', 'c'], dtype='object')\n\n        >>> idx = pd.Index(['a', 'b', 'c'])\n        >>> idx.delete([0, 2])\n        Index(['b'], dtype='object')\n        \"\"\"\n        values = self._values\n        res_values: ArrayLike\n        if isinstance(values, np.ndarray):\n            # TODO(__array_function__): special casing will be unnecessary\n            res_values = np.delete(values, loc)\n        else:\n            res_values = values.delete(loc)\n\n        # _constructor so RangeIndex->Int64Index\n        return self._constructor._simple_new(res_values, name=self.name)\n\n    def insert(self, loc: int, item) -> Index:\n        \"\"\"\n        Make new Index inserting new item at location.\n\n        Follows Python numpy.insert semantics for negative values.\n\n        Parameters\n        ----------\n        loc : int\n        item : object\n\n        Returns\n        -------\n        new_index : Index\n        \"\"\"\n        item = lib.item_from_zerodim(item)\n        if is_valid_na_for_dtype(item, self.dtype) and self.dtype != object:\n            item = self._na_value\n\n        arr = self._values\n\n        try:\n            if isinstance(arr, ExtensionArray):\n                res_values = arr.insert(loc, item)\n                return type(self)._simple_new(res_values, name=self.name)\n            else:\n                item = self._validate_fill_value(item)\n        except (TypeError, ValueError):\n            # e.g. trying to insert an integer into a DatetimeIndex\n            #  We cannot keep the same dtype, so cast to the (often object)\n            #  minimal shared dtype before doing the insert.\n            dtype = self._find_common_type_compat(item)\n            return self.astype(dtype).insert(loc, item)\n\n        if arr.dtype != object or not isinstance(\n            item, (tuple, np.datetime64, np.timedelta64)\n        ):\n            # with object-dtype we need to worry about numpy incorrectly casting\n            # dt64/td64 to integer, also about treating tuples as sequences\n            # special-casing dt64/td64 https://github.com/numpy/numpy/issues/12550\n            casted = arr.dtype.type(item)\n            new_values = np.insert(arr, loc, casted)\n\n        else:\n            # No overload variant of \"insert\" matches argument types\n            # \"ndarray[Any, Any]\", \"int\", \"None\"  [call-overload]\n            new_values = np.insert(arr, loc, None)  # type: ignore[call-overload]\n            loc = loc if loc >= 0 else loc - 1\n            new_values[loc] = item\n\n        # Use self._constructor instead of Index to retain NumericIndex GH#43921\n        # TODO(2.0) can use Index instead of self._constructor\n        return self._constructor._with_infer(new_values, name=self.name)\n\n    def drop(self, labels, errors: str_t = \"raise\") -> Index:\n        \"\"\"\n        Make new Index with passed list of labels deleted.\n\n        Parameters\n        ----------\n        labels : array-like or scalar\n        errors : {'ignore', 'raise'}, default 'raise'\n            If 'ignore', suppress error and existing labels are dropped.\n\n        Returns\n        -------\n        dropped : Index\n            Will be same type as self, except for RangeIndex.\n\n        Raises\n        ------\n        KeyError\n            If not all of the labels are found in the selected axis\n        \"\"\"\n        if not isinstance(labels, Index):\n            # avoid materializing e.g. RangeIndex\n            arr_dtype = \"object\" if self.dtype == \"object\" else None\n            labels = com.index_labels_to_array(labels, dtype=arr_dtype)\n\n        indexer = self.get_indexer_for(labels)\n        mask = indexer == -1\n        if mask.any():\n            if errors != \"ignore\":\n                raise KeyError(f\"{list(labels[mask])} not found in axis\")\n            indexer = indexer[~mask]\n        return self.delete(indexer)\n\n    # --------------------------------------------------------------------\n    # Generated Arithmetic, Comparison, and Unary Methods\n\n    def _cmp_method(self, other, op):\n        \"\"\"\n        Wrapper used to dispatch comparison operations.\n        \"\"\"\n        if self.is_(other):\n            # fastpath\n            if op in {operator.eq, operator.le, operator.ge}:\n                arr = np.ones(len(self), dtype=bool)\n                if self._can_hold_na and not isinstance(self, ABCMultiIndex):\n                    # TODO: should set MultiIndex._can_hold_na = False?\n                    arr[self.isna()] = False\n                return arr\n            elif op in {operator.ne, operator.lt, operator.gt}:\n                arr = np.zeros(len(self), dtype=bool)\n                if self._can_hold_na and not isinstance(self, ABCMultiIndex):\n                    arr[self.isna()] = True\n                return arr\n\n        if isinstance(other, (np.ndarray, Index, ABCSeries, ExtensionArray)) and len(\n            self\n        ) != len(other):\n            raise ValueError(\"Lengths must match to compare\")\n\n        if not isinstance(other, ABCMultiIndex):\n            other = extract_array(other, extract_numpy=True)\n        else:\n            other = np.asarray(other)\n\n        if is_object_dtype(self.dtype) and isinstance(other, ExtensionArray):\n            # e.g. PeriodArray, Categorical\n            with np.errstate(all=\"ignore\"):\n                result = op(self._values, other)\n\n        elif isinstance(self._values, ExtensionArray):\n            result = op(self._values, other)\n\n        elif is_object_dtype(self.dtype) and not isinstance(self, ABCMultiIndex):\n            # don't pass MultiIndex\n            with np.errstate(all=\"ignore\"):\n                result = ops.comp_method_OBJECT_ARRAY(op, self._values, other)\n\n        else:\n            with np.errstate(all=\"ignore\"):\n                result = ops.comparison_op(self._values, other, op)\n\n        return result\n\n    def _construct_result(self, result, name):\n        if isinstance(result, tuple):\n            return (\n                Index._with_infer(result[0], name=name),\n                Index._with_infer(result[1], name=name),\n            )\n        return Index._with_infer(result, name=name)\n\n    def _arith_method(self, other, op):\n        if (\n            isinstance(other, Index)\n            and is_object_dtype(other.dtype)\n            and type(other) is not Index\n        ):\n            # We return NotImplemented for object-dtype index *subclasses* so they have\n            # a chance to implement ops before we unwrap them.\n            # See https://github.com/pandas-dev/pandas/issues/31109\n            return NotImplemented\n\n        return super()._arith_method(other, op)\n\n    @final\n    def _unary_method(self, op):\n        result = op(self._values)\n        return Index(result, name=self.name)\n\n    def __abs__(self):\n        return self._unary_method(operator.abs)\n\n    def __neg__(self):\n        return self._unary_method(operator.neg)\n\n    def __pos__(self):\n        return self._unary_method(operator.pos)\n\n    def __invert__(self):\n        # GH#8875\n        return self._unary_method(operator.inv)\n\n    # --------------------------------------------------------------------\n    # Reductions\n\n    def any(self, *args, **kwargs):\n        \"\"\"\n        Return whether any element is Truthy.\n\n        Parameters\n        ----------\n        *args\n            Required for compatibility with numpy.\n        **kwargs\n            Required for compatibility with numpy.\n\n        Returns\n        -------\n        any : bool or array-like (if axis is specified)\n            A single element array-like may be converted to bool.\n\n        See Also\n        --------\n        Index.all : Return whether all elements are True.\n        Series.all : Return whether all elements are True.\n\n        Notes\n        -----\n        Not a Number (NaN), positive infinity and negative infinity\n        evaluate to True because these are not equal to zero.\n\n        Examples\n        --------\n        >>> index = pd.Index([0, 1, 2])\n        >>> index.any()\n        True\n\n        >>> index = pd.Index([0, 0, 0])\n        >>> index.any()\n        False\n        \"\"\"\n        nv.validate_any(args, kwargs)\n        self._maybe_disable_logical_methods(\"any\")\n        # error: Argument 1 to \"any\" has incompatible type \"ArrayLike\"; expected\n        # \"Union[Union[int, float, complex, str, bytes, generic], Sequence[Union[int,\n        # float, complex, str, bytes, generic]], Sequence[Sequence[Any]],\n        # _SupportsArray]\"\n        return np.any(self.values)  # type: ignore[arg-type]\n\n    def all(self, *args, **kwargs):\n        \"\"\"\n        Return whether all elements are Truthy.\n\n        Parameters\n        ----------\n        *args\n            Required for compatibility with numpy.\n        **kwargs\n            Required for compatibility with numpy.\n\n        Returns\n        -------\n        all : bool or array-like (if axis is specified)\n            A single element array-like may be converted to bool.\n\n        See Also\n        --------\n        Index.any : Return whether any element in an Index is True.\n        Series.any : Return whether any element in a Series is True.\n        Series.all : Return whether all elements in a Series are True.\n\n        Notes\n        -----\n        Not a Number (NaN), positive infinity and negative infinity\n        evaluate to True because these are not equal to zero.\n\n        Examples\n        --------\n        True, because nonzero integers are considered True.\n\n        >>> pd.Index([1, 2, 3]).all()\n        True\n\n        False, because ``0`` is considered False.\n\n        >>> pd.Index([0, 1, 2]).all()\n        False\n        \"\"\"\n        nv.validate_all(args, kwargs)\n        self._maybe_disable_logical_methods(\"all\")\n        # error: Argument 1 to \"all\" has incompatible type \"ArrayLike\"; expected\n        # \"Union[Union[int, float, complex, str, bytes, generic], Sequence[Union[int,\n        # float, complex, str, bytes, generic]], Sequence[Sequence[Any]],\n        # _SupportsArray]\"\n        return np.all(self.values)  # type: ignore[arg-type]\n\n    @final\n    def _maybe_disable_logical_methods(self, opname: str_t) -> None:\n        \"\"\"\n        raise if this Index subclass does not support any or all.\n        \"\"\"\n        if (\n            isinstance(self, ABCMultiIndex)\n            or needs_i8_conversion(self.dtype)\n            or is_interval_dtype(self.dtype)\n            or is_categorical_dtype(self.dtype)\n            or is_float_dtype(self.dtype)\n        ):\n            # This call will raise\n            make_invalid_op(opname)(self)\n\n    @Appender(IndexOpsMixin.argmin.__doc__)\n    def argmin(self, axis=None, skipna=True, *args, **kwargs):\n        nv.validate_argmin(args, kwargs)\n        nv.validate_minmax_axis(axis)\n\n        if not self._is_multi and self.hasnans:\n            # Take advantage of cache\n            mask = self._isnan\n            if not skipna or mask.all():\n                return -1\n        return super().argmin(skipna=skipna)\n\n    @Appender(IndexOpsMixin.argmax.__doc__)\n    def argmax(self, axis=None, skipna=True, *args, **kwargs):\n        nv.validate_argmax(args, kwargs)\n        nv.validate_minmax_axis(axis)\n\n        if not self._is_multi and self.hasnans:\n            # Take advantage of cache\n            mask = self._isnan\n            if not skipna or mask.all():\n                return -1\n        return super().argmax(skipna=skipna)\n\n    @doc(IndexOpsMixin.min)\n    def min(self, axis=None, skipna=True, *args, **kwargs):\n        nv.validate_min(args, kwargs)\n        nv.validate_minmax_axis(axis)\n\n        if not len(self):\n            return self._na_value\n\n        if len(self) and self.is_monotonic_increasing:\n            # quick check\n            first = self[0]\n            if not isna(first):\n                return first\n\n        if not self._is_multi and self.hasnans:\n            # Take advantage of cache\n            mask = self._isnan\n            if not skipna or mask.all():\n                return self._na_value\n\n        if not self._is_multi and not isinstance(self._values, np.ndarray):\n            # \"ExtensionArray\" has no attribute \"min\"\n            return self._values.min(skipna=skipna)  # type: ignore[attr-defined]\n\n        return super().min(skipna=skipna)\n\n    @doc(IndexOpsMixin.max)\n    def max(self, axis=None, skipna=True, *args, **kwargs):\n        nv.validate_max(args, kwargs)\n        nv.validate_minmax_axis(axis)\n\n        if not len(self):\n            return self._na_value\n\n        if len(self) and self.is_monotonic_increasing:\n            # quick check\n            last = self[-1]\n            if not isna(last):\n                return last\n\n        if not self._is_multi and self.hasnans:\n            # Take advantage of cache\n            mask = self._isnan\n            if not skipna or mask.all():\n                return self._na_value\n\n        if not self._is_multi and not isinstance(self._values, np.ndarray):\n            # \"ExtensionArray\" has no attribute \"max\"\n            return self._values.max(skipna=skipna)  # type: ignore[attr-defined]\n\n        return super().max(skipna=skipna)\n\n    # --------------------------------------------------------------------\n\n    @final\n    @property\n    def shape(self) -> Shape:\n        \"\"\"\n        Return a tuple of the shape of the underlying data.\n        \"\"\"\n        # See GH#27775, GH#27384 for history/reasoning in how this is defined.\n        return (len(self),)\n\n    @final\n    def _deprecated_arg(self, value, name: str_t, methodname: str_t) -> None:\n        \"\"\"\n        Issue a FutureWarning if the arg/kwarg is not no_default.\n        \"\"\"\n        if value is not no_default:\n            warnings.warn(\n                f\"'{name}' argument in {methodname} is deprecated \"\n                \"and will be removed in a future version.  Do not pass it.\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n\n\ndef ensure_index_from_sequences(sequences, names=None) -> Index:\n    \"\"\"\n    Construct an index from sequences of data.\n\n    A single sequence returns an Index. Many sequences returns a\n    MultiIndex.\n\n    Parameters\n    ----------\n    sequences : sequence of sequences\n    names : sequence of str\n\n    Returns\n    -------\n    index : Index or MultiIndex\n\n    Examples\n    --------\n    >>> ensure_index_from_sequences([[1, 2, 3]], names=[\"name\"])\n    Int64Index([1, 2, 3], dtype='int64', name='name')\n\n    >>> ensure_index_from_sequences([[\"a\", \"a\"], [\"a\", \"b\"]], names=[\"L1\", \"L2\"])\n    MultiIndex([('a', 'a'),\n                ('a', 'b')],\n               names=['L1', 'L2'])\n\n    See Also\n    --------\n    ensure_index\n    \"\"\"\n    from pandas.core.indexes.multi import MultiIndex\n\n    if len(sequences) == 1:\n        if names is not None:\n            names = names[0]\n        return Index(sequences[0], name=names)\n    else:\n        return MultiIndex.from_arrays(sequences, names=names)\n\n\ndef ensure_index(index_like: AnyArrayLike | Sequence, copy: bool = False) -> Index:\n    \"\"\"\n    Ensure that we have an index from some index-like object.\n\n    Parameters\n    ----------\n    index_like : sequence\n        An Index or other sequence\n    copy : bool, default False\n\n    Returns\n    -------\n    index : Index or MultiIndex\n\n    See Also\n    --------\n    ensure_index_from_sequences\n\n    Examples\n    --------\n    >>> ensure_index(['a', 'b'])\n    Index(['a', 'b'], dtype='object')\n\n    >>> ensure_index([('a', 'a'),  ('b', 'c')])\n    Index([('a', 'a'), ('b', 'c')], dtype='object')\n\n    >>> ensure_index([['a', 'a'], ['b', 'c']])\n    MultiIndex([('a', 'b'),\n            ('a', 'c')],\n           )\n    \"\"\"\n    if isinstance(index_like, Index):\n        if copy:\n            index_like = index_like.copy()\n        return index_like\n\n    if isinstance(index_like, ABCSeries):\n        name = index_like.name\n        return Index._with_infer(index_like, name=name, copy=copy)\n\n    if is_iterator(index_like):\n        index_like = list(index_like)\n\n    if isinstance(index_like, list):\n        if type(index_like) is not list:\n            # must check for exactly list here because of strict type\n            # check in clean_index_list\n            index_like = list(index_like)\n\n        if len(index_like) and lib.is_all_arraylike(index_like):\n            from pandas.core.indexes.multi import MultiIndex\n\n            return MultiIndex.from_arrays(index_like)\n        else:\n            return Index._with_infer(index_like, copy=copy, tupleize_cols=False)\n    else:\n        return Index._with_infer(index_like, copy=copy)\n\n\ndef ensure_has_len(seq):\n    \"\"\"\n    If seq is an iterator, put its values into a list.\n    \"\"\"\n    try:\n        len(seq)\n    except TypeError:\n        return list(seq)\n    else:\n        return seq\n\n\ndef trim_front(strings: list[str]) -> list[str]:\n    \"\"\"\n    Trims zeros and decimal points.\n\n    Examples\n    --------\n    >>> trim_front([\" a\", \" b\"])\n    ['a', 'b']\n\n    >>> trim_front([\" a\", \" \"])\n    ['a', '']\n    \"\"\"\n    if not strings:\n        return strings\n    while all(strings) and all(x[0] == \" \" for x in strings):\n        strings = [x[1:] for x in strings]\n    return strings\n\n\ndef _validate_join_method(method: str) -> None:\n    if method not in [\"left\", \"right\", \"inner\", \"outer\"]:\n        raise ValueError(f\"do not recognize join method {method}\")\n\n\ndef maybe_extract_name(name, obj, cls) -> Hashable:\n    \"\"\"\n    If no name is passed, then extract it from data, validating hashability.\n    \"\"\"\n    if name is None and isinstance(obj, (Index, ABCSeries)):\n        # Note we don't just check for \"name\" attribute since that would\n        #  pick up e.g. dtype.name\n        name = obj.name\n\n    # GH#29069\n    if not is_hashable(name):\n        raise TypeError(f\"{cls.__name__}.name must be a hashable type\")\n\n    return name\n\n\n_cast_depr_msg = (\n    \"In a future version, passing an object-dtype arraylike to pd.Index will \"\n    \"not infer numeric values to numeric dtype (matching the Series behavior). \"\n    \"To retain the old behavior, explicitly pass the desired dtype or use the \"\n    \"desired Index subclass\"\n)\n\n\ndef _maybe_cast_data_without_dtype(\n    subarr: np.ndarray, cast_numeric_deprecated: bool = True\n) -> ArrayLike:\n    \"\"\"\n    If we have an arraylike input but no passed dtype, try to infer\n    a supported dtype.\n\n    Parameters\n    ----------\n    subarr : np.ndarray[object]\n    cast_numeric_deprecated : bool, default True\n        Whether to issue a FutureWarning when inferring numeric dtypes.\n\n    Returns\n    -------\n    np.ndarray or ExtensionArray\n    \"\"\"\n\n    result = lib.maybe_convert_objects(\n        subarr,\n        convert_datetime=True,\n        convert_timedelta=True,\n        convert_period=True,\n        convert_interval=True,\n        dtype_if_all_nat=np.dtype(\"datetime64[ns]\"),\n    )\n    if result.dtype.kind in [\"i\", \"u\", \"f\"]:\n        if not cast_numeric_deprecated:\n            # i.e. we started with a list, not an ndarray[object]\n            return result\n\n        warnings.warn(\n            \"In a future version, the Index constructor will not infer numeric \"\n            \"dtypes when passed object-dtype sequences (matching Series behavior)\",\n            FutureWarning,\n            stacklevel=3,\n        )\n    if result.dtype.kind in [\"b\", \"c\"]:\n        return subarr\n    result = ensure_wrapped_if_datetimelike(result)\n    return result\n\n\ndef get_unanimous_names(*indexes: Index) -> tuple[Hashable, ...]:\n    \"\"\"\n    Return common name if all indices agree, otherwise None (level-by-level).\n\n    Parameters\n    ----------\n    indexes : list of Index objects\n\n    Returns\n    -------\n    list\n        A list representing the unanimous 'names' found.\n    \"\"\"\n    name_tups = [tuple(i.names) for i in indexes]\n    name_sets = [{*ns} for ns in zip_longest(*name_tups)]\n    names = tuple(ns.pop() if len(ns) == 1 else None for ns in name_sets)\n    return names\n\n\ndef unpack_nested_dtype(other: _IndexT) -> _IndexT:\n    \"\"\"\n    When checking if our dtype is comparable with another, we need\n    to unpack CategoricalDtype to look at its categories.dtype.\n\n    Parameters\n    ----------\n    other : Index\n\n    Returns\n    -------\n    Index\n    \"\"\"\n    dtype = other.dtype\n    if is_categorical_dtype(dtype):\n        # If there is ever a SparseIndex, this could get dispatched\n        #  here too.\n        # error: Item  \"dtype[Any]\"/\"ExtensionDtype\" of \"Union[dtype[Any],\n        # ExtensionDtype]\" has no attribute \"categories\"\n        return dtype.categories  # type: ignore[union-attr]\n    return other\n\n\ndef _maybe_try_sort(result, sort):\n    if sort is None:\n        try:\n            result = algos.safe_sort(result)\n        except TypeError as err:\n            warnings.warn(\n                f\"{err}, sort order is undefined for incomparable objects.\",\n                RuntimeWarning,\n                stacklevel=find_stack_level(),\n            )\n    return result\n", 7201], "/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/construction.py": ["\"\"\"\nConstructor functions intended to be shared by pd.array, Series.__init__,\nand Index.__new__.\n\nThese should not depend on core.internals.\n\"\"\"\nfrom __future__ import annotations\n\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Sequence,\n    cast,\n)\nimport warnings\n\nimport numpy as np\nimport numpy.ma as ma\n\nfrom pandas._libs import lib\nfrom pandas._typing import (\n    AnyArrayLike,\n    ArrayLike,\n    Dtype,\n    DtypeObj,\n)\nfrom pandas.errors import IntCastingNaNError\nfrom pandas.util._exceptions import find_stack_level\n\nfrom pandas.core.dtypes.base import (\n    ExtensionDtype,\n    _registry as registry,\n)\nfrom pandas.core.dtypes.cast import (\n    construct_1d_arraylike_from_scalar,\n    construct_1d_object_array_from_listlike,\n    maybe_cast_to_datetime,\n    maybe_cast_to_integer_array,\n    maybe_convert_platform,\n    maybe_infer_to_datetimelike,\n    maybe_upcast,\n    sanitize_to_nanoseconds,\n)\nfrom pandas.core.dtypes.common import (\n    is_datetime64_ns_dtype,\n    is_extension_array_dtype,\n    is_float_dtype,\n    is_integer_dtype,\n    is_list_like,\n    is_object_dtype,\n    is_timedelta64_ns_dtype,\n)\nfrom pandas.core.dtypes.dtypes import (\n    DatetimeTZDtype,\n    PandasDtype,\n)\nfrom pandas.core.dtypes.generic import (\n    ABCExtensionArray,\n    ABCIndex,\n    ABCPandasArray,\n    ABCRangeIndex,\n    ABCSeries,\n)\nfrom pandas.core.dtypes.missing import isna\n\nimport pandas.core.common as com\n\nif TYPE_CHECKING:\n    from pandas import (\n        ExtensionArray,\n        Index,\n        Series,\n    )\n\n\ndef array(\n    data: Sequence[object] | AnyArrayLike,\n    dtype: Dtype | None = None,\n    copy: bool = True,\n) -> ExtensionArray:\n    \"\"\"\n    Create an array.\n\n    Parameters\n    ----------\n    data : Sequence of objects\n        The scalars inside `data` should be instances of the\n        scalar type for `dtype`. It's expected that `data`\n        represents a 1-dimensional array of data.\n\n        When `data` is an Index or Series, the underlying array\n        will be extracted from `data`.\n\n    dtype : str, np.dtype, or ExtensionDtype, optional\n        The dtype to use for the array. This may be a NumPy\n        dtype or an extension type registered with pandas using\n        :meth:`pandas.api.extensions.register_extension_dtype`.\n\n        If not specified, there are two possibilities:\n\n        1. When `data` is a :class:`Series`, :class:`Index`, or\n           :class:`ExtensionArray`, the `dtype` will be taken\n           from the data.\n        2. Otherwise, pandas will attempt to infer the `dtype`\n           from the data.\n\n        Note that when `data` is a NumPy array, ``data.dtype`` is\n        *not* used for inferring the array type. This is because\n        NumPy cannot represent all the types of data that can be\n        held in extension arrays.\n\n        Currently, pandas will infer an extension dtype for sequences of\n\n        ============================== =======================================\n        Scalar Type                    Array Type\n        ============================== =======================================\n        :class:`pandas.Interval`       :class:`pandas.arrays.IntervalArray`\n        :class:`pandas.Period`         :class:`pandas.arrays.PeriodArray`\n        :class:`datetime.datetime`     :class:`pandas.arrays.DatetimeArray`\n        :class:`datetime.timedelta`    :class:`pandas.arrays.TimedeltaArray`\n        :class:`int`                   :class:`pandas.arrays.IntegerArray`\n        :class:`float`                 :class:`pandas.arrays.FloatingArray`\n        :class:`str`                   :class:`pandas.arrays.StringArray` or\n                                       :class:`pandas.arrays.ArrowStringArray`\n        :class:`bool`                  :class:`pandas.arrays.BooleanArray`\n        ============================== =======================================\n\n        The ExtensionArray created when the scalar type is :class:`str` is determined by\n        ``pd.options.mode.string_storage`` if the dtype is not explicitly given.\n\n        For all other cases, NumPy's usual inference rules will be used.\n\n        .. versionchanged:: 1.0.0\n\n           Pandas infers nullable-integer dtype for integer data,\n           string dtype for string data, and nullable-boolean dtype\n           for boolean data.\n\n        .. versionchanged:: 1.2.0\n\n            Pandas now also infers nullable-floating dtype for float-like\n            input data\n\n    copy : bool, default True\n        Whether to copy the data, even if not necessary. Depending\n        on the type of `data`, creating the new array may require\n        copying data, even if ``copy=False``.\n\n    Returns\n    -------\n    ExtensionArray\n        The newly created array.\n\n    Raises\n    ------\n    ValueError\n        When `data` is not 1-dimensional.\n\n    See Also\n    --------\n    numpy.array : Construct a NumPy array.\n    Series : Construct a pandas Series.\n    Index : Construct a pandas Index.\n    arrays.PandasArray : ExtensionArray wrapping a NumPy array.\n    Series.array : Extract the array stored within a Series.\n\n    Notes\n    -----\n    Omitting the `dtype` argument means pandas will attempt to infer the\n    best array type from the values in the data. As new array types are\n    added by pandas and 3rd party libraries, the \"best\" array type may\n    change. We recommend specifying `dtype` to ensure that\n\n    1. the correct array type for the data is returned\n    2. the returned array type doesn't change as new extension types\n       are added by pandas and third-party libraries\n\n    Additionally, if the underlying memory representation of the returned\n    array matters, we recommend specifying the `dtype` as a concrete object\n    rather than a string alias or allowing it to be inferred. For example,\n    a future version of pandas or a 3rd-party library may include a\n    dedicated ExtensionArray for string data. In this event, the following\n    would no longer return a :class:`arrays.PandasArray` backed by a NumPy\n    array.\n\n    >>> pd.array(['a', 'b'], dtype=str)\n    <PandasArray>\n    ['a', 'b']\n    Length: 2, dtype: str32\n\n    This would instead return the new ExtensionArray dedicated for string\n    data. If you really need the new array to be backed by a  NumPy array,\n    specify that in the dtype.\n\n    >>> pd.array(['a', 'b'], dtype=np.dtype(\"<U1\"))\n    <PandasArray>\n    ['a', 'b']\n    Length: 2, dtype: str32\n\n    Finally, Pandas has arrays that mostly overlap with NumPy\n\n      * :class:`arrays.DatetimeArray`\n      * :class:`arrays.TimedeltaArray`\n\n    When data with a ``datetime64[ns]`` or ``timedelta64[ns]`` dtype is\n    passed, pandas will always return a ``DatetimeArray`` or ``TimedeltaArray``\n    rather than a ``PandasArray``. This is for symmetry with the case of\n    timezone-aware data, which NumPy does not natively support.\n\n    >>> pd.array(['2015', '2016'], dtype='datetime64[ns]')\n    <DatetimeArray>\n    ['2015-01-01 00:00:00', '2016-01-01 00:00:00']\n    Length: 2, dtype: datetime64[ns]\n\n    >>> pd.array([\"1H\", \"2H\"], dtype='timedelta64[ns]')\n    <TimedeltaArray>\n    ['0 days 01:00:00', '0 days 02:00:00']\n    Length: 2, dtype: timedelta64[ns]\n\n    Examples\n    --------\n    If a dtype is not specified, pandas will infer the best dtype from the values.\n    See the description of `dtype` for the types pandas infers for.\n\n    >>> pd.array([1, 2])\n    <IntegerArray>\n    [1, 2]\n    Length: 2, dtype: Int64\n\n    >>> pd.array([1, 2, np.nan])\n    <IntegerArray>\n    [1, 2, <NA>]\n    Length: 3, dtype: Int64\n\n    >>> pd.array([1.1, 2.2])\n    <FloatingArray>\n    [1.1, 2.2]\n    Length: 2, dtype: Float64\n\n    >>> pd.array([\"a\", None, \"c\"])\n    <StringArray>\n    ['a', <NA>, 'c']\n    Length: 3, dtype: string\n\n    >>> with pd.option_context(\"string_storage\", \"pyarrow\"):\n    ...     arr = pd.array([\"a\", None, \"c\"])\n    ...\n    >>> arr\n    <ArrowStringArray>\n    ['a', <NA>, 'c']\n    Length: 3, dtype: string\n\n    >>> pd.array([pd.Period('2000', freq=\"D\"), pd.Period(\"2000\", freq=\"D\")])\n    <PeriodArray>\n    ['2000-01-01', '2000-01-01']\n    Length: 2, dtype: period[D]\n\n    You can use the string alias for `dtype`\n\n    >>> pd.array(['a', 'b', 'a'], dtype='category')\n    ['a', 'b', 'a']\n    Categories (2, object): ['a', 'b']\n\n    Or specify the actual dtype\n\n    >>> pd.array(['a', 'b', 'a'],\n    ...          dtype=pd.CategoricalDtype(['a', 'b', 'c'], ordered=True))\n    ['a', 'b', 'a']\n    Categories (3, object): ['a' < 'b' < 'c']\n\n    If pandas does not infer a dedicated extension type a\n    :class:`arrays.PandasArray` is returned.\n\n    >>> pd.array([1 + 1j, 3 + 2j])\n    <PandasArray>\n    [(1+1j), (3+2j)]\n    Length: 2, dtype: complex128\n\n    As mentioned in the \"Notes\" section, new extension types may be added\n    in the future (by pandas or 3rd party libraries), causing the return\n    value to no longer be a :class:`arrays.PandasArray`. Specify the `dtype`\n    as a NumPy dtype if you need to ensure there's no future change in\n    behavior.\n\n    >>> pd.array([1, 2], dtype=np.dtype(\"int32\"))\n    <PandasArray>\n    [1, 2]\n    Length: 2, dtype: int32\n\n    `data` must be 1-dimensional. A ValueError is raised when the input\n    has the wrong dimensionality.\n\n    >>> pd.array(1)\n    Traceback (most recent call last):\n      ...\n    ValueError: Cannot pass scalar '1' to 'pandas.array'.\n    \"\"\"\n    from pandas.core.arrays import (\n        BooleanArray,\n        DatetimeArray,\n        ExtensionArray,\n        FloatingArray,\n        IntegerArray,\n        IntervalArray,\n        PandasArray,\n        PeriodArray,\n        TimedeltaArray,\n    )\n    from pandas.core.arrays.string_ import StringDtype\n\n    if lib.is_scalar(data):\n        msg = f\"Cannot pass scalar '{data}' to 'pandas.array'.\"\n        raise ValueError(msg)\n\n    if dtype is None and isinstance(data, (ABCSeries, ABCIndex, ExtensionArray)):\n        # Note: we exclude np.ndarray here, will do type inference on it\n        dtype = data.dtype\n\n    data = extract_array(data, extract_numpy=True)\n\n    # this returns None for not-found dtypes.\n    if isinstance(dtype, str):\n        dtype = registry.find(dtype) or dtype\n\n    if is_extension_array_dtype(dtype):\n        cls = cast(ExtensionDtype, dtype).construct_array_type()\n        return cls._from_sequence(data, dtype=dtype, copy=copy)\n\n    if dtype is None:\n        inferred_dtype = lib.infer_dtype(data, skipna=True)\n        if inferred_dtype == \"period\":\n            return PeriodArray._from_sequence(data, copy=copy)\n\n        elif inferred_dtype == \"interval\":\n            return IntervalArray(data, copy=copy)\n\n        elif inferred_dtype.startswith(\"datetime\"):\n            # datetime, datetime64\n            try:\n                return DatetimeArray._from_sequence(data, copy=copy)\n            except ValueError:\n                # Mixture of timezones, fall back to PandasArray\n                pass\n\n        elif inferred_dtype.startswith(\"timedelta\"):\n            # timedelta, timedelta64\n            return TimedeltaArray._from_sequence(data, copy=copy)\n\n        elif inferred_dtype == \"string\":\n            # StringArray/ArrowStringArray depending on pd.options.mode.string_storage\n            return StringDtype().construct_array_type()._from_sequence(data, copy=copy)\n\n        elif inferred_dtype == \"integer\":\n            return IntegerArray._from_sequence(data, copy=copy)\n\n        elif (\n            inferred_dtype in (\"floating\", \"mixed-integer-float\")\n            and getattr(data, \"dtype\", None) != np.float16\n        ):\n            # GH#44715 Exclude np.float16 bc FloatingArray does not support it;\n            #  we will fall back to PandasArray.\n            return FloatingArray._from_sequence(data, copy=copy)\n\n        elif inferred_dtype == \"boolean\":\n            return BooleanArray._from_sequence(data, copy=copy)\n\n    # Pandas overrides NumPy for\n    #   1. datetime64[ns]\n    #   2. timedelta64[ns]\n    # so that a DatetimeArray is returned.\n    if is_datetime64_ns_dtype(dtype):\n        return DatetimeArray._from_sequence(data, dtype=dtype, copy=copy)\n    elif is_timedelta64_ns_dtype(dtype):\n        return TimedeltaArray._from_sequence(data, dtype=dtype, copy=copy)\n\n    return PandasArray._from_sequence(data, dtype=dtype, copy=copy)\n\n\ndef extract_array(\n    obj: object, extract_numpy: bool = False, extract_range: bool = False\n) -> Any | ArrayLike:\n    \"\"\"\n    Extract the ndarray or ExtensionArray from a Series or Index.\n\n    For all other types, `obj` is just returned as is.\n\n    Parameters\n    ----------\n    obj : object\n        For Series / Index, the underlying ExtensionArray is unboxed.\n\n    extract_numpy : bool, default False\n        Whether to extract the ndarray from a PandasArray.\n\n    extract_range : bool, default False\n        If we have a RangeIndex, return range._values if True\n        (which is a materialized integer ndarray), otherwise return unchanged.\n\n    Returns\n    -------\n    arr : object\n\n    Examples\n    --------\n    >>> extract_array(pd.Series(['a', 'b', 'c'], dtype='category'))\n    ['a', 'b', 'c']\n    Categories (3, object): ['a', 'b', 'c']\n\n    Other objects like lists, arrays, and DataFrames are just passed through.\n\n    >>> extract_array([1, 2, 3])\n    [1, 2, 3]\n\n    For an ndarray-backed Series / Index the ndarray is returned.\n\n    >>> extract_array(pd.Series([1, 2, 3]))\n    array([1, 2, 3])\n\n    To extract all the way down to the ndarray, pass ``extract_numpy=True``.\n\n    >>> extract_array(pd.Series([1, 2, 3]), extract_numpy=True)\n    array([1, 2, 3])\n    \"\"\"\n    if isinstance(obj, (ABCIndex, ABCSeries)):\n        if isinstance(obj, ABCRangeIndex):\n            if extract_range:\n                return obj._values\n            return obj\n\n        obj = obj._values\n\n    elif extract_numpy and isinstance(obj, ABCPandasArray):\n        obj = obj.to_numpy()\n\n    return obj\n\n\ndef ensure_wrapped_if_datetimelike(arr):\n    \"\"\"\n    Wrap datetime64 and timedelta64 ndarrays in DatetimeArray/TimedeltaArray.\n    \"\"\"\n    if isinstance(arr, np.ndarray):\n        if arr.dtype.kind == \"M\":\n            from pandas.core.arrays import DatetimeArray\n\n            return DatetimeArray._from_sequence(arr)\n\n        elif arr.dtype.kind == \"m\":\n            from pandas.core.arrays import TimedeltaArray\n\n            return TimedeltaArray._from_sequence(arr)\n\n    return arr\n\n\ndef sanitize_masked_array(data: ma.MaskedArray) -> np.ndarray:\n    \"\"\"\n    Convert numpy MaskedArray to ensure mask is softened.\n    \"\"\"\n    mask = ma.getmaskarray(data)\n    if mask.any():\n        data, fill_value = maybe_upcast(data, copy=True)\n        data.soften_mask()  # set hardmask False if it was True\n        data[mask] = fill_value\n    else:\n        data = data.copy()\n    return data\n\n\ndef sanitize_array(\n    data,\n    index: Index | None,\n    dtype: DtypeObj | None = None,\n    copy: bool = False,\n    raise_cast_failure: bool = True,\n    *,\n    allow_2d: bool = False,\n) -> ArrayLike:\n    \"\"\"\n    Sanitize input data to an ndarray or ExtensionArray, copy if specified,\n    coerce to the dtype if specified.\n\n    Parameters\n    ----------\n    data : Any\n    index : Index or None, default None\n    dtype : np.dtype, ExtensionDtype, or None, default None\n    copy : bool, default False\n    raise_cast_failure : bool, default True\n    allow_2d : bool, default False\n        If False, raise if we have a 2D Arraylike.\n\n    Returns\n    -------\n    np.ndarray or ExtensionArray\n\n    Notes\n    -----\n    raise_cast_failure=False is only intended to be True when called from the\n    DataFrame constructor, as the dtype keyword there may be interpreted as only\n    applying to a subset of columns, see GH#24435.\n    \"\"\"\n    if isinstance(data, ma.MaskedArray):\n        data = sanitize_masked_array(data)\n\n    if isinstance(dtype, PandasDtype):\n        # Avoid ending up with a PandasArray\n        dtype = dtype.numpy_dtype\n\n    # extract ndarray or ExtensionArray, ensure we have no PandasArray\n    data = extract_array(data, extract_numpy=True)\n\n    if isinstance(data, np.ndarray) and data.ndim == 0:\n        if dtype is None:\n            dtype = data.dtype\n        data = lib.item_from_zerodim(data)\n    elif isinstance(data, range):\n        # GH#16804\n        data = range_to_ndarray(data)\n        copy = False\n\n    if not is_list_like(data):\n        if index is None:\n            raise ValueError(\"index must be specified when data is not list-like\")\n        data = construct_1d_arraylike_from_scalar(data, len(index), dtype)\n        return data\n\n    # GH#846\n    if isinstance(data, np.ndarray):\n        if isinstance(data, np.matrix):\n            data = data.A\n\n        if dtype is not None and is_float_dtype(data.dtype) and is_integer_dtype(dtype):\n            # possibility of nan -> garbage\n            try:\n                subarr = _try_cast(data, dtype, copy, True)\n            except IntCastingNaNError:\n                warnings.warn(\n                    \"In a future version, passing float-dtype values containing NaN \"\n                    \"and an integer dtype will raise IntCastingNaNError \"\n                    \"(subclass of ValueError) instead of silently ignoring the \"\n                    \"passed dtype. To retain the old behavior, call Series(arr) or \"\n                    \"DataFrame(arr) without passing a dtype.\",\n                    FutureWarning,\n                    stacklevel=find_stack_level(),\n                )\n                subarr = np.array(data, copy=copy)\n            except ValueError:\n                if not raise_cast_failure:\n                    # i.e. called via DataFrame constructor\n                    warnings.warn(\n                        \"In a future version, passing float-dtype values and an \"\n                        \"integer dtype to DataFrame will retain floating dtype \"\n                        \"if they cannot be cast losslessly (matching Series behavior). \"\n                        \"To retain the old behavior, use DataFrame(data).astype(dtype)\",\n                        FutureWarning,\n                        stacklevel=find_stack_level(),\n                    )\n                    # GH#40110 until the deprecation is enforced, we _dont_\n                    #  ignore the dtype for DataFrame, and _do_ cast even though\n                    #  it is lossy.\n                    dtype = cast(np.dtype, dtype)\n                    return np.array(data, dtype=dtype, copy=copy)\n                subarr = np.array(data, copy=copy)\n        else:\n            # we will try to copy by-definition here\n            subarr = _try_cast(data, dtype, copy, raise_cast_failure)\n\n    elif isinstance(data, ABCExtensionArray):\n        # it is already ensured above this is not a PandasArray\n        subarr = data\n\n        if dtype is not None:\n            subarr = subarr.astype(dtype, copy=copy)\n        elif copy:\n            subarr = subarr.copy()\n\n    else:\n        if isinstance(data, (set, frozenset)):\n            # Raise only for unordered sets, e.g., not for dict_keys\n            raise TypeError(f\"'{type(data).__name__}' type is unordered\")\n\n        # materialize e.g. generators, convert e.g. tuples, abc.ValueView\n        if hasattr(data, \"__array__\"):\n            # e.g. dask array GH#38645\n            data = np.asarray(data)\n        else:\n            data = list(data)\n\n        if dtype is not None or len(data) == 0:\n            subarr = _try_cast(data, dtype, copy, raise_cast_failure)\n        else:\n            subarr = maybe_convert_platform(data)\n            if subarr.dtype == object:\n                subarr = cast(np.ndarray, subarr)\n                subarr = maybe_infer_to_datetimelike(subarr)\n\n    subarr = _sanitize_ndim(subarr, data, dtype, index, allow_2d=allow_2d)\n\n    if isinstance(subarr, np.ndarray):\n        # at this point we should have dtype be None or subarr.dtype == dtype\n        dtype = cast(np.dtype, dtype)\n        subarr = _sanitize_str_dtypes(subarr, data, dtype, copy)\n\n    return subarr\n\n\ndef range_to_ndarray(rng: range) -> np.ndarray:\n    \"\"\"\n    Cast a range object to ndarray.\n    \"\"\"\n    # GH#30171 perf avoid realizing range as a list in np.array\n    try:\n        arr = np.arange(rng.start, rng.stop, rng.step, dtype=\"int64\")\n    except OverflowError:\n        # GH#30173 handling for ranges that overflow int64\n        if (rng.start >= 0 and rng.step > 0) or (rng.stop >= 0 and rng.step < 0):\n            try:\n                arr = np.arange(rng.start, rng.stop, rng.step, dtype=\"uint64\")\n            except OverflowError:\n                arr = construct_1d_object_array_from_listlike(list(rng))\n        else:\n            arr = construct_1d_object_array_from_listlike(list(rng))\n    return arr\n\n\ndef _sanitize_ndim(\n    result: ArrayLike,\n    data,\n    dtype: DtypeObj | None,\n    index: Index | None,\n    *,\n    allow_2d: bool = False,\n) -> ArrayLike:\n    \"\"\"\n    Ensure we have a 1-dimensional result array.\n    \"\"\"\n    if getattr(result, \"ndim\", 0) == 0:\n        raise ValueError(\"result should be arraylike with ndim > 0\")\n\n    elif result.ndim == 1:\n        # the result that we want\n        result = _maybe_repeat(result, index)\n\n    elif result.ndim > 1:\n        if isinstance(data, np.ndarray):\n            if allow_2d:\n                return result\n            raise ValueError(\"Data must be 1-dimensional\")\n        if is_object_dtype(dtype) and isinstance(dtype, ExtensionDtype):\n            # i.e. PandasDtype(\"O\")\n\n            result = com.asarray_tuplesafe(data, dtype=np.dtype(\"object\"))\n            cls = dtype.construct_array_type()\n            result = cls._from_sequence(result, dtype=dtype)\n        else:\n            # error: Argument \"dtype\" to \"asarray_tuplesafe\" has incompatible type\n            # \"Union[dtype[Any], ExtensionDtype, None]\"; expected \"Union[str,\n            # dtype[Any], None]\"\n            result = com.asarray_tuplesafe(data, dtype=dtype)  # type: ignore[arg-type]\n    return result\n\n\ndef _sanitize_str_dtypes(\n    result: np.ndarray, data, dtype: np.dtype | None, copy: bool\n) -> np.ndarray:\n    \"\"\"\n    Ensure we have a dtype that is supported by pandas.\n    \"\"\"\n\n    # This is to prevent mixed-type Series getting all casted to\n    # NumPy string type, e.g. NaN --> '-1#IND'.\n    if issubclass(result.dtype.type, str):\n        # GH#16605\n        # If not empty convert the data to dtype\n        # GH#19853: If data is a scalar, result has already the result\n        if not lib.is_scalar(data):\n            if not np.all(isna(data)):\n                data = np.array(data, dtype=dtype, copy=False)\n            result = np.array(data, dtype=object, copy=copy)\n    return result\n\n\ndef _maybe_repeat(arr: ArrayLike, index: Index | None) -> ArrayLike:\n    \"\"\"\n    If we have a length-1 array and an index describing how long we expect\n    the result to be, repeat the array.\n    \"\"\"\n    if index is not None:\n        if 1 == len(arr) != len(index):\n            arr = arr.repeat(len(index))\n    return arr\n\n\ndef _try_cast(\n    arr: list | np.ndarray,\n    dtype: DtypeObj | None,\n    copy: bool,\n    raise_cast_failure: bool,\n) -> ArrayLike:\n    \"\"\"\n    Convert input to numpy ndarray and optionally cast to a given dtype.\n\n    Parameters\n    ----------\n    arr : ndarray or list\n        Excludes: ExtensionArray, Series, Index.\n    dtype : np.dtype, ExtensionDtype or None\n    copy : bool\n        If False, don't copy the data if not needed.\n    raise_cast_failure : bool\n        If True, and if a dtype is specified, raise errors during casting.\n        Otherwise an object array is returned.\n\n    Returns\n    -------\n    np.ndarray or ExtensionArray\n    \"\"\"\n    is_ndarray = isinstance(arr, np.ndarray)\n\n    if dtype is None:\n        # perf shortcut as this is the most common case\n        if is_ndarray:\n            arr = cast(np.ndarray, arr)\n            if arr.dtype != object:\n                return sanitize_to_nanoseconds(arr, copy=copy)\n\n            out = maybe_infer_to_datetimelike(arr)\n            if out is arr and copy:\n                out = out.copy()\n            return out\n\n        else:\n            # i.e. list\n            varr = np.array(arr, copy=False)\n            # filter out cases that we _dont_ want to go through\n            #  maybe_infer_to_datetimelike\n            if varr.dtype != object or varr.size == 0:\n                return varr\n            return maybe_infer_to_datetimelike(varr)\n\n    elif isinstance(dtype, ExtensionDtype):\n        # create an extension array from its dtype\n        if isinstance(dtype, DatetimeTZDtype):\n            # We can't go through _from_sequence because it handles dt64naive\n            #  data differently; _from_sequence treats naive as wall times,\n            #  while maybe_cast_to_datetime treats it as UTC\n            #  see test_maybe_promote_any_numpy_dtype_with_datetimetz\n\n            return maybe_cast_to_datetime(arr, dtype)\n            # TODO: copy?\n\n        array_type = dtype.construct_array_type()._from_sequence\n        subarr = array_type(arr, dtype=dtype, copy=copy)\n        return subarr\n\n    elif is_object_dtype(dtype):\n        if not is_ndarray:\n            subarr = construct_1d_object_array_from_listlike(arr)\n            return subarr\n        return ensure_wrapped_if_datetimelike(arr).astype(dtype, copy=copy)\n\n    elif dtype.kind == \"U\":\n        # TODO: test cases with arr.dtype.kind in [\"m\", \"M\"]\n        return lib.ensure_string_array(arr, convert_na_value=False, copy=copy)\n\n    elif dtype.kind in [\"m\", \"M\"]:\n        return maybe_cast_to_datetime(arr, dtype)\n\n    try:\n        # GH#15832: Check if we are requesting a numeric dtype and\n        # that we can convert the data to the requested dtype.\n        if is_integer_dtype(dtype):\n            # this will raise if we have e.g. floats\n\n            subarr = maybe_cast_to_integer_array(arr, dtype)\n        else:\n            # 4 tests fail if we move this to a try/except/else; see\n            #  test_constructor_compound_dtypes, test_constructor_cast_failure\n            #  test_constructor_dict_cast2, test_loc_setitem_dtype\n            subarr = np.array(arr, dtype=dtype, copy=copy)\n\n    except (ValueError, TypeError):\n        if raise_cast_failure:\n            raise\n        else:\n            # we only get here with raise_cast_failure False, which means\n            #  called via the DataFrame constructor\n            # GH#24435\n            warnings.warn(\n                f\"Could not cast to {dtype}, falling back to object. This \"\n                \"behavior is deprecated. In a future version, when a dtype is \"\n                \"passed to 'DataFrame', either all columns will be cast to that \"\n                \"dtype, or a TypeError will be raised.\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n            subarr = np.array(arr, dtype=object, copy=copy)\n    return subarr\n\n\ndef is_empty_data(data: Any) -> bool:\n    \"\"\"\n    Utility to check if a Series is instantiated with empty data,\n    which does not contain dtype information.\n\n    Parameters\n    ----------\n    data : array-like, Iterable, dict, or scalar value\n        Contains data stored in Series.\n\n    Returns\n    -------\n    bool\n    \"\"\"\n    is_none = data is None\n    is_list_like_without_dtype = is_list_like(data) and not hasattr(data, \"dtype\")\n    is_simple_empty = is_list_like_without_dtype and not data\n    return is_none or is_simple_empty\n\n\ndef create_series_with_explicit_dtype(\n    data: Any = None,\n    index: ArrayLike | Index | None = None,\n    dtype: Dtype | None = None,\n    name: str | None = None,\n    copy: bool = False,\n    fastpath: bool = False,\n    dtype_if_empty: Dtype = object,\n) -> Series:\n    \"\"\"\n    Helper to pass an explicit dtype when instantiating an empty Series.\n\n    This silences a DeprecationWarning described in GitHub-17261.\n\n    Parameters\n    ----------\n    data : Mirrored from Series.__init__\n    index : Mirrored from Series.__init__\n    dtype : Mirrored from Series.__init__\n    name : Mirrored from Series.__init__\n    copy : Mirrored from Series.__init__\n    fastpath : Mirrored from Series.__init__\n    dtype_if_empty : str, numpy.dtype, or ExtensionDtype\n        This dtype will be passed explicitly if an empty Series will\n        be instantiated.\n\n    Returns\n    -------\n    Series\n    \"\"\"\n    from pandas.core.series import Series\n\n    if is_empty_data(data) and dtype is None:\n        dtype = dtype_if_empty\n    return Series(\n        data=data, index=index, dtype=dtype, name=name, copy=copy, fastpath=fastpath\n    )\n", 858], "/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/abc.py": ["# Copyright 2007 Google, Inc. All Rights Reserved.\n# Licensed to PSF under a Contributor Agreement.\n\n\"\"\"Abstract Base Classes (ABCs) according to PEP 3119.\"\"\"\n\n\ndef abstractmethod(funcobj):\n    \"\"\"A decorator indicating abstract methods.\n\n    Requires that the metaclass is ABCMeta or derived from it.  A\n    class that has a metaclass derived from ABCMeta cannot be\n    instantiated unless all of its abstract methods are overridden.\n    The abstract methods can be called using any of the normal\n    'super' call mechanisms.  abstractmethod() may be used to declare\n    abstract methods for properties and descriptors.\n\n    Usage:\n\n        class C(metaclass=ABCMeta):\n            @abstractmethod\n            def my_abstract_method(self, ...):\n                ...\n    \"\"\"\n    funcobj.__isabstractmethod__ = True\n    return funcobj\n\n\nclass abstractclassmethod(classmethod):\n    \"\"\"A decorator indicating abstract classmethods.\n\n    Deprecated, use 'classmethod' with 'abstractmethod' instead:\n\n        class C(ABC):\n            @classmethod\n            @abstractmethod\n            def my_abstract_classmethod(cls, ...):\n                ...\n\n    \"\"\"\n\n    __isabstractmethod__ = True\n\n    def __init__(self, callable):\n        callable.__isabstractmethod__ = True\n        super().__init__(callable)\n\n\nclass abstractstaticmethod(staticmethod):\n    \"\"\"A decorator indicating abstract staticmethods.\n\n    Deprecated, use 'staticmethod' with 'abstractmethod' instead:\n\n        class C(ABC):\n            @staticmethod\n            @abstractmethod\n            def my_abstract_staticmethod(...):\n                ...\n\n    \"\"\"\n\n    __isabstractmethod__ = True\n\n    def __init__(self, callable):\n        callable.__isabstractmethod__ = True\n        super().__init__(callable)\n\n\nclass abstractproperty(property):\n    \"\"\"A decorator indicating abstract properties.\n\n    Deprecated, use 'property' with 'abstractmethod' instead:\n\n        class C(ABC):\n            @property\n            @abstractmethod\n            def my_abstract_property(self):\n                ...\n\n    \"\"\"\n\n    __isabstractmethod__ = True\n\n\ntry:\n    from _abc import (get_cache_token, _abc_init, _abc_register,\n                      _abc_instancecheck, _abc_subclasscheck, _get_dump,\n                      _reset_registry, _reset_caches)\nexcept ImportError:\n    from _py_abc import ABCMeta, get_cache_token\n    ABCMeta.__module__ = 'abc'\nelse:\n    class ABCMeta(type):\n        \"\"\"Metaclass for defining Abstract Base Classes (ABCs).\n\n        Use this metaclass to create an ABC.  An ABC can be subclassed\n        directly, and then acts as a mix-in class.  You can also register\n        unrelated concrete classes (even built-in classes) and unrelated\n        ABCs as 'virtual subclasses' -- these and their descendants will\n        be considered subclasses of the registering ABC by the built-in\n        issubclass() function, but the registering ABC won't show up in\n        their MRO (Method Resolution Order) nor will method\n        implementations defined by the registering ABC be callable (not\n        even via super()).\n        \"\"\"\n        def __new__(mcls, name, bases, namespace, **kwargs):\n            cls = super().__new__(mcls, name, bases, namespace, **kwargs)\n            _abc_init(cls)\n            return cls\n\n        def register(cls, subclass):\n            \"\"\"Register a virtual subclass of an ABC.\n\n            Returns the subclass, to allow usage as a class decorator.\n            \"\"\"\n            return _abc_register(cls, subclass)\n\n        def __instancecheck__(cls, instance):\n            \"\"\"Override for isinstance(instance, cls).\"\"\"\n            return _abc_instancecheck(cls, instance)\n\n        def __subclasscheck__(cls, subclass):\n            \"\"\"Override for issubclass(subclass, cls).\"\"\"\n            return _abc_subclasscheck(cls, subclass)\n\n        def _dump_registry(cls, file=None):\n            \"\"\"Debug helper to print the ABC registry.\"\"\"\n            print(f\"Class: {cls.__module__}.{cls.__qualname__}\", file=file)\n            print(f\"Inv. counter: {get_cache_token()}\", file=file)\n            (_abc_registry, _abc_cache, _abc_negative_cache,\n             _abc_negative_cache_version) = _get_dump(cls)\n            print(f\"_abc_registry: {_abc_registry!r}\", file=file)\n            print(f\"_abc_cache: {_abc_cache!r}\", file=file)\n            print(f\"_abc_negative_cache: {_abc_negative_cache!r}\", file=file)\n            print(f\"_abc_negative_cache_version: {_abc_negative_cache_version!r}\",\n                  file=file)\n\n        def _abc_registry_clear(cls):\n            \"\"\"Clear the registry (for debugging or testing).\"\"\"\n            _reset_registry(cls)\n\n        def _abc_caches_clear(cls):\n            \"\"\"Clear the caches (for debugging or testing).\"\"\"\n            _reset_caches(cls)\n\n\ndef update_abstractmethods(cls):\n    \"\"\"Recalculate the set of abstract methods of an abstract class.\n\n    If a class has had one of its abstract methods implemented after the\n    class was created, the method will not be considered implemented until\n    this function is called. Alternatively, if a new abstract method has been\n    added to the class, it will only be considered an abstract method of the\n    class after this function is called.\n\n    This function should be called before any use is made of the class,\n    usually in class decorators that add methods to the subject class.\n\n    Returns cls, to allow usage as a class decorator.\n\n    If cls is not an instance of ABCMeta, does nothing.\n    \"\"\"\n    if not hasattr(cls, '__abstractmethods__'):\n        # We check for __abstractmethods__ here because cls might by a C\n        # implementation or a python implementation (especially during\n        # testing), and we want to handle both cases.\n        return cls\n\n    abstracts = set()\n    # Check the existing abstract methods of the parents, keep only the ones\n    # that are not implemented.\n    for scls in cls.__bases__:\n        for name in getattr(scls, '__abstractmethods__', ()):\n            value = getattr(cls, name, None)\n            if getattr(value, \"__isabstractmethod__\", False):\n                abstracts.add(name)\n    # Also add any other newly added abstract methods.\n    for name, value in cls.__dict__.items():\n        if getattr(value, \"__isabstractmethod__\", False):\n            abstracts.add(name)\n    cls.__abstractmethods__ = frozenset(abstracts)\n    return cls\n\n\nclass ABC(metaclass=ABCMeta):\n    \"\"\"Helper class that provides a standard way to create an ABC using\n    inheritance.\n    \"\"\"\n    __slots__ = ()\n", 188], "/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/_collections_abc.py": ["# Copyright 2007 Google, Inc. All Rights Reserved.\n# Licensed to PSF under a Contributor Agreement.\n\n\"\"\"Abstract Base Classes (ABCs) for collections, according to PEP 3119.\n\nUnit tests are in test_collections.\n\"\"\"\n\nfrom abc import ABCMeta, abstractmethod\nimport sys\n\nGenericAlias = type(list[int])\nEllipsisType = type(...)\ndef _f(): pass\nFunctionType = type(_f)\ndel _f\n\n__all__ = [\"Awaitable\", \"Coroutine\",\n           \"AsyncIterable\", \"AsyncIterator\", \"AsyncGenerator\",\n           \"Hashable\", \"Iterable\", \"Iterator\", \"Generator\", \"Reversible\",\n           \"Sized\", \"Container\", \"Callable\", \"Collection\",\n           \"Set\", \"MutableSet\",\n           \"Mapping\", \"MutableMapping\",\n           \"MappingView\", \"KeysView\", \"ItemsView\", \"ValuesView\",\n           \"Sequence\", \"MutableSequence\",\n           \"ByteString\",\n           ]\n\n# This module has been renamed from collections.abc to _collections_abc to\n# speed up interpreter startup. Some of the types such as MutableMapping are\n# required early but collections module imports a lot of other modules.\n# See issue #19218\n__name__ = \"collections.abc\"\n\n# Private list of types that we want to register with the various ABCs\n# so that they will pass tests like:\n#       it = iter(somebytearray)\n#       assert isinstance(it, Iterable)\n# Note:  in other implementations, these types might not be distinct\n# and they may have their own implementation specific types that\n# are not included on this list.\nbytes_iterator = type(iter(b''))\nbytearray_iterator = type(iter(bytearray()))\n#callable_iterator = ???\ndict_keyiterator = type(iter({}.keys()))\ndict_valueiterator = type(iter({}.values()))\ndict_itemiterator = type(iter({}.items()))\nlist_iterator = type(iter([]))\nlist_reverseiterator = type(iter(reversed([])))\nrange_iterator = type(iter(range(0)))\nlongrange_iterator = type(iter(range(1 << 1000)))\nset_iterator = type(iter(set()))\nstr_iterator = type(iter(\"\"))\ntuple_iterator = type(iter(()))\nzip_iterator = type(iter(zip()))\n## views ##\ndict_keys = type({}.keys())\ndict_values = type({}.values())\ndict_items = type({}.items())\n## misc ##\nmappingproxy = type(type.__dict__)\ngenerator = type((lambda: (yield))())\n## coroutine ##\nasync def _coro(): pass\n_coro = _coro()\ncoroutine = type(_coro)\n_coro.close()  # Prevent ResourceWarning\ndel _coro\n## asynchronous generator ##\nasync def _ag(): yield\n_ag = _ag()\nasync_generator = type(_ag)\ndel _ag\n\n\n### ONE-TRICK PONIES ###\n\ndef _check_methods(C, *methods):\n    mro = C.__mro__\n    for method in methods:\n        for B in mro:\n            if method in B.__dict__:\n                if B.__dict__[method] is None:\n                    return NotImplemented\n                break\n        else:\n            return NotImplemented\n    return True\n\nclass Hashable(metaclass=ABCMeta):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __hash__(self):\n        return 0\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Hashable:\n            return _check_methods(C, \"__hash__\")\n        return NotImplemented\n\n\nclass Awaitable(metaclass=ABCMeta):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __await__(self):\n        yield\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Awaitable:\n            return _check_methods(C, \"__await__\")\n        return NotImplemented\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n\nclass Coroutine(Awaitable):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def send(self, value):\n        \"\"\"Send a value into the coroutine.\n        Return next yielded value or raise StopIteration.\n        \"\"\"\n        raise StopIteration\n\n    @abstractmethod\n    def throw(self, typ, val=None, tb=None):\n        \"\"\"Raise an exception in the coroutine.\n        Return next yielded value or raise StopIteration.\n        \"\"\"\n        if val is None:\n            if tb is None:\n                raise typ\n            val = typ()\n        if tb is not None:\n            val = val.with_traceback(tb)\n        raise val\n\n    def close(self):\n        \"\"\"Raise GeneratorExit inside coroutine.\n        \"\"\"\n        try:\n            self.throw(GeneratorExit)\n        except (GeneratorExit, StopIteration):\n            pass\n        else:\n            raise RuntimeError(\"coroutine ignored GeneratorExit\")\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Coroutine:\n            return _check_methods(C, '__await__', 'send', 'throw', 'close')\n        return NotImplemented\n\n\nCoroutine.register(coroutine)\n\n\nclass AsyncIterable(metaclass=ABCMeta):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __aiter__(self):\n        return AsyncIterator()\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is AsyncIterable:\n            return _check_methods(C, \"__aiter__\")\n        return NotImplemented\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n\nclass AsyncIterator(AsyncIterable):\n\n    __slots__ = ()\n\n    @abstractmethod\n    async def __anext__(self):\n        \"\"\"Return the next item or raise StopAsyncIteration when exhausted.\"\"\"\n        raise StopAsyncIteration\n\n    def __aiter__(self):\n        return self\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is AsyncIterator:\n            return _check_methods(C, \"__anext__\", \"__aiter__\")\n        return NotImplemented\n\n\nclass AsyncGenerator(AsyncIterator):\n\n    __slots__ = ()\n\n    async def __anext__(self):\n        \"\"\"Return the next item from the asynchronous generator.\n        When exhausted, raise StopAsyncIteration.\n        \"\"\"\n        return await self.asend(None)\n\n    @abstractmethod\n    async def asend(self, value):\n        \"\"\"Send a value into the asynchronous generator.\n        Return next yielded value or raise StopAsyncIteration.\n        \"\"\"\n        raise StopAsyncIteration\n\n    @abstractmethod\n    async def athrow(self, typ, val=None, tb=None):\n        \"\"\"Raise an exception in the asynchronous generator.\n        Return next yielded value or raise StopAsyncIteration.\n        \"\"\"\n        if val is None:\n            if tb is None:\n                raise typ\n            val = typ()\n        if tb is not None:\n            val = val.with_traceback(tb)\n        raise val\n\n    async def aclose(self):\n        \"\"\"Raise GeneratorExit inside coroutine.\n        \"\"\"\n        try:\n            await self.athrow(GeneratorExit)\n        except (GeneratorExit, StopAsyncIteration):\n            pass\n        else:\n            raise RuntimeError(\"asynchronous generator ignored GeneratorExit\")\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is AsyncGenerator:\n            return _check_methods(C, '__aiter__', '__anext__',\n                                  'asend', 'athrow', 'aclose')\n        return NotImplemented\n\n\nAsyncGenerator.register(async_generator)\n\n\nclass Iterable(metaclass=ABCMeta):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __iter__(self):\n        while False:\n            yield None\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Iterable:\n            return _check_methods(C, \"__iter__\")\n        return NotImplemented\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n\nclass Iterator(Iterable):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __next__(self):\n        'Return the next item from the iterator. When exhausted, raise StopIteration'\n        raise StopIteration\n\n    def __iter__(self):\n        return self\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Iterator:\n            return _check_methods(C, '__iter__', '__next__')\n        return NotImplemented\n\n\nIterator.register(bytes_iterator)\nIterator.register(bytearray_iterator)\n#Iterator.register(callable_iterator)\nIterator.register(dict_keyiterator)\nIterator.register(dict_valueiterator)\nIterator.register(dict_itemiterator)\nIterator.register(list_iterator)\nIterator.register(list_reverseiterator)\nIterator.register(range_iterator)\nIterator.register(longrange_iterator)\nIterator.register(set_iterator)\nIterator.register(str_iterator)\nIterator.register(tuple_iterator)\nIterator.register(zip_iterator)\n\n\nclass Reversible(Iterable):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __reversed__(self):\n        while False:\n            yield None\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Reversible:\n            return _check_methods(C, \"__reversed__\", \"__iter__\")\n        return NotImplemented\n\n\nclass Generator(Iterator):\n\n    __slots__ = ()\n\n    def __next__(self):\n        \"\"\"Return the next item from the generator.\n        When exhausted, raise StopIteration.\n        \"\"\"\n        return self.send(None)\n\n    @abstractmethod\n    def send(self, value):\n        \"\"\"Send a value into the generator.\n        Return next yielded value or raise StopIteration.\n        \"\"\"\n        raise StopIteration\n\n    @abstractmethod\n    def throw(self, typ, val=None, tb=None):\n        \"\"\"Raise an exception in the generator.\n        Return next yielded value or raise StopIteration.\n        \"\"\"\n        if val is None:\n            if tb is None:\n                raise typ\n            val = typ()\n        if tb is not None:\n            val = val.with_traceback(tb)\n        raise val\n\n    def close(self):\n        \"\"\"Raise GeneratorExit inside generator.\n        \"\"\"\n        try:\n            self.throw(GeneratorExit)\n        except (GeneratorExit, StopIteration):\n            pass\n        else:\n            raise RuntimeError(\"generator ignored GeneratorExit\")\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Generator:\n            return _check_methods(C, '__iter__', '__next__',\n                                  'send', 'throw', 'close')\n        return NotImplemented\n\n\nGenerator.register(generator)\n\n\nclass Sized(metaclass=ABCMeta):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __len__(self):\n        return 0\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Sized:\n            return _check_methods(C, \"__len__\")\n        return NotImplemented\n\n\nclass Container(metaclass=ABCMeta):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __contains__(self, x):\n        return False\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Container:\n            return _check_methods(C, \"__contains__\")\n        return NotImplemented\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n\nclass Collection(Sized, Iterable, Container):\n\n    __slots__ = ()\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Collection:\n            return _check_methods(C,  \"__len__\", \"__iter__\", \"__contains__\")\n        return NotImplemented\n\n\nclass _CallableGenericAlias(GenericAlias):\n    \"\"\" Represent `Callable[argtypes, resulttype]`.\n\n    This sets ``__args__`` to a tuple containing the flattened ``argtypes``\n    followed by ``resulttype``.\n\n    Example: ``Callable[[int, str], float]`` sets ``__args__`` to\n    ``(int, str, float)``.\n    \"\"\"\n\n    __slots__ = ()\n\n    def __new__(cls, origin, args):\n        if not (isinstance(args, tuple) and len(args) == 2):\n            raise TypeError(\n                \"Callable must be used as Callable[[arg, ...], result].\")\n        t_args, t_result = args\n        if isinstance(t_args, list):\n            args = (*t_args, t_result)\n        elif not _is_param_expr(t_args):\n            raise TypeError(f\"Expected a list of types, an ellipsis, \"\n                            f\"ParamSpec, or Concatenate. Got {t_args}\")\n        return super().__new__(cls, origin, args)\n\n    @property\n    def __parameters__(self):\n        params = []\n        for arg in self.__args__:\n            # Looks like a genericalias\n            if hasattr(arg, \"__parameters__\") and isinstance(arg.__parameters__, tuple):\n                params.extend(arg.__parameters__)\n            else:\n                if _is_typevarlike(arg):\n                    params.append(arg)\n        return tuple(dict.fromkeys(params))\n\n    def __repr__(self):\n        if len(self.__args__) == 2 and _is_param_expr(self.__args__[0]):\n            return super().__repr__()\n        return (f'collections.abc.Callable'\n                f'[[{\", \".join([_type_repr(a) for a in self.__args__[:-1]])}], '\n                f'{_type_repr(self.__args__[-1])}]')\n\n    def __reduce__(self):\n        args = self.__args__\n        if not (len(args) == 2 and _is_param_expr(args[0])):\n            args = list(args[:-1]), args[-1]\n        return _CallableGenericAlias, (Callable, args)\n\n    def __getitem__(self, item):\n        # Called during TypeVar substitution, returns the custom subclass\n        # rather than the default types.GenericAlias object.  Most of the\n        # code is copied from typing's _GenericAlias and the builtin\n        # types.GenericAlias.\n\n        # A special case in PEP 612 where if X = Callable[P, int],\n        # then X[int, str] == X[[int, str]].\n        param_len = len(self.__parameters__)\n        if param_len == 0:\n            raise TypeError(f'{self} is not a generic class')\n        if not isinstance(item, tuple):\n            item = (item,)\n        if (param_len == 1 and _is_param_expr(self.__parameters__[0])\n                and item and not _is_param_expr(item[0])):\n            item = (list(item),)\n        item_len = len(item)\n        if item_len != param_len:\n            raise TypeError(f'Too {\"many\" if item_len > param_len else \"few\"}'\n                            f' arguments for {self};'\n                            f' actual {item_len}, expected {param_len}')\n        subst = dict(zip(self.__parameters__, item))\n        new_args = []\n        for arg in self.__args__:\n            if _is_typevarlike(arg):\n                if _is_param_expr(arg):\n                    arg = subst[arg]\n                    if not _is_param_expr(arg):\n                        raise TypeError(f\"Expected a list of types, an ellipsis, \"\n                                        f\"ParamSpec, or Concatenate. Got {arg}\")\n                else:\n                    arg = subst[arg]\n            # Looks like a GenericAlias\n            elif hasattr(arg, '__parameters__') and isinstance(arg.__parameters__, tuple):\n                subparams = arg.__parameters__\n                if subparams:\n                    subargs = tuple(subst[x] for x in subparams)\n                    arg = arg[subargs]\n            if isinstance(arg, tuple):\n                new_args.extend(arg)\n            else:\n                new_args.append(arg)\n\n        # args[0] occurs due to things like Z[[int, str, bool]] from PEP 612\n        if not isinstance(new_args[0], list):\n            t_result = new_args[-1]\n            t_args = new_args[:-1]\n            new_args = (t_args, t_result)\n        return _CallableGenericAlias(Callable, tuple(new_args))\n\n\ndef _is_typevarlike(arg):\n    obj = type(arg)\n    # looks like a TypeVar/ParamSpec\n    return (obj.__module__ == 'typing'\n            and obj.__name__ in {'ParamSpec', 'TypeVar'})\n\ndef _is_param_expr(obj):\n    \"\"\"Checks if obj matches either a list of types, ``...``, ``ParamSpec`` or\n    ``_ConcatenateGenericAlias`` from typing.py\n    \"\"\"\n    if obj is Ellipsis:\n        return True\n    if isinstance(obj, list):\n        return True\n    obj = type(obj)\n    names = ('ParamSpec', '_ConcatenateGenericAlias')\n    return obj.__module__ == 'typing' and any(obj.__name__ == name for name in names)\n\ndef _type_repr(obj):\n    \"\"\"Return the repr() of an object, special-casing types (internal helper).\n\n    Copied from :mod:`typing` since collections.abc\n    shouldn't depend on that module.\n    \"\"\"\n    if isinstance(obj, GenericAlias):\n        return repr(obj)\n    if isinstance(obj, type):\n        if obj.__module__ == 'builtins':\n            return obj.__qualname__\n        return f'{obj.__module__}.{obj.__qualname__}'\n    if obj is Ellipsis:\n        return '...'\n    if isinstance(obj, FunctionType):\n        return obj.__name__\n    return repr(obj)\n\n\nclass Callable(metaclass=ABCMeta):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __call__(self, *args, **kwds):\n        return False\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Callable:\n            return _check_methods(C, \"__call__\")\n        return NotImplemented\n\n    __class_getitem__ = classmethod(_CallableGenericAlias)\n\n\n### SETS ###\n\n\nclass Set(Collection):\n    \"\"\"A set is a finite, iterable container.\n\n    This class provides concrete generic implementations of all\n    methods except for __contains__, __iter__ and __len__.\n\n    To override the comparisons (presumably for speed, as the\n    semantics are fixed), redefine __le__ and __ge__,\n    then the other operations will automatically follow suit.\n    \"\"\"\n\n    __slots__ = ()\n\n    def __le__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        if len(self) > len(other):\n            return False\n        for elem in self:\n            if elem not in other:\n                return False\n        return True\n\n    def __lt__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        return len(self) < len(other) and self.__le__(other)\n\n    def __gt__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        return len(self) > len(other) and self.__ge__(other)\n\n    def __ge__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        if len(self) < len(other):\n            return False\n        for elem in other:\n            if elem not in self:\n                return False\n        return True\n\n    def __eq__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        return len(self) == len(other) and self.__le__(other)\n\n    @classmethod\n    def _from_iterable(cls, it):\n        '''Construct an instance of the class from any iterable input.\n\n        Must override this method if the class constructor signature\n        does not accept an iterable for an input.\n        '''\n        return cls(it)\n\n    def __and__(self, other):\n        if not isinstance(other, Iterable):\n            return NotImplemented\n        return self._from_iterable(value for value in other if value in self)\n\n    __rand__ = __and__\n\n    def isdisjoint(self, other):\n        'Return True if two sets have a null intersection.'\n        for value in other:\n            if value in self:\n                return False\n        return True\n\n    def __or__(self, other):\n        if not isinstance(other, Iterable):\n            return NotImplemented\n        chain = (e for s in (self, other) for e in s)\n        return self._from_iterable(chain)\n\n    __ror__ = __or__\n\n    def __sub__(self, other):\n        if not isinstance(other, Set):\n            if not isinstance(other, Iterable):\n                return NotImplemented\n            other = self._from_iterable(other)\n        return self._from_iterable(value for value in self\n                                   if value not in other)\n\n    def __rsub__(self, other):\n        if not isinstance(other, Set):\n            if not isinstance(other, Iterable):\n                return NotImplemented\n            other = self._from_iterable(other)\n        return self._from_iterable(value for value in other\n                                   if value not in self)\n\n    def __xor__(self, other):\n        if not isinstance(other, Set):\n            if not isinstance(other, Iterable):\n                return NotImplemented\n            other = self._from_iterable(other)\n        return (self - other) | (other - self)\n\n    __rxor__ = __xor__\n\n    def _hash(self):\n        \"\"\"Compute the hash value of a set.\n\n        Note that we don't define __hash__: not all sets are hashable.\n        But if you define a hashable set type, its __hash__ should\n        call this function.\n\n        This must be compatible __eq__.\n\n        All sets ought to compare equal if they contain the same\n        elements, regardless of how they are implemented, and\n        regardless of the order of the elements; so there's not much\n        freedom for __eq__ or __hash__.  We match the algorithm used\n        by the built-in frozenset type.\n        \"\"\"\n        MAX = sys.maxsize\n        MASK = 2 * MAX + 1\n        n = len(self)\n        h = 1927868237 * (n + 1)\n        h &= MASK\n        for x in self:\n            hx = hash(x)\n            h ^= (hx ^ (hx << 16) ^ 89869747)  * 3644798167\n            h &= MASK\n        h ^= (h >> 11) ^ (h >> 25)\n        h = h * 69069 + 907133923\n        h &= MASK\n        if h > MAX:\n            h -= MASK + 1\n        if h == -1:\n            h = 590923713\n        return h\n\n\nSet.register(frozenset)\n\n\nclass MutableSet(Set):\n    \"\"\"A mutable set is a finite, iterable container.\n\n    This class provides concrete generic implementations of all\n    methods except for __contains__, __iter__, __len__,\n    add(), and discard().\n\n    To override the comparisons (presumably for speed, as the\n    semantics are fixed), all you have to do is redefine __le__ and\n    then the other operations will automatically follow suit.\n    \"\"\"\n\n    __slots__ = ()\n\n    @abstractmethod\n    def add(self, value):\n        \"\"\"Add an element.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def discard(self, value):\n        \"\"\"Remove an element.  Do not raise an exception if absent.\"\"\"\n        raise NotImplementedError\n\n    def remove(self, value):\n        \"\"\"Remove an element. If not a member, raise a KeyError.\"\"\"\n        if value not in self:\n            raise KeyError(value)\n        self.discard(value)\n\n    def pop(self):\n        \"\"\"Return the popped value.  Raise KeyError if empty.\"\"\"\n        it = iter(self)\n        try:\n            value = next(it)\n        except StopIteration:\n            raise KeyError from None\n        self.discard(value)\n        return value\n\n    def clear(self):\n        \"\"\"This is slow (creates N new iterators!) but effective.\"\"\"\n        try:\n            while True:\n                self.pop()\n        except KeyError:\n            pass\n\n    def __ior__(self, it):\n        for value in it:\n            self.add(value)\n        return self\n\n    def __iand__(self, it):\n        for value in (self - it):\n            self.discard(value)\n        return self\n\n    def __ixor__(self, it):\n        if it is self:\n            self.clear()\n        else:\n            if not isinstance(it, Set):\n                it = self._from_iterable(it)\n            for value in it:\n                if value in self:\n                    self.discard(value)\n                else:\n                    self.add(value)\n        return self\n\n    def __isub__(self, it):\n        if it is self:\n            self.clear()\n        else:\n            for value in it:\n                self.discard(value)\n        return self\n\n\nMutableSet.register(set)\n\n\n### MAPPINGS ###\n\nclass Mapping(Collection):\n    \"\"\"A Mapping is a generic container for associating key/value\n    pairs.\n\n    This class provides concrete generic implementations of all\n    methods except for __getitem__, __iter__, and __len__.\n    \"\"\"\n\n    __slots__ = ()\n\n    # Tell ABCMeta.__new__ that this class should have TPFLAGS_MAPPING set.\n    __abc_tpflags__ = 1 << 6 # Py_TPFLAGS_MAPPING\n\n    @abstractmethod\n    def __getitem__(self, key):\n        raise KeyError\n\n    def get(self, key, default=None):\n        'D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.'\n        try:\n            return self[key]\n        except KeyError:\n            return default\n\n    def __contains__(self, key):\n        try:\n            self[key]\n        except KeyError:\n            return False\n        else:\n            return True\n\n    def keys(self):\n        \"D.keys() -> a set-like object providing a view on D's keys\"\n        return KeysView(self)\n\n    def items(self):\n        \"D.items() -> a set-like object providing a view on D's items\"\n        return ItemsView(self)\n\n    def values(self):\n        \"D.values() -> an object providing a view on D's values\"\n        return ValuesView(self)\n\n    def __eq__(self, other):\n        if not isinstance(other, Mapping):\n            return NotImplemented\n        return dict(self.items()) == dict(other.items())\n\n    __reversed__ = None\n\nMapping.register(mappingproxy)\n\n\nclass MappingView(Sized):\n\n    __slots__ = '_mapping',\n\n    def __init__(self, mapping):\n        self._mapping = mapping\n\n    def __len__(self):\n        return len(self._mapping)\n\n    def __repr__(self):\n        return '{0.__class__.__name__}({0._mapping!r})'.format(self)\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n\nclass KeysView(MappingView, Set):\n\n    __slots__ = ()\n\n    @classmethod\n    def _from_iterable(cls, it):\n        return set(it)\n\n    def __contains__(self, key):\n        return key in self._mapping\n\n    def __iter__(self):\n        yield from self._mapping\n\n\nKeysView.register(dict_keys)\n\n\nclass ItemsView(MappingView, Set):\n\n    __slots__ = ()\n\n    @classmethod\n    def _from_iterable(cls, it):\n        return set(it)\n\n    def __contains__(self, item):\n        key, value = item\n        try:\n            v = self._mapping[key]\n        except KeyError:\n            return False\n        else:\n            return v is value or v == value\n\n    def __iter__(self):\n        for key in self._mapping:\n            yield (key, self._mapping[key])\n\n\nItemsView.register(dict_items)\n\n\nclass ValuesView(MappingView, Collection):\n\n    __slots__ = ()\n\n    def __contains__(self, value):\n        for key in self._mapping:\n            v = self._mapping[key]\n            if v is value or v == value:\n                return True\n        return False\n\n    def __iter__(self):\n        for key in self._mapping:\n            yield self._mapping[key]\n\n\nValuesView.register(dict_values)\n\n\nclass MutableMapping(Mapping):\n    \"\"\"A MutableMapping is a generic container for associating\n    key/value pairs.\n\n    This class provides concrete generic implementations of all\n    methods except for __getitem__, __setitem__, __delitem__,\n    __iter__, and __len__.\n    \"\"\"\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __setitem__(self, key, value):\n        raise KeyError\n\n    @abstractmethod\n    def __delitem__(self, key):\n        raise KeyError\n\n    __marker = object()\n\n    def pop(self, key, default=__marker):\n        '''D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n          If key is not found, d is returned if given, otherwise KeyError is raised.\n        '''\n        try:\n            value = self[key]\n        except KeyError:\n            if default is self.__marker:\n                raise\n            return default\n        else:\n            del self[key]\n            return value\n\n    def popitem(self):\n        '''D.popitem() -> (k, v), remove and return some (key, value) pair\n           as a 2-tuple; but raise KeyError if D is empty.\n        '''\n        try:\n            key = next(iter(self))\n        except StopIteration:\n            raise KeyError from None\n        value = self[key]\n        del self[key]\n        return key, value\n\n    def clear(self):\n        'D.clear() -> None.  Remove all items from D.'\n        try:\n            while True:\n                self.popitem()\n        except KeyError:\n            pass\n\n    def update(self, other=(), /, **kwds):\n        ''' D.update([E, ]**F) -> None.  Update D from mapping/iterable E and F.\n            If E present and has a .keys() method, does:     for k in E: D[k] = E[k]\n            If E present and lacks .keys() method, does:     for (k, v) in E: D[k] = v\n            In either case, this is followed by: for k, v in F.items(): D[k] = v\n        '''\n        if isinstance(other, Mapping):\n            for key in other:\n                self[key] = other[key]\n        elif hasattr(other, \"keys\"):\n            for key in other.keys():\n                self[key] = other[key]\n        else:\n            for key, value in other:\n                self[key] = value\n        for key, value in kwds.items():\n            self[key] = value\n\n    def setdefault(self, key, default=None):\n        'D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D'\n        try:\n            return self[key]\n        except KeyError:\n            self[key] = default\n        return default\n\n\nMutableMapping.register(dict)\n\n\n### SEQUENCES ###\n\nclass Sequence(Reversible, Collection):\n    \"\"\"All the operations on a read-only sequence.\n\n    Concrete subclasses must override __new__ or __init__,\n    __getitem__, and __len__.\n    \"\"\"\n\n    __slots__ = ()\n\n    # Tell ABCMeta.__new__ that this class should have TPFLAGS_SEQUENCE set.\n    __abc_tpflags__ = 1 << 5 # Py_TPFLAGS_SEQUENCE\n\n    @abstractmethod\n    def __getitem__(self, index):\n        raise IndexError\n\n    def __iter__(self):\n        i = 0\n        try:\n            while True:\n                v = self[i]\n                yield v\n                i += 1\n        except IndexError:\n            return\n\n    def __contains__(self, value):\n        for v in self:\n            if v is value or v == value:\n                return True\n        return False\n\n    def __reversed__(self):\n        for i in reversed(range(len(self))):\n            yield self[i]\n\n    def index(self, value, start=0, stop=None):\n        '''S.index(value, [start, [stop]]) -> integer -- return first index of value.\n           Raises ValueError if the value is not present.\n\n           Supporting start and stop arguments is optional, but\n           recommended.\n        '''\n        if start is not None and start < 0:\n            start = max(len(self) + start, 0)\n        if stop is not None and stop < 0:\n            stop += len(self)\n\n        i = start\n        while stop is None or i < stop:\n            try:\n                v = self[i]\n                if v is value or v == value:\n                    return i\n            except IndexError:\n                break\n            i += 1\n        raise ValueError\n\n    def count(self, value):\n        'S.count(value) -> integer -- return number of occurrences of value'\n        return sum(1 for v in self if v is value or v == value)\n\nSequence.register(tuple)\nSequence.register(str)\nSequence.register(range)\nSequence.register(memoryview)\n\n\nclass ByteString(Sequence):\n    \"\"\"This unifies bytes and bytearray.\n\n    XXX Should add all their methods.\n    \"\"\"\n\n    __slots__ = ()\n\nByteString.register(bytes)\nByteString.register(bytearray)\n\n\nclass MutableSequence(Sequence):\n    \"\"\"All the operations on a read-write sequence.\n\n    Concrete subclasses must provide __new__ or __init__,\n    __getitem__, __setitem__, __delitem__, __len__, and insert().\n    \"\"\"\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __setitem__(self, index, value):\n        raise IndexError\n\n    @abstractmethod\n    def __delitem__(self, index):\n        raise IndexError\n\n    @abstractmethod\n    def insert(self, index, value):\n        'S.insert(index, value) -- insert value before index'\n        raise IndexError\n\n    def append(self, value):\n        'S.append(value) -- append value to the end of the sequence'\n        self.insert(len(self), value)\n\n    def clear(self):\n        'S.clear() -> None -- remove all items from S'\n        try:\n            while True:\n                self.pop()\n        except IndexError:\n            pass\n\n    def reverse(self):\n        'S.reverse() -- reverse *IN PLACE*'\n        n = len(self)\n        for i in range(n//2):\n            self[i], self[n-i-1] = self[n-i-1], self[i]\n\n    def extend(self, values):\n        'S.extend(iterable) -- extend sequence by appending elements from the iterable'\n        if values is self:\n            values = list(values)\n        for v in values:\n            self.append(v)\n\n    def pop(self, index=-1):\n        '''S.pop([index]) -> item -- remove and return item at index (default last).\n           Raise IndexError if list is empty or index is out of range.\n        '''\n        v = self[index]\n        del self[index]\n        return v\n\n    def remove(self, value):\n        '''S.remove(value) -- remove first occurrence of value.\n           Raise ValueError if the value is not present.\n        '''\n        del self[self.index(value)]\n\n    def __iadd__(self, values):\n        self.extend(values)\n        return self\n\n\nMutableSequence.register(list)\nMutableSequence.register(bytearray)  # Multiply inheriting, see ByteString\n", 1166], "/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/typing.py": ["\"\"\"\nThe typing module: Support for gradual typing as defined by PEP 484.\n\nAt large scale, the structure of the module is following:\n* Imports and exports, all public names should be explicitly added to __all__.\n* Internal helper functions: these should never be used in code outside this module.\n* _SpecialForm and its instances (special forms):\n  Any, NoReturn, ClassVar, Union, Optional, Concatenate\n* Classes whose instances can be type arguments in addition to types:\n  ForwardRef, TypeVar and ParamSpec\n* The core of internal generics API: _GenericAlias and _VariadicGenericAlias, the latter is\n  currently only used by Tuple and Callable. All subscripted types like X[int], Union[int, str],\n  etc., are instances of either of these classes.\n* The public counterpart of the generics API consists of two classes: Generic and Protocol.\n* Public helper functions: get_type_hints, overload, cast, no_type_check,\n  no_type_check_decorator.\n* Generic aliases for collections.abc ABCs and few additional protocols.\n* Special types: NewType, NamedTuple, TypedDict.\n* Wrapper submodules for re and io related types.\n\"\"\"\n\nfrom abc import abstractmethod, ABCMeta\nimport collections\nimport collections.abc\nimport contextlib\nimport functools\nimport operator\nimport re as stdlib_re  # Avoid confusion with the re we export.\nimport sys\nimport types\nfrom types import WrapperDescriptorType, MethodWrapperType, MethodDescriptorType, GenericAlias\n\n# Please keep __all__ alphabetized within each category.\n__all__ = [\n    # Super-special typing primitives.\n    'Annotated',\n    'Any',\n    'Callable',\n    'ClassVar',\n    'Concatenate',\n    'Final',\n    'ForwardRef',\n    'Generic',\n    'Literal',\n    'Optional',\n    'ParamSpec',\n    'Protocol',\n    'Tuple',\n    'Type',\n    'TypeVar',\n    'Union',\n\n    # ABCs (from collections.abc).\n    'AbstractSet',  # collections.abc.Set.\n    'ByteString',\n    'Container',\n    'ContextManager',\n    'Hashable',\n    'ItemsView',\n    'Iterable',\n    'Iterator',\n    'KeysView',\n    'Mapping',\n    'MappingView',\n    'MutableMapping',\n    'MutableSequence',\n    'MutableSet',\n    'Sequence',\n    'Sized',\n    'ValuesView',\n    'Awaitable',\n    'AsyncIterator',\n    'AsyncIterable',\n    'Coroutine',\n    'Collection',\n    'AsyncGenerator',\n    'AsyncContextManager',\n\n    # Structural checks, a.k.a. protocols.\n    'Reversible',\n    'SupportsAbs',\n    'SupportsBytes',\n    'SupportsComplex',\n    'SupportsFloat',\n    'SupportsIndex',\n    'SupportsInt',\n    'SupportsRound',\n\n    # Concrete collection types.\n    'ChainMap',\n    'Counter',\n    'Deque',\n    'Dict',\n    'DefaultDict',\n    'List',\n    'OrderedDict',\n    'Set',\n    'FrozenSet',\n    'NamedTuple',  # Not really a type.\n    'TypedDict',  # Not really a type.\n    'Generator',\n\n    # Other concrete types.\n    'BinaryIO',\n    'IO',\n    'Match',\n    'Pattern',\n    'TextIO',\n\n    # One-off things.\n    'AnyStr',\n    'cast',\n    'final',\n    'get_args',\n    'get_origin',\n    'get_type_hints',\n    'is_typeddict',\n    'NewType',\n    'no_type_check',\n    'no_type_check_decorator',\n    'NoReturn',\n    'overload',\n    'ParamSpecArgs',\n    'ParamSpecKwargs',\n    'runtime_checkable',\n    'Text',\n    'TYPE_CHECKING',\n    'TypeAlias',\n    'TypeGuard',\n]\n\n# The pseudo-submodules 're' and 'io' are part of the public\n# namespace, but excluded from __all__ because they might stomp on\n# legitimate imports of those modules.\n\n\ndef _type_convert(arg, module=None, *, allow_special_forms=False):\n    \"\"\"For converting None to type(None), and strings to ForwardRef.\"\"\"\n    if arg is None:\n        return type(None)\n    if isinstance(arg, str):\n        return ForwardRef(arg, module=module, is_class=allow_special_forms)\n    return arg\n\n\ndef _type_check(arg, msg, is_argument=True, module=None, *, allow_special_forms=False):\n    \"\"\"Check that the argument is a type, and return it (internal helper).\n\n    As a special case, accept None and return type(None) instead. Also wrap strings\n    into ForwardRef instances. Consider several corner cases, for example plain\n    special forms like Union are not valid, while Union[int, str] is OK, etc.\n    The msg argument is a human-readable error message, e.g::\n\n        \"Union[arg, ...]: arg should be a type.\"\n\n    We append the repr() of the actual value (truncated to 100 chars).\n    \"\"\"\n    invalid_generic_forms = (Generic, Protocol)\n    if not allow_special_forms:\n        invalid_generic_forms += (ClassVar,)\n        if is_argument:\n            invalid_generic_forms += (Final,)\n\n    arg = _type_convert(arg, module=module, allow_special_forms=allow_special_forms)\n    if (isinstance(arg, _GenericAlias) and\n            arg.__origin__ in invalid_generic_forms):\n        raise TypeError(f\"{arg} is not valid as type argument\")\n    if arg in (Any, NoReturn, Final, TypeAlias):\n        return arg\n    if isinstance(arg, _SpecialForm) or arg in (Generic, Protocol):\n        raise TypeError(f\"Plain {arg} is not valid as type argument\")\n    if isinstance(arg, (type, TypeVar, ForwardRef, types.UnionType, ParamSpec,\n                        ParamSpecArgs, ParamSpecKwargs)):\n        return arg\n    if not callable(arg):\n        raise TypeError(f\"{msg} Got {arg!r:.100}.\")\n    return arg\n\n\ndef _is_param_expr(arg):\n    return arg is ... or isinstance(arg,\n            (tuple, list, ParamSpec, _ConcatenateGenericAlias))\n\n\ndef _type_repr(obj):\n    \"\"\"Return the repr() of an object, special-casing types (internal helper).\n\n    If obj is a type, we return a shorter version than the default\n    type.__repr__, based on the module and qualified name, which is\n    typically enough to uniquely identify a type.  For everything\n    else, we fall back on repr(obj).\n    \"\"\"\n    if isinstance(obj, types.GenericAlias):\n        return repr(obj)\n    if isinstance(obj, type):\n        if obj.__module__ == 'builtins':\n            return obj.__qualname__\n        return f'{obj.__module__}.{obj.__qualname__}'\n    if obj is ...:\n        return('...')\n    if isinstance(obj, types.FunctionType):\n        return obj.__name__\n    return repr(obj)\n\n\ndef _collect_type_vars(types_, typevar_types=None):\n    \"\"\"Collect all type variable contained\n    in types in order of first appearance (lexicographic order). For example::\n\n        _collect_type_vars((T, List[S, T])) == (T, S)\n    \"\"\"\n    if typevar_types is None:\n        typevar_types = TypeVar\n    tvars = []\n    for t in types_:\n        if isinstance(t, typevar_types) and t not in tvars:\n            tvars.append(t)\n        if isinstance(t, (_GenericAlias, GenericAlias, types.UnionType)):\n            tvars.extend([t for t in t.__parameters__ if t not in tvars])\n    return tuple(tvars)\n\n\ndef _check_generic(cls, parameters, elen):\n    \"\"\"Check correct count for parameters of a generic cls (internal helper).\n    This gives a nice error message in case of count mismatch.\n    \"\"\"\n    if not elen:\n        raise TypeError(f\"{cls} is not a generic class\")\n    alen = len(parameters)\n    if alen != elen:\n        raise TypeError(f\"Too {'many' if alen > elen else 'few'} arguments for {cls};\"\n                        f\" actual {alen}, expected {elen}\")\n\ndef _prepare_paramspec_params(cls, params):\n    \"\"\"Prepares the parameters for a Generic containing ParamSpec\n    variables (internal helper).\n    \"\"\"\n    # Special case where Z[[int, str, bool]] == Z[int, str, bool] in PEP 612.\n    if (len(cls.__parameters__) == 1\n            and params and not _is_param_expr(params[0])):\n        assert isinstance(cls.__parameters__[0], ParamSpec)\n        return (params,)\n    else:\n        _check_generic(cls, params, len(cls.__parameters__))\n        _params = []\n        # Convert lists to tuples to help other libraries cache the results.\n        for p, tvar in zip(params, cls.__parameters__):\n            if isinstance(tvar, ParamSpec) and isinstance(p, list):\n                p = tuple(p)\n            _params.append(p)\n        return tuple(_params)\n\ndef _deduplicate(params):\n    # Weed out strict duplicates, preserving the first of each occurrence.\n    all_params = set(params)\n    if len(all_params) < len(params):\n        new_params = []\n        for t in params:\n            if t in all_params:\n                new_params.append(t)\n                all_params.remove(t)\n        params = new_params\n        assert not all_params, all_params\n    return params\n\n\ndef _remove_dups_flatten(parameters):\n    \"\"\"An internal helper for Union creation and substitution: flatten Unions\n    among parameters, then remove duplicates.\n    \"\"\"\n    # Flatten out Union[Union[...], ...].\n    params = []\n    for p in parameters:\n        if isinstance(p, (_UnionGenericAlias, types.UnionType)):\n            params.extend(p.__args__)\n        elif isinstance(p, tuple) and len(p) > 0 and p[0] is Union:\n            params.extend(p[1:])\n        else:\n            params.append(p)\n\n    return tuple(_deduplicate(params))\n\n\ndef _flatten_literal_params(parameters):\n    \"\"\"An internal helper for Literal creation: flatten Literals among parameters\"\"\"\n    params = []\n    for p in parameters:\n        if isinstance(p, _LiteralGenericAlias):\n            params.extend(p.__args__)\n        else:\n            params.append(p)\n    return tuple(params)\n\n\n_cleanups = []\n\n\ndef _tp_cache(func=None, /, *, typed=False):\n    \"\"\"Internal wrapper caching __getitem__ of generic types with a fallback to\n    original function for non-hashable arguments.\n    \"\"\"\n    def decorator(func):\n        cached = functools.lru_cache(typed=typed)(func)\n        _cleanups.append(cached.cache_clear)\n\n        @functools.wraps(func)\n        def inner(*args, **kwds):\n            try:\n                return cached(*args, **kwds)\n            except TypeError:\n                pass  # All real errors (not unhashable args) are raised below.\n            return func(*args, **kwds)\n        return inner\n\n    if func is not None:\n        return decorator(func)\n\n    return decorator\n\ndef _eval_type(t, globalns, localns, recursive_guard=frozenset()):\n    \"\"\"Evaluate all forward references in the given type t.\n    For use of globalns and localns see the docstring for get_type_hints().\n    recursive_guard is used to prevent infinite recursion with a recursive\n    ForwardRef.\n    \"\"\"\n    if isinstance(t, ForwardRef):\n        return t._evaluate(globalns, localns, recursive_guard)\n    if isinstance(t, (_GenericAlias, GenericAlias, types.UnionType)):\n        ev_args = tuple(_eval_type(a, globalns, localns, recursive_guard) for a in t.__args__)\n        if ev_args == t.__args__:\n            return t\n        if isinstance(t, GenericAlias):\n            return GenericAlias(t.__origin__, ev_args)\n        if isinstance(t, types.UnionType):\n            return functools.reduce(operator.or_, ev_args)\n        else:\n            return t.copy_with(ev_args)\n    return t\n\n\nclass _Final:\n    \"\"\"Mixin to prohibit subclassing\"\"\"\n\n    __slots__ = ('__weakref__',)\n\n    def __init_subclass__(self, /, *args, **kwds):\n        if '_root' not in kwds:\n            raise TypeError(\"Cannot subclass special typing classes\")\n\nclass _Immutable:\n    \"\"\"Mixin to indicate that object should not be copied.\"\"\"\n    __slots__ = ()\n\n    def __copy__(self):\n        return self\n\n    def __deepcopy__(self, memo):\n        return self\n\n\n# Internal indicator of special typing constructs.\n# See __doc__ instance attribute for specific docs.\nclass _SpecialForm(_Final, _root=True):\n    __slots__ = ('_name', '__doc__', '_getitem')\n\n    def __init__(self, getitem):\n        self._getitem = getitem\n        self._name = getitem.__name__\n        self.__doc__ = getitem.__doc__\n\n    def __getattr__(self, item):\n        if item in {'__name__', '__qualname__'}:\n            return self._name\n\n        raise AttributeError(item)\n\n    def __mro_entries__(self, bases):\n        raise TypeError(f\"Cannot subclass {self!r}\")\n\n    def __repr__(self):\n        return 'typing.' + self._name\n\n    def __reduce__(self):\n        return self._name\n\n    def __call__(self, *args, **kwds):\n        raise TypeError(f\"Cannot instantiate {self!r}\")\n\n    def __or__(self, other):\n        return Union[self, other]\n\n    def __ror__(self, other):\n        return Union[other, self]\n\n    def __instancecheck__(self, obj):\n        raise TypeError(f\"{self} cannot be used with isinstance()\")\n\n    def __subclasscheck__(self, cls):\n        raise TypeError(f\"{self} cannot be used with issubclass()\")\n\n    @_tp_cache\n    def __getitem__(self, parameters):\n        return self._getitem(self, parameters)\n\n\nclass _LiteralSpecialForm(_SpecialForm, _root=True):\n    def __getitem__(self, parameters):\n        if not isinstance(parameters, tuple):\n            parameters = (parameters,)\n        return self._getitem(self, *parameters)\n\n\n@_SpecialForm\ndef Any(self, parameters):\n    \"\"\"Special type indicating an unconstrained type.\n\n    - Any is compatible with every type.\n    - Any assumed to have all methods.\n    - All values assumed to be instances of Any.\n\n    Note that all the above statements are true from the point of view of\n    static type checkers. At runtime, Any should not be used with instance\n    or class checks.\n    \"\"\"\n    raise TypeError(f\"{self} is not subscriptable\")\n\n@_SpecialForm\ndef NoReturn(self, parameters):\n    \"\"\"Special type indicating functions that never return.\n    Example::\n\n      from typing import NoReturn\n\n      def stop() -> NoReturn:\n          raise Exception('no way')\n\n    This type is invalid in other positions, e.g., ``List[NoReturn]``\n    will fail in static type checkers.\n    \"\"\"\n    raise TypeError(f\"{self} is not subscriptable\")\n\n@_SpecialForm\ndef ClassVar(self, parameters):\n    \"\"\"Special type construct to mark class variables.\n\n    An annotation wrapped in ClassVar indicates that a given\n    attribute is intended to be used as a class variable and\n    should not be set on instances of that class. Usage::\n\n      class Starship:\n          stats: ClassVar[Dict[str, int]] = {} # class variable\n          damage: int = 10                     # instance variable\n\n    ClassVar accepts only types and cannot be further subscribed.\n\n    Note that ClassVar is not a class itself, and should not\n    be used with isinstance() or issubclass().\n    \"\"\"\n    item = _type_check(parameters, f'{self} accepts only single type.')\n    return _GenericAlias(self, (item,))\n\n@_SpecialForm\ndef Final(self, parameters):\n    \"\"\"Special typing construct to indicate final names to type checkers.\n\n    A final name cannot be re-assigned or overridden in a subclass.\n    For example:\n\n      MAX_SIZE: Final = 9000\n      MAX_SIZE += 1  # Error reported by type checker\n\n      class Connection:\n          TIMEOUT: Final[int] = 10\n\n      class FastConnector(Connection):\n          TIMEOUT = 1  # Error reported by type checker\n\n    There is no runtime checking of these properties.\n    \"\"\"\n    item = _type_check(parameters, f'{self} accepts only single type.')\n    return _GenericAlias(self, (item,))\n\n@_SpecialForm\ndef Union(self, parameters):\n    \"\"\"Union type; Union[X, Y] means either X or Y.\n\n    To define a union, use e.g. Union[int, str].  Details:\n    - The arguments must be types and there must be at least one.\n    - None as an argument is a special case and is replaced by\n      type(None).\n    - Unions of unions are flattened, e.g.::\n\n        Union[Union[int, str], float] == Union[int, str, float]\n\n    - Unions of a single argument vanish, e.g.::\n\n        Union[int] == int  # The constructor actually returns int\n\n    - Redundant arguments are skipped, e.g.::\n\n        Union[int, str, int] == Union[int, str]\n\n    - When comparing unions, the argument order is ignored, e.g.::\n\n        Union[int, str] == Union[str, int]\n\n    - You cannot subclass or instantiate a union.\n    - You can use Optional[X] as a shorthand for Union[X, None].\n    \"\"\"\n    if parameters == ():\n        raise TypeError(\"Cannot take a Union of no types.\")\n    if not isinstance(parameters, tuple):\n        parameters = (parameters,)\n    msg = \"Union[arg, ...]: each arg must be a type.\"\n    parameters = tuple(_type_check(p, msg) for p in parameters)\n    parameters = _remove_dups_flatten(parameters)\n    if len(parameters) == 1:\n        return parameters[0]\n    if len(parameters) == 2 and type(None) in parameters:\n        return _UnionGenericAlias(self, parameters, name=\"Optional\")\n    return _UnionGenericAlias(self, parameters)\n\n@_SpecialForm\ndef Optional(self, parameters):\n    \"\"\"Optional type.\n\n    Optional[X] is equivalent to Union[X, None].\n    \"\"\"\n    arg = _type_check(parameters, f\"{self} requires a single type.\")\n    return Union[arg, type(None)]\n\n@_LiteralSpecialForm\n@_tp_cache(typed=True)\ndef Literal(self, *parameters):\n    \"\"\"Special typing form to define literal types (a.k.a. value types).\n\n    This form can be used to indicate to type checkers that the corresponding\n    variable or function parameter has a value equivalent to the provided\n    literal (or one of several literals):\n\n      def validate_simple(data: Any) -> Literal[True]:  # always returns True\n          ...\n\n      MODE = Literal['r', 'rb', 'w', 'wb']\n      def open_helper(file: str, mode: MODE) -> str:\n          ...\n\n      open_helper('/some/path', 'r')  # Passes type check\n      open_helper('/other/path', 'typo')  # Error in type checker\n\n    Literal[...] cannot be subclassed. At runtime, an arbitrary value\n    is allowed as type argument to Literal[...], but type checkers may\n    impose restrictions.\n    \"\"\"\n    # There is no '_type_check' call because arguments to Literal[...] are\n    # values, not types.\n    parameters = _flatten_literal_params(parameters)\n\n    try:\n        parameters = tuple(p for p, _ in _deduplicate(list(_value_and_type_iter(parameters))))\n    except TypeError:  # unhashable parameters\n        pass\n\n    return _LiteralGenericAlias(self, parameters)\n\n\n@_SpecialForm\ndef TypeAlias(self, parameters):\n    \"\"\"Special marker indicating that an assignment should\n    be recognized as a proper type alias definition by type\n    checkers.\n\n    For example::\n\n        Predicate: TypeAlias = Callable[..., bool]\n\n    It's invalid when used anywhere except as in the example above.\n    \"\"\"\n    raise TypeError(f\"{self} is not subscriptable\")\n\n\n@_SpecialForm\ndef Concatenate(self, parameters):\n    \"\"\"Used in conjunction with ``ParamSpec`` and ``Callable`` to represent a\n    higher order function which adds, removes or transforms parameters of a\n    callable.\n\n    For example::\n\n       Callable[Concatenate[int, P], int]\n\n    See PEP 612 for detailed information.\n    \"\"\"\n    if parameters == ():\n        raise TypeError(\"Cannot take a Concatenate of no types.\")\n    if not isinstance(parameters, tuple):\n        parameters = (parameters,)\n    if not isinstance(parameters[-1], ParamSpec):\n        raise TypeError(\"The last parameter to Concatenate should be a \"\n                        \"ParamSpec variable.\")\n    msg = \"Concatenate[arg, ...]: each arg must be a type.\"\n    parameters = (*(_type_check(p, msg) for p in parameters[:-1]), parameters[-1])\n    return _ConcatenateGenericAlias(self, parameters,\n                                    _typevar_types=(TypeVar, ParamSpec),\n                                    _paramspec_tvars=True)\n\n\n@_SpecialForm\ndef TypeGuard(self, parameters):\n    \"\"\"Special typing form used to annotate the return type of a user-defined\n    type guard function.  ``TypeGuard`` only accepts a single type argument.\n    At runtime, functions marked this way should return a boolean.\n\n    ``TypeGuard`` aims to benefit *type narrowing* -- a technique used by static\n    type checkers to determine a more precise type of an expression within a\n    program's code flow.  Usually type narrowing is done by analyzing\n    conditional code flow and applying the narrowing to a block of code.  The\n    conditional expression here is sometimes referred to as a \"type guard\".\n\n    Sometimes it would be convenient to use a user-defined boolean function\n    as a type guard.  Such a function should use ``TypeGuard[...]`` as its\n    return type to alert static type checkers to this intention.\n\n    Using  ``-> TypeGuard`` tells the static type checker that for a given\n    function:\n\n    1. The return value is a boolean.\n    2. If the return value is ``True``, the type of its argument\n       is the type inside ``TypeGuard``.\n\n       For example::\n\n          def is_str(val: Union[str, float]):\n              # \"isinstance\" type guard\n              if isinstance(val, str):\n                  # Type of ``val`` is narrowed to ``str``\n                  ...\n              else:\n                  # Else, type of ``val`` is narrowed to ``float``.\n                  ...\n\n    Strict type narrowing is not enforced -- ``TypeB`` need not be a narrower\n    form of ``TypeA`` (it can even be a wider form) and this may lead to\n    type-unsafe results.  The main reason is to allow for things like\n    narrowing ``List[object]`` to ``List[str]`` even though the latter is not\n    a subtype of the former, since ``List`` is invariant.  The responsibility of\n    writing type-safe type guards is left to the user.\n\n    ``TypeGuard`` also works with type variables.  For more information, see\n    PEP 647 (User-Defined Type Guards).\n    \"\"\"\n    item = _type_check(parameters, f'{self} accepts only single type.')\n    return _GenericAlias(self, (item,))\n\n\nclass ForwardRef(_Final, _root=True):\n    \"\"\"Internal wrapper to hold a forward reference.\"\"\"\n\n    __slots__ = ('__forward_arg__', '__forward_code__',\n                 '__forward_evaluated__', '__forward_value__',\n                 '__forward_is_argument__', '__forward_is_class__',\n                 '__forward_module__')\n\n    def __init__(self, arg, is_argument=True, module=None, *, is_class=False):\n        if not isinstance(arg, str):\n            raise TypeError(f\"Forward reference must be a string -- got {arg!r}\")\n        try:\n            code = compile(arg, '<string>', 'eval')\n        except SyntaxError:\n            raise SyntaxError(f\"Forward reference must be an expression -- got {arg!r}\")\n        self.__forward_arg__ = arg\n        self.__forward_code__ = code\n        self.__forward_evaluated__ = False\n        self.__forward_value__ = None\n        self.__forward_is_argument__ = is_argument\n        self.__forward_is_class__ = is_class\n        self.__forward_module__ = module\n\n    def _evaluate(self, globalns, localns, recursive_guard):\n        if self.__forward_arg__ in recursive_guard:\n            return self\n        if not self.__forward_evaluated__ or localns is not globalns:\n            if globalns is None and localns is None:\n                globalns = localns = {}\n            elif globalns is None:\n                globalns = localns\n            elif localns is None:\n                localns = globalns\n            if self.__forward_module__ is not None:\n                globalns = getattr(\n                    sys.modules.get(self.__forward_module__, None), '__dict__', globalns\n                )\n            type_ = _type_check(\n                eval(self.__forward_code__, globalns, localns),\n                \"Forward references must evaluate to types.\",\n                is_argument=self.__forward_is_argument__,\n                allow_special_forms=self.__forward_is_class__,\n            )\n            self.__forward_value__ = _eval_type(\n                type_, globalns, localns, recursive_guard | {self.__forward_arg__}\n            )\n            self.__forward_evaluated__ = True\n        return self.__forward_value__\n\n    def __eq__(self, other):\n        if not isinstance(other, ForwardRef):\n            return NotImplemented\n        if self.__forward_evaluated__ and other.__forward_evaluated__:\n            return (self.__forward_arg__ == other.__forward_arg__ and\n                    self.__forward_value__ == other.__forward_value__)\n        return (self.__forward_arg__ == other.__forward_arg__ and\n                self.__forward_module__ == other.__forward_module__)\n\n    def __hash__(self):\n        return hash((self.__forward_arg__, self.__forward_module__))\n\n    def __repr__(self):\n        return f'ForwardRef({self.__forward_arg__!r})'\n\nclass _TypeVarLike:\n    \"\"\"Mixin for TypeVar-like types (TypeVar and ParamSpec).\"\"\"\n    def __init__(self, bound, covariant, contravariant):\n        \"\"\"Used to setup TypeVars and ParamSpec's bound, covariant and\n        contravariant attributes.\n        \"\"\"\n        if covariant and contravariant:\n            raise ValueError(\"Bivariant types are not supported.\")\n        self.__covariant__ = bool(covariant)\n        self.__contravariant__ = bool(contravariant)\n        if bound:\n            self.__bound__ = _type_check(bound, \"Bound must be a type.\")\n        else:\n            self.__bound__ = None\n\n    def __or__(self, right):\n        return Union[self, right]\n\n    def __ror__(self, left):\n        return Union[left, self]\n\n    def __repr__(self):\n        if self.__covariant__:\n            prefix = '+'\n        elif self.__contravariant__:\n            prefix = '-'\n        else:\n            prefix = '~'\n        return prefix + self.__name__\n\n    def __reduce__(self):\n        return self.__name__\n\n\nclass TypeVar( _Final, _Immutable, _TypeVarLike, _root=True):\n    \"\"\"Type variable.\n\n    Usage::\n\n      T = TypeVar('T')  # Can be anything\n      A = TypeVar('A', str, bytes)  # Must be str or bytes\n\n    Type variables exist primarily for the benefit of static type\n    checkers.  They serve as the parameters for generic types as well\n    as for generic function definitions.  See class Generic for more\n    information on generic types.  Generic functions work as follows:\n\n      def repeat(x: T, n: int) -> List[T]:\n          '''Return a list containing n references to x.'''\n          return [x]*n\n\n      def longest(x: A, y: A) -> A:\n          '''Return the longest of two strings.'''\n          return x if len(x) >= len(y) else y\n\n    The latter example's signature is essentially the overloading\n    of (str, str) -> str and (bytes, bytes) -> bytes.  Also note\n    that if the arguments are instances of some subclass of str,\n    the return type is still plain str.\n\n    At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError.\n\n    Type variables defined with covariant=True or contravariant=True\n    can be used to declare covariant or contravariant generic types.\n    See PEP 484 for more details. By default generic types are invariant\n    in all type variables.\n\n    Type variables can be introspected. e.g.:\n\n      T.__name__ == 'T'\n      T.__constraints__ == ()\n      T.__covariant__ == False\n      T.__contravariant__ = False\n      A.__constraints__ == (str, bytes)\n\n    Note that only type variables defined in global scope can be pickled.\n    \"\"\"\n\n    __slots__ = ('__name__', '__bound__', '__constraints__',\n                 '__covariant__', '__contravariant__', '__dict__')\n\n    def __init__(self, name, *constraints, bound=None,\n                 covariant=False, contravariant=False):\n        self.__name__ = name\n        super().__init__(bound, covariant, contravariant)\n        if constraints and bound is not None:\n            raise TypeError(\"Constraints cannot be combined with bound=...\")\n        if constraints and len(constraints) == 1:\n            raise TypeError(\"A single constraint is not allowed\")\n        msg = \"TypeVar(name, constraint, ...): constraints must be types.\"\n        self.__constraints__ = tuple(_type_check(t, msg) for t in constraints)\n        try:\n            def_mod = sys._getframe(1).f_globals.get('__name__', '__main__')  # for pickling\n        except (AttributeError, ValueError):\n            def_mod = None\n        if def_mod != 'typing':\n            self.__module__ = def_mod\n\n\nclass ParamSpecArgs(_Final, _Immutable, _root=True):\n    \"\"\"The args for a ParamSpec object.\n\n    Given a ParamSpec object P, P.args is an instance of ParamSpecArgs.\n\n    ParamSpecArgs objects have a reference back to their ParamSpec:\n\n       P.args.__origin__ is P\n\n    This type is meant for runtime introspection and has no special meaning to\n    static type checkers.\n    \"\"\"\n    def __init__(self, origin):\n        self.__origin__ = origin\n\n    def __repr__(self):\n        return f\"{self.__origin__.__name__}.args\"\n\n    def __eq__(self, other):\n        if not isinstance(other, ParamSpecArgs):\n            return NotImplemented\n        return self.__origin__ == other.__origin__\n\n\nclass ParamSpecKwargs(_Final, _Immutable, _root=True):\n    \"\"\"The kwargs for a ParamSpec object.\n\n    Given a ParamSpec object P, P.kwargs is an instance of ParamSpecKwargs.\n\n    ParamSpecKwargs objects have a reference back to their ParamSpec:\n\n       P.kwargs.__origin__ is P\n\n    This type is meant for runtime introspection and has no special meaning to\n    static type checkers.\n    \"\"\"\n    def __init__(self, origin):\n        self.__origin__ = origin\n\n    def __repr__(self):\n        return f\"{self.__origin__.__name__}.kwargs\"\n\n    def __eq__(self, other):\n        if not isinstance(other, ParamSpecKwargs):\n            return NotImplemented\n        return self.__origin__ == other.__origin__\n\n\nclass ParamSpec(_Final, _Immutable, _TypeVarLike, _root=True):\n    \"\"\"Parameter specification variable.\n\n    Usage::\n\n       P = ParamSpec('P')\n\n    Parameter specification variables exist primarily for the benefit of static\n    type checkers.  They are used to forward the parameter types of one\n    callable to another callable, a pattern commonly found in higher order\n    functions and decorators.  They are only valid when used in ``Concatenate``,\n    or as the first argument to ``Callable``, or as parameters for user-defined\n    Generics.  See class Generic for more information on generic types.  An\n    example for annotating a decorator::\n\n       T = TypeVar('T')\n       P = ParamSpec('P')\n\n       def add_logging(f: Callable[P, T]) -> Callable[P, T]:\n           '''A type-safe decorator to add logging to a function.'''\n           def inner(*args: P.args, **kwargs: P.kwargs) -> T:\n               logging.info(f'{f.__name__} was called')\n               return f(*args, **kwargs)\n           return inner\n\n       @add_logging\n       def add_two(x: float, y: float) -> float:\n           '''Add two numbers together.'''\n           return x + y\n\n    Parameter specification variables defined with covariant=True or\n    contravariant=True can be used to declare covariant or contravariant\n    generic types.  These keyword arguments are valid, but their actual semantics\n    are yet to be decided.  See PEP 612 for details.\n\n    Parameter specification variables can be introspected. e.g.:\n\n       P.__name__ == 'T'\n       P.__bound__ == None\n       P.__covariant__ == False\n       P.__contravariant__ == False\n\n    Note that only parameter specification variables defined in global scope can\n    be pickled.\n    \"\"\"\n\n    __slots__ = ('__name__', '__bound__', '__covariant__', '__contravariant__',\n                 '__dict__')\n\n    @property\n    def args(self):\n        return ParamSpecArgs(self)\n\n    @property\n    def kwargs(self):\n        return ParamSpecKwargs(self)\n\n    def __init__(self, name, *, bound=None, covariant=False, contravariant=False):\n        self.__name__ = name\n        super().__init__(bound, covariant, contravariant)\n        try:\n            def_mod = sys._getframe(1).f_globals.get('__name__', '__main__')\n        except (AttributeError, ValueError):\n            def_mod = None\n        if def_mod != 'typing':\n            self.__module__ = def_mod\n\n\ndef _is_dunder(attr):\n    return attr.startswith('__') and attr.endswith('__')\n\nclass _BaseGenericAlias(_Final, _root=True):\n    \"\"\"The central part of internal API.\n\n    This represents a generic version of type 'origin' with type arguments 'params'.\n    There are two kind of these aliases: user defined and special. The special ones\n    are wrappers around builtin collections and ABCs in collections.abc. These must\n    have 'name' always set. If 'inst' is False, then the alias can't be instantiated,\n    this is used by e.g. typing.List and typing.Dict.\n    \"\"\"\n    def __init__(self, origin, *, inst=True, name=None):\n        self._inst = inst\n        self._name = name\n        self.__origin__ = origin\n        self.__slots__ = None  # This is not documented.\n\n    def __call__(self, *args, **kwargs):\n        if not self._inst:\n            raise TypeError(f\"Type {self._name} cannot be instantiated; \"\n                            f\"use {self.__origin__.__name__}() instead\")\n        result = self.__origin__(*args, **kwargs)\n        try:\n            result.__orig_class__ = self\n        except AttributeError:\n            pass\n        return result\n\n    def __mro_entries__(self, bases):\n        res = []\n        if self.__origin__ not in bases:\n            res.append(self.__origin__)\n        i = bases.index(self)\n        for b in bases[i+1:]:\n            if isinstance(b, _BaseGenericAlias) or issubclass(b, Generic):\n                break\n        else:\n            res.append(Generic)\n        return tuple(res)\n\n    def __getattr__(self, attr):\n        if attr in {'__name__', '__qualname__'}:\n            return self._name or self.__origin__.__name__\n\n        # We are careful for copy and pickle.\n        # Also for simplicity we don't relay any dunder names\n        if '__origin__' in self.__dict__ and not _is_dunder(attr):\n            return getattr(self.__origin__, attr)\n        raise AttributeError(attr)\n\n    def __setattr__(self, attr, val):\n        if _is_dunder(attr) or attr in {'_name', '_inst', '_nparams',\n                                        '_typevar_types', '_paramspec_tvars'}:\n            super().__setattr__(attr, val)\n        else:\n            setattr(self.__origin__, attr, val)\n\n    def __instancecheck__(self, obj):\n        return self.__subclasscheck__(type(obj))\n\n    def __subclasscheck__(self, cls):\n        raise TypeError(\"Subscripted generics cannot be used with\"\n                        \" class and instance checks\")\n\n    def __dir__(self):\n        return list(set(super().__dir__()\n                + [attr for attr in dir(self.__origin__) if not _is_dunder(attr)]))\n\n# Special typing constructs Union, Optional, Generic, Callable and Tuple\n# use three special attributes for internal bookkeeping of generic types:\n# * __parameters__ is a tuple of unique free type parameters of a generic\n#   type, for example, Dict[T, T].__parameters__ == (T,);\n# * __origin__ keeps a reference to a type that was subscripted,\n#   e.g., Union[T, int].__origin__ == Union, or the non-generic version of\n#   the type.\n# * __args__ is a tuple of all arguments used in subscripting,\n#   e.g., Dict[T, int].__args__ == (T, int).\n\n\nclass _GenericAlias(_BaseGenericAlias, _root=True):\n    def __init__(self, origin, params, *, inst=True, name=None,\n                 _typevar_types=TypeVar,\n                 _paramspec_tvars=False):\n        super().__init__(origin, inst=inst, name=name)\n        if not isinstance(params, tuple):\n            params = (params,)\n        self.__args__ = tuple(... if a is _TypingEllipsis else\n                              () if a is _TypingEmpty else\n                              a for a in params)\n        self.__parameters__ = _collect_type_vars(params, typevar_types=_typevar_types)\n        self._typevar_types = _typevar_types\n        self._paramspec_tvars = _paramspec_tvars\n        if not name:\n            self.__module__ = origin.__module__\n\n    def __eq__(self, other):\n        if not isinstance(other, _GenericAlias):\n            return NotImplemented\n        return (self.__origin__ == other.__origin__\n                and self.__args__ == other.__args__)\n\n    def __hash__(self):\n        return hash((self.__origin__, self.__args__))\n\n    def __or__(self, right):\n        return Union[self, right]\n\n    def __ror__(self, left):\n        return Union[left, self]\n\n    @_tp_cache\n    def __getitem__(self, params):\n        if self.__origin__ in (Generic, Protocol):\n            # Can't subscript Generic[...] or Protocol[...].\n            raise TypeError(f\"Cannot subscript already-subscripted {self}\")\n        if not isinstance(params, tuple):\n            params = (params,)\n        params = tuple(_type_convert(p) for p in params)\n        if (self._paramspec_tvars\n                and any(isinstance(t, ParamSpec) for t in self.__parameters__)):\n            params = _prepare_paramspec_params(self, params)\n        else:\n            _check_generic(self, params, len(self.__parameters__))\n\n        subst = dict(zip(self.__parameters__, params))\n        new_args = []\n        for arg in self.__args__:\n            if isinstance(arg, self._typevar_types):\n                if isinstance(arg, ParamSpec):\n                    arg = subst[arg]\n                    if not _is_param_expr(arg):\n                        raise TypeError(f\"Expected a list of types, an ellipsis, \"\n                                        f\"ParamSpec, or Concatenate. Got {arg}\")\n                else:\n                    arg = subst[arg]\n            elif isinstance(arg, (_GenericAlias, GenericAlias, types.UnionType)):\n                subparams = arg.__parameters__\n                if subparams:\n                    subargs = tuple(subst[x] for x in subparams)\n                    arg = arg[subargs]\n            # Required to flatten out the args for CallableGenericAlias\n            if self.__origin__ == collections.abc.Callable and isinstance(arg, tuple):\n                new_args.extend(arg)\n            else:\n                new_args.append(arg)\n        return self.copy_with(tuple(new_args))\n\n    def copy_with(self, params):\n        return self.__class__(self.__origin__, params, name=self._name, inst=self._inst,\n                              _typevar_types=self._typevar_types,\n                              _paramspec_tvars=self._paramspec_tvars)\n\n    def __repr__(self):\n        if self._name:\n            name = 'typing.' + self._name\n        else:\n            name = _type_repr(self.__origin__)\n        args = \", \".join([_type_repr(a) for a in self.__args__])\n        return f'{name}[{args}]'\n\n    def __reduce__(self):\n        if self._name:\n            origin = globals()[self._name]\n        else:\n            origin = self.__origin__\n        args = tuple(self.__args__)\n        if len(args) == 1 and not isinstance(args[0], tuple):\n            args, = args\n        return operator.getitem, (origin, args)\n\n    def __mro_entries__(self, bases):\n        if isinstance(self.__origin__, _SpecialForm):\n            raise TypeError(f\"Cannot subclass {self!r}\")\n\n        if self._name:  # generic version of an ABC or built-in class\n            return super().__mro_entries__(bases)\n        if self.__origin__ is Generic:\n            if Protocol in bases:\n                return ()\n            i = bases.index(self)\n            for b in bases[i+1:]:\n                if isinstance(b, _BaseGenericAlias) and b is not self:\n                    return ()\n        return (self.__origin__,)\n\n\n# _nparams is the number of accepted parameters, e.g. 0 for Hashable,\n# 1 for List and 2 for Dict.  It may be -1 if variable number of\n# parameters are accepted (needs custom __getitem__).\n\nclass _SpecialGenericAlias(_BaseGenericAlias, _root=True):\n    def __init__(self, origin, nparams, *, inst=True, name=None):\n        if name is None:\n            name = origin.__name__\n        super().__init__(origin, inst=inst, name=name)\n        self._nparams = nparams\n        if origin.__module__ == 'builtins':\n            self.__doc__ = f'A generic version of {origin.__qualname__}.'\n        else:\n            self.__doc__ = f'A generic version of {origin.__module__}.{origin.__qualname__}.'\n\n    @_tp_cache\n    def __getitem__(self, params):\n        if not isinstance(params, tuple):\n            params = (params,)\n        msg = \"Parameters to generic types must be types.\"\n        params = tuple(_type_check(p, msg) for p in params)\n        _check_generic(self, params, self._nparams)\n        return self.copy_with(params)\n\n    def copy_with(self, params):\n        return _GenericAlias(self.__origin__, params,\n                             name=self._name, inst=self._inst)\n\n    def __repr__(self):\n        return 'typing.' + self._name\n\n    def __subclasscheck__(self, cls):\n        if isinstance(cls, _SpecialGenericAlias):\n            return issubclass(cls.__origin__, self.__origin__)\n        if not isinstance(cls, _GenericAlias):\n            return issubclass(cls, self.__origin__)\n        return super().__subclasscheck__(cls)\n\n    def __reduce__(self):\n        return self._name\n\n    def __or__(self, right):\n        return Union[self, right]\n\n    def __ror__(self, left):\n        return Union[left, self]\n\nclass _CallableGenericAlias(_GenericAlias, _root=True):\n    def __repr__(self):\n        assert self._name == 'Callable'\n        args = self.__args__\n        if len(args) == 2 and _is_param_expr(args[0]):\n            return super().__repr__()\n        return (f'typing.Callable'\n                f'[[{\", \".join([_type_repr(a) for a in args[:-1]])}], '\n                f'{_type_repr(args[-1])}]')\n\n    def __reduce__(self):\n        args = self.__args__\n        if not (len(args) == 2 and _is_param_expr(args[0])):\n            args = list(args[:-1]), args[-1]\n        return operator.getitem, (Callable, args)\n\n\nclass _CallableType(_SpecialGenericAlias, _root=True):\n    def copy_with(self, params):\n        return _CallableGenericAlias(self.__origin__, params,\n                                     name=self._name, inst=self._inst,\n                                     _typevar_types=(TypeVar, ParamSpec),\n                                     _paramspec_tvars=True)\n\n    def __getitem__(self, params):\n        if not isinstance(params, tuple) or len(params) != 2:\n            raise TypeError(\"Callable must be used as \"\n                            \"Callable[[arg, ...], result].\")\n        args, result = params\n        # This relaxes what args can be on purpose to allow things like\n        # PEP 612 ParamSpec.  Responsibility for whether a user is using\n        # Callable[...] properly is deferred to static type checkers.\n        if isinstance(args, list):\n            params = (tuple(args), result)\n        else:\n            params = (args, result)\n        return self.__getitem_inner__(params)\n\n    @_tp_cache\n    def __getitem_inner__(self, params):\n        args, result = params\n        msg = \"Callable[args, result]: result must be a type.\"\n        result = _type_check(result, msg)\n        if args is Ellipsis:\n            return self.copy_with((_TypingEllipsis, result))\n        if not isinstance(args, tuple):\n            args = (args,)\n        args = tuple(_type_convert(arg) for arg in args)\n        params = args + (result,)\n        return self.copy_with(params)\n\n\nclass _TupleType(_SpecialGenericAlias, _root=True):\n    @_tp_cache\n    def __getitem__(self, params):\n        if params == ():\n            return self.copy_with((_TypingEmpty,))\n        if not isinstance(params, tuple):\n            params = (params,)\n        if len(params) == 2 and params[1] is ...:\n            msg = \"Tuple[t, ...]: t must be a type.\"\n            p = _type_check(params[0], msg)\n            return self.copy_with((p, _TypingEllipsis))\n        msg = \"Tuple[t0, t1, ...]: each t must be a type.\"\n        params = tuple(_type_check(p, msg) for p in params)\n        return self.copy_with(params)\n\n\nclass _UnionGenericAlias(_GenericAlias, _root=True):\n    def copy_with(self, params):\n        return Union[params]\n\n    def __eq__(self, other):\n        if not isinstance(other, (_UnionGenericAlias, types.UnionType)):\n            return NotImplemented\n        return set(self.__args__) == set(other.__args__)\n\n    def __hash__(self):\n        return hash(frozenset(self.__args__))\n\n    def __repr__(self):\n        args = self.__args__\n        if len(args) == 2:\n            if args[0] is type(None):\n                return f'typing.Optional[{_type_repr(args[1])}]'\n            elif args[1] is type(None):\n                return f'typing.Optional[{_type_repr(args[0])}]'\n        return super().__repr__()\n\n    def __instancecheck__(self, obj):\n        return self.__subclasscheck__(type(obj))\n\n    def __subclasscheck__(self, cls):\n        for arg in self.__args__:\n            if issubclass(cls, arg):\n                return True\n\n    def __reduce__(self):\n        func, (origin, args) = super().__reduce__()\n        return func, (Union, args)\n\n\ndef _value_and_type_iter(parameters):\n    return ((p, type(p)) for p in parameters)\n\n\nclass _LiteralGenericAlias(_GenericAlias, _root=True):\n\n    def __eq__(self, other):\n        if not isinstance(other, _LiteralGenericAlias):\n            return NotImplemented\n\n        return set(_value_and_type_iter(self.__args__)) == set(_value_and_type_iter(other.__args__))\n\n    def __hash__(self):\n        return hash(frozenset(_value_and_type_iter(self.__args__)))\n\n\nclass _ConcatenateGenericAlias(_GenericAlias, _root=True):\n    def copy_with(self, params):\n        if isinstance(params[-1], (list, tuple)):\n            return (*params[:-1], *params[-1])\n        if isinstance(params[-1], _ConcatenateGenericAlias):\n            params = (*params[:-1], *params[-1].__args__)\n        elif not isinstance(params[-1], ParamSpec):\n            raise TypeError(\"The last parameter to Concatenate should be a \"\n                            \"ParamSpec variable.\")\n        return super().copy_with(params)\n\n\nclass Generic:\n    \"\"\"Abstract base class for generic types.\n\n    A generic type is typically declared by inheriting from\n    this class parameterized with one or more type variables.\n    For example, a generic mapping type might be defined as::\n\n      class Mapping(Generic[KT, VT]):\n          def __getitem__(self, key: KT) -> VT:\n              ...\n          # Etc.\n\n    This class can then be used as follows::\n\n      def lookup_name(mapping: Mapping[KT, VT], key: KT, default: VT) -> VT:\n          try:\n              return mapping[key]\n          except KeyError:\n              return default\n    \"\"\"\n    __slots__ = ()\n    _is_protocol = False\n\n    @_tp_cache\n    def __class_getitem__(cls, params):\n        if not isinstance(params, tuple):\n            params = (params,)\n        if not params and cls is not Tuple:\n            raise TypeError(\n                f\"Parameter list to {cls.__qualname__}[...] cannot be empty\")\n        params = tuple(_type_convert(p) for p in params)\n        if cls in (Generic, Protocol):\n            # Generic and Protocol can only be subscripted with unique type variables.\n            if not all(isinstance(p, (TypeVar, ParamSpec)) for p in params):\n                raise TypeError(\n                    f\"Parameters to {cls.__name__}[...] must all be type variables \"\n                    f\"or parameter specification variables.\")\n            if len(set(params)) != len(params):\n                raise TypeError(\n                    f\"Parameters to {cls.__name__}[...] must all be unique\")\n        else:\n            # Subscripting a regular Generic subclass.\n            if any(isinstance(t, ParamSpec) for t in cls.__parameters__):\n                params = _prepare_paramspec_params(cls, params)\n            else:\n                _check_generic(cls, params, len(cls.__parameters__))\n        return _GenericAlias(cls, params,\n                             _typevar_types=(TypeVar, ParamSpec),\n                             _paramspec_tvars=True)\n\n    def __init_subclass__(cls, *args, **kwargs):\n        super().__init_subclass__(*args, **kwargs)\n        tvars = []\n        if '__orig_bases__' in cls.__dict__:\n            error = Generic in cls.__orig_bases__\n        else:\n            error = Generic in cls.__bases__ and cls.__name__ != 'Protocol'\n        if error:\n            raise TypeError(\"Cannot inherit from plain Generic\")\n        if '__orig_bases__' in cls.__dict__:\n            tvars = _collect_type_vars(cls.__orig_bases__, (TypeVar, ParamSpec))\n            # Look for Generic[T1, ..., Tn].\n            # If found, tvars must be a subset of it.\n            # If not found, tvars is it.\n            # Also check for and reject plain Generic,\n            # and reject multiple Generic[...].\n            gvars = None\n            for base in cls.__orig_bases__:\n                if (isinstance(base, _GenericAlias) and\n                        base.__origin__ is Generic):\n                    if gvars is not None:\n                        raise TypeError(\n                            \"Cannot inherit from Generic[...] multiple types.\")\n                    gvars = base.__parameters__\n            if gvars is not None:\n                tvarset = set(tvars)\n                gvarset = set(gvars)\n                if not tvarset <= gvarset:\n                    s_vars = ', '.join(str(t) for t in tvars if t not in gvarset)\n                    s_args = ', '.join(str(g) for g in gvars)\n                    raise TypeError(f\"Some type variables ({s_vars}) are\"\n                                    f\" not listed in Generic[{s_args}]\")\n                tvars = gvars\n        cls.__parameters__ = tuple(tvars)\n\n\nclass _TypingEmpty:\n    \"\"\"Internal placeholder for () or []. Used by TupleMeta and CallableMeta\n    to allow empty list/tuple in specific places, without allowing them\n    to sneak in where prohibited.\n    \"\"\"\n\n\nclass _TypingEllipsis:\n    \"\"\"Internal placeholder for ... (ellipsis).\"\"\"\n\n\n_TYPING_INTERNALS = ['__parameters__', '__orig_bases__',  '__orig_class__',\n                     '_is_protocol', '_is_runtime_protocol']\n\n_SPECIAL_NAMES = ['__abstractmethods__', '__annotations__', '__dict__', '__doc__',\n                  '__init__', '__module__', '__new__', '__slots__',\n                  '__subclasshook__', '__weakref__', '__class_getitem__']\n\n# These special attributes will be not collected as protocol members.\nEXCLUDED_ATTRIBUTES = _TYPING_INTERNALS + _SPECIAL_NAMES + ['_MutableMapping__marker']\n\n\ndef _get_protocol_attrs(cls):\n    \"\"\"Collect protocol members from a protocol class objects.\n\n    This includes names actually defined in the class dictionary, as well\n    as names that appear in annotations. Special names (above) are skipped.\n    \"\"\"\n    attrs = set()\n    for base in cls.__mro__[:-1]:  # without object\n        if base.__name__ in ('Protocol', 'Generic'):\n            continue\n        annotations = getattr(base, '__annotations__', {})\n        for attr in list(base.__dict__.keys()) + list(annotations.keys()):\n            if not attr.startswith('_abc_') and attr not in EXCLUDED_ATTRIBUTES:\n                attrs.add(attr)\n    return attrs\n\n\ndef _is_callable_members_only(cls):\n    # PEP 544 prohibits using issubclass() with protocols that have non-method members.\n    return all(callable(getattr(cls, attr, None)) for attr in _get_protocol_attrs(cls))\n\n\ndef _no_init_or_replace_init(self, *args, **kwargs):\n    cls = type(self)\n\n    if cls._is_protocol:\n        raise TypeError('Protocols cannot be instantiated')\n\n    # Already using a custom `__init__`. No need to calculate correct\n    # `__init__` to call. This can lead to RecursionError. See bpo-45121.\n    if cls.__init__ is not _no_init_or_replace_init:\n        return\n\n    # Initially, `__init__` of a protocol subclass is set to `_no_init_or_replace_init`.\n    # The first instantiation of the subclass will call `_no_init_or_replace_init` which\n    # searches for a proper new `__init__` in the MRO. The new `__init__`\n    # replaces the subclass' old `__init__` (ie `_no_init_or_replace_init`). Subsequent\n    # instantiation of the protocol subclass will thus use the new\n    # `__init__` and no longer call `_no_init_or_replace_init`.\n    for base in cls.__mro__:\n        init = base.__dict__.get('__init__', _no_init_or_replace_init)\n        if init is not _no_init_or_replace_init:\n            cls.__init__ = init\n            break\n    else:\n        # should not happen\n        cls.__init__ = object.__init__\n\n    cls.__init__(self, *args, **kwargs)\n\n\ndef _caller(depth=1, default='__main__'):\n    try:\n        return sys._getframe(depth + 1).f_globals.get('__name__', default)\n    except (AttributeError, ValueError):  # For platforms without _getframe()\n        return None\n\n\ndef _allow_reckless_class_checks(depth=3):\n    \"\"\"Allow instance and class checks for special stdlib modules.\n\n    The abc and functools modules indiscriminately call isinstance() and\n    issubclass() on the whole MRO of a user class, which may contain protocols.\n    \"\"\"\n    try:\n        return sys._getframe(depth).f_globals['__name__'] in ['abc', 'functools']\n    except (AttributeError, ValueError):  # For platforms without _getframe().\n        return True\n\n\n_PROTO_ALLOWLIST = {\n    'collections.abc': [\n        'Callable', 'Awaitable', 'Iterable', 'Iterator', 'AsyncIterable',\n        'Hashable', 'Sized', 'Container', 'Collection', 'Reversible',\n    ],\n    'contextlib': ['AbstractContextManager', 'AbstractAsyncContextManager'],\n}\n\n\nclass _ProtocolMeta(ABCMeta):\n    # This metaclass is really unfortunate and exists only because of\n    # the lack of __instancehook__.\n    def __instancecheck__(cls, instance):\n        # We need this method for situations where attributes are\n        # assigned in __init__.\n        if (\n            getattr(cls, '_is_protocol', False) and\n            not getattr(cls, '_is_runtime_protocol', False) and\n            not _allow_reckless_class_checks(depth=2)\n        ):\n            raise TypeError(\"Instance and class checks can only be used with\"\n                            \" @runtime_checkable protocols\")\n\n        if ((not getattr(cls, '_is_protocol', False) or\n                _is_callable_members_only(cls)) and\n                issubclass(instance.__class__, cls)):\n            return True\n        if cls._is_protocol:\n            if all(hasattr(instance, attr) and\n                    # All *methods* can be blocked by setting them to None.\n                    (not callable(getattr(cls, attr, None)) or\n                     getattr(instance, attr) is not None)\n                    for attr in _get_protocol_attrs(cls)):\n                return True\n        return super().__instancecheck__(instance)\n\n\nclass Protocol(Generic, metaclass=_ProtocolMeta):\n    \"\"\"Base class for protocol classes.\n\n    Protocol classes are defined as::\n\n        class Proto(Protocol):\n            def meth(self) -> int:\n                ...\n\n    Such classes are primarily used with static type checkers that recognize\n    structural subtyping (static duck-typing), for example::\n\n        class C:\n            def meth(self) -> int:\n                return 0\n\n        def func(x: Proto) -> int:\n            return x.meth()\n\n        func(C())  # Passes static type check\n\n    See PEP 544 for details. Protocol classes decorated with\n    @typing.runtime_checkable act as simple-minded runtime protocols that check\n    only the presence of given attributes, ignoring their type signatures.\n    Protocol classes can be generic, they are defined as::\n\n        class GenProto(Protocol[T]):\n            def meth(self) -> T:\n                ...\n    \"\"\"\n    __slots__ = ()\n    _is_protocol = True\n    _is_runtime_protocol = False\n\n    def __init_subclass__(cls, *args, **kwargs):\n        super().__init_subclass__(*args, **kwargs)\n\n        # Determine if this is a protocol or a concrete subclass.\n        if not cls.__dict__.get('_is_protocol', False):\n            cls._is_protocol = any(b is Protocol for b in cls.__bases__)\n\n        # Set (or override) the protocol subclass hook.\n        def _proto_hook(other):\n            if not cls.__dict__.get('_is_protocol', False):\n                return NotImplemented\n\n            # First, perform various sanity checks.\n            if not getattr(cls, '_is_runtime_protocol', False):\n                if _allow_reckless_class_checks():\n                    return NotImplemented\n                raise TypeError(\"Instance and class checks can only be used with\"\n                                \" @runtime_checkable protocols\")\n            if not _is_callable_members_only(cls):\n                if _allow_reckless_class_checks():\n                    return NotImplemented\n                raise TypeError(\"Protocols with non-method members\"\n                                \" don't support issubclass()\")\n            if not isinstance(other, type):\n                # Same error message as for issubclass(1, int).\n                raise TypeError('issubclass() arg 1 must be a class')\n\n            # Second, perform the actual structural compatibility check.\n            for attr in _get_protocol_attrs(cls):\n                for base in other.__mro__:\n                    # Check if the members appears in the class dictionary...\n                    if attr in base.__dict__:\n                        if base.__dict__[attr] is None:\n                            return NotImplemented\n                        break\n\n                    # ...or in annotations, if it is a sub-protocol.\n                    annotations = getattr(base, '__annotations__', {})\n                    if (isinstance(annotations, collections.abc.Mapping) and\n                            attr in annotations and\n                            issubclass(other, Generic) and other._is_protocol):\n                        break\n                else:\n                    return NotImplemented\n            return True\n\n        if '__subclasshook__' not in cls.__dict__:\n            cls.__subclasshook__ = _proto_hook\n\n        # We have nothing more to do for non-protocols...\n        if not cls._is_protocol:\n            return\n\n        # ... otherwise check consistency of bases, and prohibit instantiation.\n        for base in cls.__bases__:\n            if not (base in (object, Generic) or\n                    base.__module__ in _PROTO_ALLOWLIST and\n                    base.__name__ in _PROTO_ALLOWLIST[base.__module__] or\n                    issubclass(base, Generic) and base._is_protocol):\n                raise TypeError('Protocols can only inherit from other'\n                                ' protocols, got %r' % base)\n        cls.__init__ = _no_init_or_replace_init\n\n\nclass _AnnotatedAlias(_GenericAlias, _root=True):\n    \"\"\"Runtime representation of an annotated type.\n\n    At its core 'Annotated[t, dec1, dec2, ...]' is an alias for the type 't'\n    with extra annotations. The alias behaves like a normal typing alias,\n    instantiating is the same as instantiating the underlying type, binding\n    it to types is also the same.\n    \"\"\"\n    def __init__(self, origin, metadata):\n        if isinstance(origin, _AnnotatedAlias):\n            metadata = origin.__metadata__ + metadata\n            origin = origin.__origin__\n        super().__init__(origin, origin)\n        self.__metadata__ = metadata\n\n    def copy_with(self, params):\n        assert len(params) == 1\n        new_type = params[0]\n        return _AnnotatedAlias(new_type, self.__metadata__)\n\n    def __repr__(self):\n        return \"typing.Annotated[{}, {}]\".format(\n            _type_repr(self.__origin__),\n            \", \".join(repr(a) for a in self.__metadata__)\n        )\n\n    def __reduce__(self):\n        return operator.getitem, (\n            Annotated, (self.__origin__,) + self.__metadata__\n        )\n\n    def __eq__(self, other):\n        if not isinstance(other, _AnnotatedAlias):\n            return NotImplemented\n        return (self.__origin__ == other.__origin__\n                and self.__metadata__ == other.__metadata__)\n\n    def __hash__(self):\n        return hash((self.__origin__, self.__metadata__))\n\n    def __getattr__(self, attr):\n        if attr in {'__name__', '__qualname__'}:\n            return 'Annotated'\n        return super().__getattr__(attr)\n\n\nclass Annotated:\n    \"\"\"Add context specific metadata to a type.\n\n    Example: Annotated[int, runtime_check.Unsigned] indicates to the\n    hypothetical runtime_check module that this type is an unsigned int.\n    Every other consumer of this type can ignore this metadata and treat\n    this type as int.\n\n    The first argument to Annotated must be a valid type.\n\n    Details:\n\n    - It's an error to call `Annotated` with less than two arguments.\n    - Nested Annotated are flattened::\n\n        Annotated[Annotated[T, Ann1, Ann2], Ann3] == Annotated[T, Ann1, Ann2, Ann3]\n\n    - Instantiating an annotated type is equivalent to instantiating the\n    underlying type::\n\n        Annotated[C, Ann1](5) == C(5)\n\n    - Annotated can be used as a generic type alias::\n\n        Optimized = Annotated[T, runtime.Optimize()]\n        Optimized[int] == Annotated[int, runtime.Optimize()]\n\n        OptimizedList = Annotated[List[T], runtime.Optimize()]\n        OptimizedList[int] == Annotated[List[int], runtime.Optimize()]\n    \"\"\"\n\n    __slots__ = ()\n\n    def __new__(cls, *args, **kwargs):\n        raise TypeError(\"Type Annotated cannot be instantiated.\")\n\n    @_tp_cache\n    def __class_getitem__(cls, params):\n        if not isinstance(params, tuple) or len(params) < 2:\n            raise TypeError(\"Annotated[...] should be used \"\n                            \"with at least two arguments (a type and an \"\n                            \"annotation).\")\n        msg = \"Annotated[t, ...]: t must be a type.\"\n        origin = _type_check(params[0], msg, allow_special_forms=True)\n        metadata = tuple(params[1:])\n        return _AnnotatedAlias(origin, metadata)\n\n    def __init_subclass__(cls, *args, **kwargs):\n        raise TypeError(\n            \"Cannot subclass {}.Annotated\".format(cls.__module__)\n        )\n\n\ndef runtime_checkable(cls):\n    \"\"\"Mark a protocol class as a runtime protocol.\n\n    Such protocol can be used with isinstance() and issubclass().\n    Raise TypeError if applied to a non-protocol class.\n    This allows a simple-minded structural check very similar to\n    one trick ponies in collections.abc such as Iterable.\n    For example::\n\n        @runtime_checkable\n        class Closable(Protocol):\n            def close(self): ...\n\n        assert isinstance(open('/some/file'), Closable)\n\n    Warning: this will check only the presence of the required methods,\n    not their type signatures!\n    \"\"\"\n    if not issubclass(cls, Generic) or not cls._is_protocol:\n        raise TypeError('@runtime_checkable can be only applied to protocol classes,'\n                        ' got %r' % cls)\n    cls._is_runtime_protocol = True\n    return cls\n\n\ndef cast(typ, val):\n    \"\"\"Cast a value to a type.\n\n    This returns the value unchanged.  To the type checker this\n    signals that the return value has the designated type, but at\n    runtime we intentionally don't check anything (we want this\n    to be as fast as possible).\n    \"\"\"\n    return val\n\n\ndef _get_defaults(func):\n    \"\"\"Internal helper to extract the default arguments, by name.\"\"\"\n    try:\n        code = func.__code__\n    except AttributeError:\n        # Some built-in functions don't have __code__, __defaults__, etc.\n        return {}\n    pos_count = code.co_argcount\n    arg_names = code.co_varnames\n    arg_names = arg_names[:pos_count]\n    defaults = func.__defaults__ or ()\n    kwdefaults = func.__kwdefaults__\n    res = dict(kwdefaults) if kwdefaults else {}\n    pos_offset = pos_count - len(defaults)\n    for name, value in zip(arg_names[pos_offset:], defaults):\n        assert name not in res\n        res[name] = value\n    return res\n\n\n_allowed_types = (types.FunctionType, types.BuiltinFunctionType,\n                  types.MethodType, types.ModuleType,\n                  WrapperDescriptorType, MethodWrapperType, MethodDescriptorType)\n\n\ndef get_type_hints(obj, globalns=None, localns=None, include_extras=False):\n    \"\"\"Return type hints for an object.\n\n    This is often the same as obj.__annotations__, but it handles\n    forward references encoded as string literals, adds Optional[t] if a\n    default value equal to None is set and recursively replaces all\n    'Annotated[T, ...]' with 'T' (unless 'include_extras=True').\n\n    The argument may be a module, class, method, or function. The annotations\n    are returned as a dictionary. For classes, annotations include also\n    inherited members.\n\n    TypeError is raised if the argument is not of a type that can contain\n    annotations, and an empty dictionary is returned if no annotations are\n    present.\n\n    BEWARE -- the behavior of globalns and localns is counterintuitive\n    (unless you are familiar with how eval() and exec() work).  The\n    search order is locals first, then globals.\n\n    - If no dict arguments are passed, an attempt is made to use the\n      globals from obj (or the respective module's globals for classes),\n      and these are also used as the locals.  If the object does not appear\n      to have globals, an empty dictionary is used.  For classes, the search\n      order is globals first then locals.\n\n    - If one dict argument is passed, it is used for both globals and\n      locals.\n\n    - If two dict arguments are passed, they specify globals and\n      locals, respectively.\n    \"\"\"\n\n    if getattr(obj, '__no_type_check__', None):\n        return {}\n    # Classes require a special treatment.\n    if isinstance(obj, type):\n        hints = {}\n        for base in reversed(obj.__mro__):\n            if globalns is None:\n                base_globals = getattr(sys.modules.get(base.__module__, None), '__dict__', {})\n            else:\n                base_globals = globalns\n            ann = base.__dict__.get('__annotations__', {})\n            if isinstance(ann, types.GetSetDescriptorType):\n                ann = {}\n            base_locals = dict(vars(base)) if localns is None else localns\n            if localns is None and globalns is None:\n                # This is surprising, but required.  Before Python 3.10,\n                # get_type_hints only evaluated the globalns of\n                # a class.  To maintain backwards compatibility, we reverse\n                # the globalns and localns order so that eval() looks into\n                # *base_globals* first rather than *base_locals*.\n                # This only affects ForwardRefs.\n                base_globals, base_locals = base_locals, base_globals\n            for name, value in ann.items():\n                if value is None:\n                    value = type(None)\n                if isinstance(value, str):\n                    value = ForwardRef(value, is_argument=False, is_class=True)\n                value = _eval_type(value, base_globals, base_locals)\n                hints[name] = value\n        return hints if include_extras else {k: _strip_annotations(t) for k, t in hints.items()}\n\n    if globalns is None:\n        if isinstance(obj, types.ModuleType):\n            globalns = obj.__dict__\n        else:\n            nsobj = obj\n            # Find globalns for the unwrapped object.\n            while hasattr(nsobj, '__wrapped__'):\n                nsobj = nsobj.__wrapped__\n            globalns = getattr(nsobj, '__globals__', {})\n        if localns is None:\n            localns = globalns\n    elif localns is None:\n        localns = globalns\n    hints = getattr(obj, '__annotations__', None)\n    if hints is None:\n        # Return empty annotations for something that _could_ have them.\n        if isinstance(obj, _allowed_types):\n            return {}\n        else:\n            raise TypeError('{!r} is not a module, class, method, '\n                            'or function.'.format(obj))\n    defaults = _get_defaults(obj)\n    hints = dict(hints)\n    for name, value in hints.items():\n        if value is None:\n            value = type(None)\n        if isinstance(value, str):\n            # class-level forward refs were handled above, this must be either\n            # a module-level annotation or a function argument annotation\n            value = ForwardRef(\n                value,\n                is_argument=not isinstance(obj, types.ModuleType),\n                is_class=False,\n            )\n        value = _eval_type(value, globalns, localns)\n        if name in defaults and defaults[name] is None:\n            value = Optional[value]\n        hints[name] = value\n    return hints if include_extras else {k: _strip_annotations(t) for k, t in hints.items()}\n\n\ndef _strip_annotations(t):\n    \"\"\"Strips the annotations from a given type.\n    \"\"\"\n    if isinstance(t, _AnnotatedAlias):\n        return _strip_annotations(t.__origin__)\n    if isinstance(t, _GenericAlias):\n        stripped_args = tuple(_strip_annotations(a) for a in t.__args__)\n        if stripped_args == t.__args__:\n            return t\n        return t.copy_with(stripped_args)\n    if isinstance(t, GenericAlias):\n        stripped_args = tuple(_strip_annotations(a) for a in t.__args__)\n        if stripped_args == t.__args__:\n            return t\n        return GenericAlias(t.__origin__, stripped_args)\n    if isinstance(t, types.UnionType):\n        stripped_args = tuple(_strip_annotations(a) for a in t.__args__)\n        if stripped_args == t.__args__:\n            return t\n        return functools.reduce(operator.or_, stripped_args)\n\n    return t\n\n\ndef get_origin(tp):\n    \"\"\"Get the unsubscripted version of a type.\n\n    This supports generic types, Callable, Tuple, Union, Literal, Final, ClassVar\n    and Annotated. Return None for unsupported types. Examples::\n\n        get_origin(Literal[42]) is Literal\n        get_origin(int) is None\n        get_origin(ClassVar[int]) is ClassVar\n        get_origin(Generic) is Generic\n        get_origin(Generic[T]) is Generic\n        get_origin(Union[T, int]) is Union\n        get_origin(List[Tuple[T, T]][int]) == list\n        get_origin(P.args) is P\n    \"\"\"\n    if isinstance(tp, _AnnotatedAlias):\n        return Annotated\n    if isinstance(tp, (_BaseGenericAlias, GenericAlias,\n                       ParamSpecArgs, ParamSpecKwargs)):\n        return tp.__origin__\n    if tp is Generic:\n        return Generic\n    if isinstance(tp, types.UnionType):\n        return types.UnionType\n    return None\n\n\ndef get_args(tp):\n    \"\"\"Get type arguments with all substitutions performed.\n\n    For unions, basic simplifications used by Union constructor are performed.\n    Examples::\n        get_args(Dict[str, int]) == (str, int)\n        get_args(int) == ()\n        get_args(Union[int, Union[T, int], str][int]) == (int, str)\n        get_args(Union[int, Tuple[T, int]][str]) == (int, Tuple[str, int])\n        get_args(Callable[[], T][int]) == ([], int)\n    \"\"\"\n    if isinstance(tp, _AnnotatedAlias):\n        return (tp.__origin__,) + tp.__metadata__\n    if isinstance(tp, (_GenericAlias, GenericAlias)):\n        res = tp.__args__\n        if (tp.__origin__ is collections.abc.Callable\n                and not (len(res) == 2 and _is_param_expr(res[0]))):\n            res = (list(res[:-1]), res[-1])\n        return res\n    if isinstance(tp, types.UnionType):\n        return tp.__args__\n    return ()\n\n\ndef is_typeddict(tp):\n    \"\"\"Check if an annotation is a TypedDict class\n\n    For example::\n        class Film(TypedDict):\n            title: str\n            year: int\n\n        is_typeddict(Film)  # => True\n        is_typeddict(Union[list, str])  # => False\n    \"\"\"\n    return isinstance(tp, _TypedDictMeta)\n\n\ndef no_type_check(arg):\n    \"\"\"Decorator to indicate that annotations are not type hints.\n\n    The argument must be a class or function; if it is a class, it\n    applies recursively to all methods and classes defined in that class\n    (but not to methods defined in its superclasses or subclasses).\n\n    This mutates the function(s) or class(es) in place.\n    \"\"\"\n    if isinstance(arg, type):\n        arg_attrs = arg.__dict__.copy()\n        for attr, val in arg.__dict__.items():\n            if val in arg.__bases__ + (arg,):\n                arg_attrs.pop(attr)\n        for obj in arg_attrs.values():\n            if isinstance(obj, types.FunctionType):\n                obj.__no_type_check__ = True\n            if isinstance(obj, type):\n                no_type_check(obj)\n    try:\n        arg.__no_type_check__ = True\n    except TypeError:  # built-in classes\n        pass\n    return arg\n\n\ndef no_type_check_decorator(decorator):\n    \"\"\"Decorator to give another decorator the @no_type_check effect.\n\n    This wraps the decorator with something that wraps the decorated\n    function in @no_type_check.\n    \"\"\"\n\n    @functools.wraps(decorator)\n    def wrapped_decorator(*args, **kwds):\n        func = decorator(*args, **kwds)\n        func = no_type_check(func)\n        return func\n\n    return wrapped_decorator\n\n\ndef _overload_dummy(*args, **kwds):\n    \"\"\"Helper for @overload to raise when called.\"\"\"\n    raise NotImplementedError(\n        \"You should not call an overloaded function. \"\n        \"A series of @overload-decorated functions \"\n        \"outside a stub module should always be followed \"\n        \"by an implementation that is not @overload-ed.\")\n\n\ndef overload(func):\n    \"\"\"Decorator for overloaded functions/methods.\n\n    In a stub file, place two or more stub definitions for the same\n    function in a row, each decorated with @overload.  For example:\n\n      @overload\n      def utf8(value: None) -> None: ...\n      @overload\n      def utf8(value: bytes) -> bytes: ...\n      @overload\n      def utf8(value: str) -> bytes: ...\n\n    In a non-stub file (i.e. a regular .py file), do the same but\n    follow it with an implementation.  The implementation should *not*\n    be decorated with @overload.  For example:\n\n      @overload\n      def utf8(value: None) -> None: ...\n      @overload\n      def utf8(value: bytes) -> bytes: ...\n      @overload\n      def utf8(value: str) -> bytes: ...\n      def utf8(value):\n          # implementation goes here\n    \"\"\"\n    return _overload_dummy\n\n\ndef final(f):\n    \"\"\"A decorator to indicate final methods and final classes.\n\n    Use this decorator to indicate to type checkers that the decorated\n    method cannot be overridden, and decorated class cannot be subclassed.\n    For example:\n\n      class Base:\n          @final\n          def done(self) -> None:\n              ...\n      class Sub(Base):\n          def done(self) -> None:  # Error reported by type checker\n                ...\n\n      @final\n      class Leaf:\n          ...\n      class Other(Leaf):  # Error reported by type checker\n          ...\n\n    There is no runtime checking of these properties.\n    \"\"\"\n    return f\n\n\n# Some unconstrained type variables.  These are used by the container types.\n# (These are not for export.)\nT = TypeVar('T')  # Any type.\nKT = TypeVar('KT')  # Key type.\nVT = TypeVar('VT')  # Value type.\nT_co = TypeVar('T_co', covariant=True)  # Any type covariant containers.\nV_co = TypeVar('V_co', covariant=True)  # Any type covariant containers.\nVT_co = TypeVar('VT_co', covariant=True)  # Value type covariant containers.\nT_contra = TypeVar('T_contra', contravariant=True)  # Ditto contravariant.\n# Internal type variable used for Type[].\nCT_co = TypeVar('CT_co', covariant=True, bound=type)\n\n# A useful type variable with constraints.  This represents string types.\n# (This one *is* for export!)\nAnyStr = TypeVar('AnyStr', bytes, str)\n\n\n# Various ABCs mimicking those in collections.abc.\n_alias = _SpecialGenericAlias\n\nHashable = _alias(collections.abc.Hashable, 0)  # Not generic.\nAwaitable = _alias(collections.abc.Awaitable, 1)\nCoroutine = _alias(collections.abc.Coroutine, 3)\nAsyncIterable = _alias(collections.abc.AsyncIterable, 1)\nAsyncIterator = _alias(collections.abc.AsyncIterator, 1)\nIterable = _alias(collections.abc.Iterable, 1)\nIterator = _alias(collections.abc.Iterator, 1)\nReversible = _alias(collections.abc.Reversible, 1)\nSized = _alias(collections.abc.Sized, 0)  # Not generic.\nContainer = _alias(collections.abc.Container, 1)\nCollection = _alias(collections.abc.Collection, 1)\nCallable = _CallableType(collections.abc.Callable, 2)\nCallable.__doc__ = \\\n    \"\"\"Callable type; Callable[[int], str] is a function of (int) -> str.\n\n    The subscription syntax must always be used with exactly two\n    values: the argument list and the return type.  The argument list\n    must be a list of types or ellipsis; the return type must be a single type.\n\n    There is no syntax to indicate optional or keyword arguments,\n    such function types are rarely used as callback types.\n    \"\"\"\nAbstractSet = _alias(collections.abc.Set, 1, name='AbstractSet')\nMutableSet = _alias(collections.abc.MutableSet, 1)\n# NOTE: Mapping is only covariant in the value type.\nMapping = _alias(collections.abc.Mapping, 2)\nMutableMapping = _alias(collections.abc.MutableMapping, 2)\nSequence = _alias(collections.abc.Sequence, 1)\nMutableSequence = _alias(collections.abc.MutableSequence, 1)\nByteString = _alias(collections.abc.ByteString, 0)  # Not generic\n# Tuple accepts variable number of parameters.\nTuple = _TupleType(tuple, -1, inst=False, name='Tuple')\nTuple.__doc__ = \\\n    \"\"\"Tuple type; Tuple[X, Y] is the cross-product type of X and Y.\n\n    Example: Tuple[T1, T2] is a tuple of two elements corresponding\n    to type variables T1 and T2.  Tuple[int, float, str] is a tuple\n    of an int, a float and a string.\n\n    To specify a variable-length tuple of homogeneous type, use Tuple[T, ...].\n    \"\"\"\nList = _alias(list, 1, inst=False, name='List')\nDeque = _alias(collections.deque, 1, name='Deque')\nSet = _alias(set, 1, inst=False, name='Set')\nFrozenSet = _alias(frozenset, 1, inst=False, name='FrozenSet')\nMappingView = _alias(collections.abc.MappingView, 1)\nKeysView = _alias(collections.abc.KeysView, 1)\nItemsView = _alias(collections.abc.ItemsView, 2)\nValuesView = _alias(collections.abc.ValuesView, 1)\nContextManager = _alias(contextlib.AbstractContextManager, 1, name='ContextManager')\nAsyncContextManager = _alias(contextlib.AbstractAsyncContextManager, 1, name='AsyncContextManager')\nDict = _alias(dict, 2, inst=False, name='Dict')\nDefaultDict = _alias(collections.defaultdict, 2, name='DefaultDict')\nOrderedDict = _alias(collections.OrderedDict, 2)\nCounter = _alias(collections.Counter, 1)\nChainMap = _alias(collections.ChainMap, 2)\nGenerator = _alias(collections.abc.Generator, 3)\nAsyncGenerator = _alias(collections.abc.AsyncGenerator, 2)\nType = _alias(type, 1, inst=False, name='Type')\nType.__doc__ = \\\n    \"\"\"A special construct usable to annotate class objects.\n\n    For example, suppose we have the following classes::\n\n      class User: ...  # Abstract base for User classes\n      class BasicUser(User): ...\n      class ProUser(User): ...\n      class TeamUser(User): ...\n\n    And a function that takes a class argument that's a subclass of\n    User and returns an instance of the corresponding class::\n\n      U = TypeVar('U', bound=User)\n      def new_user(user_class: Type[U]) -> U:\n          user = user_class()\n          # (Here we could write the user object to a database)\n          return user\n\n      joe = new_user(BasicUser)\n\n    At this point the type checker knows that joe has type BasicUser.\n    \"\"\"\n\n\n@runtime_checkable\nclass SupportsInt(Protocol):\n    \"\"\"An ABC with one abstract method __int__.\"\"\"\n    __slots__ = ()\n\n    @abstractmethod\n    def __int__(self) -> int:\n        pass\n\n\n@runtime_checkable\nclass SupportsFloat(Protocol):\n    \"\"\"An ABC with one abstract method __float__.\"\"\"\n    __slots__ = ()\n\n    @abstractmethod\n    def __float__(self) -> float:\n        pass\n\n\n@runtime_checkable\nclass SupportsComplex(Protocol):\n    \"\"\"An ABC with one abstract method __complex__.\"\"\"\n    __slots__ = ()\n\n    @abstractmethod\n    def __complex__(self) -> complex:\n        pass\n\n\n@runtime_checkable\nclass SupportsBytes(Protocol):\n    \"\"\"An ABC with one abstract method __bytes__.\"\"\"\n    __slots__ = ()\n\n    @abstractmethod\n    def __bytes__(self) -> bytes:\n        pass\n\n\n@runtime_checkable\nclass SupportsIndex(Protocol):\n    \"\"\"An ABC with one abstract method __index__.\"\"\"\n    __slots__ = ()\n\n    @abstractmethod\n    def __index__(self) -> int:\n        pass\n\n\n@runtime_checkable\nclass SupportsAbs(Protocol[T_co]):\n    \"\"\"An ABC with one abstract method __abs__ that is covariant in its return type.\"\"\"\n    __slots__ = ()\n\n    @abstractmethod\n    def __abs__(self) -> T_co:\n        pass\n\n\n@runtime_checkable\nclass SupportsRound(Protocol[T_co]):\n    \"\"\"An ABC with one abstract method __round__ that is covariant in its return type.\"\"\"\n    __slots__ = ()\n\n    @abstractmethod\n    def __round__(self, ndigits: int = 0) -> T_co:\n        pass\n\n\ndef _make_nmtuple(name, types, module, defaults = ()):\n    fields = [n for n, t in types]\n    types = {n: _type_check(t, f\"field {n} annotation must be a type\")\n             for n, t in types}\n    nm_tpl = collections.namedtuple(name, fields,\n                                    defaults=defaults, module=module)\n    nm_tpl.__annotations__ = nm_tpl.__new__.__annotations__ = types\n    return nm_tpl\n\n\n# attributes prohibited to set in NamedTuple class syntax\n_prohibited = frozenset({'__new__', '__init__', '__slots__', '__getnewargs__',\n                         '_fields', '_field_defaults',\n                         '_make', '_replace', '_asdict', '_source'})\n\n_special = frozenset({'__module__', '__name__', '__annotations__'})\n\n\nclass NamedTupleMeta(type):\n\n    def __new__(cls, typename, bases, ns):\n        assert bases[0] is _NamedTuple\n        types = ns.get('__annotations__', {})\n        default_names = []\n        for field_name in types:\n            if field_name in ns:\n                default_names.append(field_name)\n            elif default_names:\n                raise TypeError(f\"Non-default namedtuple field {field_name} \"\n                                f\"cannot follow default field\"\n                                f\"{'s' if len(default_names) > 1 else ''} \"\n                                f\"{', '.join(default_names)}\")\n        nm_tpl = _make_nmtuple(typename, types.items(),\n                               defaults=[ns[n] for n in default_names],\n                               module=ns['__module__'])\n        # update from user namespace without overriding special namedtuple attributes\n        for key in ns:\n            if key in _prohibited:\n                raise AttributeError(\"Cannot overwrite NamedTuple attribute \" + key)\n            elif key not in _special and key not in nm_tpl._fields:\n                setattr(nm_tpl, key, ns[key])\n        return nm_tpl\n\n\ndef NamedTuple(typename, fields=None, /, **kwargs):\n    \"\"\"Typed version of namedtuple.\n\n    Usage in Python versions >= 3.6::\n\n        class Employee(NamedTuple):\n            name: str\n            id: int\n\n    This is equivalent to::\n\n        Employee = collections.namedtuple('Employee', ['name', 'id'])\n\n    The resulting class has an extra __annotations__ attribute, giving a\n    dict that maps field names to types.  (The field names are also in\n    the _fields attribute, which is part of the namedtuple API.)\n    Alternative equivalent keyword syntax is also accepted::\n\n        Employee = NamedTuple('Employee', name=str, id=int)\n\n    In Python versions <= 3.5 use::\n\n        Employee = NamedTuple('Employee', [('name', str), ('id', int)])\n    \"\"\"\n    if fields is None:\n        fields = kwargs.items()\n    elif kwargs:\n        raise TypeError(\"Either list of fields or keywords\"\n                        \" can be provided to NamedTuple, not both\")\n    try:\n        module = sys._getframe(1).f_globals.get('__name__', '__main__')\n    except (AttributeError, ValueError):\n        module = None\n    return _make_nmtuple(typename, fields, module=module)\n\n_NamedTuple = type.__new__(NamedTupleMeta, 'NamedTuple', (), {})\n\ndef _namedtuple_mro_entries(bases):\n    if len(bases) > 1:\n        raise TypeError(\"Multiple inheritance with NamedTuple is not supported\")\n    assert bases[0] is NamedTuple\n    return (_NamedTuple,)\n\nNamedTuple.__mro_entries__ = _namedtuple_mro_entries\n\n\nclass _TypedDictMeta(type):\n    def __new__(cls, name, bases, ns, total=True):\n        \"\"\"Create new typed dict class object.\n\n        This method is called when TypedDict is subclassed,\n        or when TypedDict is instantiated. This way\n        TypedDict supports all three syntax forms described in its docstring.\n        Subclasses and instances of TypedDict return actual dictionaries.\n        \"\"\"\n        for base in bases:\n            if type(base) is not _TypedDictMeta:\n                raise TypeError('cannot inherit from both a TypedDict type '\n                                'and a non-TypedDict base class')\n        tp_dict = type.__new__(_TypedDictMeta, name, (dict,), ns)\n\n        annotations = {}\n        own_annotations = ns.get('__annotations__', {})\n        own_annotation_keys = set(own_annotations.keys())\n        msg = \"TypedDict('Name', {f0: t0, f1: t1, ...}); each t must be a type\"\n        own_annotations = {\n            n: _type_check(tp, msg, module=tp_dict.__module__)\n            for n, tp in own_annotations.items()\n        }\n        required_keys = set()\n        optional_keys = set()\n\n        for base in bases:\n            annotations.update(base.__dict__.get('__annotations__', {}))\n            required_keys.update(base.__dict__.get('__required_keys__', ()))\n            optional_keys.update(base.__dict__.get('__optional_keys__', ()))\n\n        annotations.update(own_annotations)\n        if total:\n            required_keys.update(own_annotation_keys)\n        else:\n            optional_keys.update(own_annotation_keys)\n\n        tp_dict.__annotations__ = annotations\n        tp_dict.__required_keys__ = frozenset(required_keys)\n        tp_dict.__optional_keys__ = frozenset(optional_keys)\n        if not hasattr(tp_dict, '__total__'):\n            tp_dict.__total__ = total\n        return tp_dict\n\n    __call__ = dict  # static method\n\n    def __subclasscheck__(cls, other):\n        # Typed dicts are only for static structural subtyping.\n        raise TypeError('TypedDict does not support instance and class checks')\n\n    __instancecheck__ = __subclasscheck__\n\n\ndef TypedDict(typename, fields=None, /, *, total=True, **kwargs):\n    \"\"\"A simple typed namespace. At runtime it is equivalent to a plain dict.\n\n    TypedDict creates a dictionary type that expects all of its\n    instances to have a certain set of keys, where each key is\n    associated with a value of a consistent type. This expectation\n    is not checked at runtime but is only enforced by type checkers.\n    Usage::\n\n        class Point2D(TypedDict):\n            x: int\n            y: int\n            label: str\n\n        a: Point2D = {'x': 1, 'y': 2, 'label': 'good'}  # OK\n        b: Point2D = {'z': 3, 'label': 'bad'}           # Fails type check\n\n        assert Point2D(x=1, y=2, label='first') == dict(x=1, y=2, label='first')\n\n    The type info can be accessed via the Point2D.__annotations__ dict, and\n    the Point2D.__required_keys__ and Point2D.__optional_keys__ frozensets.\n    TypedDict supports two additional equivalent forms::\n\n        Point2D = TypedDict('Point2D', x=int, y=int, label=str)\n        Point2D = TypedDict('Point2D', {'x': int, 'y': int, 'label': str})\n\n    By default, all keys must be present in a TypedDict. It is possible\n    to override this by specifying totality.\n    Usage::\n\n        class point2D(TypedDict, total=False):\n            x: int\n            y: int\n\n    This means that a point2D TypedDict can have any of the keys omitted.A type\n    checker is only expected to support a literal False or True as the value of\n    the total argument. True is the default, and makes all items defined in the\n    class body be required.\n\n    The class syntax is only supported in Python 3.6+, while two other\n    syntax forms work for Python 2.7 and 3.2+\n    \"\"\"\n    if fields is None:\n        fields = kwargs\n    elif kwargs:\n        raise TypeError(\"TypedDict takes either a dict or keyword arguments,\"\n                        \" but not both\")\n\n    ns = {'__annotations__': dict(fields)}\n    try:\n        # Setting correct module is necessary to make typed dict classes pickleable.\n        ns['__module__'] = sys._getframe(1).f_globals.get('__name__', '__main__')\n    except (AttributeError, ValueError):\n        pass\n\n    return _TypedDictMeta(typename, (), ns, total=total)\n\n_TypedDict = type.__new__(_TypedDictMeta, 'TypedDict', (), {})\nTypedDict.__mro_entries__ = lambda bases: (_TypedDict,)\n\n\nclass NewType:\n    \"\"\"NewType creates simple unique types with almost zero\n    runtime overhead. NewType(name, tp) is considered a subtype of tp\n    by static type checkers. At runtime, NewType(name, tp) returns\n    a dummy callable that simply returns its argument. Usage::\n\n        UserId = NewType('UserId', int)\n\n        def name_by_id(user_id: UserId) -> str:\n            ...\n\n        UserId('user')          # Fails type check\n\n        name_by_id(42)          # Fails type check\n        name_by_id(UserId(42))  # OK\n\n        num = UserId(5) + 1     # type: int\n    \"\"\"\n\n    def __init__(self, name, tp):\n        self.__qualname__ = name\n        if '.' in name:\n            name = name.rpartition('.')[-1]\n        self.__name__ = name\n        self.__supertype__ = tp\n        def_mod = _caller()\n        if def_mod != 'typing':\n            self.__module__ = def_mod\n\n    def __repr__(self):\n        return f'{self.__module__}.{self.__qualname__}'\n\n    def __call__(self, x):\n        return x\n\n    def __reduce__(self):\n        return self.__qualname__\n\n    def __or__(self, other):\n        return Union[self, other]\n\n    def __ror__(self, other):\n        return Union[other, self]\n\n\n# Python-version-specific alias (Python 2: unicode; Python 3: str)\nText = str\n\n\n# Constant that's True when type checking, but False here.\nTYPE_CHECKING = False\n\n\nclass IO(Generic[AnyStr]):\n    \"\"\"Generic base class for TextIO and BinaryIO.\n\n    This is an abstract, generic version of the return of open().\n\n    NOTE: This does not distinguish between the different possible\n    classes (text vs. binary, read vs. write vs. read/write,\n    append-only, unbuffered).  The TextIO and BinaryIO subclasses\n    below capture the distinctions between text vs. binary, which is\n    pervasive in the interface; however we currently do not offer a\n    way to track the other distinctions in the type system.\n    \"\"\"\n\n    __slots__ = ()\n\n    @property\n    @abstractmethod\n    def mode(self) -> str:\n        pass\n\n    @property\n    @abstractmethod\n    def name(self) -> str:\n        pass\n\n    @abstractmethod\n    def close(self) -> None:\n        pass\n\n    @property\n    @abstractmethod\n    def closed(self) -> bool:\n        pass\n\n    @abstractmethod\n    def fileno(self) -> int:\n        pass\n\n    @abstractmethod\n    def flush(self) -> None:\n        pass\n\n    @abstractmethod\n    def isatty(self) -> bool:\n        pass\n\n    @abstractmethod\n    def read(self, n: int = -1) -> AnyStr:\n        pass\n\n    @abstractmethod\n    def readable(self) -> bool:\n        pass\n\n    @abstractmethod\n    def readline(self, limit: int = -1) -> AnyStr:\n        pass\n\n    @abstractmethod\n    def readlines(self, hint: int = -1) -> List[AnyStr]:\n        pass\n\n    @abstractmethod\n    def seek(self, offset: int, whence: int = 0) -> int:\n        pass\n\n    @abstractmethod\n    def seekable(self) -> bool:\n        pass\n\n    @abstractmethod\n    def tell(self) -> int:\n        pass\n\n    @abstractmethod\n    def truncate(self, size: int = None) -> int:\n        pass\n\n    @abstractmethod\n    def writable(self) -> bool:\n        pass\n\n    @abstractmethod\n    def write(self, s: AnyStr) -> int:\n        pass\n\n    @abstractmethod\n    def writelines(self, lines: List[AnyStr]) -> None:\n        pass\n\n    @abstractmethod\n    def __enter__(self) -> 'IO[AnyStr]':\n        pass\n\n    @abstractmethod\n    def __exit__(self, type, value, traceback) -> None:\n        pass\n\n\nclass BinaryIO(IO[bytes]):\n    \"\"\"Typed version of the return of open() in binary mode.\"\"\"\n\n    __slots__ = ()\n\n    @abstractmethod\n    def write(self, s: Union[bytes, bytearray]) -> int:\n        pass\n\n    @abstractmethod\n    def __enter__(self) -> 'BinaryIO':\n        pass\n\n\nclass TextIO(IO[str]):\n    \"\"\"Typed version of the return of open() in text mode.\"\"\"\n\n    __slots__ = ()\n\n    @property\n    @abstractmethod\n    def buffer(self) -> BinaryIO:\n        pass\n\n    @property\n    @abstractmethod\n    def encoding(self) -> str:\n        pass\n\n    @property\n    @abstractmethod\n    def errors(self) -> Optional[str]:\n        pass\n\n    @property\n    @abstractmethod\n    def line_buffering(self) -> bool:\n        pass\n\n    @property\n    @abstractmethod\n    def newlines(self) -> Any:\n        pass\n\n    @abstractmethod\n    def __enter__(self) -> 'TextIO':\n        pass\n\n\nclass io:\n    \"\"\"Wrapper namespace for IO generic classes.\"\"\"\n\n    __all__ = ['IO', 'TextIO', 'BinaryIO']\n    IO = IO\n    TextIO = TextIO\n    BinaryIO = BinaryIO\n\n\nio.__name__ = __name__ + '.io'\nsys.modules[io.__name__] = io\n\nPattern = _alias(stdlib_re.Pattern, 1)\nMatch = _alias(stdlib_re.Match, 1)\n\nclass re:\n    \"\"\"Wrapper namespace for re type aliases.\"\"\"\n\n    __all__ = ['Pattern', 'Match']\n    Pattern = Pattern\n    Match = Match\n\n\nre.__name__ = __name__ + '.re'\nsys.modules[re.__name__] = re\n", 2675], "/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/common.py": ["\"\"\"\nMisc tools for implementing data structures\n\nNote: pandas.core.common is *not* part of the public API.\n\"\"\"\nfrom __future__ import annotations\n\nimport builtins\nfrom collections import (\n    abc,\n    defaultdict,\n)\nimport contextlib\nfrom functools import partial\nimport inspect\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Collection,\n    Hashable,\n    Iterable,\n    Iterator,\n    Sequence,\n    cast,\n    overload,\n)\nimport warnings\n\nimport numpy as np\n\nfrom pandas._libs import lib\nfrom pandas._typing import (\n    AnyArrayLike,\n    ArrayLike,\n    NpDtype,\n    RandomState,\n    Scalar,\n    T,\n)\nfrom pandas.util._exceptions import find_stack_level\n\nfrom pandas.core.dtypes.cast import construct_1d_object_array_from_listlike\nfrom pandas.core.dtypes.common import (\n    is_array_like,\n    is_bool_dtype,\n    is_extension_array_dtype,\n    is_integer,\n)\nfrom pandas.core.dtypes.generic import (\n    ABCExtensionArray,\n    ABCIndex,\n    ABCSeries,\n)\nfrom pandas.core.dtypes.inference import iterable_not_string\nfrom pandas.core.dtypes.missing import isna\n\nif TYPE_CHECKING:\n    from pandas import Index\n\n\nclass SettingWithCopyError(ValueError):\n    pass\n\n\nclass SettingWithCopyWarning(Warning):\n    pass\n\n\ndef flatten(line):\n    \"\"\"\n    Flatten an arbitrarily nested sequence.\n\n    Parameters\n    ----------\n    line : sequence\n        The non string sequence to flatten\n\n    Notes\n    -----\n    This doesn't consider strings sequences.\n\n    Returns\n    -------\n    flattened : generator\n    \"\"\"\n    for element in line:\n        if iterable_not_string(element):\n            yield from flatten(element)\n        else:\n            yield element\n\n\ndef consensus_name_attr(objs):\n    name = objs[0].name\n    for obj in objs[1:]:\n        try:\n            if obj.name != name:\n                name = None\n        except ValueError:\n            name = None\n    return name\n\n\ndef is_bool_indexer(key: Any) -> bool:\n    \"\"\"\n    Check whether `key` is a valid boolean indexer.\n\n    Parameters\n    ----------\n    key : Any\n        Only list-likes may be considered boolean indexers.\n        All other types are not considered a boolean indexer.\n        For array-like input, boolean ndarrays or ExtensionArrays\n        with ``_is_boolean`` set are considered boolean indexers.\n\n    Returns\n    -------\n    bool\n        Whether `key` is a valid boolean indexer.\n\n    Raises\n    ------\n    ValueError\n        When the array is an object-dtype ndarray or ExtensionArray\n        and contains missing values.\n\n    See Also\n    --------\n    check_array_indexer : Check that `key` is a valid array to index,\n        and convert to an ndarray.\n    \"\"\"\n    if isinstance(key, (ABCSeries, np.ndarray, ABCIndex)) or (\n        is_array_like(key) and is_extension_array_dtype(key.dtype)\n    ):\n        if key.dtype == np.object_:\n            key = np.asarray(key)\n\n            if not lib.is_bool_array(key):\n                na_msg = \"Cannot mask with non-boolean array containing NA / NaN values\"\n                if lib.infer_dtype(key) == \"boolean\" and isna(key).any():\n                    # Don't raise on e.g. [\"A\", \"B\", np.nan], see\n                    #  test_loc_getitem_list_of_labels_categoricalindex_with_na\n                    raise ValueError(na_msg)\n                return False\n            return True\n        elif is_bool_dtype(key.dtype):\n            return True\n    elif isinstance(key, list):\n        # check if np.array(key).dtype would be bool\n        if len(key) > 0:\n            if type(key) is not list:\n                # GH#42461 cython will raise TypeError if we pass a subclass\n                key = list(key)\n            return lib.is_bool_list(key)\n\n    return False\n\n\ndef cast_scalar_indexer(val, warn_float: bool = False):\n    \"\"\"\n    To avoid numpy DeprecationWarnings, cast float to integer where valid.\n\n    Parameters\n    ----------\n    val : scalar\n    warn_float : bool, default False\n        If True, issue deprecation warning for a float indexer.\n\n    Returns\n    -------\n    outval : scalar\n    \"\"\"\n    # assumes lib.is_scalar(val)\n    if lib.is_float(val) and val.is_integer():\n        if warn_float:\n            warnings.warn(\n                \"Indexing with a float is deprecated, and will raise an IndexError \"\n                \"in pandas 2.0. You can manually convert to an integer key instead.\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n        return int(val)\n    return val\n\n\ndef not_none(*args):\n    \"\"\"\n    Returns a generator consisting of the arguments that are not None.\n    \"\"\"\n    return (arg for arg in args if arg is not None)\n\n\ndef any_none(*args) -> bool:\n    \"\"\"\n    Returns a boolean indicating if any argument is None.\n    \"\"\"\n    return any(arg is None for arg in args)\n\n\ndef all_none(*args) -> bool:\n    \"\"\"\n    Returns a boolean indicating if all arguments are None.\n    \"\"\"\n    return all(arg is None for arg in args)\n\n\ndef any_not_none(*args) -> bool:\n    \"\"\"\n    Returns a boolean indicating if any argument is not None.\n    \"\"\"\n    return any(arg is not None for arg in args)\n\n\ndef all_not_none(*args) -> bool:\n    \"\"\"\n    Returns a boolean indicating if all arguments are not None.\n    \"\"\"\n    return all(arg is not None for arg in args)\n\n\ndef count_not_none(*args) -> int:\n    \"\"\"\n    Returns the count of arguments that are not None.\n    \"\"\"\n    return sum(x is not None for x in args)\n\n\ndef asarray_tuplesafe(values, dtype: NpDtype | None = None) -> np.ndarray:\n\n    if not (isinstance(values, (list, tuple)) or hasattr(values, \"__array__\")):\n        values = list(values)\n    elif isinstance(values, ABCIndex):\n        # error: Incompatible return value type (got \"Union[ExtensionArray, ndarray]\",\n        # expected \"ndarray\")\n        return values._values  # type: ignore[return-value]\n\n    if isinstance(values, list) and dtype in [np.object_, object]:\n        return construct_1d_object_array_from_listlike(values)\n\n    result = np.asarray(values, dtype=dtype)\n\n    if issubclass(result.dtype.type, str):\n        result = np.asarray(values, dtype=object)\n\n    if result.ndim == 2:\n        # Avoid building an array of arrays:\n        values = [tuple(x) for x in values]\n        result = construct_1d_object_array_from_listlike(values)\n\n    return result\n\n\ndef index_labels_to_array(labels, dtype: NpDtype | None = None) -> np.ndarray:\n    \"\"\"\n    Transform label or iterable of labels to array, for use in Index.\n\n    Parameters\n    ----------\n    dtype : dtype\n        If specified, use as dtype of the resulting array, otherwise infer.\n\n    Returns\n    -------\n    array\n    \"\"\"\n    if isinstance(labels, (str, tuple)):\n        labels = [labels]\n\n    if not isinstance(labels, (list, np.ndarray)):\n        try:\n            labels = list(labels)\n        except TypeError:  # non-iterable\n            labels = [labels]\n\n    labels = asarray_tuplesafe(labels, dtype=dtype)\n\n    return labels\n\n\ndef maybe_make_list(obj):\n    if obj is not None and not isinstance(obj, (tuple, list)):\n        return [obj]\n    return obj\n\n\ndef maybe_iterable_to_list(obj: Iterable[T] | T) -> Collection[T] | T:\n    \"\"\"\n    If obj is Iterable but not list-like, consume into list.\n    \"\"\"\n    if isinstance(obj, abc.Iterable) and not isinstance(obj, abc.Sized):\n        return list(obj)\n    obj = cast(Collection, obj)\n    return obj\n\n\ndef is_null_slice(obj) -> bool:\n    \"\"\"\n    We have a null slice.\n    \"\"\"\n    return (\n        isinstance(obj, slice)\n        and obj.start is None\n        and obj.stop is None\n        and obj.step is None\n    )\n\n\ndef is_true_slices(line) -> list[bool]:\n    \"\"\"\n    Find non-trivial slices in \"line\": return a list of booleans with same length.\n    \"\"\"\n    return [isinstance(k, slice) and not is_null_slice(k) for k in line]\n\n\n# TODO: used only once in indexing; belongs elsewhere?\ndef is_full_slice(obj, line: int) -> bool:\n    \"\"\"\n    We have a full length slice.\n    \"\"\"\n    return (\n        isinstance(obj, slice)\n        and obj.start == 0\n        and obj.stop == line\n        and obj.step is None\n    )\n\n\ndef get_callable_name(obj):\n    # typical case has name\n    if hasattr(obj, \"__name__\"):\n        return getattr(obj, \"__name__\")\n    # some objects don't; could recurse\n    if isinstance(obj, partial):\n        return get_callable_name(obj.func)\n    # fall back to class name\n    if callable(obj):\n        return type(obj).__name__\n    # everything failed (probably because the argument\n    # wasn't actually callable); we return None\n    # instead of the empty string in this case to allow\n    # distinguishing between no name and a name of ''\n    return None\n\n\ndef apply_if_callable(maybe_callable, obj, **kwargs):\n    \"\"\"\n    Evaluate possibly callable input using obj and kwargs if it is callable,\n    otherwise return as it is.\n\n    Parameters\n    ----------\n    maybe_callable : possibly a callable\n    obj : NDFrame\n    **kwargs\n    \"\"\"\n    if callable(maybe_callable):\n        return maybe_callable(obj, **kwargs)\n\n    return maybe_callable\n\n\ndef standardize_mapping(into):\n    \"\"\"\n    Helper function to standardize a supplied mapping.\n\n    Parameters\n    ----------\n    into : instance or subclass of collections.abc.Mapping\n        Must be a class, an initialized collections.defaultdict,\n        or an instance of a collections.abc.Mapping subclass.\n\n    Returns\n    -------\n    mapping : a collections.abc.Mapping subclass or other constructor\n        a callable object that can accept an iterator to create\n        the desired Mapping.\n\n    See Also\n    --------\n    DataFrame.to_dict\n    Series.to_dict\n    \"\"\"\n    if not inspect.isclass(into):\n        if isinstance(into, defaultdict):\n            return partial(defaultdict, into.default_factory)\n        into = type(into)\n    if not issubclass(into, abc.Mapping):\n        raise TypeError(f\"unsupported type: {into}\")\n    elif into == defaultdict:\n        raise TypeError(\"to_dict() only accepts initialized defaultdicts\")\n    return into\n\n\n@overload\ndef random_state(state: np.random.Generator) -> np.random.Generator:\n    ...\n\n\n@overload\ndef random_state(\n    state: int | ArrayLike | np.random.BitGenerator | np.random.RandomState | None,\n) -> np.random.RandomState:\n    ...\n\n\ndef random_state(state: RandomState | None = None):\n    \"\"\"\n    Helper function for processing random_state arguments.\n\n    Parameters\n    ----------\n    state : int, array-like, BitGenerator, Generator, np.random.RandomState, None.\n        If receives an int, array-like, or BitGenerator, passes to\n        np.random.RandomState() as seed.\n        If receives an np.random RandomState or Generator, just returns that unchanged.\n        If receives `None`, returns np.random.\n        If receives anything else, raises an informative ValueError.\n\n        .. versionchanged:: 1.1.0\n\n            array-like and BitGenerator object now passed to np.random.RandomState()\n            as seed\n\n        Default None.\n\n    Returns\n    -------\n    np.random.RandomState or np.random.Generator. If state is None, returns np.random\n\n    \"\"\"\n    if (\n        is_integer(state)\n        or is_array_like(state)\n        or isinstance(state, np.random.BitGenerator)\n    ):\n        # error: Argument 1 to \"RandomState\" has incompatible type \"Optional[Union[int,\n        # Union[ExtensionArray, ndarray[Any, Any]], Generator, RandomState]]\"; expected\n        # \"Union[None, Union[Union[_SupportsArray[dtype[Union[bool_, integer[Any]]]],\n        # Sequence[_SupportsArray[dtype[Union[bool_, integer[Any]]]]],\n        # Sequence[Sequence[_SupportsArray[dtype[Union[bool_, integer[Any]]]]]],\n        # Sequence[Sequence[Sequence[_SupportsArray[dtype[Union[bool_,\n        # integer[Any]]]]]]],\n        # Sequence[Sequence[Sequence[Sequence[_SupportsArray[dtype[Union[bool_,\n        # integer[Any]]]]]]]]], Union[bool, int, Sequence[Union[bool, int]],\n        # Sequence[Sequence[Union[bool, int]]], Sequence[Sequence[Sequence[Union[bool,\n        # int]]]], Sequence[Sequence[Sequence[Sequence[Union[bool, int]]]]]]],\n        # BitGenerator]\"\n        return np.random.RandomState(state)  # type: ignore[arg-type]\n    elif isinstance(state, np.random.RandomState):\n        return state\n    elif isinstance(state, np.random.Generator):\n        return state\n    elif state is None:\n        return np.random\n    else:\n        raise ValueError(\n            \"random_state must be an integer, array-like, a BitGenerator, Generator, \"\n            \"a numpy RandomState, or None\"\n        )\n\n\ndef pipe(\n    obj, func: Callable[..., T] | tuple[Callable[..., T], str], *args, **kwargs\n) -> T:\n    \"\"\"\n    Apply a function ``func`` to object ``obj`` either by passing obj as the\n    first argument to the function or, in the case that the func is a tuple,\n    interpret the first element of the tuple as a function and pass the obj to\n    that function as a keyword argument whose key is the value of the second\n    element of the tuple.\n\n    Parameters\n    ----------\n    func : callable or tuple of (callable, str)\n        Function to apply to this object or, alternatively, a\n        ``(callable, data_keyword)`` tuple where ``data_keyword`` is a\n        string indicating the keyword of `callable`` that expects the\n        object.\n    *args : iterable, optional\n        Positional arguments passed into ``func``.\n    **kwargs : dict, optional\n        A dictionary of keyword arguments passed into ``func``.\n\n    Returns\n    -------\n    object : the return type of ``func``.\n    \"\"\"\n    if isinstance(func, tuple):\n        func, target = func\n        if target in kwargs:\n            msg = f\"{target} is both the pipe target and a keyword argument\"\n            raise ValueError(msg)\n        kwargs[target] = obj\n        return func(*args, **kwargs)\n    else:\n        return func(obj, *args, **kwargs)\n\n\ndef get_rename_function(mapper):\n    \"\"\"\n    Returns a function that will map names/labels, dependent if mapper\n    is a dict, Series or just a function.\n    \"\"\"\n    if isinstance(mapper, (abc.Mapping, ABCSeries)):\n\n        def f(x):\n            if x in mapper:\n                return mapper[x]\n            else:\n                return x\n\n    else:\n        f = mapper\n\n    return f\n\n\ndef convert_to_list_like(\n    values: Scalar | Iterable | AnyArrayLike,\n) -> list | AnyArrayLike:\n    \"\"\"\n    Convert list-like or scalar input to list-like. List, numpy and pandas array-like\n    inputs are returned unmodified whereas others are converted to list.\n    \"\"\"\n    if isinstance(values, (list, np.ndarray, ABCIndex, ABCSeries, ABCExtensionArray)):\n        return values\n    elif isinstance(values, abc.Iterable) and not isinstance(values, str):\n        return list(values)\n\n    return [values]\n\n\n@contextlib.contextmanager\ndef temp_setattr(obj, attr: str, value) -> Iterator[None]:\n    \"\"\"Temporarily set attribute on an object.\n\n    Args:\n        obj: Object whose attribute will be modified.\n        attr: Attribute to modify.\n        value: Value to temporarily set attribute to.\n\n    Yields:\n        obj with modified attribute.\n    \"\"\"\n    old_value = getattr(obj, attr)\n    setattr(obj, attr, value)\n    yield obj\n    setattr(obj, attr, old_value)\n\n\ndef require_length_match(data, index: Index):\n    \"\"\"\n    Check the length of data matches the length of the index.\n    \"\"\"\n    if len(data) != len(index):\n        raise ValueError(\n            \"Length of values \"\n            f\"({len(data)}) \"\n            \"does not match length of index \"\n            f\"({len(index)})\"\n        )\n\n\n# the ufuncs np.maximum.reduce and np.minimum.reduce default to axis=0,\n#  whereas np.min and np.max (which directly call obj.min and obj.max)\n#  default to axis=None.\n_builtin_table = {\n    builtins.sum: np.sum,\n    builtins.max: np.maximum.reduce,\n    builtins.min: np.minimum.reduce,\n}\n\n_cython_table = {\n    builtins.sum: \"sum\",\n    builtins.max: \"max\",\n    builtins.min: \"min\",\n    np.all: \"all\",\n    np.any: \"any\",\n    np.sum: \"sum\",\n    np.nansum: \"sum\",\n    np.mean: \"mean\",\n    np.nanmean: \"mean\",\n    np.prod: \"prod\",\n    np.nanprod: \"prod\",\n    np.std: \"std\",\n    np.nanstd: \"std\",\n    np.var: \"var\",\n    np.nanvar: \"var\",\n    np.median: \"median\",\n    np.nanmedian: \"median\",\n    np.max: \"max\",\n    np.nanmax: \"max\",\n    np.min: \"min\",\n    np.nanmin: \"min\",\n    np.cumprod: \"cumprod\",\n    np.nancumprod: \"cumprod\",\n    np.cumsum: \"cumsum\",\n    np.nancumsum: \"cumsum\",\n}\n\n\ndef get_cython_func(arg: Callable) -> str | None:\n    \"\"\"\n    if we define an internal function for this argument, return it\n    \"\"\"\n    return _cython_table.get(arg)\n\n\ndef is_builtin_func(arg):\n    \"\"\"\n    if we define an builtin function for this argument, return it,\n    otherwise return the arg\n    \"\"\"\n    return _builtin_table.get(arg, arg)\n\n\ndef fill_missing_names(names: Sequence[Hashable | None]) -> list[Hashable]:\n    \"\"\"\n    If a name is missing then replace it by level_n, where n is the count\n\n    .. versionadded:: 1.4.0\n\n    Parameters\n    ----------\n    names : list-like\n        list of column names or None values.\n\n    Returns\n    -------\n    list\n        list of column names with the None values replaced.\n    \"\"\"\n    return [f\"level_{i}\" if name is None else name for i, name in enumerate(names)]\n", 634], "/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/indexes/range.py": ["from __future__ import annotations\n\nfrom datetime import timedelta\nimport operator\nfrom sys import getsizeof\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Hashable,\n    List,\n    cast,\n)\nimport warnings\n\nimport numpy as np\n\nfrom pandas._libs import (\n    index as libindex,\n    lib,\n)\nfrom pandas._libs.lib import no_default\nfrom pandas._typing import (\n    Dtype,\n    npt,\n)\nfrom pandas.compat.numpy import function as nv\nfrom pandas.util._decorators import (\n    cache_readonly,\n    doc,\n)\nfrom pandas.util._exceptions import find_stack_level\n\nfrom pandas.core.dtypes.common import (\n    ensure_platform_int,\n    ensure_python_int,\n    is_float,\n    is_integer,\n    is_scalar,\n    is_signed_integer_dtype,\n    is_timedelta64_dtype,\n)\nfrom pandas.core.dtypes.generic import ABCTimedeltaIndex\n\nfrom pandas.core import ops\nimport pandas.core.common as com\nfrom pandas.core.construction import extract_array\nimport pandas.core.indexes.base as ibase\nfrom pandas.core.indexes.base import maybe_extract_name\nfrom pandas.core.indexes.numeric import (\n    Float64Index,\n    Int64Index,\n    NumericIndex,\n)\nfrom pandas.core.ops.common import unpack_zerodim_and_defer\n\nif TYPE_CHECKING:\n    from pandas import Index\n\n_empty_range = range(0)\n\n\nclass RangeIndex(NumericIndex):\n    \"\"\"\n    Immutable Index implementing a monotonic integer range.\n\n    RangeIndex is a memory-saving special case of Int64Index limited to\n    representing monotonic ranges. Using RangeIndex may in some instances\n    improve computing speed.\n\n    This is the default index type used\n    by DataFrame and Series when no explicit index is provided by the user.\n\n    Parameters\n    ----------\n    start : int (default: 0), range, or other RangeIndex instance\n        If int and \"stop\" is not given, interpreted as \"stop\" instead.\n    stop : int (default: 0)\n    step : int (default: 1)\n    dtype : np.int64\n        Unused, accepted for homogeneity with other index types.\n    copy : bool, default False\n        Unused, accepted for homogeneity with other index types.\n    name : object, optional\n        Name to be stored in the index.\n\n    Attributes\n    ----------\n    start\n    stop\n    step\n\n    Methods\n    -------\n    from_range\n\n    See Also\n    --------\n    Index : The base pandas Index type.\n    Int64Index : Index of int64 data.\n    \"\"\"\n\n    _typ = \"rangeindex\"\n    _engine_type = libindex.Int64Engine\n    _dtype_validation_metadata = (is_signed_integer_dtype, \"signed integer\")\n    _range: range\n    _is_backward_compat_public_numeric_index: bool = False\n\n    # --------------------------------------------------------------------\n    # Constructors\n\n    def __new__(\n        cls,\n        start=None,\n        stop=None,\n        step=None,\n        dtype: Dtype | None = None,\n        copy: bool = False,\n        name: Hashable = None,\n    ) -> RangeIndex:\n        cls._validate_dtype(dtype)\n        name = maybe_extract_name(name, start, cls)\n\n        # RangeIndex\n        if isinstance(start, RangeIndex):\n            return start.copy(name=name)\n        elif isinstance(start, range):\n            return cls._simple_new(start, name=name)\n\n        # validate the arguments\n        if com.all_none(start, stop, step):\n            raise TypeError(\"RangeIndex(...) must be called with integers\")\n\n        start = ensure_python_int(start) if start is not None else 0\n\n        if stop is None:\n            start, stop = 0, start\n        else:\n            stop = ensure_python_int(stop)\n\n        step = ensure_python_int(step) if step is not None else 1\n        if step == 0:\n            raise ValueError(\"Step must not be zero\")\n\n        rng = range(start, stop, step)\n        return cls._simple_new(rng, name=name)\n\n    @classmethod\n    def from_range(\n        cls, data: range, name=None, dtype: Dtype | None = None\n    ) -> RangeIndex:\n        \"\"\"\n        Create RangeIndex from a range object.\n\n        Returns\n        -------\n        RangeIndex\n        \"\"\"\n        if not isinstance(data, range):\n            raise TypeError(\n                f\"{cls.__name__}(...) must be called with object coercible to a \"\n                f\"range, {repr(data)} was passed\"\n            )\n        cls._validate_dtype(dtype)\n        return cls._simple_new(data, name=name)\n\n    @classmethod\n    def _simple_new(cls, values: range, name: Hashable = None) -> RangeIndex:\n        result = object.__new__(cls)\n\n        assert isinstance(values, range)\n\n        result._range = values\n        result._name = name\n        result._cache = {}\n        result._reset_identity()\n        return result\n\n    # --------------------------------------------------------------------\n\n    # error: Return type \"Type[Int64Index]\" of \"_constructor\" incompatible with return\n    # type \"Type[RangeIndex]\" in supertype \"Index\"\n    @cache_readonly\n    def _constructor(self) -> type[Int64Index]:  # type: ignore[override]\n        \"\"\"return the class to use for construction\"\"\"\n        return Int64Index\n\n    # error: Signature of \"_data\" incompatible with supertype \"Index\"\n    @cache_readonly\n    def _data(self) -> np.ndarray:  # type: ignore[override]\n        \"\"\"\n        An int array that for performance reasons is created only when needed.\n\n        The constructed array is saved in ``_cache``.\n        \"\"\"\n        return np.arange(self.start, self.stop, self.step, dtype=np.int64)\n\n    def _get_data_as_items(self):\n        \"\"\"return a list of tuples of start, stop, step\"\"\"\n        rng = self._range\n        return [(\"start\", rng.start), (\"stop\", rng.stop), (\"step\", rng.step)]\n\n    def __reduce__(self):\n        d = {\"name\": self.name}\n        d.update(dict(self._get_data_as_items()))\n        return ibase._new_Index, (type(self), d), None\n\n    # --------------------------------------------------------------------\n    # Rendering Methods\n\n    def _format_attrs(self):\n        \"\"\"\n        Return a list of tuples of the (attr, formatted_value)\n        \"\"\"\n        attrs = self._get_data_as_items()\n        if self.name is not None:\n            attrs.append((\"name\", ibase.default_pprint(self.name)))\n        return attrs\n\n    def _format_data(self, name=None):\n        # we are formatting thru the attributes\n        return None\n\n    def _format_with_header(self, header: list[str], na_rep: str) -> list[str]:\n        # Equivalent to Index implementation, but faster\n        if not len(self._range):\n            return header\n        first_val_str = str(self._range[0])\n        last_val_str = str(self._range[-1])\n        max_length = max(len(first_val_str), len(last_val_str))\n\n        return header + [f\"{x:<{max_length}}\" for x in self._range]\n\n    # --------------------------------------------------------------------\n    _deprecation_message = (\n        \"RangeIndex.{} is deprecated and will be \"\n        \"removed in a future version. Use RangeIndex.{} \"\n        \"instead\"\n    )\n\n    @property\n    def start(self) -> int:\n        \"\"\"\n        The value of the `start` parameter (``0`` if this was not supplied).\n        \"\"\"\n        # GH 25710\n        return self._range.start\n\n    @property\n    def _start(self) -> int:\n        \"\"\"\n        The value of the `start` parameter (``0`` if this was not supplied).\n\n         .. deprecated:: 0.25.0\n            Use ``start`` instead.\n        \"\"\"\n        warnings.warn(\n            self._deprecation_message.format(\"_start\", \"start\"),\n            FutureWarning,\n            stacklevel=find_stack_level(),\n        )\n        return self.start\n\n    @property\n    def stop(self) -> int:\n        \"\"\"\n        The value of the `stop` parameter.\n        \"\"\"\n        return self._range.stop\n\n    @property\n    def _stop(self) -> int:\n        \"\"\"\n        The value of the `stop` parameter.\n\n         .. deprecated:: 0.25.0\n            Use ``stop`` instead.\n        \"\"\"\n        # GH 25710\n        warnings.warn(\n            self._deprecation_message.format(\"_stop\", \"stop\"),\n            FutureWarning,\n            stacklevel=find_stack_level(),\n        )\n        return self.stop\n\n    @property\n    def step(self) -> int:\n        \"\"\"\n        The value of the `step` parameter (``1`` if this was not supplied).\n        \"\"\"\n        # GH 25710\n        return self._range.step\n\n    @property\n    def _step(self) -> int:\n        \"\"\"\n        The value of the `step` parameter (``1`` if this was not supplied).\n\n         .. deprecated:: 0.25.0\n            Use ``step`` instead.\n        \"\"\"\n        # GH 25710\n        warnings.warn(\n            self._deprecation_message.format(\"_step\", \"step\"),\n            FutureWarning,\n            stacklevel=find_stack_level(),\n        )\n        return self.step\n\n    @cache_readonly\n    def nbytes(self) -> int:\n        \"\"\"\n        Return the number of bytes in the underlying data.\n        \"\"\"\n        rng = self._range\n        return getsizeof(rng) + sum(\n            getsizeof(getattr(rng, attr_name))\n            for attr_name in [\"start\", \"stop\", \"step\"]\n        )\n\n    def memory_usage(self, deep: bool = False) -> int:\n        \"\"\"\n        Memory usage of my values\n\n        Parameters\n        ----------\n        deep : bool\n            Introspect the data deeply, interrogate\n            `object` dtypes for system-level memory consumption\n\n        Returns\n        -------\n        bytes used\n\n        Notes\n        -----\n        Memory usage does not include memory consumed by elements that\n        are not components of the array if deep=False\n\n        See Also\n        --------\n        numpy.ndarray.nbytes\n        \"\"\"\n        return self.nbytes\n\n    @property\n    def dtype(self) -> np.dtype:\n        return np.dtype(np.int64)\n\n    @property\n    def is_unique(self) -> bool:\n        \"\"\"return if the index has unique values\"\"\"\n        return True\n\n    @cache_readonly\n    def is_monotonic_increasing(self) -> bool:\n        return self._range.step > 0 or len(self) <= 1\n\n    @cache_readonly\n    def is_monotonic_decreasing(self) -> bool:\n        return self._range.step < 0 or len(self) <= 1\n\n    def __contains__(self, key: Any) -> bool:\n        hash(key)\n        try:\n            key = ensure_python_int(key)\n        except TypeError:\n            return False\n        return key in self._range\n\n    @property\n    def inferred_type(self) -> str:\n        return \"integer\"\n\n    # --------------------------------------------------------------------\n    # Indexing Methods\n\n    @doc(Int64Index.get_loc)\n    def get_loc(self, key, method=None, tolerance=None):\n        if method is None and tolerance is None:\n            if is_integer(key) or (is_float(key) and key.is_integer()):\n                new_key = int(key)\n                try:\n                    return self._range.index(new_key)\n                except ValueError as err:\n                    raise KeyError(key) from err\n            self._check_indexing_error(key)\n            raise KeyError(key)\n        return super().get_loc(key, method=method, tolerance=tolerance)\n\n    def _get_indexer(\n        self,\n        target: Index,\n        method: str | None = None,\n        limit: int | None = None,\n        tolerance=None,\n    ) -> npt.NDArray[np.intp]:\n        if com.any_not_none(method, tolerance, limit):\n            return super()._get_indexer(\n                target, method=method, tolerance=tolerance, limit=limit\n            )\n\n        if self.step > 0:\n            start, stop, step = self.start, self.stop, self.step\n        else:\n            # GH 28678: work on reversed range for simplicity\n            reverse = self._range[::-1]\n            start, stop, step = reverse.start, reverse.stop, reverse.step\n\n        target_array = np.asarray(target)\n        locs = target_array - start\n        valid = (locs % step == 0) & (locs >= 0) & (target_array < stop)\n        locs[~valid] = -1\n        locs[valid] = locs[valid] / step\n\n        if step != self.step:\n            # We reversed this range: transform to original locs\n            locs[valid] = len(self) - 1 - locs[valid]\n        return ensure_platform_int(locs)\n\n    # --------------------------------------------------------------------\n\n    def tolist(self) -> list[int]:\n        return list(self._range)\n\n    @doc(Int64Index.__iter__)\n    def __iter__(self):\n        yield from self._range\n\n    @doc(Int64Index._shallow_copy)\n    def _shallow_copy(self, values, name: Hashable = no_default):\n        name = self.name if name is no_default else name\n\n        if values.dtype.kind == \"f\":\n            return Float64Index(values, name=name)\n        return Int64Index._simple_new(values, name=name)\n\n    def _view(self: RangeIndex) -> RangeIndex:\n        result = type(self)._simple_new(self._range, name=self._name)\n        result._cache = self._cache\n        return result\n\n    @doc(Int64Index.copy)\n    def copy(\n        self,\n        name: Hashable = None,\n        deep: bool = False,\n        dtype: Dtype | None = None,\n        names=None,\n    ):\n        name = self._validate_names(name=name, names=names, deep=deep)[0]\n        new_index = self._rename(name=name)\n\n        if dtype:\n            warnings.warn(\n                \"parameter dtype is deprecated and will be removed in a future \"\n                \"version. Use the astype method instead.\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n            new_index = new_index.astype(dtype)\n        return new_index\n\n    def _minmax(self, meth: str):\n        no_steps = len(self) - 1\n        if no_steps == -1:\n            return np.nan\n        elif (meth == \"min\" and self.step > 0) or (meth == \"max\" and self.step < 0):\n            return self.start\n\n        return self.start + self.step * no_steps\n\n    def min(self, axis=None, skipna: bool = True, *args, **kwargs) -> int:\n        \"\"\"The minimum value of the RangeIndex\"\"\"\n        nv.validate_minmax_axis(axis)\n        nv.validate_min(args, kwargs)\n        return self._minmax(\"min\")\n\n    def max(self, axis=None, skipna: bool = True, *args, **kwargs) -> int:\n        \"\"\"The maximum value of the RangeIndex\"\"\"\n        nv.validate_minmax_axis(axis)\n        nv.validate_max(args, kwargs)\n        return self._minmax(\"max\")\n\n    def argsort(self, *args, **kwargs) -> npt.NDArray[np.intp]:\n        \"\"\"\n        Returns the indices that would sort the index and its\n        underlying data.\n\n        Returns\n        -------\n        np.ndarray[np.intp]\n\n        See Also\n        --------\n        numpy.ndarray.argsort\n        \"\"\"\n        ascending = kwargs.pop(\"ascending\", True)  # EA compat\n        kwargs.pop(\"kind\", None)  # e.g. \"mergesort\" is irrelevant\n        nv.validate_argsort(args, kwargs)\n\n        if self._range.step > 0:\n            result = np.arange(len(self), dtype=np.intp)\n        else:\n            result = np.arange(len(self) - 1, -1, -1, dtype=np.intp)\n\n        if not ascending:\n            result = result[::-1]\n        return result\n\n    def factorize(\n        self, sort: bool = False, na_sentinel: int | None = -1\n    ) -> tuple[npt.NDArray[np.intp], RangeIndex]:\n        codes = np.arange(len(self), dtype=np.intp)\n        uniques = self\n        if sort and self.step < 0:\n            codes = codes[::-1]\n            uniques = uniques[::-1]\n        return codes, uniques\n\n    def equals(self, other: object) -> bool:\n        \"\"\"\n        Determines if two Index objects contain the same elements.\n        \"\"\"\n        if isinstance(other, RangeIndex):\n            return self._range == other._range\n        return super().equals(other)\n\n    def sort_values(\n        self,\n        return_indexer: bool = False,\n        ascending: bool = True,\n        na_position: str = \"last\",\n        key: Callable | None = None,\n    ):\n        sorted_index = self\n        indexer = RangeIndex(range(len(self)))\n        if key is not None:\n            return super().sort_values(\n                return_indexer=return_indexer,\n                ascending=ascending,\n                na_position=na_position,\n                key=key,\n            )\n        else:\n            sorted_index = self\n            if ascending:\n                if self.step < 0:\n                    sorted_index = self[::-1]\n                    indexer = indexer[::-1]\n            else:\n                if self.step > 0:\n                    sorted_index = self[::-1]\n                    indexer = indexer = indexer[::-1]\n\n        if return_indexer:\n            return sorted_index, indexer\n        else:\n            return sorted_index\n\n    # --------------------------------------------------------------------\n    # Set Operations\n\n    def _intersection(self, other: Index, sort=False):\n        # caller is responsible for checking self and other are both non-empty\n\n        if not isinstance(other, RangeIndex):\n            # Int64Index\n            return super()._intersection(other, sort=sort)\n\n        first = self._range[::-1] if self.step < 0 else self._range\n        second = other._range[::-1] if other.step < 0 else other._range\n\n        # check whether intervals intersect\n        # deals with in- and decreasing ranges\n        int_low = max(first.start, second.start)\n        int_high = min(first.stop, second.stop)\n        if int_high <= int_low:\n            return self._simple_new(_empty_range)\n\n        # Method hint: linear Diophantine equation\n        # solve intersection problem\n        # performance hint: for identical step sizes, could use\n        # cheaper alternative\n        gcd, s, _ = self._extended_gcd(first.step, second.step)\n\n        # check whether element sets intersect\n        if (first.start - second.start) % gcd:\n            return self._simple_new(_empty_range)\n\n        # calculate parameters for the RangeIndex describing the\n        # intersection disregarding the lower bounds\n        tmp_start = first.start + (second.start - first.start) * first.step // gcd * s\n        new_step = first.step * second.step // gcd\n        new_range = range(tmp_start, int_high, new_step)\n        new_index = self._simple_new(new_range)\n\n        # adjust index to limiting interval\n        new_start = new_index._min_fitting_element(int_low)\n        new_range = range(new_start, new_index.stop, new_index.step)\n        new_index = self._simple_new(new_range)\n\n        if (self.step < 0 and other.step < 0) is not (new_index.step < 0):\n            new_index = new_index[::-1]\n\n        if sort is None:\n            new_index = new_index.sort_values()\n\n        return new_index\n\n    def _min_fitting_element(self, lower_limit: int) -> int:\n        \"\"\"Returns the smallest element greater than or equal to the limit\"\"\"\n        no_steps = -(-(lower_limit - self.start) // abs(self.step))\n        return self.start + abs(self.step) * no_steps\n\n    def _extended_gcd(self, a: int, b: int) -> tuple[int, int, int]:\n        \"\"\"\n        Extended Euclidean algorithms to solve Bezout's identity:\n           a*x + b*y = gcd(x, y)\n        Finds one particular solution for x, y: s, t\n        Returns: gcd, s, t\n        \"\"\"\n        s, old_s = 0, 1\n        t, old_t = 1, 0\n        r, old_r = b, a\n        while r:\n            quotient = old_r // r\n            old_r, r = r, old_r - quotient * r\n            old_s, s = s, old_s - quotient * s\n            old_t, t = t, old_t - quotient * t\n        return old_r, old_s, old_t\n\n    def _union(self, other: Index, sort):\n        \"\"\"\n        Form the union of two Index objects and sorts if possible\n\n        Parameters\n        ----------\n        other : Index or array-like\n\n        sort : False or None, default None\n            Whether to sort resulting index. ``sort=None`` returns a\n            monotonically increasing ``RangeIndex`` if possible or a sorted\n            ``Int64Index`` if not. ``sort=False`` always returns an\n            unsorted ``Int64Index``\n\n            .. versionadded:: 0.25.0\n\n        Returns\n        -------\n        union : Index\n        \"\"\"\n        if isinstance(other, RangeIndex) and sort is None:\n            start_s, step_s = self.start, self.step\n            end_s = self.start + self.step * (len(self) - 1)\n            start_o, step_o = other.start, other.step\n            end_o = other.start + other.step * (len(other) - 1)\n            if self.step < 0:\n                start_s, step_s, end_s = end_s, -step_s, start_s\n            if other.step < 0:\n                start_o, step_o, end_o = end_o, -step_o, start_o\n            if len(self) == 1 and len(other) == 1:\n                step_s = step_o = abs(self.start - other.start)\n            elif len(self) == 1:\n                step_s = step_o\n            elif len(other) == 1:\n                step_o = step_s\n            start_r = min(start_s, start_o)\n            end_r = max(end_s, end_o)\n            if step_o == step_s:\n                if (\n                    (start_s - start_o) % step_s == 0\n                    and (start_s - end_o) <= step_s\n                    and (start_o - end_s) <= step_s\n                ):\n                    return type(self)(start_r, end_r + step_s, step_s)\n                if (\n                    (step_s % 2 == 0)\n                    and (abs(start_s - start_o) == step_s / 2)\n                    and (abs(end_s - end_o) == step_s / 2)\n                ):\n                    # e.g. range(0, 10, 2) and range(1, 11, 2)\n                    #  but not range(0, 20, 4) and range(1, 21, 4) GH#44019\n                    return type(self)(start_r, end_r + step_s / 2, step_s / 2)\n\n            elif step_o % step_s == 0:\n                if (\n                    (start_o - start_s) % step_s == 0\n                    and (start_o + step_s >= start_s)\n                    and (end_o - step_s <= end_s)\n                ):\n                    return type(self)(start_r, end_r + step_s, step_s)\n            elif step_s % step_o == 0:\n                if (\n                    (start_s - start_o) % step_o == 0\n                    and (start_s + step_o >= start_o)\n                    and (end_s - step_o <= end_o)\n                ):\n                    return type(self)(start_r, end_r + step_o, step_o)\n\n        return super()._union(other, sort=sort)\n\n    def _difference(self, other, sort=None):\n        # optimized set operation if we have another RangeIndex\n        self._validate_sort_keyword(sort)\n        self._assert_can_do_setop(other)\n        other, result_name = self._convert_can_do_setop(other)\n\n        if not isinstance(other, RangeIndex):\n            return super()._difference(other, sort=sort)\n\n        if sort is None and self.step < 0:\n            return self[::-1]._difference(other)\n\n        res_name = ops.get_op_result_name(self, other)\n\n        first = self._range[::-1] if self.step < 0 else self._range\n        overlap = self.intersection(other)\n        if overlap.step < 0:\n            overlap = overlap[::-1]\n\n        if len(overlap) == 0:\n            return self.rename(name=res_name)\n        if len(overlap) == len(self):\n            return self[:0].rename(res_name)\n\n        # overlap.step will always be a multiple of self.step (see _intersection)\n\n        if len(overlap) == 1:\n            if overlap[0] == self[0]:\n                return self[1:]\n\n            elif overlap[0] == self[-1]:\n                return self[:-1]\n\n            elif len(self) == 3 and overlap[0] == self[1]:\n                return self[::2]\n\n            else:\n                return super()._difference(other, sort=sort)\n\n        elif len(overlap) == 2 and overlap[0] == first[0] and overlap[-1] == first[-1]:\n            # e.g. range(-8, 20, 7) and range(13, -9, -3)\n            return self[1:-1]\n\n        if overlap.step == first.step:\n            if overlap[0] == first.start:\n                # The difference is everything after the intersection\n                new_rng = range(overlap[-1] + first.step, first.stop, first.step)\n            elif overlap[-1] == first[-1]:\n                # The difference is everything before the intersection\n                new_rng = range(first.start, overlap[0], first.step)\n            elif overlap._range == first[1:-1]:\n                # e.g. range(4) and range(1, 3)\n                step = len(first) - 1\n                new_rng = first[::step]\n            else:\n                # The difference is not range-like\n                # e.g. range(1, 10, 1) and range(3, 7, 1)\n                return super()._difference(other, sort=sort)\n\n        else:\n            # We must have len(self) > 1, bc we ruled out above\n            #  len(overlap) == 0 and len(overlap) == len(self)\n            assert len(self) > 1\n\n            if overlap.step == first.step * 2:\n                if overlap[0] == first[0] and overlap[-1] in (first[-1], first[-2]):\n                    # e.g. range(1, 10, 1) and range(1, 10, 2)\n                    new_rng = first[1::2]\n\n                elif overlap[0] == first[1] and overlap[-1] in (first[-1], first[-2]):\n                    # e.g. range(1, 10, 1) and range(2, 10, 2)\n                    new_rng = first[::2]\n\n                else:\n                    # We can get here with  e.g. range(20) and range(0, 10, 2)\n                    return super()._difference(other, sort=sort)\n\n            else:\n                # e.g. range(10) and range(0, 10, 3)\n                return super()._difference(other, sort=sort)\n\n        new_index = type(self)._simple_new(new_rng, name=res_name)\n        if first is not self._range:\n            new_index = new_index[::-1]\n\n        return new_index\n\n    def symmetric_difference(self, other, result_name: Hashable = None, sort=None):\n        if not isinstance(other, RangeIndex) or sort is not None:\n            return super().symmetric_difference(other, result_name, sort)\n\n        left = self.difference(other)\n        right = other.difference(self)\n        result = left.union(right)\n\n        if result_name is not None:\n            result = result.rename(result_name)\n        return result\n\n    # --------------------------------------------------------------------\n\n    # error: Return type \"Index\" of \"delete\" incompatible with return type\n    #  \"RangeIndex\" in supertype \"Index\"\n    def delete(self, loc) -> Index:  # type: ignore[override]\n        # In some cases we can retain RangeIndex, see also\n        #  DatetimeTimedeltaMixin._get_delete_Freq\n        if is_integer(loc):\n            if loc == 0 or loc == -len(self):\n                return self[1:]\n            if loc == -1 or loc == len(self) - 1:\n                return self[:-1]\n            if len(self) == 3 and (loc == 1 or loc == -2):\n                return self[::2]\n\n        elif lib.is_list_like(loc):\n            slc = lib.maybe_indices_to_slice(np.asarray(loc, dtype=np.intp), len(self))\n\n            if isinstance(slc, slice):\n                # defer to RangeIndex._difference, which is optimized to return\n                #  a RangeIndex whenever possible\n                other = self[slc]\n                return self.difference(other, sort=False)\n\n        return super().delete(loc)\n\n    def insert(self, loc: int, item) -> Index:\n        if len(self) and (is_integer(item) or is_float(item)):\n            # We can retain RangeIndex is inserting at the beginning or end,\n            #  or right in the middle.\n            rng = self._range\n            if loc == 0 and item == self[0] - self.step:\n                new_rng = range(rng.start - rng.step, rng.stop, rng.step)\n                return type(self)._simple_new(new_rng, name=self.name)\n\n            elif loc == len(self) and item == self[-1] + self.step:\n                new_rng = range(rng.start, rng.stop + rng.step, rng.step)\n                return type(self)._simple_new(new_rng, name=self.name)\n\n            elif len(self) == 2 and item == self[0] + self.step / 2:\n                # e.g. inserting 1 into [0, 2]\n                step = int(self.step / 2)\n                new_rng = range(self.start, self.stop, step)\n                return type(self)._simple_new(new_rng, name=self.name)\n\n        return super().insert(loc, item)\n\n    def _concat(self, indexes: list[Index], name: Hashable) -> Index:\n        \"\"\"\n        Overriding parent method for the case of all RangeIndex instances.\n\n        When all members of \"indexes\" are of type RangeIndex: result will be\n        RangeIndex if possible, Int64Index otherwise. E.g.:\n        indexes = [RangeIndex(3), RangeIndex(3, 6)] -> RangeIndex(6)\n        indexes = [RangeIndex(3), RangeIndex(4, 6)] -> Int64Index([0,1,2,4,5])\n        \"\"\"\n        if not all(isinstance(x, RangeIndex) for x in indexes):\n            return super()._concat(indexes, name)\n\n        elif len(indexes) == 1:\n            return indexes[0]\n\n        rng_indexes = cast(List[RangeIndex], indexes)\n\n        start = step = next_ = None\n\n        # Filter the empty indexes\n        non_empty_indexes = [obj for obj in rng_indexes if len(obj)]\n\n        for obj in non_empty_indexes:\n            rng = obj._range\n\n            if start is None:\n                # This is set by the first non-empty index\n                start = rng.start\n                if step is None and len(rng) > 1:\n                    step = rng.step\n            elif step is None:\n                # First non-empty index had only one element\n                if rng.start == start:\n                    values = np.concatenate([x._values for x in rng_indexes])\n                    result = Int64Index(values)\n                    return result.rename(name)\n\n                step = rng.start - start\n\n            non_consecutive = (step != rng.step and len(rng) > 1) or (\n                next_ is not None and rng.start != next_\n            )\n            if non_consecutive:\n                result = Int64Index(np.concatenate([x._values for x in rng_indexes]))\n                return result.rename(name)\n\n            if step is not None:\n                next_ = rng[-1] + step\n\n        if non_empty_indexes:\n            # Get the stop value from \"next\" or alternatively\n            # from the last non-empty index\n            stop = non_empty_indexes[-1].stop if next_ is None else next_\n            return RangeIndex(start, stop, step).rename(name)\n\n        # Here all \"indexes\" had 0 length, i.e. were empty.\n        # In this case return an empty range index.\n        return RangeIndex(0, 0).rename(name)\n\n    def __len__(self) -> int:\n        \"\"\"\n        return the length of the RangeIndex\n        \"\"\"\n        return len(self._range)\n\n    @property\n    def size(self) -> int:\n        return len(self)\n\n    def __getitem__(self, key):\n        \"\"\"\n        Conserve RangeIndex type for scalar and slice keys.\n        \"\"\"\n        if isinstance(key, slice):\n            new_range = self._range[key]\n            return self._simple_new(new_range, name=self._name)\n        elif is_integer(key):\n            new_key = int(key)\n            try:\n                return self._range[new_key]\n            except IndexError as err:\n                raise IndexError(\n                    f\"index {key} is out of bounds for axis 0 with size {len(self)}\"\n                ) from err\n        elif is_scalar(key):\n            raise IndexError(\n                \"only integers, slices (`:`), \"\n                \"ellipsis (`...`), numpy.newaxis (`None`) \"\n                \"and integer or boolean \"\n                \"arrays are valid indices\"\n            )\n        # fall back to Int64Index\n        return super().__getitem__(key)\n\n    def _getitem_slice(self: RangeIndex, slobj: slice) -> RangeIndex:\n        \"\"\"\n        Fastpath for __getitem__ when we know we have a slice.\n        \"\"\"\n        res = self._range[slobj]\n        return type(self)._simple_new(res, name=self._name)\n\n    @unpack_zerodim_and_defer(\"__floordiv__\")\n    def __floordiv__(self, other):\n\n        if is_integer(other) and other != 0:\n            if len(self) == 0 or self.start % other == 0 and self.step % other == 0:\n                start = self.start // other\n                step = self.step // other\n                stop = start + len(self) * step\n                new_range = range(start, stop, step or 1)\n                return self._simple_new(new_range, name=self.name)\n            if len(self) == 1:\n                start = self.start // other\n                new_range = range(start, start + 1, 1)\n                return self._simple_new(new_range, name=self.name)\n\n        return super().__floordiv__(other)\n\n    # --------------------------------------------------------------------\n    # Reductions\n\n    def all(self, *args, **kwargs) -> bool:\n        return 0 not in self._range\n\n    def any(self, *args, **kwargs) -> bool:\n        return any(self._range)\n\n    # --------------------------------------------------------------------\n\n    def _cmp_method(self, other, op):\n        if isinstance(other, RangeIndex) and self._range == other._range:\n            # Both are immutable so if ._range attr. are equal, shortcut is possible\n            return super()._cmp_method(self, op)\n        return super()._cmp_method(other, op)\n\n    def _arith_method(self, other, op):\n        \"\"\"\n        Parameters\n        ----------\n        other : Any\n        op : callable that accepts 2 params\n            perform the binary op\n        \"\"\"\n\n        if isinstance(other, ABCTimedeltaIndex):\n            # Defer to TimedeltaIndex implementation\n            return NotImplemented\n        elif isinstance(other, (timedelta, np.timedelta64)):\n            # GH#19333 is_integer evaluated True on timedelta64,\n            # so we need to catch these explicitly\n            return super()._arith_method(other, op)\n        elif is_timedelta64_dtype(other):\n            # Must be an np.ndarray; GH#22390\n            return super()._arith_method(other, op)\n\n        if op in [\n            operator.pow,\n            ops.rpow,\n            operator.mod,\n            ops.rmod,\n            operator.floordiv,\n            ops.rfloordiv,\n            divmod,\n            ops.rdivmod,\n        ]:\n            return super()._arith_method(other, op)\n\n        step: Callable | None = None\n        if op in [operator.mul, ops.rmul, operator.truediv, ops.rtruediv]:\n            step = op\n\n        # TODO: if other is a RangeIndex we may have more efficient options\n        right = extract_array(other, extract_numpy=True, extract_range=True)\n        left = self\n\n        try:\n            # apply if we have an override\n            if step:\n                with np.errstate(all=\"ignore\"):\n                    rstep = step(left.step, right)\n\n                # we don't have a representable op\n                # so return a base index\n                if not is_integer(rstep) or not rstep:\n                    raise ValueError\n\n            else:\n                rstep = left.step\n\n            with np.errstate(all=\"ignore\"):\n                rstart = op(left.start, right)\n                rstop = op(left.stop, right)\n\n            res_name = ops.get_op_result_name(self, other)\n            result = type(self)(rstart, rstop, rstep, name=res_name)\n\n            # for compat with numpy / Int64Index\n            # even if we can represent as a RangeIndex, return\n            # as a Float64Index if we have float-like descriptors\n            if not all(is_integer(x) for x in [rstart, rstop, rstep]):\n                result = result.astype(\"float64\")\n\n            return result\n\n        except (ValueError, TypeError, ZeroDivisionError):\n            # Defer to Int64Index implementation\n            # test_arithmetic_explicit_conversions\n            return super()._arith_method(other, op)\n", 1057], "/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/indexes/api.py": ["from __future__ import annotations\n\nimport textwrap\n\nfrom pandas._libs import (\n    NaT,\n    lib,\n)\nfrom pandas.errors import InvalidIndexError\n\nfrom pandas.core.dtypes.common import is_dtype_equal\n\nfrom pandas.core.indexes.base import (\n    Index,\n    _new_Index,\n    ensure_index,\n    ensure_index_from_sequences,\n    get_unanimous_names,\n)\nfrom pandas.core.indexes.category import CategoricalIndex\nfrom pandas.core.indexes.datetimes import DatetimeIndex\nfrom pandas.core.indexes.interval import IntervalIndex\nfrom pandas.core.indexes.multi import MultiIndex\nfrom pandas.core.indexes.numeric import (\n    Float64Index,\n    Int64Index,\n    NumericIndex,\n    UInt64Index,\n)\nfrom pandas.core.indexes.period import PeriodIndex\nfrom pandas.core.indexes.range import RangeIndex\nfrom pandas.core.indexes.timedeltas import TimedeltaIndex\n\n_sort_msg = textwrap.dedent(\n    \"\"\"\\\nSorting because non-concatenation axis is not aligned. A future version\nof pandas will change to not sort by default.\n\nTo accept the future behavior, pass 'sort=False'.\n\nTo retain the current behavior and silence the warning, pass 'sort=True'.\n\"\"\"\n)\n\n\n__all__ = [\n    \"Index\",\n    \"MultiIndex\",\n    \"NumericIndex\",\n    \"Float64Index\",\n    \"Int64Index\",\n    \"CategoricalIndex\",\n    \"IntervalIndex\",\n    \"RangeIndex\",\n    \"UInt64Index\",\n    \"InvalidIndexError\",\n    \"TimedeltaIndex\",\n    \"PeriodIndex\",\n    \"DatetimeIndex\",\n    \"_new_Index\",\n    \"NaT\",\n    \"ensure_index\",\n    \"ensure_index_from_sequences\",\n    \"get_objs_combined_axis\",\n    \"union_indexes\",\n    \"get_unanimous_names\",\n    \"all_indexes_same\",\n    \"default_index\",\n]\n\n\ndef get_objs_combined_axis(\n    objs, intersect: bool = False, axis=0, sort: bool = True, copy: bool = False\n) -> Index:\n    \"\"\"\n    Extract combined index: return intersection or union (depending on the\n    value of \"intersect\") of indexes on given axis, or None if all objects\n    lack indexes (e.g. they are numpy arrays).\n\n    Parameters\n    ----------\n    objs : list\n        Series or DataFrame objects, may be mix of the two.\n    intersect : bool, default False\n        If True, calculate the intersection between indexes. Otherwise,\n        calculate the union.\n    axis : {0 or 'index', 1 or 'outer'}, default 0\n        The axis to extract indexes from.\n    sort : bool, default True\n        Whether the result index should come out sorted or not.\n    copy : bool, default False\n        If True, return a copy of the combined index.\n\n    Returns\n    -------\n    Index\n    \"\"\"\n    obs_idxes = [obj._get_axis(axis) for obj in objs]\n    return _get_combined_index(obs_idxes, intersect=intersect, sort=sort, copy=copy)\n\n\ndef _get_distinct_objs(objs: list[Index]) -> list[Index]:\n    \"\"\"\n    Return a list with distinct elements of \"objs\" (different ids).\n    Preserves order.\n    \"\"\"\n    ids: set[int] = set()\n    res = []\n    for obj in objs:\n        if id(obj) not in ids:\n            ids.add(id(obj))\n            res.append(obj)\n    return res\n\n\ndef _get_combined_index(\n    indexes: list[Index],\n    intersect: bool = False,\n    sort: bool = False,\n    copy: bool = False,\n) -> Index:\n    \"\"\"\n    Return the union or intersection of indexes.\n\n    Parameters\n    ----------\n    indexes : list of Index or list objects\n        When intersect=True, do not accept list of lists.\n    intersect : bool, default False\n        If True, calculate the intersection between indexes. Otherwise,\n        calculate the union.\n    sort : bool, default False\n        Whether the result index should come out sorted or not.\n    copy : bool, default False\n        If True, return a copy of the combined index.\n\n    Returns\n    -------\n    Index\n    \"\"\"\n    # TODO: handle index names!\n    indexes = _get_distinct_objs(indexes)\n    if len(indexes) == 0:\n        index = Index([])\n    elif len(indexes) == 1:\n        index = indexes[0]\n    elif intersect:\n        index = indexes[0]\n        for other in indexes[1:]:\n            index = index.intersection(other)\n    else:\n        index = union_indexes(indexes, sort=False)\n        index = ensure_index(index)\n\n    if sort:\n        try:\n            index = index.sort_values()\n        except TypeError:\n            pass\n\n    # GH 29879\n    if copy:\n        index = index.copy()\n\n    return index\n\n\ndef union_indexes(indexes, sort: bool | None = True) -> Index:\n    \"\"\"\n    Return the union of indexes.\n\n    The behavior of sort and names is not consistent.\n\n    Parameters\n    ----------\n    indexes : list of Index or list objects\n    sort : bool, default True\n        Whether the result index should come out sorted or not.\n\n    Returns\n    -------\n    Index\n    \"\"\"\n    if len(indexes) == 0:\n        raise AssertionError(\"Must have at least 1 Index to union\")\n    if len(indexes) == 1:\n        result = indexes[0]\n        if isinstance(result, list):\n            result = Index(sorted(result))\n        return result\n\n    indexes, kind = _sanitize_and_check(indexes)\n\n    def _unique_indices(inds) -> Index:\n        \"\"\"\n        Convert indexes to lists and concatenate them, removing duplicates.\n\n        The final dtype is inferred.\n\n        Parameters\n        ----------\n        inds : list of Index or list objects\n\n        Returns\n        -------\n        Index\n        \"\"\"\n\n        def conv(i):\n            if isinstance(i, Index):\n                i = i.tolist()\n            return i\n\n        return Index(lib.fast_unique_multiple_list([conv(i) for i in inds], sort=sort))\n\n    if kind == \"special\":\n        result = indexes[0]\n        first = result\n\n        dtis = [x for x in indexes if isinstance(x, DatetimeIndex)]\n        dti_tzs = [x for x in dtis if x.tz is not None]\n        if len(dti_tzs) not in [0, len(dtis)]:\n            # TODO: this behavior is not tested (so may not be desired),\n            #  but is kept in order to keep behavior the same when\n            #  deprecating union_many\n            # test_frame_from_dict_with_mixed_indexes\n            raise TypeError(\"Cannot join tz-naive with tz-aware DatetimeIndex\")\n\n        if len(dtis) == len(indexes):\n            sort = True\n            if not all(is_dtype_equal(x.dtype, first.dtype) for x in indexes):\n                # i.e. timezones mismatch\n                # TODO(2.0): once deprecation is enforced, this union will\n                #  cast to UTC automatically.\n                indexes = [x.tz_convert(\"UTC\") for x in indexes]\n\n            result = indexes[0]\n\n        elif len(dtis) > 1:\n            # If we have mixed timezones, our casting behavior may depend on\n            #  the order of indexes, which we don't want.\n            sort = False\n\n            # TODO: what about Categorical[dt64]?\n            # test_frame_from_dict_with_mixed_indexes\n            indexes = [x.astype(object, copy=False) for x in indexes]\n            result = indexes[0]\n\n        for other in indexes[1:]:\n            result = result.union(other, sort=None if sort else False)\n        return result\n\n    elif kind == \"array\":\n        index = indexes[0]\n        if not all(index.equals(other) for other in indexes[1:]):\n            index = _unique_indices(indexes)\n\n        name = get_unanimous_names(*indexes)[0]\n        if name != index.name:\n            index = index.rename(name)\n        return index\n    else:  # kind='list'\n        return _unique_indices(indexes)\n\n\ndef _sanitize_and_check(indexes):\n    \"\"\"\n    Verify the type of indexes and convert lists to Index.\n\n    Cases:\n\n    - [list, list, ...]: Return ([list, list, ...], 'list')\n    - [list, Index, ...]: Return _sanitize_and_check([Index, Index, ...])\n        Lists are sorted and converted to Index.\n    - [Index, Index, ...]: Return ([Index, Index, ...], TYPE)\n        TYPE = 'special' if at least one special type, 'array' otherwise.\n\n    Parameters\n    ----------\n    indexes : list of Index or list objects\n\n    Returns\n    -------\n    sanitized_indexes : list of Index or list objects\n    type : {'list', 'array', 'special'}\n    \"\"\"\n    kinds = list({type(index) for index in indexes})\n\n    if list in kinds:\n        if len(kinds) > 1:\n            indexes = [\n                Index(list(x)) if not isinstance(x, Index) else x for x in indexes\n            ]\n            kinds.remove(list)\n        else:\n            return indexes, \"list\"\n\n    if len(kinds) > 1 or Index not in kinds:\n        return indexes, \"special\"\n    else:\n        return indexes, \"array\"\n\n\ndef all_indexes_same(indexes) -> bool:\n    \"\"\"\n    Determine if all indexes contain the same elements.\n\n    Parameters\n    ----------\n    indexes : iterable of Index objects\n\n    Returns\n    -------\n    bool\n        True if all indexes contain the same elements, False otherwise.\n    \"\"\"\n    itr = iter(indexes)\n    first = next(itr)\n    return all(first.equals(index) for index in itr)\n\n\ndef default_index(n: int) -> RangeIndex:\n    rng = range(0, n)\n    return RangeIndex._simple_new(rng, name=None)\n", 324], "/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/cast.py": ["\"\"\"\nRoutines for casting.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom datetime import (\n    date,\n    datetime,\n    timedelta,\n)\nimport functools\nimport inspect\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Sized,\n    TypeVar,\n    cast,\n    overload,\n)\nimport warnings\n\nimport numpy as np\n\nfrom pandas._libs import lib\nfrom pandas._libs.tslibs import (\n    NaT,\n    OutOfBoundsDatetime,\n    OutOfBoundsTimedelta,\n    Timedelta,\n    Timestamp,\n    conversion,\n)\nfrom pandas._libs.tslibs.timedeltas import array_to_timedelta64\nfrom pandas._typing import (\n    ArrayLike,\n    Dtype,\n    DtypeObj,\n    Scalar,\n)\nfrom pandas.errors import IntCastingNaNError\nfrom pandas.util._exceptions import find_stack_level\nfrom pandas.util._validators import validate_bool_kwarg\n\nfrom pandas.core.dtypes.common import (\n    DT64NS_DTYPE,\n    TD64NS_DTYPE,\n    ensure_int8,\n    ensure_int16,\n    ensure_int32,\n    ensure_int64,\n    ensure_object,\n    ensure_str,\n    is_bool,\n    is_bool_dtype,\n    is_complex,\n    is_complex_dtype,\n    is_datetime64_dtype,\n    is_datetime64tz_dtype,\n    is_dtype_equal,\n    is_extension_array_dtype,\n    is_float,\n    is_float_dtype,\n    is_integer,\n    is_integer_dtype,\n    is_numeric_dtype,\n    is_object_dtype,\n    is_scalar,\n    is_string_dtype,\n    is_timedelta64_dtype,\n    is_unsigned_integer_dtype,\n    pandas_dtype,\n)\nfrom pandas.core.dtypes.dtypes import (\n    CategoricalDtype,\n    DatetimeTZDtype,\n    ExtensionDtype,\n    IntervalDtype,\n    PandasDtype,\n    PeriodDtype,\n)\nfrom pandas.core.dtypes.generic import (\n    ABCExtensionArray,\n    ABCSeries,\n)\nfrom pandas.core.dtypes.inference import is_list_like\nfrom pandas.core.dtypes.missing import (\n    is_valid_na_for_dtype,\n    isna,\n    na_value_for_dtype,\n    notna,\n)\n\nif TYPE_CHECKING:\n\n    from pandas.core.arrays import (\n        DatetimeArray,\n        ExtensionArray,\n        IntervalArray,\n        PeriodArray,\n        TimedeltaArray,\n    )\n\n_int8_max = np.iinfo(np.int8).max\n_int16_max = np.iinfo(np.int16).max\n_int32_max = np.iinfo(np.int32).max\n_int64_max = np.iinfo(np.int64).max\n\n_dtype_obj = np.dtype(object)\n\nNumpyArrayT = TypeVar(\"NumpyArrayT\", bound=np.ndarray)\n\n\ndef maybe_convert_platform(\n    values: list | tuple | range | np.ndarray | ExtensionArray,\n) -> ArrayLike:\n    \"\"\"try to do platform conversion, allow ndarray or list here\"\"\"\n    arr: ArrayLike\n\n    if isinstance(values, (list, tuple, range)):\n        arr = construct_1d_object_array_from_listlike(values)\n    else:\n        # The caller is responsible for ensuring that we have np.ndarray\n        #  or ExtensionArray here.\n        arr = values\n\n    if arr.dtype == _dtype_obj:\n        arr = cast(np.ndarray, arr)\n        arr = lib.maybe_convert_objects(arr)\n\n    return arr\n\n\ndef is_nested_object(obj) -> bool:\n    \"\"\"\n    return a boolean if we have a nested object, e.g. a Series with 1 or\n    more Series elements\n\n    This may not be necessarily be performant.\n\n    \"\"\"\n    return bool(\n        isinstance(obj, ABCSeries)\n        and is_object_dtype(obj.dtype)\n        and any(isinstance(v, ABCSeries) for v in obj._values)\n    )\n\n\ndef maybe_box_datetimelike(value: Scalar, dtype: Dtype | None = None) -> Scalar:\n    \"\"\"\n    Cast scalar to Timestamp or Timedelta if scalar is datetime-like\n    and dtype is not object.\n\n    Parameters\n    ----------\n    value : scalar\n    dtype : Dtype, optional\n\n    Returns\n    -------\n    scalar\n    \"\"\"\n    if dtype == _dtype_obj:\n        pass\n    elif isinstance(value, (np.datetime64, datetime)):\n        value = Timestamp(value)\n    elif isinstance(value, (np.timedelta64, timedelta)):\n        value = Timedelta(value)\n\n    return value\n\n\ndef maybe_box_native(value: Scalar) -> Scalar:\n    \"\"\"\n    If passed a scalar cast the scalar to a python native type.\n\n    Parameters\n    ----------\n    value : scalar or Series\n\n    Returns\n    -------\n    scalar or Series\n    \"\"\"\n    if is_float(value):\n        # error: Argument 1 to \"float\" has incompatible type\n        # \"Union[Union[str, int, float, bool], Union[Any, Timestamp, Timedelta, Any]]\";\n        # expected \"Union[SupportsFloat, _SupportsIndex, str]\"\n        value = float(value)  # type: ignore[arg-type]\n    elif is_integer(value):\n        # error: Argument 1 to \"int\" has incompatible type\n        # \"Union[Union[str, int, float, bool], Union[Any, Timestamp, Timedelta, Any]]\";\n        # expected \"Union[str, SupportsInt, _SupportsIndex, _SupportsTrunc]\"\n        value = int(value)  # type: ignore[arg-type]\n    elif is_bool(value):\n        value = bool(value)\n    elif isinstance(value, (np.datetime64, np.timedelta64)):\n        value = maybe_box_datetimelike(value)\n    return value\n\n\ndef maybe_unbox_datetimelike(value: Scalar, dtype: DtypeObj) -> Scalar:\n    \"\"\"\n    Convert a Timedelta or Timestamp to timedelta64 or datetime64 for setting\n    into a numpy array.  Failing to unbox would risk dropping nanoseconds.\n\n    Notes\n    -----\n    Caller is responsible for checking dtype.kind in [\"m\", \"M\"]\n    \"\"\"\n    if is_valid_na_for_dtype(value, dtype):\n        # GH#36541: can't fill array directly with pd.NaT\n        # > np.empty(10, dtype=\"datetime64[64]\").fill(pd.NaT)\n        # ValueError: cannot convert float NaN to integer\n        value = dtype.type(\"NaT\", \"ns\")\n    elif isinstance(value, Timestamp):\n        if value.tz is None:\n            value = value.to_datetime64()\n        elif not isinstance(dtype, DatetimeTZDtype):\n            raise TypeError(\"Cannot unbox tzaware Timestamp to tznaive dtype\")\n    elif isinstance(value, Timedelta):\n        value = value.to_timedelta64()\n\n    _disallow_mismatched_datetimelike(value, dtype)\n    return value\n\n\ndef _disallow_mismatched_datetimelike(value, dtype: DtypeObj):\n    \"\"\"\n    numpy allows np.array(dt64values, dtype=\"timedelta64[ns]\") and\n    vice-versa, but we do not want to allow this, so we need to\n    check explicitly\n    \"\"\"\n    vdtype = getattr(value, \"dtype\", None)\n    if vdtype is None:\n        return\n    elif (vdtype.kind == \"m\" and dtype.kind == \"M\") or (\n        vdtype.kind == \"M\" and dtype.kind == \"m\"\n    ):\n        raise TypeError(f\"Cannot cast {repr(value)} to {dtype}\")\n\n\ndef maybe_downcast_to_dtype(result: ArrayLike, dtype: str | np.dtype) -> ArrayLike:\n    \"\"\"\n    try to cast to the specified dtype (e.g. convert back to bool/int\n    or could be an astype of float64->float32\n    \"\"\"\n    do_round = False\n\n    if isinstance(dtype, str):\n        if dtype == \"infer\":\n            inferred_type = lib.infer_dtype(result, skipna=False)\n            if inferred_type == \"boolean\":\n                dtype = \"bool\"\n            elif inferred_type == \"integer\":\n                dtype = \"int64\"\n            elif inferred_type == \"datetime64\":\n                dtype = \"datetime64[ns]\"\n            elif inferred_type == \"timedelta64\":\n                dtype = \"timedelta64[ns]\"\n\n            # try to upcast here\n            elif inferred_type == \"floating\":\n                dtype = \"int64\"\n                if issubclass(result.dtype.type, np.number):\n                    do_round = True\n\n            else:\n                # TODO: complex?  what if result is already non-object?\n                dtype = \"object\"\n\n        dtype = np.dtype(dtype)\n\n    if not isinstance(dtype, np.dtype):\n        # enforce our signature annotation\n        raise TypeError(dtype)  # pragma: no cover\n\n    converted = maybe_downcast_numeric(result, dtype, do_round)\n    if converted is not result:\n        return converted\n\n    # a datetimelike\n    # GH12821, iNaT is cast to float\n    if dtype.kind in [\"M\", \"m\"] and result.dtype.kind in [\"i\", \"f\"]:\n        result = result.astype(dtype)\n\n    return result\n\n\ndef maybe_downcast_numeric(\n    result: ArrayLike, dtype: DtypeObj, do_round: bool = False\n) -> ArrayLike:\n    \"\"\"\n    Subset of maybe_downcast_to_dtype restricted to numeric dtypes.\n\n    Parameters\n    ----------\n    result : ndarray or ExtensionArray\n    dtype : np.dtype or ExtensionDtype\n    do_round : bool\n\n    Returns\n    -------\n    ndarray or ExtensionArray\n    \"\"\"\n    if not isinstance(dtype, np.dtype) or not isinstance(result.dtype, np.dtype):\n        # e.g. SparseDtype has no itemsize attr\n        return result\n\n    def trans(x):\n        if do_round:\n            return x.round()\n        return x\n\n    if dtype.kind == result.dtype.kind:\n        # don't allow upcasts here (except if empty)\n        if result.dtype.itemsize <= dtype.itemsize and result.size:\n            return result\n\n    if is_bool_dtype(dtype) or is_integer_dtype(dtype):\n\n        if not result.size:\n            # if we don't have any elements, just astype it\n            return trans(result).astype(dtype)\n\n        # do a test on the first element, if it fails then we are done\n        r = result.ravel()\n        arr = np.array([r[0]])\n\n        if isna(arr).any():\n            # if we have any nulls, then we are done\n            return result\n\n        elif not isinstance(r[0], (np.integer, np.floating, int, float, bool)):\n            # a comparable, e.g. a Decimal may slip in here\n            return result\n\n        if (\n            issubclass(result.dtype.type, (np.object_, np.number))\n            and notna(result).all()\n        ):\n            new_result = trans(result).astype(dtype)\n            if new_result.dtype.kind == \"O\" or result.dtype.kind == \"O\":\n                # np.allclose may raise TypeError on object-dtype\n                if (new_result == result).all():\n                    return new_result\n            else:\n                if np.allclose(new_result, result, rtol=0):\n                    return new_result\n\n    elif (\n        issubclass(dtype.type, np.floating)\n        and not is_bool_dtype(result.dtype)\n        and not is_string_dtype(result.dtype)\n    ):\n        return result.astype(dtype)\n\n    return result\n\n\ndef maybe_cast_pointwise_result(\n    result: ArrayLike,\n    dtype: DtypeObj,\n    numeric_only: bool = False,\n    same_dtype: bool = True,\n) -> ArrayLike:\n    \"\"\"\n    Try casting result of a pointwise operation back to the original dtype if\n    appropriate.\n\n    Parameters\n    ----------\n    result : array-like\n        Result to cast.\n    dtype : np.dtype or ExtensionDtype\n        Input Series from which result was calculated.\n    numeric_only : bool, default False\n        Whether to cast only numerics or datetimes as well.\n    same_dtype : bool, default True\n        Specify dtype when calling _from_sequence\n\n    Returns\n    -------\n    result : array-like\n        result maybe casted to the dtype.\n    \"\"\"\n\n    assert not is_scalar(result)\n\n    if isinstance(dtype, ExtensionDtype):\n        if not isinstance(dtype, (CategoricalDtype, DatetimeTZDtype)):\n            # TODO: avoid this special-casing\n            # We have to special case categorical so as not to upcast\n            # things like counts back to categorical\n\n            cls = dtype.construct_array_type()\n            if same_dtype:\n                result = maybe_cast_to_extension_array(cls, result, dtype=dtype)\n            else:\n                result = maybe_cast_to_extension_array(cls, result)\n\n    elif (numeric_only and is_numeric_dtype(dtype)) or not numeric_only:\n        result = maybe_downcast_to_dtype(result, dtype)\n\n    return result\n\n\ndef maybe_cast_to_extension_array(\n    cls: type[ExtensionArray], obj: ArrayLike, dtype: ExtensionDtype | None = None\n) -> ArrayLike:\n    \"\"\"\n    Call to `_from_sequence` that returns the object unchanged on Exception.\n\n    Parameters\n    ----------\n    cls : class, subclass of ExtensionArray\n    obj : arraylike\n        Values to pass to cls._from_sequence\n    dtype : ExtensionDtype, optional\n\n    Returns\n    -------\n    ExtensionArray or obj\n    \"\"\"\n    from pandas.core.arrays.string_ import BaseStringArray\n\n    assert isinstance(cls, type), f\"must pass a type: {cls}\"\n    assertion_msg = f\"must pass a subclass of ExtensionArray: {cls}\"\n    assert issubclass(cls, ABCExtensionArray), assertion_msg\n\n    # Everything can be converted to StringArrays, but we may not want to convert\n    if issubclass(cls, BaseStringArray) and lib.infer_dtype(obj) != \"string\":\n        return obj\n\n    try:\n        result = cls._from_sequence(obj, dtype=dtype)\n    except Exception:\n        # We can't predict what downstream EA constructors may raise\n        result = obj\n    return result\n\n\n@overload\ndef ensure_dtype_can_hold_na(dtype: np.dtype) -> np.dtype:\n    ...\n\n\n@overload\ndef ensure_dtype_can_hold_na(dtype: ExtensionDtype) -> ExtensionDtype:\n    ...\n\n\ndef ensure_dtype_can_hold_na(dtype: DtypeObj) -> DtypeObj:\n    \"\"\"\n    If we have a dtype that cannot hold NA values, find the best match that can.\n    \"\"\"\n    if isinstance(dtype, ExtensionDtype):\n        # TODO: ExtensionDtype.can_hold_na?\n        return dtype\n    elif dtype.kind == \"b\":\n        return _dtype_obj\n    elif dtype.kind in [\"i\", \"u\"]:\n        return np.dtype(np.float64)\n    return dtype\n\n\ndef maybe_promote(dtype: np.dtype, fill_value=np.nan):\n    \"\"\"\n    Find the minimal dtype that can hold both the given dtype and fill_value.\n\n    Parameters\n    ----------\n    dtype : np.dtype\n    fill_value : scalar, default np.nan\n\n    Returns\n    -------\n    dtype\n        Upcasted from dtype argument if necessary.\n    fill_value\n        Upcasted from fill_value argument if necessary.\n\n    Raises\n    ------\n    ValueError\n        If fill_value is a non-scalar and dtype is not object.\n    \"\"\"\n    # TODO(2.0): need to directly use the non-cached version as long as we\n    # possibly raise a deprecation warning for datetime dtype\n    if dtype.kind == \"M\":\n        return _maybe_promote(dtype, fill_value)\n    # for performance, we are using a cached version of the actual implementation\n    # of the function in _maybe_promote. However, this doesn't always work (in case\n    # of non-hashable arguments), so we fallback to the actual implementation if needed\n    try:\n        # error: Argument 3 to \"__call__\" of \"_lru_cache_wrapper\" has incompatible type\n        # \"Type[Any]\"; expected \"Hashable\"  [arg-type]\n        return _maybe_promote_cached(\n            dtype, fill_value, type(fill_value)  # type: ignore[arg-type]\n        )\n    except TypeError:\n        # if fill_value is not hashable (required for caching)\n        return _maybe_promote(dtype, fill_value)\n\n\n@functools.lru_cache(maxsize=128)\ndef _maybe_promote_cached(dtype, fill_value, fill_value_type):\n    # The cached version of _maybe_promote below\n    # This also use fill_value_type as (unused) argument to use this in the\n    # cache lookup -> to differentiate 1 and True\n    return _maybe_promote(dtype, fill_value)\n\n\ndef _maybe_promote(dtype: np.dtype, fill_value=np.nan):\n    # The actual implementation of the function, use `maybe_promote` above for\n    # a cached version.\n    if not is_scalar(fill_value):\n        # with object dtype there is nothing to promote, and the user can\n        #  pass pretty much any weird fill_value they like\n        if not is_object_dtype(dtype):\n            # with object dtype there is nothing to promote, and the user can\n            #  pass pretty much any weird fill_value they like\n            raise ValueError(\"fill_value must be a scalar\")\n        dtype = _dtype_obj\n        return dtype, fill_value\n\n    kinds = [\"i\", \"u\", \"f\", \"c\", \"m\", \"M\"]\n    if is_valid_na_for_dtype(fill_value, dtype) and dtype.kind in kinds:\n        dtype = ensure_dtype_can_hold_na(dtype)\n        fv = na_value_for_dtype(dtype)\n        return dtype, fv\n\n    elif isna(fill_value):\n        dtype = _dtype_obj\n        if fill_value is None:\n            # but we retain e.g. pd.NA\n            fill_value = np.nan\n        return dtype, fill_value\n\n    # returns tuple of (dtype, fill_value)\n    if issubclass(dtype.type, np.datetime64):\n        inferred, fv = infer_dtype_from_scalar(fill_value, pandas_dtype=True)\n        if inferred == dtype:\n            return dtype, fv\n\n        # TODO(2.0): once this deprecation is enforced, this whole case\n        # becomes equivalent to:\n        #  dta = DatetimeArray._from_sequence([], dtype=\"M8[ns]\")\n        #  try:\n        #      fv = dta._validate_setitem_value(fill_value)\n        #      return dta.dtype, fv\n        #  except (ValueError, TypeError):\n        #      return _dtype_obj, fill_value\n        if isinstance(fill_value, date) and not isinstance(fill_value, datetime):\n            # deprecate casting of date object to match infer_dtype_from_scalar\n            #  and DatetimeArray._validate_setitem_value\n            try:\n                fv = Timestamp(fill_value).to_datetime64()\n            except OutOfBoundsDatetime:\n                pass\n            else:\n                warnings.warn(\n                    \"Using a `date` object for fill_value with `datetime64[ns]` \"\n                    \"dtype is deprecated. In a future version, this will be cast \"\n                    \"to object dtype. Pass `fill_value=Timestamp(date_obj)` instead.\",\n                    FutureWarning,\n                    stacklevel=find_stack_level(),\n                )\n                return dtype, fv\n        elif isinstance(fill_value, str):\n            try:\n                # explicitly wrap in str to convert np.str_\n                fv = Timestamp(str(fill_value))\n            except (ValueError, TypeError):\n                pass\n            else:\n                if isna(fv) or fv.tz is None:\n                    return dtype, fv.asm8\n\n        return np.dtype(\"object\"), fill_value\n\n    elif issubclass(dtype.type, np.timedelta64):\n        inferred, fv = infer_dtype_from_scalar(fill_value, pandas_dtype=True)\n        if inferred == dtype:\n            return dtype, fv\n\n        return np.dtype(\"object\"), fill_value\n\n    elif is_float(fill_value):\n        if issubclass(dtype.type, np.bool_):\n            dtype = np.dtype(np.object_)\n\n        elif issubclass(dtype.type, np.integer):\n            dtype = np.dtype(np.float64)\n\n        elif dtype.kind == \"f\":\n            mst = np.min_scalar_type(fill_value)\n            if mst > dtype:\n                # e.g. mst is np.float64 and dtype is np.float32\n                dtype = mst\n\n        elif dtype.kind == \"c\":\n            mst = np.min_scalar_type(fill_value)\n            dtype = np.promote_types(dtype, mst)\n\n    elif is_bool(fill_value):\n        if not issubclass(dtype.type, np.bool_):\n            dtype = np.dtype(np.object_)\n\n    elif is_integer(fill_value):\n        if issubclass(dtype.type, np.bool_):\n            dtype = np.dtype(np.object_)\n\n        elif issubclass(dtype.type, np.integer):\n            if not np.can_cast(fill_value, dtype):\n                # upcast to prevent overflow\n                mst = np.min_scalar_type(fill_value)\n                dtype = np.promote_types(dtype, mst)\n                if dtype.kind == \"f\":\n                    # Case where we disagree with numpy\n                    dtype = np.dtype(np.object_)\n\n    elif is_complex(fill_value):\n        if issubclass(dtype.type, np.bool_):\n            dtype = np.dtype(np.object_)\n\n        elif issubclass(dtype.type, (np.integer, np.floating)):\n            mst = np.min_scalar_type(fill_value)\n            dtype = np.promote_types(dtype, mst)\n\n        elif dtype.kind == \"c\":\n            mst = np.min_scalar_type(fill_value)\n            if mst > dtype:\n                # e.g. mst is np.complex128 and dtype is np.complex64\n                dtype = mst\n\n    else:\n        dtype = np.dtype(np.object_)\n\n    # in case we have a string that looked like a number\n    if issubclass(dtype.type, (bytes, str)):\n        dtype = np.dtype(np.object_)\n\n    fill_value = _ensure_dtype_type(fill_value, dtype)\n    return dtype, fill_value\n\n\ndef _ensure_dtype_type(value, dtype: np.dtype):\n    \"\"\"\n    Ensure that the given value is an instance of the given dtype.\n\n    e.g. if out dtype is np.complex64_, we should have an instance of that\n    as opposed to a python complex object.\n\n    Parameters\n    ----------\n    value : object\n    dtype : np.dtype\n\n    Returns\n    -------\n    object\n    \"\"\"\n    # Start with exceptions in which we do _not_ cast to numpy types\n\n    if dtype == _dtype_obj:\n        return value\n\n    # Note: before we get here we have already excluded isna(value)\n    return dtype.type(value)\n\n\ndef infer_dtype_from(val, pandas_dtype: bool = False) -> tuple[DtypeObj, Any]:\n    \"\"\"\n    Interpret the dtype from a scalar or array.\n\n    Parameters\n    ----------\n    val : object\n    pandas_dtype : bool, default False\n        whether to infer dtype including pandas extension types.\n        If False, scalar/array belongs to pandas extension types is inferred as\n        object\n    \"\"\"\n    if not is_list_like(val):\n        return infer_dtype_from_scalar(val, pandas_dtype=pandas_dtype)\n    return infer_dtype_from_array(val, pandas_dtype=pandas_dtype)\n\n\ndef infer_dtype_from_scalar(val, pandas_dtype: bool = False) -> tuple[DtypeObj, Any]:\n    \"\"\"\n    Interpret the dtype from a scalar.\n\n    Parameters\n    ----------\n    pandas_dtype : bool, default False\n        whether to infer dtype including pandas extension types.\n        If False, scalar belongs to pandas extension types is inferred as\n        object\n    \"\"\"\n    dtype: DtypeObj = _dtype_obj\n\n    # a 1-element ndarray\n    if isinstance(val, np.ndarray):\n        if val.ndim != 0:\n            msg = \"invalid ndarray passed to infer_dtype_from_scalar\"\n            raise ValueError(msg)\n\n        dtype = val.dtype\n        val = lib.item_from_zerodim(val)\n\n    elif isinstance(val, str):\n\n        # If we create an empty array using a string to infer\n        # the dtype, NumPy will only allocate one character per entry\n        # so this is kind of bad. Alternately we could use np.repeat\n        # instead of np.empty (but then you still don't want things\n        # coming out as np.str_!\n\n        dtype = _dtype_obj\n\n    elif isinstance(val, (np.datetime64, datetime)):\n        try:\n            val = Timestamp(val)\n        except OutOfBoundsDatetime:\n            return _dtype_obj, val\n\n        # error: Non-overlapping identity check (left operand type: \"Timestamp\",\n        # right operand type: \"NaTType\")\n        if val is NaT or val.tz is None:  # type: ignore[comparison-overlap]\n            dtype = np.dtype(\"M8[ns]\")\n            val = val.to_datetime64()\n        else:\n            if pandas_dtype:\n                dtype = DatetimeTZDtype(unit=\"ns\", tz=val.tz)\n            else:\n                # return datetimetz as object\n                return _dtype_obj, val\n\n    elif isinstance(val, (np.timedelta64, timedelta)):\n        try:\n            val = Timedelta(val)\n        except (OutOfBoundsTimedelta, OverflowError):\n            dtype = _dtype_obj\n        else:\n            dtype = np.dtype(\"m8[ns]\")\n            val = np.timedelta64(val.value, \"ns\")\n\n    elif is_bool(val):\n        dtype = np.dtype(np.bool_)\n\n    elif is_integer(val):\n        if isinstance(val, np.integer):\n            dtype = np.dtype(type(val))\n        else:\n            dtype = np.dtype(np.int64)\n\n        try:\n            np.array(val, dtype=dtype)\n        except OverflowError:\n            dtype = np.array(val).dtype\n\n    elif is_float(val):\n        if isinstance(val, np.floating):\n            dtype = np.dtype(type(val))\n        else:\n            dtype = np.dtype(np.float64)\n\n    elif is_complex(val):\n        dtype = np.dtype(np.complex_)\n\n    elif pandas_dtype:\n        if lib.is_period(val):\n            dtype = PeriodDtype(freq=val.freq)\n        elif lib.is_interval(val):\n            subtype = infer_dtype_from_scalar(val.left, pandas_dtype=True)[0]\n            dtype = IntervalDtype(subtype=subtype, closed=val.closed)\n\n    return dtype, val\n\n\ndef dict_compat(d: dict[Scalar, Scalar]) -> dict[Scalar, Scalar]:\n    \"\"\"\n    Convert datetimelike-keyed dicts to a Timestamp-keyed dict.\n\n    Parameters\n    ----------\n    d: dict-like object\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    return {maybe_box_datetimelike(key): value for key, value in d.items()}\n\n\ndef infer_dtype_from_array(\n    arr, pandas_dtype: bool = False\n) -> tuple[DtypeObj, ArrayLike]:\n    \"\"\"\n    Infer the dtype from an array.\n\n    Parameters\n    ----------\n    arr : array\n    pandas_dtype : bool, default False\n        whether to infer dtype including pandas extension types.\n        If False, array belongs to pandas extension types\n        is inferred as object\n\n    Returns\n    -------\n    tuple (numpy-compat/pandas-compat dtype, array)\n\n    Notes\n    -----\n    if pandas_dtype=False. these infer to numpy dtypes\n    exactly with the exception that mixed / object dtypes\n    are not coerced by stringifying or conversion\n\n    if pandas_dtype=True. datetime64tz-aware/categorical\n    types will retain there character.\n\n    Examples\n    --------\n    >>> np.asarray([1, '1'])\n    array(['1', '1'], dtype='<U21')\n\n    >>> infer_dtype_from_array([1, '1'])\n    (dtype('O'), [1, '1'])\n    \"\"\"\n    if isinstance(arr, np.ndarray):\n        return arr.dtype, arr\n\n    if not is_list_like(arr):\n        raise TypeError(\"'arr' must be list-like\")\n\n    if pandas_dtype and is_extension_array_dtype(arr):\n        return arr.dtype, arr\n\n    elif isinstance(arr, ABCSeries):\n        return arr.dtype, np.asarray(arr)\n\n    # don't force numpy coerce with nan's\n    inferred = lib.infer_dtype(arr, skipna=False)\n    if inferred in [\"string\", \"bytes\", \"mixed\", \"mixed-integer\"]:\n        return (np.dtype(np.object_), arr)\n\n    arr = np.asarray(arr)\n    return arr.dtype, arr\n\n\ndef maybe_infer_dtype_type(element):\n    \"\"\"\n    Try to infer an object's dtype, for use in arithmetic ops.\n\n    Uses `element.dtype` if that's available.\n    Objects implementing the iterator protocol are cast to a NumPy array,\n    and from there the array's type is used.\n\n    Parameters\n    ----------\n    element : object\n        Possibly has a `.dtype` attribute, and possibly the iterator\n        protocol.\n\n    Returns\n    -------\n    tipo : type\n\n    Examples\n    --------\n    >>> from collections import namedtuple\n    >>> Foo = namedtuple(\"Foo\", \"dtype\")\n    >>> maybe_infer_dtype_type(Foo(np.dtype(\"i8\")))\n    dtype('int64')\n    \"\"\"\n    tipo = None\n    if hasattr(element, \"dtype\"):\n        tipo = element.dtype\n    elif is_list_like(element):\n        element = np.asarray(element)\n        tipo = element.dtype\n    return tipo\n\n\ndef maybe_upcast(\n    values: NumpyArrayT,\n    fill_value: Scalar = np.nan,\n    copy: bool = False,\n) -> tuple[NumpyArrayT, Scalar]:\n    \"\"\"\n    Provide explicit type promotion and coercion.\n\n    Parameters\n    ----------\n    values : np.ndarray\n        The array that we may want to upcast.\n    fill_value : what we want to fill with\n    copy : bool, default True\n        If True always make a copy even if no upcast is required.\n\n    Returns\n    -------\n    values: np.ndarray\n        the original array, possibly upcast\n    fill_value:\n        the fill value, possibly upcast\n    \"\"\"\n    new_dtype, fill_value = maybe_promote(values.dtype, fill_value)\n    # We get a copy in all cases _except_ (values.dtype == new_dtype and not copy)\n    upcast_values = values.astype(new_dtype, copy=copy)\n\n    # error: Incompatible return value type (got \"Tuple[ndarray[Any, dtype[Any]],\n    # Union[Union[str, int, float, bool] Union[Period, Timestamp, Timedelta, Any]]]\",\n    # expected \"Tuple[NumpyArrayT, Union[Union[str, int, float, bool], Union[Period,\n    # Timestamp, Timedelta, Any]]]\")\n    return upcast_values, fill_value  # type: ignore[return-value]\n\n\ndef invalidate_string_dtypes(dtype_set: set[DtypeObj]):\n    \"\"\"\n    Change string like dtypes to object for\n    ``DataFrame.select_dtypes()``.\n    \"\"\"\n    # error: Argument 1 to <set> has incompatible type \"Type[generic]\"; expected\n    # \"Union[dtype[Any], ExtensionDtype, None]\"\n    # error: Argument 2 to <set> has incompatible type \"Type[generic]\"; expected\n    # \"Union[dtype[Any], ExtensionDtype, None]\"\n    non_string_dtypes = dtype_set - {\n        np.dtype(\"S\").type,  # type: ignore[arg-type]\n        np.dtype(\"<U\").type,  # type: ignore[arg-type]\n    }\n    if non_string_dtypes != dtype_set:\n        raise TypeError(\"string dtypes are not allowed, use 'object' instead\")\n\n\ndef coerce_indexer_dtype(indexer, categories):\n    \"\"\"coerce the indexer input array to the smallest dtype possible\"\"\"\n    length = len(categories)\n    if length < _int8_max:\n        return ensure_int8(indexer)\n    elif length < _int16_max:\n        return ensure_int16(indexer)\n    elif length < _int32_max:\n        return ensure_int32(indexer)\n    return ensure_int64(indexer)\n\n\ndef astype_dt64_to_dt64tz(\n    values: ArrayLike, dtype: DtypeObj, copy: bool, via_utc: bool = False\n) -> DatetimeArray:\n    # GH#33401 we have inconsistent behaviors between\n    #  Datetimeindex[naive].astype(tzaware)\n    #  Series[dt64].astype(tzaware)\n    # This collects them in one place to prevent further fragmentation.\n\n    from pandas.core.construction import ensure_wrapped_if_datetimelike\n\n    values = ensure_wrapped_if_datetimelike(values)\n    values = cast(\"DatetimeArray\", values)\n    aware = isinstance(dtype, DatetimeTZDtype)\n\n    if via_utc:\n        # Series.astype behavior\n\n        # caller is responsible for checking this\n        assert values.tz is None and aware\n        dtype = cast(DatetimeTZDtype, dtype)\n\n        if copy:\n            # this should be the only copy\n            values = values.copy()\n\n        warnings.warn(\n            \"Using .astype to convert from timezone-naive dtype to \"\n            \"timezone-aware dtype is deprecated and will raise in a \"\n            \"future version.  Use ser.dt.tz_localize instead.\",\n            FutureWarning,\n            stacklevel=find_stack_level(),\n        )\n\n        # GH#33401 this doesn't match DatetimeArray.astype, which\n        #  goes through the `not via_utc` path\n        return values.tz_localize(\"UTC\").tz_convert(dtype.tz)\n\n    else:\n        # DatetimeArray/DatetimeIndex.astype behavior\n        if values.tz is None and aware:\n            dtype = cast(DatetimeTZDtype, dtype)\n            warnings.warn(\n                \"Using .astype to convert from timezone-naive dtype to \"\n                \"timezone-aware dtype is deprecated and will raise in a \"\n                \"future version.  Use obj.tz_localize instead.\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n\n            return values.tz_localize(dtype.tz)\n\n        elif aware:\n            # GH#18951: datetime64_tz dtype but not equal means different tz\n            dtype = cast(DatetimeTZDtype, dtype)\n            result = values.tz_convert(dtype.tz)\n            if copy:\n                result = result.copy()\n            return result\n\n        elif values.tz is not None:\n            warnings.warn(\n                \"Using .astype to convert from timezone-aware dtype to \"\n                \"timezone-naive dtype is deprecated and will raise in a \"\n                \"future version.  Use obj.tz_localize(None) or \"\n                \"obj.tz_convert('UTC').tz_localize(None) instead\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n\n            result = values.tz_convert(\"UTC\").tz_localize(None)\n            if copy:\n                result = result.copy()\n            return result\n\n        raise NotImplementedError(\"dtype_equal case should be handled elsewhere\")\n\n\ndef astype_td64_unit_conversion(\n    values: np.ndarray, dtype: np.dtype, copy: bool\n) -> np.ndarray:\n    \"\"\"\n    By pandas convention, converting to non-nano timedelta64\n    returns an int64-dtyped array with ints representing multiples\n    of the desired timedelta unit.  This is essentially division.\n\n    Parameters\n    ----------\n    values : np.ndarray[timedelta64[ns]]\n    dtype : np.dtype\n        timedelta64 with unit not-necessarily nano\n    copy : bool\n\n    Returns\n    -------\n    np.ndarray\n    \"\"\"\n    if is_dtype_equal(values.dtype, dtype):\n        if copy:\n            return values.copy()\n        return values\n\n    # otherwise we are converting to non-nano\n    result = values.astype(dtype, copy=False)  # avoid double-copying\n    result = result.astype(np.float64)\n\n    mask = isna(values)\n    np.putmask(result, mask, np.nan)\n    return result\n\n\n@overload\ndef astype_nansafe(\n    arr: np.ndarray, dtype: np.dtype, copy: bool = ..., skipna: bool = ...\n) -> np.ndarray:\n    ...\n\n\n@overload\ndef astype_nansafe(\n    arr: np.ndarray, dtype: ExtensionDtype, copy: bool = ..., skipna: bool = ...\n) -> ExtensionArray:\n    ...\n\n\ndef astype_nansafe(\n    arr: np.ndarray, dtype: DtypeObj, copy: bool = True, skipna: bool = False\n) -> ArrayLike:\n    \"\"\"\n    Cast the elements of an array to a given dtype a nan-safe manner.\n\n    Parameters\n    ----------\n    arr : ndarray\n    dtype : np.dtype or ExtensionDtype\n    copy : bool, default True\n        If False, a view will be attempted but may fail, if\n        e.g. the item sizes don't align.\n    skipna: bool, default False\n        Whether or not we should skip NaN when casting as a string-type.\n\n    Raises\n    ------\n    ValueError\n        The dtype was a datetime64/timedelta64 dtype, but it had no unit.\n    \"\"\"\n    if arr.ndim > 1:\n        flat = arr.ravel()\n        result = astype_nansafe(flat, dtype, copy=copy, skipna=skipna)\n        # error: Item \"ExtensionArray\" of \"Union[ExtensionArray, ndarray]\" has no\n        # attribute \"reshape\"\n        return result.reshape(arr.shape)  # type: ignore[union-attr]\n\n    # We get here with 0-dim from sparse\n    arr = np.atleast_1d(arr)\n\n    # dispatch on extension dtype if needed\n    if isinstance(dtype, ExtensionDtype):\n        return dtype.construct_array_type()._from_sequence(arr, dtype=dtype, copy=copy)\n\n    elif not isinstance(dtype, np.dtype):  # pragma: no cover\n        raise ValueError(\"dtype must be np.dtype or ExtensionDtype\")\n\n    if arr.dtype.kind in [\"m\", \"M\"] and (\n        issubclass(dtype.type, str) or dtype == _dtype_obj\n    ):\n        from pandas.core.construction import ensure_wrapped_if_datetimelike\n\n        arr = ensure_wrapped_if_datetimelike(arr)\n        return arr.astype(dtype, copy=copy)\n\n    if issubclass(dtype.type, str):\n        return lib.ensure_string_array(arr, skipna=skipna, convert_na_value=False)\n\n    elif is_datetime64_dtype(arr.dtype):\n        if dtype == np.int64:\n            if isna(arr).any():\n                raise ValueError(\"Cannot convert NaT values to integer\")\n            return arr.view(dtype)\n\n        # allow frequency conversions\n        if dtype.kind == \"M\":\n            return arr.astype(dtype)\n\n        raise TypeError(f\"cannot astype a datetimelike from [{arr.dtype}] to [{dtype}]\")\n\n    elif is_timedelta64_dtype(arr.dtype):\n        if dtype == np.int64:\n            if isna(arr).any():\n                raise ValueError(\"Cannot convert NaT values to integer\")\n            return arr.view(dtype)\n\n        elif dtype.kind == \"m\":\n            return astype_td64_unit_conversion(arr, dtype, copy=copy)\n\n        raise TypeError(f\"cannot astype a timedelta from [{arr.dtype}] to [{dtype}]\")\n\n    elif np.issubdtype(arr.dtype, np.floating) and np.issubdtype(dtype, np.integer):\n        return astype_float_to_int_nansafe(arr, dtype, copy)\n\n    elif is_object_dtype(arr.dtype):\n\n        # work around NumPy brokenness, #1987\n        if np.issubdtype(dtype.type, np.integer):\n            return lib.astype_intsafe(arr, dtype)\n\n        # if we have a datetime/timedelta array of objects\n        # then coerce to a proper dtype and recall astype_nansafe\n\n        elif is_datetime64_dtype(dtype):\n            from pandas import to_datetime\n\n            return astype_nansafe(\n                to_datetime(arr).values,\n                dtype,\n                copy=copy,\n            )\n        elif is_timedelta64_dtype(dtype):\n            from pandas import to_timedelta\n\n            return astype_nansafe(to_timedelta(arr)._values, dtype, copy=copy)\n\n    if dtype.name in (\"datetime64\", \"timedelta64\"):\n        msg = (\n            f\"The '{dtype.name}' dtype has no unit. Please pass in \"\n            f\"'{dtype.name}[ns]' instead.\"\n        )\n        raise ValueError(msg)\n\n    if copy or is_object_dtype(arr.dtype) or is_object_dtype(dtype):\n        # Explicit copy, or required since NumPy can't view from / to object.\n        return arr.astype(dtype, copy=True)\n\n    return arr.astype(dtype, copy=copy)\n\n\ndef astype_float_to_int_nansafe(\n    values: np.ndarray, dtype: np.dtype, copy: bool\n) -> np.ndarray:\n    \"\"\"\n    astype with a check preventing converting NaN to an meaningless integer value.\n    \"\"\"\n    if not np.isfinite(values).all():\n        raise IntCastingNaNError(\n            \"Cannot convert non-finite values (NA or inf) to integer\"\n        )\n    return values.astype(dtype, copy=copy)\n\n\ndef astype_array(values: ArrayLike, dtype: DtypeObj, copy: bool = False) -> ArrayLike:\n    \"\"\"\n    Cast array (ndarray or ExtensionArray) to the new dtype.\n\n    Parameters\n    ----------\n    values : ndarray or ExtensionArray\n    dtype : dtype object\n    copy : bool, default False\n        copy if indicated\n\n    Returns\n    -------\n    ndarray or ExtensionArray\n    \"\"\"\n    if (\n        values.dtype.kind in [\"m\", \"M\"]\n        and dtype.kind in [\"i\", \"u\"]\n        and isinstance(dtype, np.dtype)\n        and dtype.itemsize != 8\n    ):\n        # TODO(2.0) remove special case once deprecation on DTA/TDA is enforced\n        msg = rf\"cannot astype a datetimelike from [{values.dtype}] to [{dtype}]\"\n        raise TypeError(msg)\n\n    if is_datetime64tz_dtype(dtype) and is_datetime64_dtype(values.dtype):\n        return astype_dt64_to_dt64tz(values, dtype, copy, via_utc=True)\n\n    if is_dtype_equal(values.dtype, dtype):\n        if copy:\n            return values.copy()\n        return values\n\n    if not isinstance(values, np.ndarray):\n        # i.e. ExtensionArray\n        values = values.astype(dtype, copy=copy)\n\n    else:\n        values = astype_nansafe(values, dtype, copy=copy)\n\n    # in pandas we don't store numpy str dtypes, so convert to object\n    if isinstance(dtype, np.dtype) and issubclass(values.dtype.type, str):\n        values = np.array(values, dtype=object)\n\n    return values\n\n\ndef astype_array_safe(\n    values: ArrayLike, dtype, copy: bool = False, errors: str = \"raise\"\n) -> ArrayLike:\n    \"\"\"\n    Cast array (ndarray or ExtensionArray) to the new dtype.\n\n    This basically is the implementation for DataFrame/Series.astype and\n    includes all custom logic for pandas (NaN-safety, converting str to object,\n    not allowing )\n\n    Parameters\n    ----------\n    values : ndarray or ExtensionArray\n    dtype : str, dtype convertible\n    copy : bool, default False\n        copy if indicated\n    errors : str, {'raise', 'ignore'}, default 'raise'\n        - ``raise`` : allow exceptions to be raised\n        - ``ignore`` : suppress exceptions. On error return original object\n\n    Returns\n    -------\n    ndarray or ExtensionArray\n    \"\"\"\n    errors_legal_values = (\"raise\", \"ignore\")\n\n    if errors not in errors_legal_values:\n        invalid_arg = (\n            \"Expected value of kwarg 'errors' to be one of \"\n            f\"{list(errors_legal_values)}. Supplied value is '{errors}'\"\n        )\n        raise ValueError(invalid_arg)\n\n    if inspect.isclass(dtype) and issubclass(dtype, ExtensionDtype):\n        msg = (\n            f\"Expected an instance of {dtype.__name__}, \"\n            \"but got the class instead. Try instantiating 'dtype'.\"\n        )\n        raise TypeError(msg)\n\n    dtype = pandas_dtype(dtype)\n    if isinstance(dtype, PandasDtype):\n        # Ensure we don't end up with a PandasArray\n        dtype = dtype.numpy_dtype\n\n    try:\n        new_values = astype_array(values, dtype, copy=copy)\n    except (ValueError, TypeError):\n        # e.g. astype_nansafe can fail on object-dtype of strings\n        #  trying to convert to float\n        if errors == \"ignore\":\n            new_values = values\n        else:\n            raise\n\n    return new_values\n\n\ndef soft_convert_objects(\n    values: np.ndarray,\n    datetime: bool = True,\n    numeric: bool = True,\n    timedelta: bool = True,\n    period: bool = True,\n    copy: bool = True,\n) -> ArrayLike:\n    \"\"\"\n    Try to coerce datetime, timedelta, and numeric object-dtype columns\n    to inferred dtype.\n\n    Parameters\n    ----------\n    values : np.ndarray[object]\n    datetime : bool, default True\n    numeric: bool, default True\n    timedelta : bool, default True\n    period : bool, default True\n    copy : bool, default True\n\n    Returns\n    -------\n    np.ndarray or ExtensionArray\n    \"\"\"\n    validate_bool_kwarg(datetime, \"datetime\")\n    validate_bool_kwarg(numeric, \"numeric\")\n    validate_bool_kwarg(timedelta, \"timedelta\")\n    validate_bool_kwarg(copy, \"copy\")\n\n    conversion_count = sum((datetime, numeric, timedelta))\n    if conversion_count == 0:\n        raise ValueError(\"At least one of datetime, numeric or timedelta must be True.\")\n\n    # Soft conversions\n    if datetime or timedelta:\n        # GH 20380, when datetime is beyond year 2262, hence outside\n        # bound of nanosecond-resolution 64-bit integers.\n        try:\n            converted = lib.maybe_convert_objects(\n                values,\n                convert_datetime=datetime,\n                convert_timedelta=timedelta,\n                convert_period=period,\n            )\n        except (OutOfBoundsDatetime, ValueError):\n            return values\n        if converted is not values:\n            return converted\n\n    if numeric and is_object_dtype(values.dtype):\n        converted, _ = lib.maybe_convert_numeric(values, set(), coerce_numeric=True)\n\n        # If all NaNs, then do not-alter\n        values = converted if not isna(converted).all() else values\n        values = values.copy() if copy else values\n\n    return values\n\n\ndef convert_dtypes(\n    input_array: ArrayLike,\n    convert_string: bool = True,\n    convert_integer: bool = True,\n    convert_boolean: bool = True,\n    convert_floating: bool = True,\n) -> DtypeObj:\n    \"\"\"\n    Convert objects to best possible type, and optionally,\n    to types supporting ``pd.NA``.\n\n    Parameters\n    ----------\n    input_array : ExtensionArray or np.ndarray\n    convert_string : bool, default True\n        Whether object dtypes should be converted to ``StringDtype()``.\n    convert_integer : bool, default True\n        Whether, if possible, conversion can be done to integer extension types.\n    convert_boolean : bool, defaults True\n        Whether object dtypes should be converted to ``BooleanDtypes()``.\n    convert_floating : bool, defaults True\n        Whether, if possible, conversion can be done to floating extension types.\n        If `convert_integer` is also True, preference will be give to integer\n        dtypes if the floats can be faithfully casted to integers.\n\n    Returns\n    -------\n    np.dtype, or ExtensionDtype\n    \"\"\"\n    inferred_dtype: str | DtypeObj\n\n    if (\n        convert_string or convert_integer or convert_boolean or convert_floating\n    ) and isinstance(input_array, np.ndarray):\n\n        if is_object_dtype(input_array.dtype):\n            inferred_dtype = lib.infer_dtype(input_array)\n        else:\n            inferred_dtype = input_array.dtype\n\n        if is_string_dtype(inferred_dtype):\n            if not convert_string or inferred_dtype == \"bytes\":\n                return input_array.dtype\n            else:\n                return pandas_dtype(\"string\")\n\n        if convert_integer:\n            target_int_dtype = pandas_dtype(\"Int64\")\n\n            if is_integer_dtype(input_array.dtype):\n                from pandas.core.arrays.integer import INT_STR_TO_DTYPE\n\n                inferred_dtype = INT_STR_TO_DTYPE.get(\n                    input_array.dtype.name, target_int_dtype\n                )\n            elif is_numeric_dtype(input_array.dtype):\n                # TODO: de-dup with maybe_cast_to_integer_array?\n                arr = input_array[notna(input_array)]\n                if (arr.astype(int) == arr).all():\n                    inferred_dtype = target_int_dtype\n                else:\n                    inferred_dtype = input_array.dtype\n\n        if convert_floating:\n            if not is_integer_dtype(input_array.dtype) and is_numeric_dtype(\n                input_array.dtype\n            ):\n                from pandas.core.arrays.floating import FLOAT_STR_TO_DTYPE\n\n                inferred_float_dtype: DtypeObj = FLOAT_STR_TO_DTYPE.get(\n                    input_array.dtype.name, pandas_dtype(\"Float64\")\n                )\n                # if we could also convert to integer, check if all floats\n                # are actually integers\n                if convert_integer:\n                    # TODO: de-dup with maybe_cast_to_integer_array?\n                    arr = input_array[notna(input_array)]\n                    if (arr.astype(int) == arr).all():\n                        inferred_dtype = pandas_dtype(\"Int64\")\n                    else:\n                        inferred_dtype = inferred_float_dtype\n                else:\n                    inferred_dtype = inferred_float_dtype\n\n        if convert_boolean:\n            if is_bool_dtype(input_array.dtype):\n                inferred_dtype = pandas_dtype(\"boolean\")\n            elif isinstance(inferred_dtype, str) and inferred_dtype == \"boolean\":\n                inferred_dtype = pandas_dtype(\"boolean\")\n\n        if isinstance(inferred_dtype, str):\n            # If we couldn't do anything else, then we retain the dtype\n            inferred_dtype = input_array.dtype\n\n    else:\n        return input_array.dtype\n\n    # error: Incompatible return value type (got \"Union[str, Union[dtype[Any],\n    # ExtensionDtype]]\", expected \"Union[dtype[Any], ExtensionDtype]\")\n    return inferred_dtype  # type: ignore[return-value]\n\n\ndef maybe_infer_to_datetimelike(\n    value: np.ndarray,\n) -> np.ndarray | DatetimeArray | TimedeltaArray | PeriodArray | IntervalArray:\n    \"\"\"\n    we might have a array (or single object) that is datetime like,\n    and no dtype is passed don't change the value unless we find a\n    datetime/timedelta set\n\n    this is pretty strict in that a datetime/timedelta is REQUIRED\n    in addition to possible nulls/string likes\n\n    Parameters\n    ----------\n    value : np.ndarray[object]\n\n    Returns\n    -------\n    np.ndarray, DatetimeArray, TimedeltaArray, PeriodArray, or IntervalArray\n\n    \"\"\"\n    if not isinstance(value, np.ndarray) or value.dtype != object:\n        # Caller is responsible for passing only ndarray[object]\n        raise TypeError(type(value))  # pragma: no cover\n\n    v = np.array(value, copy=False)\n\n    shape = v.shape\n    if v.ndim != 1:\n        v = v.ravel()\n\n    if not len(v):\n        return value\n\n    def try_datetime(v: np.ndarray) -> ArrayLike:\n        # Coerce to datetime64, datetime64tz, or in corner cases\n        #  object[datetimes]\n        from pandas.core.arrays.datetimes import sequence_to_datetimes\n\n        try:\n            # GH#19671 we pass require_iso8601 to be relatively strict\n            #  when parsing strings.\n            dta = sequence_to_datetimes(v, require_iso8601=True)\n        except (ValueError, TypeError):\n            # e.g. <class 'numpy.timedelta64'> is not convertible to datetime\n            return v.reshape(shape)\n        else:\n            # GH#19761 we may have mixed timezones, in which cast 'dta' is\n            #  an ndarray[object].  Only 1 test\n            #  relies on this behavior, see GH#40111\n            return dta.reshape(shape)\n\n    def try_timedelta(v: np.ndarray) -> np.ndarray:\n        # safe coerce to timedelta64\n\n        # will try first with a string & object conversion\n        try:\n            # bc we know v.dtype == object, this is equivalent to\n            #  `np.asarray(to_timedelta(v))`, but using a lower-level API that\n            #  does not require a circular import.\n            td_values = array_to_timedelta64(v).view(\"m8[ns]\")\n        except (ValueError, OverflowError):\n            return v.reshape(shape)\n        else:\n            return td_values.reshape(shape)\n\n    inferred_type, seen_str = lib.infer_datetimelike_array(ensure_object(v))\n    if inferred_type in [\"period\", \"interval\"]:\n        # Incompatible return value type (got \"Union[ExtensionArray, ndarray]\",\n        # expected \"Union[ndarray, DatetimeArray, TimedeltaArray, PeriodArray,\n        # IntervalArray]\")\n        return lib.maybe_convert_objects(  # type: ignore[return-value]\n            v, convert_period=True, convert_interval=True\n        )\n\n    if inferred_type == \"datetime\":\n        # error: Incompatible types in assignment (expression has type \"ExtensionArray\",\n        # variable has type \"Union[ndarray, List[Any]]\")\n        value = try_datetime(v)  # type: ignore[assignment]\n    elif inferred_type == \"timedelta\":\n        value = try_timedelta(v)\n    elif inferred_type == \"nat\":\n\n        # if all NaT, return as datetime\n        if isna(v).all():\n            # error: Incompatible types in assignment (expression has type\n            # \"ExtensionArray\", variable has type \"Union[ndarray, List[Any]]\")\n            value = try_datetime(v)  # type: ignore[assignment]\n        else:\n\n            # We have at least a NaT and a string\n            # try timedelta first to avoid spurious datetime conversions\n            # e.g. '00:00:01' is a timedelta but technically is also a datetime\n            value = try_timedelta(v)\n            if lib.infer_dtype(value, skipna=False) in [\"mixed\"]:\n                # cannot skip missing values, as NaT implies that the string\n                # is actually a datetime\n\n                # error: Incompatible types in assignment (expression has type\n                # \"ExtensionArray\", variable has type \"Union[ndarray, List[Any]]\")\n                value = try_datetime(v)  # type: ignore[assignment]\n\n    if value.dtype.kind in [\"m\", \"M\"] and seen_str:\n        # TODO(2.0): enforcing this deprecation should close GH#40111\n        warnings.warn(\n            f\"Inferring {value.dtype} from data containing strings is deprecated \"\n            \"and will be removed in a future version. To retain the old behavior \"\n            f\"explicitly pass Series(data, dtype={value.dtype})\",\n            FutureWarning,\n            stacklevel=find_stack_level(),\n        )\n    return value\n\n\ndef maybe_cast_to_datetime(\n    value: ExtensionArray | np.ndarray | list, dtype: DtypeObj | None\n) -> ExtensionArray | np.ndarray:\n    \"\"\"\n    try to cast the array/value to a datetimelike dtype, converting float\n    nan to iNaT\n\n    We allow a list *only* when dtype is not None.\n    \"\"\"\n    from pandas.core.arrays.datetimes import sequence_to_datetimes\n    from pandas.core.arrays.timedeltas import TimedeltaArray\n\n    if not is_list_like(value):\n        raise TypeError(\"value must be listlike\")\n\n    if is_timedelta64_dtype(dtype):\n        # TODO: _from_sequence would raise ValueError in cases where\n        #  ensure_nanosecond_dtype raises TypeError\n        dtype = cast(np.dtype, dtype)\n        dtype = ensure_nanosecond_dtype(dtype)\n        res = TimedeltaArray._from_sequence(value, dtype=dtype)\n        return res\n\n    if dtype is not None:\n        is_datetime64 = is_datetime64_dtype(dtype)\n        is_datetime64tz = is_datetime64tz_dtype(dtype)\n\n        vdtype = getattr(value, \"dtype\", None)\n\n        if is_datetime64 or is_datetime64tz:\n            dtype = ensure_nanosecond_dtype(dtype)\n\n            value = np.array(value, copy=False)\n\n            # we have an array of datetime or timedeltas & nulls\n            if value.size or not is_dtype_equal(value.dtype, dtype):\n                _disallow_mismatched_datetimelike(value, dtype)\n\n                try:\n                    if is_datetime64:\n                        dta = sequence_to_datetimes(value)\n                        # GH 25843: Remove tz information since the dtype\n                        # didn't specify one\n\n                        if dta.tz is not None:\n                            warnings.warn(\n                                \"Data is timezone-aware. Converting \"\n                                \"timezone-aware data to timezone-naive by \"\n                                \"passing dtype='datetime64[ns]' to \"\n                                \"DataFrame or Series is deprecated and will \"\n                                \"raise in a future version. Use \"\n                                \"`pd.Series(values).dt.tz_localize(None)` \"\n                                \"instead.\",\n                                FutureWarning,\n                                stacklevel=find_stack_level(),\n                            )\n                            # equiv: dta.view(dtype)\n                            # Note: NOT equivalent to dta.astype(dtype)\n                            dta = dta.tz_localize(None)\n\n                        value = dta\n                    elif is_datetime64tz:\n                        dtype = cast(DatetimeTZDtype, dtype)\n                        # The string check can be removed once issue #13712\n                        # is solved. String data that is passed with a\n                        # datetime64tz is assumed to be naive which should\n                        # be localized to the timezone.\n                        is_dt_string = is_string_dtype(value.dtype)\n                        dta = sequence_to_datetimes(value)\n                        if dta.tz is not None:\n                            value = dta.astype(dtype, copy=False)\n                        elif is_dt_string:\n                            # Strings here are naive, so directly localize\n                            # equiv: dta.astype(dtype)  # though deprecated\n\n                            value = dta.tz_localize(dtype.tz)\n                        else:\n                            # Numeric values are UTC at this point,\n                            # so localize and convert\n                            # equiv: Series(dta).astype(dtype) # though deprecated\n                            if getattr(vdtype, \"kind\", None) == \"M\":\n                                # GH#24559, GH#33401 deprecate behavior inconsistent\n                                #  with DatetimeArray/DatetimeIndex\n                                warnings.warn(\n                                    \"In a future version, constructing a Series \"\n                                    \"from datetime64[ns] data and a \"\n                                    \"DatetimeTZDtype will interpret the data \"\n                                    \"as wall-times instead of \"\n                                    \"UTC times, matching the behavior of \"\n                                    \"DatetimeIndex. To treat the data as UTC \"\n                                    \"times, use pd.Series(data).dt\"\n                                    \".tz_localize('UTC').tz_convert(dtype.tz) \"\n                                    \"or pd.Series(data.view('int64'), dtype=dtype)\",\n                                    FutureWarning,\n                                    stacklevel=find_stack_level(),\n                                )\n\n                            value = dta.tz_localize(\"UTC\").tz_convert(dtype.tz)\n                except OutOfBoundsDatetime:\n                    raise\n                except ValueError:\n                    # TODO(GH#40048): only catch dateutil's ParserError\n                    #  once we can reliably import it in all supported versions\n                    pass\n\n        elif getattr(vdtype, \"kind\", None) in [\"m\", \"M\"]:\n            # we are already datetimelike and want to coerce to non-datetimelike;\n            #  astype_nansafe will raise for anything other than object, then upcast.\n            #  see test_datetimelike_values_with_object_dtype\n            # error: Argument 2 to \"astype_nansafe\" has incompatible type\n            # \"Union[dtype[Any], ExtensionDtype]\"; expected \"dtype[Any]\"\n            return astype_nansafe(value, dtype)  # type: ignore[arg-type]\n\n    elif isinstance(value, np.ndarray):\n        if value.dtype.kind in [\"M\", \"m\"]:\n            # catch a datetime/timedelta that is not of ns variety\n            # and no coercion specified\n            value = sanitize_to_nanoseconds(value)\n\n        elif value.dtype == _dtype_obj:\n            value = maybe_infer_to_datetimelike(value)\n\n    elif isinstance(value, list):\n        # we only get here with dtype=None, which we do not allow\n        raise ValueError(\n            \"maybe_cast_to_datetime allows a list *only* if dtype is not None\"\n        )\n\n    # at this point we have converted or raised in all cases where we had a list\n    return cast(ArrayLike, value)\n\n\ndef sanitize_to_nanoseconds(values: np.ndarray, copy: bool = False) -> np.ndarray:\n    \"\"\"\n    Safely convert non-nanosecond datetime64 or timedelta64 values to nanosecond.\n    \"\"\"\n    dtype = values.dtype\n    if dtype.kind == \"M\" and dtype != DT64NS_DTYPE:\n        values = conversion.ensure_datetime64ns(values)\n\n    elif dtype.kind == \"m\" and dtype != TD64NS_DTYPE:\n        values = conversion.ensure_timedelta64ns(values)\n\n    elif copy:\n        values = values.copy()\n\n    return values\n\n\ndef ensure_nanosecond_dtype(dtype: DtypeObj) -> DtypeObj:\n    \"\"\"\n    Convert dtypes with granularity less than nanosecond to nanosecond\n\n    >>> ensure_nanosecond_dtype(np.dtype(\"M8[s]\"))\n    dtype('<M8[ns]')\n\n    >>> ensure_nanosecond_dtype(np.dtype(\"m8[ps]\"))\n    Traceback (most recent call last):\n        ...\n    TypeError: cannot convert timedeltalike to dtype [timedelta64[ps]]\n    \"\"\"\n    msg = (\n        f\"The '{dtype.name}' dtype has no unit. \"\n        f\"Please pass in '{dtype.name}[ns]' instead.\"\n    )\n\n    # unpack e.g. SparseDtype\n    dtype = getattr(dtype, \"subtype\", dtype)\n\n    if not isinstance(dtype, np.dtype):\n        # i.e. datetime64tz\n        pass\n\n    elif dtype.kind == \"M\" and dtype != DT64NS_DTYPE:\n        # pandas supports dtype whose granularity is less than [ns]\n        # e.g., [ps], [fs], [as]\n        if dtype <= np.dtype(\"M8[ns]\"):\n            if dtype.name == \"datetime64\":\n                raise ValueError(msg)\n            dtype = DT64NS_DTYPE\n        else:\n            raise TypeError(f\"cannot convert datetimelike to dtype [{dtype}]\")\n\n    elif dtype.kind == \"m\" and dtype != TD64NS_DTYPE:\n        # pandas supports dtype whose granularity is less than [ns]\n        # e.g., [ps], [fs], [as]\n        if dtype <= np.dtype(\"m8[ns]\"):\n            if dtype.name == \"timedelta64\":\n                raise ValueError(msg)\n            dtype = TD64NS_DTYPE\n        else:\n            raise TypeError(f\"cannot convert timedeltalike to dtype [{dtype}]\")\n    return dtype\n\n\n@overload\ndef find_common_type(types: list[np.dtype]) -> np.dtype:\n    ...\n\n\n@overload\ndef find_common_type(types: list[ExtensionDtype]) -> DtypeObj:\n    ...\n\n\n@overload\ndef find_common_type(types: list[DtypeObj]) -> DtypeObj:\n    ...\n\n\ndef find_common_type(types):\n    \"\"\"\n    Find a common data type among the given dtypes.\n\n    Parameters\n    ----------\n    types : list of dtypes\n\n    Returns\n    -------\n    pandas extension or numpy dtype\n\n    See Also\n    --------\n    numpy.find_common_type\n\n    \"\"\"\n    if not types:\n        raise ValueError(\"no types given\")\n\n    first = types[0]\n\n    # workaround for find_common_type([np.dtype('datetime64[ns]')] * 2)\n    # => object\n    if lib.dtypes_all_equal(list(types)):\n        return first\n\n    # get unique types (dict.fromkeys is used as order-preserving set())\n    types = list(dict.fromkeys(types).keys())\n\n    if any(isinstance(t, ExtensionDtype) for t in types):\n        for t in types:\n            if isinstance(t, ExtensionDtype):\n                res = t._get_common_dtype(types)\n                if res is not None:\n                    return res\n        return np.dtype(\"object\")\n\n    # take lowest unit\n    if all(is_datetime64_dtype(t) for t in types):\n        return np.dtype(\"datetime64[ns]\")\n    if all(is_timedelta64_dtype(t) for t in types):\n        return np.dtype(\"timedelta64[ns]\")\n\n    # don't mix bool / int or float or complex\n    # this is different from numpy, which casts bool with float/int as int\n    has_bools = any(is_bool_dtype(t) for t in types)\n    if has_bools:\n        for t in types:\n            if is_integer_dtype(t) or is_float_dtype(t) or is_complex_dtype(t):\n                return np.dtype(\"object\")\n\n    return np.find_common_type(types, [])\n\n\ndef construct_2d_arraylike_from_scalar(\n    value: Scalar, length: int, width: int, dtype: np.dtype, copy: bool\n) -> np.ndarray:\n\n    shape = (length, width)\n\n    if dtype.kind in [\"m\", \"M\"]:\n        value = maybe_unbox_datetimelike_tz_deprecation(value, dtype)\n    elif dtype == _dtype_obj:\n        if isinstance(value, (np.timedelta64, np.datetime64)):\n            # calling np.array below would cast to pytimedelta/pydatetime\n            out = np.empty(shape, dtype=object)\n            out.fill(value)\n            return out\n\n    # Attempt to coerce to a numpy array\n    try:\n        arr = np.array(value, dtype=dtype, copy=copy)\n    except (ValueError, TypeError) as err:\n        raise TypeError(\n            f\"DataFrame constructor called with incompatible data and dtype: {err}\"\n        ) from err\n\n    if arr.ndim != 0:\n        raise ValueError(\"DataFrame constructor not properly called!\")\n\n    return np.full(shape, arr)\n\n\ndef construct_1d_arraylike_from_scalar(\n    value: Scalar, length: int, dtype: DtypeObj | None\n) -> ArrayLike:\n    \"\"\"\n    create a np.ndarray / pandas type of specified shape and dtype\n    filled with values\n\n    Parameters\n    ----------\n    value : scalar value\n    length : int\n    dtype : pandas_dtype or np.dtype\n\n    Returns\n    -------\n    np.ndarray / pandas type of length, filled with value\n\n    \"\"\"\n\n    if dtype is None:\n        try:\n            dtype, value = infer_dtype_from_scalar(value, pandas_dtype=True)\n        except OutOfBoundsDatetime:\n            dtype = _dtype_obj\n\n    if isinstance(dtype, ExtensionDtype):\n        cls = dtype.construct_array_type()\n        subarr = cls._from_sequence([value] * length, dtype=dtype)\n\n    else:\n\n        if length and is_integer_dtype(dtype) and isna(value):\n            # coerce if we have nan for an integer dtype\n            dtype = np.dtype(\"float64\")\n        elif isinstance(dtype, np.dtype) and dtype.kind in (\"U\", \"S\"):\n            # we need to coerce to object dtype to avoid\n            # to allow numpy to take our string as a scalar value\n            dtype = np.dtype(\"object\")\n            if not isna(value):\n                value = ensure_str(value)\n        elif dtype.kind in [\"M\", \"m\"]:\n            value = maybe_unbox_datetimelike_tz_deprecation(value, dtype)\n\n        subarr = np.empty(length, dtype=dtype)\n        subarr.fill(value)\n\n    return subarr\n\n\ndef maybe_unbox_datetimelike_tz_deprecation(value: Scalar, dtype: DtypeObj):\n    \"\"\"\n    Wrap maybe_unbox_datetimelike with a check for a timezone-aware Timestamp\n    along with a timezone-naive datetime64 dtype, which is deprecated.\n    \"\"\"\n    # Caller is responsible for checking dtype.kind in [\"m\", \"M\"]\n\n    if isinstance(value, datetime):\n        # we dont want to box dt64, in particular datetime64(\"NaT\")\n        value = maybe_box_datetimelike(value, dtype)\n\n    try:\n        value = maybe_unbox_datetimelike(value, dtype)\n    except TypeError:\n        if (\n            isinstance(value, Timestamp)\n            and value.tzinfo is not None\n            and isinstance(dtype, np.dtype)\n            and dtype.kind == \"M\"\n        ):\n            warnings.warn(\n                \"Data is timezone-aware. Converting \"\n                \"timezone-aware data to timezone-naive by \"\n                \"passing dtype='datetime64[ns]' to \"\n                \"DataFrame or Series is deprecated and will \"\n                \"raise in a future version. Use \"\n                \"`pd.Series(values).dt.tz_localize(None)` \"\n                \"instead.\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n            new_value = value.tz_localize(None)\n            return maybe_unbox_datetimelike(new_value, dtype)\n        else:\n            raise\n    return value\n\n\ndef construct_1d_object_array_from_listlike(values: Sized) -> np.ndarray:\n    \"\"\"\n    Transform any list-like object in a 1-dimensional numpy array of object\n    dtype.\n\n    Parameters\n    ----------\n    values : any iterable which has a len()\n\n    Raises\n    ------\n    TypeError\n        * If `values` does not have a len()\n\n    Returns\n    -------\n    1-dimensional numpy array of dtype object\n    \"\"\"\n    # numpy will try to interpret nested lists as further dimensions, hence\n    # making a 1D array that contains list-likes is a bit tricky:\n    result = np.empty(len(values), dtype=\"object\")\n    result[:] = values\n    return result\n\n\ndef maybe_cast_to_integer_array(\n    arr: list | np.ndarray, dtype: np.dtype, copy: bool = False\n) -> np.ndarray:\n    \"\"\"\n    Takes any dtype and returns the casted version, raising for when data is\n    incompatible with integer/unsigned integer dtypes.\n\n    Parameters\n    ----------\n    arr : np.ndarray or list\n        The array to cast.\n    dtype : np.dtype\n        The integer dtype to cast the array to.\n    copy: bool, default False\n        Whether to make a copy of the array before returning.\n\n    Returns\n    -------\n    ndarray\n        Array of integer or unsigned integer dtype.\n\n    Raises\n    ------\n    OverflowError : the dtype is incompatible with the data\n    ValueError : loss of precision has occurred during casting\n\n    Examples\n    --------\n    If you try to coerce negative values to unsigned integers, it raises:\n\n    >>> pd.Series([-1], dtype=\"uint64\")\n    Traceback (most recent call last):\n        ...\n    OverflowError: Trying to coerce negative values to unsigned integers\n\n    Also, if you try to coerce float values to integers, it raises:\n\n    >>> pd.Series([1, 2, 3.5], dtype=\"int64\")\n    Traceback (most recent call last):\n        ...\n    ValueError: Trying to coerce float values to integers\n    \"\"\"\n    assert is_integer_dtype(dtype)\n\n    try:\n        if not isinstance(arr, np.ndarray):\n            casted = np.array(arr, dtype=dtype, copy=copy)\n        else:\n            casted = arr.astype(dtype, copy=copy)\n    except OverflowError as err:\n        raise OverflowError(\n            \"The elements provided in the data cannot all be \"\n            f\"casted to the dtype {dtype}\"\n        ) from err\n\n    if np.array_equal(arr, casted):\n        return casted\n\n    # We do this casting to allow for proper\n    # data and dtype checking.\n    #\n    # We didn't do this earlier because NumPy\n    # doesn't handle `uint64` correctly.\n    arr = np.asarray(arr)\n\n    if is_unsigned_integer_dtype(dtype) and (arr < 0).any():\n        raise OverflowError(\"Trying to coerce negative values to unsigned integers\")\n\n    if is_float_dtype(arr.dtype):\n        if not np.isfinite(arr).all():\n            raise IntCastingNaNError(\n                \"Cannot convert non-finite values (NA or inf) to integer\"\n            )\n        raise ValueError(\"Trying to coerce float values to integers\")\n    if is_object_dtype(arr.dtype):\n        raise ValueError(\"Trying to coerce float values to integers\")\n\n    if casted.dtype < arr.dtype:\n        # GH#41734 e.g. [1, 200, 923442] and dtype=\"int8\" -> overflows\n        warnings.warn(\n            f\"Values are too large to be losslessly cast to {dtype}. \"\n            \"In a future version this will raise OverflowError. To retain the \"\n            f\"old behavior, use pd.Series(values).astype({dtype})\",\n            FutureWarning,\n            stacklevel=find_stack_level(),\n        )\n        return casted\n\n    if arr.dtype.kind in [\"m\", \"M\"]:\n        # test_constructor_maskedarray_nonfloat\n        warnings.warn(\n            f\"Constructing Series or DataFrame from {arr.dtype} values and \"\n            f\"dtype={dtype} is deprecated and will raise in a future version. \"\n            \"Use values.view(dtype) instead.\",\n            FutureWarning,\n            stacklevel=find_stack_level(),\n        )\n        return casted\n\n    # No known cases that get here, but raising explicitly to cover our bases.\n    raise ValueError(f\"values cannot be losslessly cast to {dtype}\")\n\n\ndef convert_scalar_for_putitemlike(scalar: Scalar, dtype: np.dtype) -> Scalar:\n    \"\"\"\n    Convert datetimelike scalar if we are setting into a datetime64\n    or timedelta64 ndarray.\n\n    Parameters\n    ----------\n    scalar : scalar\n    dtype : np.dtype\n\n    Returns\n    -------\n    scalar\n    \"\"\"\n    if dtype.kind in [\"m\", \"M\"]:\n        scalar = maybe_box_datetimelike(scalar, dtype)\n        return maybe_unbox_datetimelike(scalar, dtype)\n    else:\n        _validate_numeric_casting(dtype, scalar)\n    return scalar\n\n\ndef _validate_numeric_casting(dtype: np.dtype, value: Scalar) -> None:\n    \"\"\"\n    Check that we can losslessly insert the given value into an array\n    with the given dtype.\n\n    Parameters\n    ----------\n    dtype : np.dtype\n    value : scalar\n\n    Raises\n    ------\n    ValueError\n    \"\"\"\n    # error: Argument 1 to \"__call__\" of \"ufunc\" has incompatible type\n    # \"Union[Union[str, int, float, bool], Union[Any, Timestamp, Timedelta, Any]]\";\n    # expected \"Union[Union[int, float, complex, str, bytes, generic],\n    # Sequence[Union[int, float, complex, str, bytes, generic]],\n    # Sequence[Sequence[Any]], _SupportsArray]\"\n    if (\n        issubclass(dtype.type, (np.integer, np.bool_))\n        and is_float(value)\n        and np.isnan(value)  # type: ignore[arg-type]\n    ):\n        raise ValueError(\"Cannot assign nan to integer series\")\n\n    elif dtype.kind in [\"i\", \"u\", \"f\", \"c\"]:\n        if is_bool(value) or isinstance(value, np.timedelta64):\n            # numpy will cast td64 to integer if we're not careful\n            raise ValueError(\n                f\"Cannot assign {type(value).__name__} to float/integer series\"\n            )\n    elif dtype.kind == \"b\":\n        if is_scalar(value) and not is_bool(value):\n            raise ValueError(f\"Cannot assign {type(value).__name__} to bool series\")\n\n\ndef can_hold_element(arr: ArrayLike, element: Any) -> bool:\n    \"\"\"\n    Can we do an inplace setitem with this element in an array with this dtype?\n\n    Parameters\n    ----------\n    arr : np.ndarray or ExtensionArray\n    element : Any\n\n    Returns\n    -------\n    bool\n    \"\"\"\n    dtype = arr.dtype\n    if not isinstance(dtype, np.dtype) or dtype.kind in [\"m\", \"M\"]:\n        if isinstance(dtype, (PeriodDtype, IntervalDtype, DatetimeTZDtype, np.dtype)):\n            # np.dtype here catches datetime64ns and timedelta64ns; we assume\n            #  in this case that we have DatetimeArray/TimedeltaArray\n            arr = cast(\n                \"PeriodArray | DatetimeArray | TimedeltaArray | IntervalArray\", arr\n            )\n            try:\n                arr._validate_setitem_value(element)\n                return True\n            except (ValueError, TypeError):\n                return False\n\n        # This is technically incorrect, but maintains the behavior of\n        # ExtensionBlock._can_hold_element\n        return True\n\n    try:\n        np_can_hold_element(dtype, element)\n        return True\n    except (TypeError, ValueError):\n        return False\n\n\ndef np_can_hold_element(dtype: np.dtype, element: Any) -> Any:\n    \"\"\"\n    Raise if we cannot losslessly set this element into an ndarray with this dtype.\n\n    Specifically about places where we disagree with numpy.  i.e. there are\n    cases where numpy will raise in doing the setitem that we do not check\n    for here, e.g. setting str \"X\" into a numeric ndarray.\n\n    Returns\n    -------\n    Any\n        The element, potentially cast to the dtype.\n\n    Raises\n    ------\n    ValueError : If we cannot losslessly store this element with this dtype.\n    \"\"\"\n    if dtype == _dtype_obj:\n        return element\n\n    tipo = maybe_infer_dtype_type(element)\n\n    if dtype.kind in [\"i\", \"u\"]:\n        if isinstance(element, range):\n            if _dtype_can_hold_range(element, dtype):\n                return element\n            raise ValueError\n\n        if tipo is not None:\n            if tipo.kind not in [\"i\", \"u\"]:\n                if is_float(element) and element.is_integer():\n                    return element\n\n                if isinstance(element, np.ndarray) and element.dtype.kind == \"f\":\n                    # If all can be losslessly cast to integers, then we can hold them\n                    #  We do something similar in putmask_smart\n                    casted = element.astype(dtype)\n                    comp = casted == element\n                    if comp.all():\n                        return element\n                    raise ValueError\n\n                # Anything other than integer we cannot hold\n                raise ValueError\n            elif dtype.itemsize < tipo.itemsize:\n                if is_integer(element):\n                    # e.g. test_setitem_series_int8 if we have a python int 1\n                    #  tipo may be np.int32, despite the fact that it will fit\n                    #  in smaller int dtypes.\n                    info = np.iinfo(dtype)\n                    if info.min <= element <= info.max:\n                        return element\n                    raise ValueError\n                raise ValueError\n            elif not isinstance(tipo, np.dtype):\n                # i.e. nullable IntegerDtype; we can put this into an ndarray\n                #  losslessly iff it has no NAs\n                hasnas = element._mask.any()\n                # TODO: don't rely on implementation detail\n                if hasnas:\n                    raise ValueError\n                return element\n\n            return element\n\n        # We have not inferred an integer from the dtype\n        # check if we have a builtin int or a float equal to an int\n        if is_integer(element) or (is_float(element) and element.is_integer()):\n            return element\n        raise ValueError\n\n    elif dtype.kind == \"f\":\n        if tipo is not None:\n            # TODO: itemsize check?\n            if tipo.kind not in [\"f\", \"i\", \"u\"]:\n                # Anything other than float/integer we cannot hold\n                raise ValueError\n            elif not isinstance(tipo, np.dtype):\n                # i.e. nullable IntegerDtype or FloatingDtype;\n                #  we can put this into an ndarray losslessly iff it has no NAs\n                hasnas = element._mask.any()\n                # TODO: don't rely on implementation detail\n                if hasnas:\n                    raise ValueError\n                return element\n            return element\n\n        if lib.is_integer(element) or lib.is_float(element):\n            return element\n        raise ValueError\n\n    elif dtype.kind == \"c\":\n        if tipo is not None:\n            if tipo.kind in [\"c\", \"f\", \"i\", \"u\"]:\n                return element\n            raise ValueError\n        if lib.is_integer(element) or lib.is_complex(element) or lib.is_float(element):\n            return element\n        raise ValueError\n\n    elif dtype.kind == \"b\":\n        if tipo is not None:\n            if tipo.kind == \"b\":  # FIXME: wrong with BooleanArray?\n                return element\n            raise ValueError\n        if lib.is_bool(element):\n            return element\n        raise ValueError\n\n    elif dtype.kind == \"S\":\n        # TODO: test tests.frame.methods.test_replace tests get here,\n        #  need more targeted tests.  xref phofl has a PR about this\n        if tipo is not None:\n            if tipo.kind == \"S\" and tipo.itemsize <= dtype.itemsize:\n                return element\n            raise ValueError\n        if isinstance(element, bytes) and len(element) <= dtype.itemsize:\n            return element\n        raise ValueError\n\n    raise NotImplementedError(dtype)\n\n\ndef _dtype_can_hold_range(rng: range, dtype: np.dtype) -> bool:\n    \"\"\"\n    maybe_infer_dtype_type infers to int64 (and float64 for very large endpoints),\n    but in many cases a range can be held by a smaller integer dtype.\n    Check if this is one of those cases.\n    \"\"\"\n    if not len(rng):\n        return True\n    return np.can_cast(rng[0], dtype) and np.can_cast(rng[-1], dtype)\n", 2316], "/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/numpy/core/multiarray.py": ["\"\"\"\nCreate the numpy.core.multiarray namespace for backward compatibility. In v1.16\nthe multiarray and umath c-extension modules were merged into a single\n_multiarray_umath extension module. So we replicate the old namespace\nby importing from the extension module.\n\n\"\"\"\n\nimport functools\nimport warnings\n\nfrom . import overrides\nfrom . import _multiarray_umath\nfrom ._multiarray_umath import *  # noqa: F403\n# These imports are needed for backward compatibility,\n# do not change them. issue gh-15518\n# _get_ndarray_c_version is semi-public, on purpose not added to __all__\nfrom ._multiarray_umath import (\n    _fastCopyAndTranspose, _flagdict, _insert, _reconstruct, _vec_string,\n    _ARRAY_API, _monotonicity, _get_ndarray_c_version, _set_madvise_hugepage,\n    )\n\n__all__ = [\n    '_ARRAY_API', 'ALLOW_THREADS', 'BUFSIZE', 'CLIP', 'DATETIMEUNITS',\n    'ITEM_HASOBJECT', 'ITEM_IS_POINTER', 'LIST_PICKLE', 'MAXDIMS',\n    'MAY_SHARE_BOUNDS', 'MAY_SHARE_EXACT', 'NEEDS_INIT', 'NEEDS_PYAPI',\n    'RAISE', 'USE_GETITEM', 'USE_SETITEM', 'WRAP', '_fastCopyAndTranspose',\n    '_flagdict', '_insert', '_reconstruct', '_vec_string', '_monotonicity',\n    'add_docstring', 'arange', 'array', 'asarray', 'asanyarray',\n    'ascontiguousarray', 'asfortranarray', 'bincount', 'broadcast',\n    'busday_count', 'busday_offset', 'busdaycalendar', 'can_cast',\n    'compare_chararrays', 'concatenate', 'copyto', 'correlate', 'correlate2',\n    'count_nonzero', 'c_einsum', 'datetime_as_string', 'datetime_data',\n    'digitize', 'dot', 'dragon4_positional', 'dragon4_scientific', 'dtype',\n    'empty', 'empty_like', 'error', 'flagsobj', 'flatiter', 'format_longfloat',\n    'frombuffer', 'fromfile', 'fromiter', 'fromstring', 'inner',\n    'interp', 'interp_complex', 'is_busday', 'lexsort',\n    'matmul', 'may_share_memory', 'min_scalar_type', 'ndarray', 'nditer',\n    'nested_iters', 'normalize_axis_index', 'packbits',\n    'promote_types', 'putmask', 'ravel_multi_index', 'result_type', 'scalar',\n    'set_datetimeparse_function', 'set_legacy_print_mode', 'set_numeric_ops',\n    'set_string_function', 'set_typeDict', 'shares_memory',\n    'tracemalloc_domain', 'typeinfo', 'unpackbits', 'unravel_index', 'vdot',\n    'where', 'zeros']\n\n# For backward compatibility, make sure pickle imports these functions from here\n_reconstruct.__module__ = 'numpy.core.multiarray'\nscalar.__module__ = 'numpy.core.multiarray'\n\n\narange.__module__ = 'numpy'\narray.__module__ = 'numpy'\nasarray.__module__ = 'numpy'\nasanyarray.__module__ = 'numpy'\nascontiguousarray.__module__ = 'numpy'\nasfortranarray.__module__ = 'numpy'\ndatetime_data.__module__ = 'numpy'\nempty.__module__ = 'numpy'\nfrombuffer.__module__ = 'numpy'\nfromfile.__module__ = 'numpy'\nfromiter.__module__ = 'numpy'\nfrompyfunc.__module__ = 'numpy'\nfromstring.__module__ = 'numpy'\ngeterrobj.__module__ = 'numpy'\nmay_share_memory.__module__ = 'numpy'\nnested_iters.__module__ = 'numpy'\npromote_types.__module__ = 'numpy'\nset_numeric_ops.__module__ = 'numpy'\nseterrobj.__module__ = 'numpy'\nzeros.__module__ = 'numpy'\n\n\n# We can't verify dispatcher signatures because NumPy's C functions don't\n# support introspection.\narray_function_from_c_func_and_dispatcher = functools.partial(\n    overrides.array_function_from_dispatcher,\n    module='numpy', docs_from_dispatcher=True, verify=False)\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.empty_like)\ndef empty_like(prototype, dtype=None, order=None, subok=None, shape=None):\n    \"\"\"\n    empty_like(prototype, dtype=None, order='K', subok=True, shape=None)\n\n    Return a new array with the same shape and type as a given array.\n\n    Parameters\n    ----------\n    prototype : array_like\n        The shape and data-type of `prototype` define these same attributes\n        of the returned array.\n    dtype : data-type, optional\n        Overrides the data type of the result.\n\n        .. versionadded:: 1.6.0\n    order : {'C', 'F', 'A', or 'K'}, optional\n        Overrides the memory layout of the result. 'C' means C-order,\n        'F' means F-order, 'A' means 'F' if `prototype` is Fortran\n        contiguous, 'C' otherwise. 'K' means match the layout of `prototype`\n        as closely as possible.\n\n        .. versionadded:: 1.6.0\n    subok : bool, optional.\n        If True, then the newly created array will use the sub-class\n        type of `prototype`, otherwise it will be a base-class array. Defaults\n        to True.\n    shape : int or sequence of ints, optional.\n        Overrides the shape of the result. If order='K' and the number of\n        dimensions is unchanged, will try to keep order, otherwise,\n        order='C' is implied.\n\n        .. versionadded:: 1.17.0\n\n    Returns\n    -------\n    out : ndarray\n        Array of uninitialized (arbitrary) data with the same\n        shape and type as `prototype`.\n\n    See Also\n    --------\n    ones_like : Return an array of ones with shape and type of input.\n    zeros_like : Return an array of zeros with shape and type of input.\n    full_like : Return a new array with shape of input filled with value.\n    empty : Return a new uninitialized array.\n\n    Notes\n    -----\n    This function does *not* initialize the returned array; to do that use\n    `zeros_like` or `ones_like` instead.  It may be marginally faster than\n    the functions that do set the array values.\n\n    Examples\n    --------\n    >>> a = ([1,2,3], [4,5,6])                         # a is array-like\n    >>> np.empty_like(a)\n    array([[-1073741821, -1073741821,           3],    # uninitialized\n           [          0,           0, -1073741821]])\n    >>> a = np.array([[1., 2., 3.],[4.,5.,6.]])\n    >>> np.empty_like(a)\n    array([[ -2.00000715e+000,   1.48219694e-323,  -2.00000572e+000], # uninitialized\n           [  4.38791518e-305,  -2.00000715e+000,   4.17269252e-309]])\n\n    \"\"\"\n    return (prototype,)\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.concatenate)\ndef concatenate(arrays, axis=None, out=None, *, dtype=None, casting=None):\n    \"\"\"\n    concatenate((a1, a2, ...), axis=0, out=None, dtype=None, casting=\"same_kind\")\n\n    Join a sequence of arrays along an existing axis.\n\n    Parameters\n    ----------\n    a1, a2, ... : sequence of array_like\n        The arrays must have the same shape, except in the dimension\n        corresponding to `axis` (the first, by default).\n    axis : int, optional\n        The axis along which the arrays will be joined.  If axis is None,\n        arrays are flattened before use.  Default is 0.\n    out : ndarray, optional\n        If provided, the destination to place the result. The shape must be\n        correct, matching that of what concatenate would have returned if no\n        out argument were specified.\n    dtype : str or dtype\n        If provided, the destination array will have this dtype. Cannot be\n        provided together with `out`.\n\n        .. versionadded:: 1.20.0\n\n    casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional\n        Controls what kind of data casting may occur. Defaults to 'same_kind'.\n\n        .. versionadded:: 1.20.0\n\n    Returns\n    -------\n    res : ndarray\n        The concatenated array.\n\n    See Also\n    --------\n    ma.concatenate : Concatenate function that preserves input masks.\n    array_split : Split an array into multiple sub-arrays of equal or\n                  near-equal size.\n    split : Split array into a list of multiple sub-arrays of equal size.\n    hsplit : Split array into multiple sub-arrays horizontally (column wise).\n    vsplit : Split array into multiple sub-arrays vertically (row wise).\n    dsplit : Split array into multiple sub-arrays along the 3rd axis (depth).\n    stack : Stack a sequence of arrays along a new axis.\n    block : Assemble arrays from blocks.\n    hstack : Stack arrays in sequence horizontally (column wise).\n    vstack : Stack arrays in sequence vertically (row wise).\n    dstack : Stack arrays in sequence depth wise (along third dimension).\n    column_stack : Stack 1-D arrays as columns into a 2-D array.\n\n    Notes\n    -----\n    When one or more of the arrays to be concatenated is a MaskedArray,\n    this function will return a MaskedArray object instead of an ndarray,\n    but the input masks are *not* preserved. In cases where a MaskedArray\n    is expected as input, use the ma.concatenate function from the masked\n    array module instead.\n\n    Examples\n    --------\n    >>> a = np.array([[1, 2], [3, 4]])\n    >>> b = np.array([[5, 6]])\n    >>> np.concatenate((a, b), axis=0)\n    array([[1, 2],\n           [3, 4],\n           [5, 6]])\n    >>> np.concatenate((a, b.T), axis=1)\n    array([[1, 2, 5],\n           [3, 4, 6]])\n    >>> np.concatenate((a, b), axis=None)\n    array([1, 2, 3, 4, 5, 6])\n\n    This function will not preserve masking of MaskedArray inputs.\n\n    >>> a = np.ma.arange(3)\n    >>> a[1] = np.ma.masked\n    >>> b = np.arange(2, 5)\n    >>> a\n    masked_array(data=[0, --, 2],\n                 mask=[False,  True, False],\n           fill_value=999999)\n    >>> b\n    array([2, 3, 4])\n    >>> np.concatenate([a, b])\n    masked_array(data=[0, 1, 2, 2, 3, 4],\n                 mask=False,\n           fill_value=999999)\n    >>> np.ma.concatenate([a, b])\n    masked_array(data=[0, --, 2, 2, 3, 4],\n                 mask=[False,  True, False, False, False, False],\n           fill_value=999999)\n\n    \"\"\"\n    if out is not None:\n        # optimize for the typical case where only arrays is provided\n        arrays = list(arrays)\n        arrays.append(out)\n    return arrays\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.inner)\ndef inner(a, b):\n    \"\"\"\n    inner(a, b)\n\n    Inner product of two arrays.\n\n    Ordinary inner product of vectors for 1-D arrays (without complex\n    conjugation), in higher dimensions a sum product over the last axes.\n\n    Parameters\n    ----------\n    a, b : array_like\n        If `a` and `b` are nonscalar, their last dimensions must match.\n\n    Returns\n    -------\n    out : ndarray\n        If `a` and `b` are both\n        scalars or both 1-D arrays then a scalar is returned; otherwise\n        an array is returned.\n        ``out.shape = (*a.shape[:-1], *b.shape[:-1])``\n\n    Raises\n    ------\n    ValueError\n        If both `a` and `b` are nonscalar and their last dimensions have\n        different sizes.\n\n    See Also\n    --------\n    tensordot : Sum products over arbitrary axes.\n    dot : Generalised matrix product, using second last dimension of `b`.\n    einsum : Einstein summation convention.\n\n    Notes\n    -----\n    For vectors (1-D arrays) it computes the ordinary inner-product::\n\n        np.inner(a, b) = sum(a[:]*b[:])\n\n    More generally, if `ndim(a) = r > 0` and `ndim(b) = s > 0`::\n\n        np.inner(a, b) = np.tensordot(a, b, axes=(-1,-1))\n\n    or explicitly::\n\n        np.inner(a, b)[i0,...,ir-2,j0,...,js-2]\n             = sum(a[i0,...,ir-2,:]*b[j0,...,js-2,:])\n\n    In addition `a` or `b` may be scalars, in which case::\n\n       np.inner(a,b) = a*b\n\n    Examples\n    --------\n    Ordinary inner product for vectors:\n\n    >>> a = np.array([1,2,3])\n    >>> b = np.array([0,1,0])\n    >>> np.inner(a, b)\n    2\n\n    Some multidimensional examples:\n\n    >>> a = np.arange(24).reshape((2,3,4))\n    >>> b = np.arange(4)\n    >>> c = np.inner(a, b)\n    >>> c.shape\n    (2, 3)\n    >>> c\n    array([[ 14,  38,  62],\n           [ 86, 110, 134]])\n\n    >>> a = np.arange(2).reshape((1,1,2))\n    >>> b = np.arange(6).reshape((3,2))\n    >>> c = np.inner(a, b)\n    >>> c.shape\n    (1, 1, 3)\n    >>> c\n    array([[[1, 3, 5]]])\n\n    An example where `b` is a scalar:\n\n    >>> np.inner(np.eye(2), 7)\n    array([[7., 0.],\n           [0., 7.]])\n\n    \"\"\"\n    return (a, b)\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.where)\ndef where(condition, x=None, y=None):\n    \"\"\"\n    where(condition, [x, y])\n\n    Return elements chosen from `x` or `y` depending on `condition`.\n\n    .. note::\n        When only `condition` is provided, this function is a shorthand for\n        ``np.asarray(condition).nonzero()``. Using `nonzero` directly should be\n        preferred, as it behaves correctly for subclasses. The rest of this\n        documentation covers only the case where all three arguments are\n        provided.\n\n    Parameters\n    ----------\n    condition : array_like, bool\n        Where True, yield `x`, otherwise yield `y`.\n    x, y : array_like\n        Values from which to choose. `x`, `y` and `condition` need to be\n        broadcastable to some shape.\n\n    Returns\n    -------\n    out : ndarray\n        An array with elements from `x` where `condition` is True, and elements\n        from `y` elsewhere.\n\n    See Also\n    --------\n    choose\n    nonzero : The function that is called when x and y are omitted\n\n    Notes\n    -----\n    If all the arrays are 1-D, `where` is equivalent to::\n\n        [xv if c else yv\n         for c, xv, yv in zip(condition, x, y)]\n\n    Examples\n    --------\n    >>> a = np.arange(10)\n    >>> a\n    array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n    >>> np.where(a < 5, a, 10*a)\n    array([ 0,  1,  2,  3,  4, 50, 60, 70, 80, 90])\n\n    This can be used on multidimensional arrays too:\n\n    >>> np.where([[True, False], [True, True]],\n    ...          [[1, 2], [3, 4]],\n    ...          [[9, 8], [7, 6]])\n    array([[1, 8],\n           [3, 4]])\n\n    The shapes of x, y, and the condition are broadcast together:\n\n    >>> x, y = np.ogrid[:3, :4]\n    >>> np.where(x < y, x, 10 + y)  # both x and 10+y are broadcast\n    array([[10,  0,  0,  0],\n           [10, 11,  1,  1],\n           [10, 11, 12,  2]])\n\n    >>> a = np.array([[0, 1, 2],\n    ...               [0, 2, 4],\n    ...               [0, 3, 6]])\n    >>> np.where(a < 4, a, -1)  # -1 is broadcast\n    array([[ 0,  1,  2],\n           [ 0,  2, -1],\n           [ 0,  3, -1]])\n    \"\"\"\n    return (condition, x, y)\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.lexsort)\ndef lexsort(keys, axis=None):\n    \"\"\"\n    lexsort(keys, axis=-1)\n\n    Perform an indirect stable sort using a sequence of keys.\n\n    Given multiple sorting keys, which can be interpreted as columns in a\n    spreadsheet, lexsort returns an array of integer indices that describes\n    the sort order by multiple columns. The last key in the sequence is used\n    for the primary sort order, the second-to-last key for the secondary sort\n    order, and so on. The keys argument must be a sequence of objects that\n    can be converted to arrays of the same shape. If a 2D array is provided\n    for the keys argument, its rows are interpreted as the sorting keys and\n    sorting is according to the last row, second last row etc.\n\n    Parameters\n    ----------\n    keys : (k, N) array or tuple containing k (N,)-shaped sequences\n        The `k` different \"columns\" to be sorted.  The last column (or row if\n        `keys` is a 2D array) is the primary sort key.\n    axis : int, optional\n        Axis to be indirectly sorted.  By default, sort over the last axis.\n\n    Returns\n    -------\n    indices : (N,) ndarray of ints\n        Array of indices that sort the keys along the specified axis.\n\n    See Also\n    --------\n    argsort : Indirect sort.\n    ndarray.sort : In-place sort.\n    sort : Return a sorted copy of an array.\n\n    Examples\n    --------\n    Sort names: first by surname, then by name.\n\n    >>> surnames =    ('Hertz',    'Galilei', 'Hertz')\n    >>> first_names = ('Heinrich', 'Galileo', 'Gustav')\n    >>> ind = np.lexsort((first_names, surnames))\n    >>> ind\n    array([1, 2, 0])\n\n    >>> [surnames[i] + \", \" + first_names[i] for i in ind]\n    ['Galilei, Galileo', 'Hertz, Gustav', 'Hertz, Heinrich']\n\n    Sort two columns of numbers:\n\n    >>> a = [1,5,1,4,3,4,4] # First column\n    >>> b = [9,4,0,4,0,2,1] # Second column\n    >>> ind = np.lexsort((b,a)) # Sort by a, then by b\n    >>> ind\n    array([2, 0, 4, 6, 5, 3, 1])\n\n    >>> [(a[i],b[i]) for i in ind]\n    [(1, 0), (1, 9), (3, 0), (4, 1), (4, 2), (4, 4), (5, 4)]\n\n    Note that sorting is first according to the elements of ``a``.\n    Secondary sorting is according to the elements of ``b``.\n\n    A normal ``argsort`` would have yielded:\n\n    >>> [(a[i],b[i]) for i in np.argsort(a)]\n    [(1, 9), (1, 0), (3, 0), (4, 4), (4, 2), (4, 1), (5, 4)]\n\n    Structured arrays are sorted lexically by ``argsort``:\n\n    >>> x = np.array([(1,9), (5,4), (1,0), (4,4), (3,0), (4,2), (4,1)],\n    ...              dtype=np.dtype([('x', int), ('y', int)]))\n\n    >>> np.argsort(x) # or np.argsort(x, order=('x', 'y'))\n    array([2, 0, 4, 6, 5, 3, 1])\n\n    \"\"\"\n    if isinstance(keys, tuple):\n        return keys\n    else:\n        return (keys,)\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.can_cast)\ndef can_cast(from_, to, casting=None):\n    \"\"\"\n    can_cast(from_, to, casting='safe')\n\n    Returns True if cast between data types can occur according to the\n    casting rule.  If from is a scalar or array scalar, also returns\n    True if the scalar value can be cast without overflow or truncation\n    to an integer.\n\n    Parameters\n    ----------\n    from_ : dtype, dtype specifier, scalar, or array\n        Data type, scalar, or array to cast from.\n    to : dtype or dtype specifier\n        Data type to cast to.\n    casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional\n        Controls what kind of data casting may occur.\n\n          * 'no' means the data types should not be cast at all.\n          * 'equiv' means only byte-order changes are allowed.\n          * 'safe' means only casts which can preserve values are allowed.\n          * 'same_kind' means only safe casts or casts within a kind,\n            like float64 to float32, are allowed.\n          * 'unsafe' means any data conversions may be done.\n\n    Returns\n    -------\n    out : bool\n        True if cast can occur according to the casting rule.\n\n    Notes\n    -----\n    .. versionchanged:: 1.17.0\n       Casting between a simple data type and a structured one is possible only\n       for \"unsafe\" casting.  Casting to multiple fields is allowed, but\n       casting from multiple fields is not.\n\n    .. versionchanged:: 1.9.0\n       Casting from numeric to string types in 'safe' casting mode requires\n       that the string dtype length is long enough to store the maximum\n       integer/float value converted.\n\n    See also\n    --------\n    dtype, result_type\n\n    Examples\n    --------\n    Basic examples\n\n    >>> np.can_cast(np.int32, np.int64)\n    True\n    >>> np.can_cast(np.float64, complex)\n    True\n    >>> np.can_cast(complex, float)\n    False\n\n    >>> np.can_cast('i8', 'f8')\n    True\n    >>> np.can_cast('i8', 'f4')\n    False\n    >>> np.can_cast('i4', 'S4')\n    False\n\n    Casting scalars\n\n    >>> np.can_cast(100, 'i1')\n    True\n    >>> np.can_cast(150, 'i1')\n    False\n    >>> np.can_cast(150, 'u1')\n    True\n\n    >>> np.can_cast(3.5e100, np.float32)\n    False\n    >>> np.can_cast(1000.0, np.float32)\n    True\n\n    Array scalar checks the value, array does not\n\n    >>> np.can_cast(np.array(1000.0), np.float32)\n    True\n    >>> np.can_cast(np.array([1000.0]), np.float32)\n    False\n\n    Using the casting rules\n\n    >>> np.can_cast('i8', 'i8', 'no')\n    True\n    >>> np.can_cast('<i8', '>i8', 'no')\n    False\n\n    >>> np.can_cast('<i8', '>i8', 'equiv')\n    True\n    >>> np.can_cast('<i4', '>i8', 'equiv')\n    False\n\n    >>> np.can_cast('<i4', '>i8', 'safe')\n    True\n    >>> np.can_cast('<i8', '>i4', 'safe')\n    False\n\n    >>> np.can_cast('<i8', '>i4', 'same_kind')\n    True\n    >>> np.can_cast('<i8', '>u4', 'same_kind')\n    False\n\n    >>> np.can_cast('<i8', '>u4', 'unsafe')\n    True\n\n    \"\"\"\n    return (from_,)\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.min_scalar_type)\ndef min_scalar_type(a):\n    \"\"\"\n    min_scalar_type(a)\n\n    For scalar ``a``, returns the data type with the smallest size\n    and smallest scalar kind which can hold its value.  For non-scalar\n    array ``a``, returns the vector's dtype unmodified.\n\n    Floating point values are not demoted to integers,\n    and complex values are not demoted to floats.\n\n    Parameters\n    ----------\n    a : scalar or array_like\n        The value whose minimal data type is to be found.\n\n    Returns\n    -------\n    out : dtype\n        The minimal data type.\n\n    Notes\n    -----\n    .. versionadded:: 1.6.0\n\n    See Also\n    --------\n    result_type, promote_types, dtype, can_cast\n\n    Examples\n    --------\n    >>> np.min_scalar_type(10)\n    dtype('uint8')\n\n    >>> np.min_scalar_type(-260)\n    dtype('int16')\n\n    >>> np.min_scalar_type(3.1)\n    dtype('float16')\n\n    >>> np.min_scalar_type(1e50)\n    dtype('float64')\n\n    >>> np.min_scalar_type(np.arange(4,dtype='f8'))\n    dtype('float64')\n\n    \"\"\"\n    return (a,)\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.result_type)\ndef result_type(*arrays_and_dtypes):\n    \"\"\"\n    result_type(*arrays_and_dtypes)\n\n    Returns the type that results from applying the NumPy\n    type promotion rules to the arguments.\n\n    Type promotion in NumPy works similarly to the rules in languages\n    like C++, with some slight differences.  When both scalars and\n    arrays are used, the array's type takes precedence and the actual value\n    of the scalar is taken into account.\n\n    For example, calculating 3*a, where a is an array of 32-bit floats,\n    intuitively should result in a 32-bit float output.  If the 3 is a\n    32-bit integer, the NumPy rules indicate it can't convert losslessly\n    into a 32-bit float, so a 64-bit float should be the result type.\n    By examining the value of the constant, '3', we see that it fits in\n    an 8-bit integer, which can be cast losslessly into the 32-bit float.\n\n    Parameters\n    ----------\n    arrays_and_dtypes : list of arrays and dtypes\n        The operands of some operation whose result type is needed.\n\n    Returns\n    -------\n    out : dtype\n        The result type.\n\n    See also\n    --------\n    dtype, promote_types, min_scalar_type, can_cast\n\n    Notes\n    -----\n    .. versionadded:: 1.6.0\n\n    The specific algorithm used is as follows.\n\n    Categories are determined by first checking which of boolean,\n    integer (int/uint), or floating point (float/complex) the maximum\n    kind of all the arrays and the scalars are.\n\n    If there are only scalars or the maximum category of the scalars\n    is higher than the maximum category of the arrays,\n    the data types are combined with :func:`promote_types`\n    to produce the return value.\n\n    Otherwise, `min_scalar_type` is called on each array, and\n    the resulting data types are all combined with :func:`promote_types`\n    to produce the return value.\n\n    The set of int values is not a subset of the uint values for types\n    with the same number of bits, something not reflected in\n    :func:`min_scalar_type`, but handled as a special case in `result_type`.\n\n    Examples\n    --------\n    >>> np.result_type(3, np.arange(7, dtype='i1'))\n    dtype('int8')\n\n    >>> np.result_type('i4', 'c8')\n    dtype('complex128')\n\n    >>> np.result_type(3.0, -2)\n    dtype('float64')\n\n    \"\"\"\n    return arrays_and_dtypes\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.dot)\ndef dot(a, b, out=None):\n    \"\"\"\n    dot(a, b, out=None)\n\n    Dot product of two arrays. Specifically,\n\n    - If both `a` and `b` are 1-D arrays, it is inner product of vectors\n      (without complex conjugation).\n\n    - If both `a` and `b` are 2-D arrays, it is matrix multiplication,\n      but using :func:`matmul` or ``a @ b`` is preferred.\n\n    - If either `a` or `b` is 0-D (scalar), it is equivalent to :func:`multiply`\n      and using ``numpy.multiply(a, b)`` or ``a * b`` is preferred.\n\n    - If `a` is an N-D array and `b` is a 1-D array, it is a sum product over\n      the last axis of `a` and `b`.\n\n    - If `a` is an N-D array and `b` is an M-D array (where ``M>=2``), it is a\n      sum product over the last axis of `a` and the second-to-last axis of `b`::\n\n        dot(a, b)[i,j,k,m] = sum(a[i,j,:] * b[k,:,m])\n\n    Parameters\n    ----------\n    a : array_like\n        First argument.\n    b : array_like\n        Second argument.\n    out : ndarray, optional\n        Output argument. This must have the exact kind that would be returned\n        if it was not used. In particular, it must have the right type, must be\n        C-contiguous, and its dtype must be the dtype that would be returned\n        for `dot(a,b)`. This is a performance feature. Therefore, if these\n        conditions are not met, an exception is raised, instead of attempting\n        to be flexible.\n\n    Returns\n    -------\n    output : ndarray\n        Returns the dot product of `a` and `b`.  If `a` and `b` are both\n        scalars or both 1-D arrays then a scalar is returned; otherwise\n        an array is returned.\n        If `out` is given, then it is returned.\n\n    Raises\n    ------\n    ValueError\n        If the last dimension of `a` is not the same size as\n        the second-to-last dimension of `b`.\n\n    See Also\n    --------\n    vdot : Complex-conjugating dot product.\n    tensordot : Sum products over arbitrary axes.\n    einsum : Einstein summation convention.\n    matmul : '@' operator as method with out parameter.\n    linalg.multi_dot : Chained dot product.\n\n    Examples\n    --------\n    >>> np.dot(3, 4)\n    12\n\n    Neither argument is complex-conjugated:\n\n    >>> np.dot([2j, 3j], [2j, 3j])\n    (-13+0j)\n\n    For 2-D arrays it is the matrix product:\n\n    >>> a = [[1, 0], [0, 1]]\n    >>> b = [[4, 1], [2, 2]]\n    >>> np.dot(a, b)\n    array([[4, 1],\n           [2, 2]])\n\n    >>> a = np.arange(3*4*5*6).reshape((3,4,5,6))\n    >>> b = np.arange(3*4*5*6)[::-1].reshape((5,4,6,3))\n    >>> np.dot(a, b)[2,3,2,1,2,2]\n    499128\n    >>> sum(a[2,3,2,:] * b[1,2,:,2])\n    499128\n\n    \"\"\"\n    return (a, b, out)\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.vdot)\ndef vdot(a, b):\n    \"\"\"\n    vdot(a, b)\n\n    Return the dot product of two vectors.\n\n    The vdot(`a`, `b`) function handles complex numbers differently than\n    dot(`a`, `b`).  If the first argument is complex the complex conjugate\n    of the first argument is used for the calculation of the dot product.\n\n    Note that `vdot` handles multidimensional arrays differently than `dot`:\n    it does *not* perform a matrix product, but flattens input arguments\n    to 1-D vectors first. Consequently, it should only be used for vectors.\n\n    Parameters\n    ----------\n    a : array_like\n        If `a` is complex the complex conjugate is taken before calculation\n        of the dot product.\n    b : array_like\n        Second argument to the dot product.\n\n    Returns\n    -------\n    output : ndarray\n        Dot product of `a` and `b`.  Can be an int, float, or\n        complex depending on the types of `a` and `b`.\n\n    See Also\n    --------\n    dot : Return the dot product without using the complex conjugate of the\n          first argument.\n\n    Examples\n    --------\n    >>> a = np.array([1+2j,3+4j])\n    >>> b = np.array([5+6j,7+8j])\n    >>> np.vdot(a, b)\n    (70-8j)\n    >>> np.vdot(b, a)\n    (70+8j)\n\n    Note that higher-dimensional arrays are flattened!\n\n    >>> a = np.array([[1, 4], [5, 6]])\n    >>> b = np.array([[4, 1], [2, 2]])\n    >>> np.vdot(a, b)\n    30\n    >>> np.vdot(b, a)\n    30\n    >>> 1*4 + 4*1 + 5*2 + 6*2\n    30\n\n    \"\"\"\n    return (a, b)\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.bincount)\ndef bincount(x, weights=None, minlength=None):\n    \"\"\"\n    bincount(x, weights=None, minlength=0)\n\n    Count number of occurrences of each value in array of non-negative ints.\n\n    The number of bins (of size 1) is one larger than the largest value in\n    `x`. If `minlength` is specified, there will be at least this number\n    of bins in the output array (though it will be longer if necessary,\n    depending on the contents of `x`).\n    Each bin gives the number of occurrences of its index value in `x`.\n    If `weights` is specified the input array is weighted by it, i.e. if a\n    value ``n`` is found at position ``i``, ``out[n] += weight[i]`` instead\n    of ``out[n] += 1``.\n\n    Parameters\n    ----------\n    x : array_like, 1 dimension, nonnegative ints\n        Input array.\n    weights : array_like, optional\n        Weights, array of the same shape as `x`.\n    minlength : int, optional\n        A minimum number of bins for the output array.\n\n        .. versionadded:: 1.6.0\n\n    Returns\n    -------\n    out : ndarray of ints\n        The result of binning the input array.\n        The length of `out` is equal to ``np.amax(x)+1``.\n\n    Raises\n    ------\n    ValueError\n        If the input is not 1-dimensional, or contains elements with negative\n        values, or if `minlength` is negative.\n    TypeError\n        If the type of the input is float or complex.\n\n    See Also\n    --------\n    histogram, digitize, unique\n\n    Examples\n    --------\n    >>> np.bincount(np.arange(5))\n    array([1, 1, 1, 1, 1])\n    >>> np.bincount(np.array([0, 1, 1, 3, 2, 1, 7]))\n    array([1, 3, 1, 1, 0, 0, 0, 1])\n\n    >>> x = np.array([0, 1, 1, 3, 2, 1, 7, 23])\n    >>> np.bincount(x).size == np.amax(x)+1\n    True\n\n    The input array needs to be of integer dtype, otherwise a\n    TypeError is raised:\n\n    >>> np.bincount(np.arange(5, dtype=float))\n    Traceback (most recent call last):\n      ...\n    TypeError: Cannot cast array data from dtype('float64') to dtype('int64')\n    according to the rule 'safe'\n\n    A possible use of ``bincount`` is to perform sums over\n    variable-size chunks of an array, using the ``weights`` keyword.\n\n    >>> w = np.array([0.3, 0.5, 0.2, 0.7, 1., -0.6]) # weights\n    >>> x = np.array([0, 1, 1, 2, 2, 2])\n    >>> np.bincount(x,  weights=w)\n    array([ 0.3,  0.7,  1.1])\n\n    \"\"\"\n    return (x, weights)\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.ravel_multi_index)\ndef ravel_multi_index(multi_index, dims, mode=None, order=None):\n    \"\"\"\n    ravel_multi_index(multi_index, dims, mode='raise', order='C')\n\n    Converts a tuple of index arrays into an array of flat\n    indices, applying boundary modes to the multi-index.\n\n    Parameters\n    ----------\n    multi_index : tuple of array_like\n        A tuple of integer arrays, one array for each dimension.\n    dims : tuple of ints\n        The shape of array into which the indices from ``multi_index`` apply.\n    mode : {'raise', 'wrap', 'clip'}, optional\n        Specifies how out-of-bounds indices are handled.  Can specify\n        either one mode or a tuple of modes, one mode per index.\n\n        * 'raise' -- raise an error (default)\n        * 'wrap' -- wrap around\n        * 'clip' -- clip to the range\n\n        In 'clip' mode, a negative index which would normally\n        wrap will clip to 0 instead.\n    order : {'C', 'F'}, optional\n        Determines whether the multi-index should be viewed as\n        indexing in row-major (C-style) or column-major\n        (Fortran-style) order.\n\n    Returns\n    -------\n    raveled_indices : ndarray\n        An array of indices into the flattened version of an array\n        of dimensions ``dims``.\n\n    See Also\n    --------\n    unravel_index\n\n    Notes\n    -----\n    .. versionadded:: 1.6.0\n\n    Examples\n    --------\n    >>> arr = np.array([[3,6,6],[4,5,1]])\n    >>> np.ravel_multi_index(arr, (7,6))\n    array([22, 41, 37])\n    >>> np.ravel_multi_index(arr, (7,6), order='F')\n    array([31, 41, 13])\n    >>> np.ravel_multi_index(arr, (4,6), mode='clip')\n    array([22, 23, 19])\n    >>> np.ravel_multi_index(arr, (4,4), mode=('clip','wrap'))\n    array([12, 13, 13])\n\n    >>> np.ravel_multi_index((3,1,4,1), (6,7,8,9))\n    1621\n    \"\"\"\n    return multi_index\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.unravel_index)\ndef unravel_index(indices, shape=None, order=None):\n    \"\"\"\n    unravel_index(indices, shape, order='C')\n\n    Converts a flat index or array of flat indices into a tuple\n    of coordinate arrays.\n\n    Parameters\n    ----------\n    indices : array_like\n        An integer array whose elements are indices into the flattened\n        version of an array of dimensions ``shape``. Before version 1.6.0,\n        this function accepted just one index value.\n    shape : tuple of ints\n        The shape of the array to use for unraveling ``indices``.\n\n        .. versionchanged:: 1.16.0\n            Renamed from ``dims`` to ``shape``.\n\n    order : {'C', 'F'}, optional\n        Determines whether the indices should be viewed as indexing in\n        row-major (C-style) or column-major (Fortran-style) order.\n\n        .. versionadded:: 1.6.0\n\n    Returns\n    -------\n    unraveled_coords : tuple of ndarray\n        Each array in the tuple has the same shape as the ``indices``\n        array.\n\n    See Also\n    --------\n    ravel_multi_index\n\n    Examples\n    --------\n    >>> np.unravel_index([22, 41, 37], (7,6))\n    (array([3, 6, 6]), array([4, 5, 1]))\n    >>> np.unravel_index([31, 41, 13], (7,6), order='F')\n    (array([3, 6, 6]), array([4, 5, 1]))\n\n    >>> np.unravel_index(1621, (6,7,8,9))\n    (3, 1, 4, 1)\n\n    \"\"\"\n    return (indices,)\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.copyto)\ndef copyto(dst, src, casting=None, where=None):\n    \"\"\"\n    copyto(dst, src, casting='same_kind', where=True)\n\n    Copies values from one array to another, broadcasting as necessary.\n\n    Raises a TypeError if the `casting` rule is violated, and if\n    `where` is provided, it selects which elements to copy.\n\n    .. versionadded:: 1.7.0\n\n    Parameters\n    ----------\n    dst : ndarray\n        The array into which values are copied.\n    src : array_like\n        The array from which values are copied.\n    casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional\n        Controls what kind of data casting may occur when copying.\n\n          * 'no' means the data types should not be cast at all.\n          * 'equiv' means only byte-order changes are allowed.\n          * 'safe' means only casts which can preserve values are allowed.\n          * 'same_kind' means only safe casts or casts within a kind,\n            like float64 to float32, are allowed.\n          * 'unsafe' means any data conversions may be done.\n    where : array_like of bool, optional\n        A boolean array which is broadcasted to match the dimensions\n        of `dst`, and selects elements to copy from `src` to `dst`\n        wherever it contains the value True.\n    \"\"\"\n    return (dst, src, where)\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.putmask)\ndef putmask(a, mask, values):\n    \"\"\"\n    putmask(a, mask, values)\n\n    Changes elements of an array based on conditional and input values.\n\n    Sets ``a.flat[n] = values[n]`` for each n where ``mask.flat[n]==True``.\n\n    If `values` is not the same size as `a` and `mask` then it will repeat.\n    This gives behavior different from ``a[mask] = values``.\n\n    Parameters\n    ----------\n    a : ndarray\n        Target array.\n    mask : array_like\n        Boolean mask array. It has to be the same shape as `a`.\n    values : array_like\n        Values to put into `a` where `mask` is True. If `values` is smaller\n        than `a` it will be repeated.\n\n    See Also\n    --------\n    place, put, take, copyto\n\n    Examples\n    --------\n    >>> x = np.arange(6).reshape(2, 3)\n    >>> np.putmask(x, x>2, x**2)\n    >>> x\n    array([[ 0,  1,  2],\n           [ 9, 16, 25]])\n\n    If `values` is smaller than `a` it is repeated:\n\n    >>> x = np.arange(5)\n    >>> np.putmask(x, x>1, [-33, -44])\n    >>> x\n    array([  0,   1, -33, -44, -33])\n\n    \"\"\"\n    return (a, mask, values)\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.packbits)\ndef packbits(a, axis=None, bitorder='big'):\n    \"\"\"\n    packbits(a, axis=None, bitorder='big')\n\n    Packs the elements of a binary-valued array into bits in a uint8 array.\n\n    The result is padded to full bytes by inserting zero bits at the end.\n\n    Parameters\n    ----------\n    a : array_like\n        An array of integers or booleans whose elements should be packed to\n        bits.\n    axis : int, optional\n        The dimension over which bit-packing is done.\n        ``None`` implies packing the flattened array.\n    bitorder : {'big', 'little'}, optional\n        The order of the input bits. 'big' will mimic bin(val),\n        ``[0, 0, 0, 0, 0, 0, 1, 1] => 3 = 0b00000011``, 'little' will\n        reverse the order so ``[1, 1, 0, 0, 0, 0, 0, 0] => 3``.\n        Defaults to 'big'.\n\n        .. versionadded:: 1.17.0\n\n    Returns\n    -------\n    packed : ndarray\n        Array of type uint8 whose elements represent bits corresponding to the\n        logical (0 or nonzero) value of the input elements. The shape of\n        `packed` has the same number of dimensions as the input (unless `axis`\n        is None, in which case the output is 1-D).\n\n    See Also\n    --------\n    unpackbits: Unpacks elements of a uint8 array into a binary-valued output\n                array.\n\n    Examples\n    --------\n    >>> a = np.array([[[1,0,1],\n    ...                [0,1,0]],\n    ...               [[1,1,0],\n    ...                [0,0,1]]])\n    >>> b = np.packbits(a, axis=-1)\n    >>> b\n    array([[[160],\n            [ 64]],\n           [[192],\n            [ 32]]], dtype=uint8)\n\n    Note that in binary 160 = 1010 0000, 64 = 0100 0000, 192 = 1100 0000,\n    and 32 = 0010 0000.\n\n    \"\"\"\n    return (a,)\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.unpackbits)\ndef unpackbits(a, axis=None, count=None, bitorder='big'):\n    \"\"\"\n    unpackbits(a, axis=None, count=None, bitorder='big')\n\n    Unpacks elements of a uint8 array into a binary-valued output array.\n\n    Each element of `a` represents a bit-field that should be unpacked\n    into a binary-valued output array. The shape of the output array is\n    either 1-D (if `axis` is ``None``) or the same shape as the input\n    array with unpacking done along the axis specified.\n\n    Parameters\n    ----------\n    a : ndarray, uint8 type\n       Input array.\n    axis : int, optional\n        The dimension over which bit-unpacking is done.\n        ``None`` implies unpacking the flattened array.\n    count : int or None, optional\n        The number of elements to unpack along `axis`, provided as a way\n        of undoing the effect of packing a size that is not a multiple\n        of eight. A non-negative number means to only unpack `count`\n        bits. A negative number means to trim off that many bits from\n        the end. ``None`` means to unpack the entire array (the\n        default). Counts larger than the available number of bits will\n        add zero padding to the output. Negative counts must not\n        exceed the available number of bits.\n\n        .. versionadded:: 1.17.0\n\n    bitorder : {'big', 'little'}, optional\n        The order of the returned bits. 'big' will mimic bin(val),\n        ``3 = 0b00000011 => [0, 0, 0, 0, 0, 0, 1, 1]``, 'little' will reverse\n        the order to ``[1, 1, 0, 0, 0, 0, 0, 0]``.\n        Defaults to 'big'.\n\n        .. versionadded:: 1.17.0\n\n    Returns\n    -------\n    unpacked : ndarray, uint8 type\n       The elements are binary-valued (0 or 1).\n\n    See Also\n    --------\n    packbits : Packs the elements of a binary-valued array into bits in\n               a uint8 array.\n\n    Examples\n    --------\n    >>> a = np.array([[2], [7], [23]], dtype=np.uint8)\n    >>> a\n    array([[ 2],\n           [ 7],\n           [23]], dtype=uint8)\n    >>> b = np.unpackbits(a, axis=1)\n    >>> b\n    array([[0, 0, 0, 0, 0, 0, 1, 0],\n           [0, 0, 0, 0, 0, 1, 1, 1],\n           [0, 0, 0, 1, 0, 1, 1, 1]], dtype=uint8)\n    >>> c = np.unpackbits(a, axis=1, count=-3)\n    >>> c\n    array([[0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 0],\n           [0, 0, 0, 1, 0]], dtype=uint8)\n\n    >>> p = np.packbits(b, axis=0)\n    >>> np.unpackbits(p, axis=0)\n    array([[0, 0, 0, 0, 0, 0, 1, 0],\n           [0, 0, 0, 0, 0, 1, 1, 1],\n           [0, 0, 0, 1, 0, 1, 1, 1],\n           [0, 0, 0, 0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n    >>> np.array_equal(b, np.unpackbits(p, axis=0, count=b.shape[0]))\n    True\n\n    \"\"\"\n    return (a,)\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.shares_memory)\ndef shares_memory(a, b, max_work=None):\n    \"\"\"\n    shares_memory(a, b, max_work=None)\n\n    Determine if two arrays share memory.\n\n    .. warning::\n\n       This function can be exponentially slow for some inputs, unless\n       `max_work` is set to a finite number or ``MAY_SHARE_BOUNDS``.\n       If in doubt, use `numpy.may_share_memory` instead.\n\n    Parameters\n    ----------\n    a, b : ndarray\n        Input arrays\n    max_work : int, optional\n        Effort to spend on solving the overlap problem (maximum number\n        of candidate solutions to consider). The following special\n        values are recognized:\n\n        max_work=MAY_SHARE_EXACT  (default)\n            The problem is solved exactly. In this case, the function returns\n            True only if there is an element shared between the arrays. Finding\n            the exact solution may take extremely long in some cases.\n        max_work=MAY_SHARE_BOUNDS\n            Only the memory bounds of a and b are checked.\n\n    Raises\n    ------\n    numpy.TooHardError\n        Exceeded max_work.\n\n    Returns\n    -------\n    out : bool\n\n    See Also\n    --------\n    may_share_memory\n\n    Examples\n    --------\n    >>> x = np.array([1, 2, 3, 4])\n    >>> np.shares_memory(x, np.array([5, 6, 7]))\n    False\n    >>> np.shares_memory(x[::2], x)\n    True\n    >>> np.shares_memory(x[::2], x[1::2])\n    False\n\n    Checking whether two arrays share memory is NP-complete, and\n    runtime may increase exponentially in the number of\n    dimensions. Hence, `max_work` should generally be set to a finite\n    number, as it is possible to construct examples that take\n    extremely long to run:\n\n    >>> from numpy.lib.stride_tricks import as_strided\n    >>> x = np.zeros([192163377], dtype=np.int8)\n    >>> x1 = as_strided(x, strides=(36674, 61119, 85569), shape=(1049, 1049, 1049))\n    >>> x2 = as_strided(x[64023025:], strides=(12223, 12224, 1), shape=(1049, 1049, 1))\n    >>> np.shares_memory(x1, x2, max_work=1000)\n    Traceback (most recent call last):\n    ...\n    numpy.TooHardError: Exceeded max_work\n\n    Running ``np.shares_memory(x1, x2)`` without `max_work` set takes\n    around 1 minute for this case. It is possible to find problems\n    that take still significantly longer.\n\n    \"\"\"\n    return (a, b)\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.may_share_memory)\ndef may_share_memory(a, b, max_work=None):\n    \"\"\"\n    may_share_memory(a, b, max_work=None)\n\n    Determine if two arrays might share memory\n\n    A return of True does not necessarily mean that the two arrays\n    share any element.  It just means that they *might*.\n\n    Only the memory bounds of a and b are checked by default.\n\n    Parameters\n    ----------\n    a, b : ndarray\n        Input arrays\n    max_work : int, optional\n        Effort to spend on solving the overlap problem.  See\n        `shares_memory` for details.  Default for ``may_share_memory``\n        is to do a bounds check.\n\n    Returns\n    -------\n    out : bool\n\n    See Also\n    --------\n    shares_memory\n\n    Examples\n    --------\n    >>> np.may_share_memory(np.array([1,2]), np.array([5,8,9]))\n    False\n    >>> x = np.zeros([3, 4])\n    >>> np.may_share_memory(x[:,0], x[:,1])\n    True\n\n    \"\"\"\n    return (a, b)\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.is_busday)\ndef is_busday(dates, weekmask=None, holidays=None, busdaycal=None, out=None):\n    \"\"\"\n    is_busday(dates, weekmask='1111100', holidays=None, busdaycal=None, out=None)\n\n    Calculates which of the given dates are valid days, and which are not.\n\n    .. versionadded:: 1.7.0\n\n    Parameters\n    ----------\n    dates : array_like of datetime64[D]\n        The array of dates to process.\n    weekmask : str or array_like of bool, optional\n        A seven-element array indicating which of Monday through Sunday are\n        valid days. May be specified as a length-seven list or array, like\n        [1,1,1,1,1,0,0]; a length-seven string, like '1111100'; or a string\n        like \"Mon Tue Wed Thu Fri\", made up of 3-character abbreviations for\n        weekdays, optionally separated by white space. Valid abbreviations\n        are: Mon Tue Wed Thu Fri Sat Sun\n    holidays : array_like of datetime64[D], optional\n        An array of dates to consider as invalid dates.  They may be\n        specified in any order, and NaT (not-a-time) dates are ignored.\n        This list is saved in a normalized form that is suited for\n        fast calculations of valid days.\n    busdaycal : busdaycalendar, optional\n        A `busdaycalendar` object which specifies the valid days. If this\n        parameter is provided, neither weekmask nor holidays may be\n        provided.\n    out : array of bool, optional\n        If provided, this array is filled with the result.\n\n    Returns\n    -------\n    out : array of bool\n        An array with the same shape as ``dates``, containing True for\n        each valid day, and False for each invalid day.\n\n    See Also\n    --------\n    busdaycalendar : An object that specifies a custom set of valid days.\n    busday_offset : Applies an offset counted in valid days.\n    busday_count : Counts how many valid days are in a half-open date range.\n\n    Examples\n    --------\n    >>> # The weekdays are Friday, Saturday, and Monday\n    ... np.is_busday(['2011-07-01', '2011-07-02', '2011-07-18'],\n    ...                 holidays=['2011-07-01', '2011-07-04', '2011-07-17'])\n    array([False, False,  True])\n    \"\"\"\n    return (dates, weekmask, holidays, out)\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.busday_offset)\ndef busday_offset(dates, offsets, roll=None, weekmask=None, holidays=None,\n                  busdaycal=None, out=None):\n    \"\"\"\n    busday_offset(dates, offsets, roll='raise', weekmask='1111100', holidays=None, busdaycal=None, out=None)\n\n    First adjusts the date to fall on a valid day according to\n    the ``roll`` rule, then applies offsets to the given dates\n    counted in valid days.\n\n    .. versionadded:: 1.7.0\n\n    Parameters\n    ----------\n    dates : array_like of datetime64[D]\n        The array of dates to process.\n    offsets : array_like of int\n        The array of offsets, which is broadcast with ``dates``.\n    roll : {'raise', 'nat', 'forward', 'following', 'backward', 'preceding', 'modifiedfollowing', 'modifiedpreceding'}, optional\n        How to treat dates that do not fall on a valid day. The default\n        is 'raise'.\n\n          * 'raise' means to raise an exception for an invalid day.\n          * 'nat' means to return a NaT (not-a-time) for an invalid day.\n          * 'forward' and 'following' mean to take the first valid day\n            later in time.\n          * 'backward' and 'preceding' mean to take the first valid day\n            earlier in time.\n          * 'modifiedfollowing' means to take the first valid day\n            later in time unless it is across a Month boundary, in which\n            case to take the first valid day earlier in time.\n          * 'modifiedpreceding' means to take the first valid day\n            earlier in time unless it is across a Month boundary, in which\n            case to take the first valid day later in time.\n    weekmask : str or array_like of bool, optional\n        A seven-element array indicating which of Monday through Sunday are\n        valid days. May be specified as a length-seven list or array, like\n        [1,1,1,1,1,0,0]; a length-seven string, like '1111100'; or a string\n        like \"Mon Tue Wed Thu Fri\", made up of 3-character abbreviations for\n        weekdays, optionally separated by white space. Valid abbreviations\n        are: Mon Tue Wed Thu Fri Sat Sun\n    holidays : array_like of datetime64[D], optional\n        An array of dates to consider as invalid dates.  They may be\n        specified in any order, and NaT (not-a-time) dates are ignored.\n        This list is saved in a normalized form that is suited for\n        fast calculations of valid days.\n    busdaycal : busdaycalendar, optional\n        A `busdaycalendar` object which specifies the valid days. If this\n        parameter is provided, neither weekmask nor holidays may be\n        provided.\n    out : array of datetime64[D], optional\n        If provided, this array is filled with the result.\n\n    Returns\n    -------\n    out : array of datetime64[D]\n        An array with a shape from broadcasting ``dates`` and ``offsets``\n        together, containing the dates with offsets applied.\n\n    See Also\n    --------\n    busdaycalendar : An object that specifies a custom set of valid days.\n    is_busday : Returns a boolean array indicating valid days.\n    busday_count : Counts how many valid days are in a half-open date range.\n\n    Examples\n    --------\n    >>> # First business day in October 2011 (not accounting for holidays)\n    ... np.busday_offset('2011-10', 0, roll='forward')\n    numpy.datetime64('2011-10-03')\n    >>> # Last business day in February 2012 (not accounting for holidays)\n    ... np.busday_offset('2012-03', -1, roll='forward')\n    numpy.datetime64('2012-02-29')\n    >>> # Third Wednesday in January 2011\n    ... np.busday_offset('2011-01', 2, roll='forward', weekmask='Wed')\n    numpy.datetime64('2011-01-19')\n    >>> # 2012 Mother's Day in Canada and the U.S.\n    ... np.busday_offset('2012-05', 1, roll='forward', weekmask='Sun')\n    numpy.datetime64('2012-05-13')\n\n    >>> # First business day on or after a date\n    ... np.busday_offset('2011-03-20', 0, roll='forward')\n    numpy.datetime64('2011-03-21')\n    >>> np.busday_offset('2011-03-22', 0, roll='forward')\n    numpy.datetime64('2011-03-22')\n    >>> # First business day after a date\n    ... np.busday_offset('2011-03-20', 1, roll='backward')\n    numpy.datetime64('2011-03-21')\n    >>> np.busday_offset('2011-03-22', 1, roll='backward')\n    numpy.datetime64('2011-03-23')\n    \"\"\"\n    return (dates, offsets, weekmask, holidays, out)\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.busday_count)\ndef busday_count(begindates, enddates, weekmask=None, holidays=None,\n                 busdaycal=None, out=None):\n    \"\"\"\n    busday_count(begindates, enddates, weekmask='1111100', holidays=[], busdaycal=None, out=None)\n\n    Counts the number of valid days between `begindates` and\n    `enddates`, not including the day of `enddates`.\n\n    If ``enddates`` specifies a date value that is earlier than the\n    corresponding ``begindates`` date value, the count will be negative.\n\n    .. versionadded:: 1.7.0\n\n    Parameters\n    ----------\n    begindates : array_like of datetime64[D]\n        The array of the first dates for counting.\n    enddates : array_like of datetime64[D]\n        The array of the end dates for counting, which are excluded\n        from the count themselves.\n    weekmask : str or array_like of bool, optional\n        A seven-element array indicating which of Monday through Sunday are\n        valid days. May be specified as a length-seven list or array, like\n        [1,1,1,1,1,0,0]; a length-seven string, like '1111100'; or a string\n        like \"Mon Tue Wed Thu Fri\", made up of 3-character abbreviations for\n        weekdays, optionally separated by white space. Valid abbreviations\n        are: Mon Tue Wed Thu Fri Sat Sun\n    holidays : array_like of datetime64[D], optional\n        An array of dates to consider as invalid dates.  They may be\n        specified in any order, and NaT (not-a-time) dates are ignored.\n        This list is saved in a normalized form that is suited for\n        fast calculations of valid days.\n    busdaycal : busdaycalendar, optional\n        A `busdaycalendar` object which specifies the valid days. If this\n        parameter is provided, neither weekmask nor holidays may be\n        provided.\n    out : array of int, optional\n        If provided, this array is filled with the result.\n\n    Returns\n    -------\n    out : array of int\n        An array with a shape from broadcasting ``begindates`` and ``enddates``\n        together, containing the number of valid days between\n        the begin and end dates.\n\n    See Also\n    --------\n    busdaycalendar : An object that specifies a custom set of valid days.\n    is_busday : Returns a boolean array indicating valid days.\n    busday_offset : Applies an offset counted in valid days.\n\n    Examples\n    --------\n    >>> # Number of weekdays in January 2011\n    ... np.busday_count('2011-01', '2011-02')\n    21\n    >>> # Number of weekdays in 2011\n    >>> np.busday_count('2011', '2012')\n    260\n    >>> # Number of Saturdays in 2011\n    ... np.busday_count('2011', '2012', weekmask='Sat')\n    53\n    \"\"\"\n    return (begindates, enddates, weekmask, holidays, out)\n\n\n@array_function_from_c_func_and_dispatcher(\n    _multiarray_umath.datetime_as_string)\ndef datetime_as_string(arr, unit=None, timezone=None, casting=None):\n    \"\"\"\n    datetime_as_string(arr, unit=None, timezone='naive', casting='same_kind')\n\n    Convert an array of datetimes into an array of strings.\n\n    Parameters\n    ----------\n    arr : array_like of datetime64\n        The array of UTC timestamps to format.\n    unit : str\n        One of None, 'auto', or a :ref:`datetime unit <arrays.dtypes.dateunits>`.\n    timezone : {'naive', 'UTC', 'local'} or tzinfo\n        Timezone information to use when displaying the datetime. If 'UTC', end\n        with a Z to indicate UTC time. If 'local', convert to the local timezone\n        first, and suffix with a +-#### timezone offset. If a tzinfo object,\n        then do as with 'local', but use the specified timezone.\n    casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}\n        Casting to allow when changing between datetime units.\n\n    Returns\n    -------\n    str_arr : ndarray\n        An array of strings the same shape as `arr`.\n\n    Examples\n    --------\n    >>> import pytz\n    >>> d = np.arange('2002-10-27T04:30', 4*60, 60, dtype='M8[m]')\n    >>> d\n    array(['2002-10-27T04:30', '2002-10-27T05:30', '2002-10-27T06:30',\n           '2002-10-27T07:30'], dtype='datetime64[m]')\n\n    Setting the timezone to UTC shows the same information, but with a Z suffix\n\n    >>> np.datetime_as_string(d, timezone='UTC')\n    array(['2002-10-27T04:30Z', '2002-10-27T05:30Z', '2002-10-27T06:30Z',\n           '2002-10-27T07:30Z'], dtype='<U35')\n\n    Note that we picked datetimes that cross a DST boundary. Passing in a\n    ``pytz`` timezone object will print the appropriate offset\n\n    >>> np.datetime_as_string(d, timezone=pytz.timezone('US/Eastern'))\n    array(['2002-10-27T00:30-0400', '2002-10-27T01:30-0400',\n           '2002-10-27T01:30-0500', '2002-10-27T02:30-0500'], dtype='<U39')\n\n    Passing in a unit will change the precision\n\n    >>> np.datetime_as_string(d, unit='h')\n    array(['2002-10-27T04', '2002-10-27T05', '2002-10-27T06', '2002-10-27T07'],\n          dtype='<U32')\n    >>> np.datetime_as_string(d, unit='s')\n    array(['2002-10-27T04:30:00', '2002-10-27T05:30:00', '2002-10-27T06:30:00',\n           '2002-10-27T07:30:00'], dtype='<U38')\n\n    'casting' can be used to specify whether precision can be changed\n\n    >>> np.datetime_as_string(d, unit='h', casting='safe')\n    Traceback (most recent call last):\n        ...\n    TypeError: Cannot create a datetime string as units 'h' from a NumPy\n    datetime with units 'm' according to the rule 'safe'\n    \"\"\"\n    return (arr,)\n", 1690], "/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/numpy/core/numeric.py": ["import functools\nimport itertools\nimport operator\nimport sys\nimport warnings\nimport numbers\n\nimport numpy as np\nfrom . import multiarray\nfrom .multiarray import (\n    _fastCopyAndTranspose as fastCopyAndTranspose, ALLOW_THREADS,\n    BUFSIZE, CLIP, MAXDIMS, MAY_SHARE_BOUNDS, MAY_SHARE_EXACT, RAISE,\n    WRAP, arange, array, asarray, asanyarray, ascontiguousarray,\n    asfortranarray, broadcast, can_cast, compare_chararrays,\n    concatenate, copyto, dot, dtype, empty,\n    empty_like, flatiter, frombuffer, fromfile, fromiter, fromstring,\n    inner, lexsort, matmul, may_share_memory,\n    min_scalar_type, ndarray, nditer, nested_iters, promote_types,\n    putmask, result_type, set_numeric_ops, shares_memory, vdot, where,\n    zeros, normalize_axis_index)\n\nfrom . import overrides\nfrom . import umath\nfrom . import shape_base\nfrom .overrides import set_array_function_like_doc, set_module\nfrom .umath import (multiply, invert, sin, PINF, NAN)\nfrom . import numerictypes\nfrom .numerictypes import longlong, intc, int_, float_, complex_, bool_\nfrom ._exceptions import TooHardError, AxisError\nfrom ._ufunc_config import errstate\n\nbitwise_not = invert\nufunc = type(sin)\nnewaxis = None\n\narray_function_dispatch = functools.partial(\n    overrides.array_function_dispatch, module='numpy')\n\n\n__all__ = [\n    'newaxis', 'ndarray', 'flatiter', 'nditer', 'nested_iters', 'ufunc',\n    'arange', 'array', 'asarray', 'asanyarray', 'ascontiguousarray',\n    'asfortranarray', 'zeros', 'count_nonzero', 'empty', 'broadcast', 'dtype',\n    'fromstring', 'fromfile', 'frombuffer', 'where',\n    'argwhere', 'copyto', 'concatenate', 'fastCopyAndTranspose', 'lexsort',\n    'set_numeric_ops', 'can_cast', 'promote_types', 'min_scalar_type',\n    'result_type', 'isfortran', 'empty_like', 'zeros_like', 'ones_like',\n    'correlate', 'convolve', 'inner', 'dot', 'outer', 'vdot', 'roll',\n    'rollaxis', 'moveaxis', 'cross', 'tensordot', 'little_endian',\n    'fromiter', 'array_equal', 'array_equiv', 'indices', 'fromfunction',\n    'isclose', 'isscalar', 'binary_repr', 'base_repr', 'ones',\n    'identity', 'allclose', 'compare_chararrays', 'putmask',\n    'flatnonzero', 'Inf', 'inf', 'infty', 'Infinity', 'nan', 'NaN',\n    'False_', 'True_', 'bitwise_not', 'CLIP', 'RAISE', 'WRAP', 'MAXDIMS',\n    'BUFSIZE', 'ALLOW_THREADS', 'ComplexWarning', 'full', 'full_like',\n    'matmul', 'shares_memory', 'may_share_memory', 'MAY_SHARE_BOUNDS',\n    'MAY_SHARE_EXACT', 'TooHardError', 'AxisError']\n\n\n@set_module('numpy')\nclass ComplexWarning(RuntimeWarning):\n    \"\"\"\n    The warning raised when casting a complex dtype to a real dtype.\n\n    As implemented, casting a complex number to a real discards its imaginary\n    part, but this behavior may not be what the user actually wants.\n\n    \"\"\"\n    pass\n\n\ndef _zeros_like_dispatcher(a, dtype=None, order=None, subok=None, shape=None):\n    return (a,)\n\n\n@array_function_dispatch(_zeros_like_dispatcher)\ndef zeros_like(a, dtype=None, order='K', subok=True, shape=None):\n    \"\"\"\n    Return an array of zeros with the same shape and type as a given array.\n\n    Parameters\n    ----------\n    a : array_like\n        The shape and data-type of `a` define these same attributes of\n        the returned array.\n    dtype : data-type, optional\n        Overrides the data type of the result.\n\n        .. versionadded:: 1.6.0\n    order : {'C', 'F', 'A', or 'K'}, optional\n        Overrides the memory layout of the result. 'C' means C-order,\n        'F' means F-order, 'A' means 'F' if `a` is Fortran contiguous,\n        'C' otherwise. 'K' means match the layout of `a` as closely\n        as possible.\n\n        .. versionadded:: 1.6.0\n    subok : bool, optional.\n        If True, then the newly created array will use the sub-class\n        type of `a`, otherwise it will be a base-class array. Defaults\n        to True.\n    shape : int or sequence of ints, optional.\n        Overrides the shape of the result. If order='K' and the number of\n        dimensions is unchanged, will try to keep order, otherwise,\n        order='C' is implied.\n\n        .. versionadded:: 1.17.0\n\n    Returns\n    -------\n    out : ndarray\n        Array of zeros with the same shape and type as `a`.\n\n    See Also\n    --------\n    empty_like : Return an empty array with shape and type of input.\n    ones_like : Return an array of ones with shape and type of input.\n    full_like : Return a new array with shape of input filled with value.\n    zeros : Return a new array setting values to zero.\n\n    Examples\n    --------\n    >>> x = np.arange(6)\n    >>> x = x.reshape((2, 3))\n    >>> x\n    array([[0, 1, 2],\n           [3, 4, 5]])\n    >>> np.zeros_like(x)\n    array([[0, 0, 0],\n           [0, 0, 0]])\n\n    >>> y = np.arange(3, dtype=float)\n    >>> y\n    array([0., 1., 2.])\n    >>> np.zeros_like(y)\n    array([0.,  0.,  0.])\n\n    \"\"\"\n    res = empty_like(a, dtype=dtype, order=order, subok=subok, shape=shape)\n    # needed instead of a 0 to get same result as zeros for for string dtypes\n    z = zeros(1, dtype=res.dtype)\n    multiarray.copyto(res, z, casting='unsafe')\n    return res\n\n\ndef _ones_dispatcher(shape, dtype=None, order=None, *, like=None):\n    return(like,)\n\n\n@set_array_function_like_doc\n@set_module('numpy')\ndef ones(shape, dtype=None, order='C', *, like=None):\n    \"\"\"\n    Return a new array of given shape and type, filled with ones.\n\n    Parameters\n    ----------\n    shape : int or sequence of ints\n        Shape of the new array, e.g., ``(2, 3)`` or ``2``.\n    dtype : data-type, optional\n        The desired data-type for the array, e.g., `numpy.int8`.  Default is\n        `numpy.float64`.\n    order : {'C', 'F'}, optional, default: C\n        Whether to store multi-dimensional data in row-major\n        (C-style) or column-major (Fortran-style) order in\n        memory.\n    ${ARRAY_FUNCTION_LIKE}\n\n        .. versionadded:: 1.20.0\n\n    Returns\n    -------\n    out : ndarray\n        Array of ones with the given shape, dtype, and order.\n\n    See Also\n    --------\n    ones_like : Return an array of ones with shape and type of input.\n    empty : Return a new uninitialized array.\n    zeros : Return a new array setting values to zero.\n    full : Return a new array of given shape filled with value.\n\n\n    Examples\n    --------\n    >>> np.ones(5)\n    array([1., 1., 1., 1., 1.])\n\n    >>> np.ones((5,), dtype=int)\n    array([1, 1, 1, 1, 1])\n\n    >>> np.ones((2, 1))\n    array([[1.],\n           [1.]])\n\n    >>> s = (2,2)\n    >>> np.ones(s)\n    array([[1.,  1.],\n           [1.,  1.]])\n\n    \"\"\"\n    if like is not None:\n        return _ones_with_like(shape, dtype=dtype, order=order, like=like)\n\n    a = empty(shape, dtype, order)\n    multiarray.copyto(a, 1, casting='unsafe')\n    return a\n\n\n_ones_with_like = array_function_dispatch(\n    _ones_dispatcher\n)(ones)\n\n\ndef _ones_like_dispatcher(a, dtype=None, order=None, subok=None, shape=None):\n    return (a,)\n\n\n@array_function_dispatch(_ones_like_dispatcher)\ndef ones_like(a, dtype=None, order='K', subok=True, shape=None):\n    \"\"\"\n    Return an array of ones with the same shape and type as a given array.\n\n    Parameters\n    ----------\n    a : array_like\n        The shape and data-type of `a` define these same attributes of\n        the returned array.\n    dtype : data-type, optional\n        Overrides the data type of the result.\n\n        .. versionadded:: 1.6.0\n    order : {'C', 'F', 'A', or 'K'}, optional\n        Overrides the memory layout of the result. 'C' means C-order,\n        'F' means F-order, 'A' means 'F' if `a` is Fortran contiguous,\n        'C' otherwise. 'K' means match the layout of `a` as closely\n        as possible.\n\n        .. versionadded:: 1.6.0\n    subok : bool, optional.\n        If True, then the newly created array will use the sub-class\n        type of `a`, otherwise it will be a base-class array. Defaults\n        to True.\n    shape : int or sequence of ints, optional.\n        Overrides the shape of the result. If order='K' and the number of\n        dimensions is unchanged, will try to keep order, otherwise,\n        order='C' is implied.\n\n        .. versionadded:: 1.17.0\n\n    Returns\n    -------\n    out : ndarray\n        Array of ones with the same shape and type as `a`.\n\n    See Also\n    --------\n    empty_like : Return an empty array with shape and type of input.\n    zeros_like : Return an array of zeros with shape and type of input.\n    full_like : Return a new array with shape of input filled with value.\n    ones : Return a new array setting values to one.\n\n    Examples\n    --------\n    >>> x = np.arange(6)\n    >>> x = x.reshape((2, 3))\n    >>> x\n    array([[0, 1, 2],\n           [3, 4, 5]])\n    >>> np.ones_like(x)\n    array([[1, 1, 1],\n           [1, 1, 1]])\n\n    >>> y = np.arange(3, dtype=float)\n    >>> y\n    array([0., 1., 2.])\n    >>> np.ones_like(y)\n    array([1.,  1.,  1.])\n\n    \"\"\"\n    res = empty_like(a, dtype=dtype, order=order, subok=subok, shape=shape)\n    multiarray.copyto(res, 1, casting='unsafe')\n    return res\n\n\ndef _full_dispatcher(shape, fill_value, dtype=None, order=None, *, like=None):\n    return(like,)\n\n\n@set_array_function_like_doc\n@set_module('numpy')\ndef full(shape, fill_value, dtype=None, order='C', *, like=None):\n    \"\"\"\n    Return a new array of given shape and type, filled with `fill_value`.\n\n    Parameters\n    ----------\n    shape : int or sequence of ints\n        Shape of the new array, e.g., ``(2, 3)`` or ``2``.\n    fill_value : scalar or array_like\n        Fill value.\n    dtype : data-type, optional\n        The desired data-type for the array  The default, None, means\n         ``np.array(fill_value).dtype``.\n    order : {'C', 'F'}, optional\n        Whether to store multidimensional data in C- or Fortran-contiguous\n        (row- or column-wise) order in memory.\n    ${ARRAY_FUNCTION_LIKE}\n\n        .. versionadded:: 1.20.0\n\n    Returns\n    -------\n    out : ndarray\n        Array of `fill_value` with the given shape, dtype, and order.\n\n    See Also\n    --------\n    full_like : Return a new array with shape of input filled with value.\n    empty : Return a new uninitialized array.\n    ones : Return a new array setting values to one.\n    zeros : Return a new array setting values to zero.\n\n    Examples\n    --------\n    >>> np.full((2, 2), np.inf)\n    array([[inf, inf],\n           [inf, inf]])\n    >>> np.full((2, 2), 10)\n    array([[10, 10],\n           [10, 10]])\n\n    >>> np.full((2, 2), [1, 2])\n    array([[1, 2],\n           [1, 2]])\n\n    \"\"\"\n    if like is not None:\n        return _full_with_like(shape, fill_value, dtype=dtype, order=order, like=like)\n\n    if dtype is None:\n        fill_value = asarray(fill_value)\n        dtype = fill_value.dtype\n    a = empty(shape, dtype, order)\n    multiarray.copyto(a, fill_value, casting='unsafe')\n    return a\n\n\n_full_with_like = array_function_dispatch(\n    _full_dispatcher\n)(full)\n\n\ndef _full_like_dispatcher(a, fill_value, dtype=None, order=None, subok=None, shape=None):\n    return (a,)\n\n\n@array_function_dispatch(_full_like_dispatcher)\ndef full_like(a, fill_value, dtype=None, order='K', subok=True, shape=None):\n    \"\"\"\n    Return a full array with the same shape and type as a given array.\n\n    Parameters\n    ----------\n    a : array_like\n        The shape and data-type of `a` define these same attributes of\n        the returned array.\n    fill_value : scalar\n        Fill value.\n    dtype : data-type, optional\n        Overrides the data type of the result.\n    order : {'C', 'F', 'A', or 'K'}, optional\n        Overrides the memory layout of the result. 'C' means C-order,\n        'F' means F-order, 'A' means 'F' if `a` is Fortran contiguous,\n        'C' otherwise. 'K' means match the layout of `a` as closely\n        as possible.\n    subok : bool, optional.\n        If True, then the newly created array will use the sub-class\n        type of `a`, otherwise it will be a base-class array. Defaults\n        to True.\n    shape : int or sequence of ints, optional.\n        Overrides the shape of the result. If order='K' and the number of\n        dimensions is unchanged, will try to keep order, otherwise,\n        order='C' is implied.\n\n        .. versionadded:: 1.17.0\n\n    Returns\n    -------\n    out : ndarray\n        Array of `fill_value` with the same shape and type as `a`.\n\n    See Also\n    --------\n    empty_like : Return an empty array with shape and type of input.\n    ones_like : Return an array of ones with shape and type of input.\n    zeros_like : Return an array of zeros with shape and type of input.\n    full : Return a new array of given shape filled with value.\n\n    Examples\n    --------\n    >>> x = np.arange(6, dtype=int)\n    >>> np.full_like(x, 1)\n    array([1, 1, 1, 1, 1, 1])\n    >>> np.full_like(x, 0.1)\n    array([0, 0, 0, 0, 0, 0])\n    >>> np.full_like(x, 0.1, dtype=np.double)\n    array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1])\n    >>> np.full_like(x, np.nan, dtype=np.double)\n    array([nan, nan, nan, nan, nan, nan])\n\n    >>> y = np.arange(6, dtype=np.double)\n    >>> np.full_like(y, 0.1)\n    array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1])\n\n    \"\"\"\n    res = empty_like(a, dtype=dtype, order=order, subok=subok, shape=shape)\n    multiarray.copyto(res, fill_value, casting='unsafe')\n    return res\n\n\ndef _count_nonzero_dispatcher(a, axis=None, *, keepdims=None):\n    return (a,)\n\n\n@array_function_dispatch(_count_nonzero_dispatcher)\ndef count_nonzero(a, axis=None, *, keepdims=False):\n    \"\"\"\n    Counts the number of non-zero values in the array ``a``.\n\n    The word \"non-zero\" is in reference to the Python 2.x\n    built-in method ``__nonzero__()`` (renamed ``__bool__()``\n    in Python 3.x) of Python objects that tests an object's\n    \"truthfulness\". For example, any number is considered\n    truthful if it is nonzero, whereas any string is considered\n    truthful if it is not the empty string. Thus, this function\n    (recursively) counts how many elements in ``a`` (and in\n    sub-arrays thereof) have their ``__nonzero__()`` or ``__bool__()``\n    method evaluated to ``True``.\n\n    Parameters\n    ----------\n    a : array_like\n        The array for which to count non-zeros.\n    axis : int or tuple, optional\n        Axis or tuple of axes along which to count non-zeros.\n        Default is None, meaning that non-zeros will be counted\n        along a flattened version of ``a``.\n\n        .. versionadded:: 1.12.0\n\n    keepdims : bool, optional\n        If this is set to True, the axes that are counted are left\n        in the result as dimensions with size one. With this option,\n        the result will broadcast correctly against the input array.\n\n        .. versionadded:: 1.19.0\n\n    Returns\n    -------\n    count : int or array of int\n        Number of non-zero values in the array along a given axis.\n        Otherwise, the total number of non-zero values in the array\n        is returned.\n\n    See Also\n    --------\n    nonzero : Return the coordinates of all the non-zero values.\n\n    Examples\n    --------\n    >>> np.count_nonzero(np.eye(4))\n    4\n    >>> a = np.array([[0, 1, 7, 0],\n    ...               [3, 0, 2, 19]])\n    >>> np.count_nonzero(a)\n    5\n    >>> np.count_nonzero(a, axis=0)\n    array([1, 1, 2, 1])\n    >>> np.count_nonzero(a, axis=1)\n    array([2, 3])\n    >>> np.count_nonzero(a, axis=1, keepdims=True)\n    array([[2],\n           [3]])\n    \"\"\"\n    if axis is None and not keepdims:\n        return multiarray.count_nonzero(a)\n\n    a = asanyarray(a)\n\n    # TODO: this works around .astype(bool) not working properly (gh-9847)\n    if np.issubdtype(a.dtype, np.character):\n        a_bool = a != a.dtype.type()\n    else:\n        a_bool = a.astype(np.bool_, copy=False)\n\n    return a_bool.sum(axis=axis, dtype=np.intp, keepdims=keepdims)\n\n\n@set_module('numpy')\ndef isfortran(a):\n    \"\"\"\n    Check if the array is Fortran contiguous but *not* C contiguous.\n\n    This function is obsolete and, because of changes due to relaxed stride\n    checking, its return value for the same array may differ for versions\n    of NumPy >= 1.10.0 and previous versions. If you only want to check if an\n    array is Fortran contiguous use ``a.flags.f_contiguous`` instead.\n\n    Parameters\n    ----------\n    a : ndarray\n        Input array.\n\n    Returns\n    -------\n    isfortran : bool\n        Returns True if the array is Fortran contiguous but *not* C contiguous.\n\n\n    Examples\n    --------\n\n    np.array allows to specify whether the array is written in C-contiguous\n    order (last index varies the fastest), or FORTRAN-contiguous order in\n    memory (first index varies the fastest).\n\n    >>> a = np.array([[1, 2, 3], [4, 5, 6]], order='C')\n    >>> a\n    array([[1, 2, 3],\n           [4, 5, 6]])\n    >>> np.isfortran(a)\n    False\n\n    >>> b = np.array([[1, 2, 3], [4, 5, 6]], order='F')\n    >>> b\n    array([[1, 2, 3],\n           [4, 5, 6]])\n    >>> np.isfortran(b)\n    True\n\n\n    The transpose of a C-ordered array is a FORTRAN-ordered array.\n\n    >>> a = np.array([[1, 2, 3], [4, 5, 6]], order='C')\n    >>> a\n    array([[1, 2, 3],\n           [4, 5, 6]])\n    >>> np.isfortran(a)\n    False\n    >>> b = a.T\n    >>> b\n    array([[1, 4],\n           [2, 5],\n           [3, 6]])\n    >>> np.isfortran(b)\n    True\n\n    C-ordered arrays evaluate as False even if they are also FORTRAN-ordered.\n\n    >>> np.isfortran(np.array([1, 2], order='F'))\n    False\n\n    \"\"\"\n    return a.flags.fnc\n\n\ndef _argwhere_dispatcher(a):\n    return (a,)\n\n\n@array_function_dispatch(_argwhere_dispatcher)\ndef argwhere(a):\n    \"\"\"\n    Find the indices of array elements that are non-zero, grouped by element.\n\n    Parameters\n    ----------\n    a : array_like\n        Input data.\n\n    Returns\n    -------\n    index_array : (N, a.ndim) ndarray\n        Indices of elements that are non-zero. Indices are grouped by element.\n        This array will have shape ``(N, a.ndim)`` where ``N`` is the number of\n        non-zero items.\n\n    See Also\n    --------\n    where, nonzero\n\n    Notes\n    -----\n    ``np.argwhere(a)`` is almost the same as ``np.transpose(np.nonzero(a))``,\n    but produces a result of the correct shape for a 0D array.\n\n    The output of ``argwhere`` is not suitable for indexing arrays.\n    For this purpose use ``nonzero(a)`` instead.\n\n    Examples\n    --------\n    >>> x = np.arange(6).reshape(2,3)\n    >>> x\n    array([[0, 1, 2],\n           [3, 4, 5]])\n    >>> np.argwhere(x>1)\n    array([[0, 2],\n           [1, 0],\n           [1, 1],\n           [1, 2]])\n\n    \"\"\"\n    # nonzero does not behave well on 0d, so promote to 1d\n    if np.ndim(a) == 0:\n        a = shape_base.atleast_1d(a)\n        # then remove the added dimension\n        return argwhere(a)[:,:0]\n    return transpose(nonzero(a))\n\n\ndef _flatnonzero_dispatcher(a):\n    return (a,)\n\n\n@array_function_dispatch(_flatnonzero_dispatcher)\ndef flatnonzero(a):\n    \"\"\"\n    Return indices that are non-zero in the flattened version of a.\n\n    This is equivalent to np.nonzero(np.ravel(a))[0].\n\n    Parameters\n    ----------\n    a : array_like\n        Input data.\n\n    Returns\n    -------\n    res : ndarray\n        Output array, containing the indices of the elements of `a.ravel()`\n        that are non-zero.\n\n    See Also\n    --------\n    nonzero : Return the indices of the non-zero elements of the input array.\n    ravel : Return a 1-D array containing the elements of the input array.\n\n    Examples\n    --------\n    >>> x = np.arange(-2, 3)\n    >>> x\n    array([-2, -1,  0,  1,  2])\n    >>> np.flatnonzero(x)\n    array([0, 1, 3, 4])\n\n    Use the indices of the non-zero elements as an index array to extract\n    these elements:\n\n    >>> x.ravel()[np.flatnonzero(x)]\n    array([-2, -1,  1,  2])\n\n    \"\"\"\n    return np.nonzero(np.ravel(a))[0]\n\n\ndef _correlate_dispatcher(a, v, mode=None):\n    return (a, v)\n\n\n@array_function_dispatch(_correlate_dispatcher)\ndef correlate(a, v, mode='valid'):\n    \"\"\"\n    Cross-correlation of two 1-dimensional sequences.\n\n    This function computes the correlation as generally defined in signal\n    processing texts::\n\n        c_{av}[k] = sum_n a[n+k] * conj(v[n])\n\n    with a and v sequences being zero-padded where necessary and conj being\n    the conjugate.\n\n    Parameters\n    ----------\n    a, v : array_like\n        Input sequences.\n    mode : {'valid', 'same', 'full'}, optional\n        Refer to the `convolve` docstring.  Note that the default\n        is 'valid', unlike `convolve`, which uses 'full'.\n    old_behavior : bool\n        `old_behavior` was removed in NumPy 1.10. If you need the old\n        behavior, use `multiarray.correlate`.\n\n    Returns\n    -------\n    out : ndarray\n        Discrete cross-correlation of `a` and `v`.\n\n    See Also\n    --------\n    convolve : Discrete, linear convolution of two one-dimensional sequences.\n    multiarray.correlate : Old, no conjugate, version of correlate.\n    scipy.signal.correlate : uses FFT which has superior performance on large arrays. \n\n    Notes\n    -----\n    The definition of correlation above is not unique and sometimes correlation\n    may be defined differently. Another common definition is::\n\n        c'_{av}[k] = sum_n a[n] conj(v[n+k])\n\n    which is related to ``c_{av}[k]`` by ``c'_{av}[k] = c_{av}[-k]``.\n\n    `numpy.correlate` may perform slowly in large arrays (i.e. n = 1e5) because it does\n    not use the FFT to compute the convolution; in that case, `scipy.signal.correlate` might\n    be preferable.\n    \n\n    Examples\n    --------\n    >>> np.correlate([1, 2, 3], [0, 1, 0.5])\n    array([3.5])\n    >>> np.correlate([1, 2, 3], [0, 1, 0.5], \"same\")\n    array([2. ,  3.5,  3. ])\n    >>> np.correlate([1, 2, 3], [0, 1, 0.5], \"full\")\n    array([0.5,  2. ,  3.5,  3. ,  0. ])\n\n    Using complex sequences:\n\n    >>> np.correlate([1+1j, 2, 3-1j], [0, 1, 0.5j], 'full')\n    array([ 0.5-0.5j,  1.0+0.j ,  1.5-1.5j,  3.0-1.j ,  0.0+0.j ])\n\n    Note that you get the time reversed, complex conjugated result\n    when the two input sequences change places, i.e.,\n    ``c_{va}[k] = c^{*}_{av}[-k]``:\n\n    >>> np.correlate([0, 1, 0.5j], [1+1j, 2, 3-1j], 'full')\n    array([ 0.0+0.j ,  3.0+1.j ,  1.5+1.5j,  1.0+0.j ,  0.5+0.5j])\n\n    \"\"\"\n    return multiarray.correlate2(a, v, mode)\n\n\ndef _convolve_dispatcher(a, v, mode=None):\n    return (a, v)\n\n\n@array_function_dispatch(_convolve_dispatcher)\ndef convolve(a, v, mode='full'):\n    \"\"\"\n    Returns the discrete, linear convolution of two one-dimensional sequences.\n\n    The convolution operator is often seen in signal processing, where it\n    models the effect of a linear time-invariant system on a signal [1]_.  In\n    probability theory, the sum of two independent random variables is\n    distributed according to the convolution of their individual\n    distributions.\n\n    If `v` is longer than `a`, the arrays are swapped before computation.\n\n    Parameters\n    ----------\n    a : (N,) array_like\n        First one-dimensional input array.\n    v : (M,) array_like\n        Second one-dimensional input array.\n    mode : {'full', 'valid', 'same'}, optional\n        'full':\n          By default, mode is 'full'.  This returns the convolution\n          at each point of overlap, with an output shape of (N+M-1,). At\n          the end-points of the convolution, the signals do not overlap\n          completely, and boundary effects may be seen.\n\n        'same':\n          Mode 'same' returns output of length ``max(M, N)``.  Boundary\n          effects are still visible.\n\n        'valid':\n          Mode 'valid' returns output of length\n          ``max(M, N) - min(M, N) + 1``.  The convolution product is only given\n          for points where the signals overlap completely.  Values outside\n          the signal boundary have no effect.\n\n    Returns\n    -------\n    out : ndarray\n        Discrete, linear convolution of `a` and `v`.\n\n    See Also\n    --------\n    scipy.signal.fftconvolve : Convolve two arrays using the Fast Fourier\n                               Transform.\n    scipy.linalg.toeplitz : Used to construct the convolution operator.\n    polymul : Polynomial multiplication. Same output as convolve, but also\n              accepts poly1d objects as input.\n\n    Notes\n    -----\n    The discrete convolution operation is defined as\n\n    .. math:: (a * v)[n] = \\\\sum_{m = -\\\\infty}^{\\\\infty} a[m] v[n - m]\n\n    It can be shown that a convolution :math:`x(t) * y(t)` in time/space\n    is equivalent to the multiplication :math:`X(f) Y(f)` in the Fourier\n    domain, after appropriate padding (padding is necessary to prevent\n    circular convolution).  Since multiplication is more efficient (faster)\n    than convolution, the function `scipy.signal.fftconvolve` exploits the\n    FFT to calculate the convolution of large data-sets.\n\n    References\n    ----------\n    .. [1] Wikipedia, \"Convolution\",\n        https://en.wikipedia.org/wiki/Convolution\n\n    Examples\n    --------\n    Note how the convolution operator flips the second array\n    before \"sliding\" the two across one another:\n\n    >>> np.convolve([1, 2, 3], [0, 1, 0.5])\n    array([0. , 1. , 2.5, 4. , 1.5])\n\n    Only return the middle values of the convolution.\n    Contains boundary effects, where zeros are taken\n    into account:\n\n    >>> np.convolve([1,2,3],[0,1,0.5], 'same')\n    array([1. ,  2.5,  4. ])\n\n    The two arrays are of the same length, so there\n    is only one position where they completely overlap:\n\n    >>> np.convolve([1,2,3],[0,1,0.5], 'valid')\n    array([2.5])\n\n    \"\"\"\n    a, v = array(a, copy=False, ndmin=1), array(v, copy=False, ndmin=1)\n    if (len(v) > len(a)):\n        a, v = v, a\n    if len(a) == 0:\n        raise ValueError('a cannot be empty')\n    if len(v) == 0:\n        raise ValueError('v cannot be empty')\n    return multiarray.correlate(a, v[::-1], mode)\n\n\ndef _outer_dispatcher(a, b, out=None):\n    return (a, b, out)\n\n\n@array_function_dispatch(_outer_dispatcher)\ndef outer(a, b, out=None):\n    \"\"\"\n    Compute the outer product of two vectors.\n\n    Given two vectors, ``a = [a0, a1, ..., aM]`` and\n    ``b = [b0, b1, ..., bN]``,\n    the outer product [1]_ is::\n\n      [[a0*b0  a0*b1 ... a0*bN ]\n       [a1*b0    .\n       [ ...          .\n       [aM*b0            aM*bN ]]\n\n    Parameters\n    ----------\n    a : (M,) array_like\n        First input vector.  Input is flattened if\n        not already 1-dimensional.\n    b : (N,) array_like\n        Second input vector.  Input is flattened if\n        not already 1-dimensional.\n    out : (M, N) ndarray, optional\n        A location where the result is stored\n\n        .. versionadded:: 1.9.0\n\n    Returns\n    -------\n    out : (M, N) ndarray\n        ``out[i, j] = a[i] * b[j]``\n\n    See also\n    --------\n    inner\n    einsum : ``einsum('i,j->ij', a.ravel(), b.ravel())`` is the equivalent.\n    ufunc.outer : A generalization to dimensions other than 1D and other\n                  operations. ``np.multiply.outer(a.ravel(), b.ravel())``\n                  is the equivalent.\n    tensordot : ``np.tensordot(a.ravel(), b.ravel(), axes=((), ()))``\n                is the equivalent.\n\n    References\n    ----------\n    .. [1] : G. H. Golub and C. F. Van Loan, *Matrix Computations*, 3rd\n             ed., Baltimore, MD, Johns Hopkins University Press, 1996,\n             pg. 8.\n\n    Examples\n    --------\n    Make a (*very* coarse) grid for computing a Mandelbrot set:\n\n    >>> rl = np.outer(np.ones((5,)), np.linspace(-2, 2, 5))\n    >>> rl\n    array([[-2., -1.,  0.,  1.,  2.],\n           [-2., -1.,  0.,  1.,  2.],\n           [-2., -1.,  0.,  1.,  2.],\n           [-2., -1.,  0.,  1.,  2.],\n           [-2., -1.,  0.,  1.,  2.]])\n    >>> im = np.outer(1j*np.linspace(2, -2, 5), np.ones((5,)))\n    >>> im\n    array([[0.+2.j, 0.+2.j, 0.+2.j, 0.+2.j, 0.+2.j],\n           [0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j],\n           [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n           [0.-1.j, 0.-1.j, 0.-1.j, 0.-1.j, 0.-1.j],\n           [0.-2.j, 0.-2.j, 0.-2.j, 0.-2.j, 0.-2.j]])\n    >>> grid = rl + im\n    >>> grid\n    array([[-2.+2.j, -1.+2.j,  0.+2.j,  1.+2.j,  2.+2.j],\n           [-2.+1.j, -1.+1.j,  0.+1.j,  1.+1.j,  2.+1.j],\n           [-2.+0.j, -1.+0.j,  0.+0.j,  1.+0.j,  2.+0.j],\n           [-2.-1.j, -1.-1.j,  0.-1.j,  1.-1.j,  2.-1.j],\n           [-2.-2.j, -1.-2.j,  0.-2.j,  1.-2.j,  2.-2.j]])\n\n    An example using a \"vector\" of letters:\n\n    >>> x = np.array(['a', 'b', 'c'], dtype=object)\n    >>> np.outer(x, [1, 2, 3])\n    array([['a', 'aa', 'aaa'],\n           ['b', 'bb', 'bbb'],\n           ['c', 'cc', 'ccc']], dtype=object)\n\n    \"\"\"\n    a = asarray(a)\n    b = asarray(b)\n    return multiply(a.ravel()[:, newaxis], b.ravel()[newaxis, :], out)\n\n\ndef _tensordot_dispatcher(a, b, axes=None):\n    return (a, b)\n\n\n@array_function_dispatch(_tensordot_dispatcher)\ndef tensordot(a, b, axes=2):\n    \"\"\"\n    Compute tensor dot product along specified axes.\n\n    Given two tensors, `a` and `b`, and an array_like object containing\n    two array_like objects, ``(a_axes, b_axes)``, sum the products of\n    `a`'s and `b`'s elements (components) over the axes specified by\n    ``a_axes`` and ``b_axes``. The third argument can be a single non-negative\n    integer_like scalar, ``N``; if it is such, then the last ``N`` dimensions\n    of `a` and the first ``N`` dimensions of `b` are summed over.\n\n    Parameters\n    ----------\n    a, b : array_like\n        Tensors to \"dot\".\n\n    axes : int or (2,) array_like\n        * integer_like\n          If an int N, sum over the last N axes of `a` and the first N axes\n          of `b` in order. The sizes of the corresponding axes must match.\n        * (2,) array_like\n          Or, a list of axes to be summed over, first sequence applying to `a`,\n          second to `b`. Both elements array_like must be of the same length.\n\n    Returns\n    -------\n    output : ndarray\n        The tensor dot product of the input.\n\n    See Also\n    --------\n    dot, einsum\n\n    Notes\n    -----\n    Three common use cases are:\n        * ``axes = 0`` : tensor product :math:`a\\\\otimes b`\n        * ``axes = 1`` : tensor dot product :math:`a\\\\cdot b`\n        * ``axes = 2`` : (default) tensor double contraction :math:`a:b`\n\n    When `axes` is integer_like, the sequence for evaluation will be: first\n    the -Nth axis in `a` and 0th axis in `b`, and the -1th axis in `a` and\n    Nth axis in `b` last.\n\n    When there is more than one axis to sum over - and they are not the last\n    (first) axes of `a` (`b`) - the argument `axes` should consist of\n    two sequences of the same length, with the first axis to sum over given\n    first in both sequences, the second axis second, and so forth.\n\n    The shape of the result consists of the non-contracted axes of the\n    first tensor, followed by the non-contracted axes of the second.\n\n    Examples\n    --------\n    A \"traditional\" example:\n\n    >>> a = np.arange(60.).reshape(3,4,5)\n    >>> b = np.arange(24.).reshape(4,3,2)\n    >>> c = np.tensordot(a,b, axes=([1,0],[0,1]))\n    >>> c.shape\n    (5, 2)\n    >>> c\n    array([[4400., 4730.],\n           [4532., 4874.],\n           [4664., 5018.],\n           [4796., 5162.],\n           [4928., 5306.]])\n    >>> # A slower but equivalent way of computing the same...\n    >>> d = np.zeros((5,2))\n    >>> for i in range(5):\n    ...   for j in range(2):\n    ...     for k in range(3):\n    ...       for n in range(4):\n    ...         d[i,j] += a[k,n,i] * b[n,k,j]\n    >>> c == d\n    array([[ True,  True],\n           [ True,  True],\n           [ True,  True],\n           [ True,  True],\n           [ True,  True]])\n\n    An extended example taking advantage of the overloading of + and \\\\*:\n\n    >>> a = np.array(range(1, 9))\n    >>> a.shape = (2, 2, 2)\n    >>> A = np.array(('a', 'b', 'c', 'd'), dtype=object)\n    >>> A.shape = (2, 2)\n    >>> a; A\n    array([[[1, 2],\n            [3, 4]],\n           [[5, 6],\n            [7, 8]]])\n    array([['a', 'b'],\n           ['c', 'd']], dtype=object)\n\n    >>> np.tensordot(a, A) # third argument default is 2 for double-contraction\n    array(['abbcccdddd', 'aaaaabbbbbbcccccccdddddddd'], dtype=object)\n\n    >>> np.tensordot(a, A, 1)\n    array([[['acc', 'bdd'],\n            ['aaacccc', 'bbbdddd']],\n           [['aaaaacccccc', 'bbbbbdddddd'],\n            ['aaaaaaacccccccc', 'bbbbbbbdddddddd']]], dtype=object)\n\n    >>> np.tensordot(a, A, 0) # tensor product (result too long to incl.)\n    array([[[[['a', 'b'],\n              ['c', 'd']],\n              ...\n\n    >>> np.tensordot(a, A, (0, 1))\n    array([[['abbbbb', 'cddddd'],\n            ['aabbbbbb', 'ccdddddd']],\n           [['aaabbbbbbb', 'cccddddddd'],\n            ['aaaabbbbbbbb', 'ccccdddddddd']]], dtype=object)\n\n    >>> np.tensordot(a, A, (2, 1))\n    array([[['abb', 'cdd'],\n            ['aaabbbb', 'cccdddd']],\n           [['aaaaabbbbbb', 'cccccdddddd'],\n            ['aaaaaaabbbbbbbb', 'cccccccdddddddd']]], dtype=object)\n\n    >>> np.tensordot(a, A, ((0, 1), (0, 1)))\n    array(['abbbcccccddddddd', 'aabbbbccccccdddddddd'], dtype=object)\n\n    >>> np.tensordot(a, A, ((2, 1), (1, 0)))\n    array(['acccbbdddd', 'aaaaacccccccbbbbbbdddddddd'], dtype=object)\n\n    \"\"\"\n    try:\n        iter(axes)\n    except Exception:\n        axes_a = list(range(-axes, 0))\n        axes_b = list(range(0, axes))\n    else:\n        axes_a, axes_b = axes\n    try:\n        na = len(axes_a)\n        axes_a = list(axes_a)\n    except TypeError:\n        axes_a = [axes_a]\n        na = 1\n    try:\n        nb = len(axes_b)\n        axes_b = list(axes_b)\n    except TypeError:\n        axes_b = [axes_b]\n        nb = 1\n\n    a, b = asarray(a), asarray(b)\n    as_ = a.shape\n    nda = a.ndim\n    bs = b.shape\n    ndb = b.ndim\n    equal = True\n    if na != nb:\n        equal = False\n    else:\n        for k in range(na):\n            if as_[axes_a[k]] != bs[axes_b[k]]:\n                equal = False\n                break\n            if axes_a[k] < 0:\n                axes_a[k] += nda\n            if axes_b[k] < 0:\n                axes_b[k] += ndb\n    if not equal:\n        raise ValueError(\"shape-mismatch for sum\")\n\n    # Move the axes to sum over to the end of \"a\"\n    # and to the front of \"b\"\n    notin = [k for k in range(nda) if k not in axes_a]\n    newaxes_a = notin + axes_a\n    N2 = 1\n    for axis in axes_a:\n        N2 *= as_[axis]\n    newshape_a = (int(multiply.reduce([as_[ax] for ax in notin])), N2)\n    olda = [as_[axis] for axis in notin]\n\n    notin = [k for k in range(ndb) if k not in axes_b]\n    newaxes_b = axes_b + notin\n    N2 = 1\n    for axis in axes_b:\n        N2 *= bs[axis]\n    newshape_b = (N2, int(multiply.reduce([bs[ax] for ax in notin])))\n    oldb = [bs[axis] for axis in notin]\n\n    at = a.transpose(newaxes_a).reshape(newshape_a)\n    bt = b.transpose(newaxes_b).reshape(newshape_b)\n    res = dot(at, bt)\n    return res.reshape(olda + oldb)\n\n\ndef _roll_dispatcher(a, shift, axis=None):\n    return (a,)\n\n\n@array_function_dispatch(_roll_dispatcher)\ndef roll(a, shift, axis=None):\n    \"\"\"\n    Roll array elements along a given axis.\n\n    Elements that roll beyond the last position are re-introduced at\n    the first.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    shift : int or tuple of ints\n        The number of places by which elements are shifted.  If a tuple,\n        then `axis` must be a tuple of the same size, and each of the\n        given axes is shifted by the corresponding number.  If an int\n        while `axis` is a tuple of ints, then the same value is used for\n        all given axes.\n    axis : int or tuple of ints, optional\n        Axis or axes along which elements are shifted.  By default, the\n        array is flattened before shifting, after which the original\n        shape is restored.\n\n    Returns\n    -------\n    res : ndarray\n        Output array, with the same shape as `a`.\n\n    See Also\n    --------\n    rollaxis : Roll the specified axis backwards, until it lies in a\n               given position.\n\n    Notes\n    -----\n    .. versionadded:: 1.12.0\n\n    Supports rolling over multiple dimensions simultaneously.\n\n    Examples\n    --------\n    >>> x = np.arange(10)\n    >>> np.roll(x, 2)\n    array([8, 9, 0, 1, 2, 3, 4, 5, 6, 7])\n    >>> np.roll(x, -2)\n    array([2, 3, 4, 5, 6, 7, 8, 9, 0, 1])\n\n    >>> x2 = np.reshape(x, (2,5))\n    >>> x2\n    array([[0, 1, 2, 3, 4],\n           [5, 6, 7, 8, 9]])\n    >>> np.roll(x2, 1)\n    array([[9, 0, 1, 2, 3],\n           [4, 5, 6, 7, 8]])\n    >>> np.roll(x2, -1)\n    array([[1, 2, 3, 4, 5],\n           [6, 7, 8, 9, 0]])\n    >>> np.roll(x2, 1, axis=0)\n    array([[5, 6, 7, 8, 9],\n           [0, 1, 2, 3, 4]])\n    >>> np.roll(x2, -1, axis=0)\n    array([[5, 6, 7, 8, 9],\n           [0, 1, 2, 3, 4]])\n    >>> np.roll(x2, 1, axis=1)\n    array([[4, 0, 1, 2, 3],\n           [9, 5, 6, 7, 8]])\n    >>> np.roll(x2, -1, axis=1)\n    array([[1, 2, 3, 4, 0],\n           [6, 7, 8, 9, 5]])\n\n    \"\"\"\n    a = asanyarray(a)\n    if axis is None:\n        return roll(a.ravel(), shift, 0).reshape(a.shape)\n\n    else:\n        axis = normalize_axis_tuple(axis, a.ndim, allow_duplicate=True)\n        broadcasted = broadcast(shift, axis)\n        if broadcasted.ndim > 1:\n            raise ValueError(\n                \"'shift' and 'axis' should be scalars or 1D sequences\")\n        shifts = {ax: 0 for ax in range(a.ndim)}\n        for sh, ax in broadcasted:\n            shifts[ax] += sh\n\n        rolls = [((slice(None), slice(None)),)] * a.ndim\n        for ax, offset in shifts.items():\n            offset %= a.shape[ax] or 1  # If `a` is empty, nothing matters.\n            if offset:\n                # (original, result), (original, result)\n                rolls[ax] = ((slice(None, -offset), slice(offset, None)),\n                             (slice(-offset, None), slice(None, offset)))\n\n        result = empty_like(a)\n        for indices in itertools.product(*rolls):\n            arr_index, res_index = zip(*indices)\n            result[res_index] = a[arr_index]\n\n        return result\n\n\ndef _rollaxis_dispatcher(a, axis, start=None):\n    return (a,)\n\n\n@array_function_dispatch(_rollaxis_dispatcher)\ndef rollaxis(a, axis, start=0):\n    \"\"\"\n    Roll the specified axis backwards, until it lies in a given position.\n\n    This function continues to be supported for backward compatibility, but you\n    should prefer `moveaxis`. The `moveaxis` function was added in NumPy\n    1.11.\n\n    Parameters\n    ----------\n    a : ndarray\n        Input array.\n    axis : int\n        The axis to be rolled. The positions of the other axes do not\n        change relative to one another.\n    start : int, optional\n        When ``start <= axis``, the axis is rolled back until it lies in\n        this position. When ``start > axis``, the axis is rolled until it\n        lies before this position. The default, 0, results in a \"complete\"\n        roll. The following table describes how negative values of ``start``\n        are interpreted:\n\n        .. table::\n           :align: left\n\n           +-------------------+----------------------+\n           |     ``start``     | Normalized ``start`` |\n           +===================+======================+\n           | ``-(arr.ndim+1)`` | raise ``AxisError``  |\n           +-------------------+----------------------+\n           | ``-arr.ndim``     | 0                    |\n           +-------------------+----------------------+\n           | |vdots|           | |vdots|              |\n           +-------------------+----------------------+\n           | ``-1``            | ``arr.ndim-1``       |\n           +-------------------+----------------------+\n           | ``0``             | ``0``                |\n           +-------------------+----------------------+\n           | |vdots|           | |vdots|              |\n           +-------------------+----------------------+\n           | ``arr.ndim``      | ``arr.ndim``         |\n           +-------------------+----------------------+\n           | ``arr.ndim + 1``  | raise ``AxisError``  |\n           +-------------------+----------------------+\n           \n        .. |vdots|   unicode:: U+22EE .. Vertical Ellipsis\n\n    Returns\n    -------\n    res : ndarray\n        For NumPy >= 1.10.0 a view of `a` is always returned. For earlier\n        NumPy versions a view of `a` is returned only if the order of the\n        axes is changed, otherwise the input array is returned.\n\n    See Also\n    --------\n    moveaxis : Move array axes to new positions.\n    roll : Roll the elements of an array by a number of positions along a\n        given axis.\n\n    Examples\n    --------\n    >>> a = np.ones((3,4,5,6))\n    >>> np.rollaxis(a, 3, 1).shape\n    (3, 6, 4, 5)\n    >>> np.rollaxis(a, 2).shape\n    (5, 3, 4, 6)\n    >>> np.rollaxis(a, 1, 4).shape\n    (3, 5, 6, 4)\n\n    \"\"\"\n    n = a.ndim\n    axis = normalize_axis_index(axis, n)\n    if start < 0:\n        start += n\n    msg = \"'%s' arg requires %d <= %s < %d, but %d was passed in\"\n    if not (0 <= start < n + 1):\n        raise AxisError(msg % ('start', -n, 'start', n + 1, start))\n    if axis < start:\n        # it's been removed\n        start -= 1\n    if axis == start:\n        return a[...]\n    axes = list(range(0, n))\n    axes.remove(axis)\n    axes.insert(start, axis)\n    return a.transpose(axes)\n\n\ndef normalize_axis_tuple(axis, ndim, argname=None, allow_duplicate=False):\n    \"\"\"\n    Normalizes an axis argument into a tuple of non-negative integer axes.\n\n    This handles shorthands such as ``1`` and converts them to ``(1,)``,\n    as well as performing the handling of negative indices covered by\n    `normalize_axis_index`.\n\n    By default, this forbids axes from being specified multiple times.\n\n    Used internally by multi-axis-checking logic.\n\n    .. versionadded:: 1.13.0\n\n    Parameters\n    ----------\n    axis : int, iterable of int\n        The un-normalized index or indices of the axis.\n    ndim : int\n        The number of dimensions of the array that `axis` should be normalized\n        against.\n    argname : str, optional\n        A prefix to put before the error message, typically the name of the\n        argument.\n    allow_duplicate : bool, optional\n        If False, the default, disallow an axis from being specified twice.\n\n    Returns\n    -------\n    normalized_axes : tuple of int\n        The normalized axis index, such that `0 <= normalized_axis < ndim`\n\n    Raises\n    ------\n    AxisError\n        If any axis provided is out of range\n    ValueError\n        If an axis is repeated\n\n    See also\n    --------\n    normalize_axis_index : normalizing a single scalar axis\n    \"\"\"\n    # Optimization to speed-up the most common cases.\n    if type(axis) not in (tuple, list):\n        try:\n            axis = [operator.index(axis)]\n        except TypeError:\n            pass\n    # Going via an iterator directly is slower than via list comprehension.\n    axis = tuple([normalize_axis_index(ax, ndim, argname) for ax in axis])\n    if not allow_duplicate and len(set(axis)) != len(axis):\n        if argname:\n            raise ValueError('repeated axis in `{}` argument'.format(argname))\n        else:\n            raise ValueError('repeated axis')\n    return axis\n\n\ndef _moveaxis_dispatcher(a, source, destination):\n    return (a,)\n\n\n@array_function_dispatch(_moveaxis_dispatcher)\ndef moveaxis(a, source, destination):\n    \"\"\"\n    Move axes of an array to new positions.\n\n    Other axes remain in their original order.\n\n    .. versionadded:: 1.11.0\n\n    Parameters\n    ----------\n    a : np.ndarray\n        The array whose axes should be reordered.\n    source : int or sequence of int\n        Original positions of the axes to move. These must be unique.\n    destination : int or sequence of int\n        Destination positions for each of the original axes. These must also be\n        unique.\n\n    Returns\n    -------\n    result : np.ndarray\n        Array with moved axes. This array is a view of the input array.\n\n    See Also\n    --------\n    transpose : Permute the dimensions of an array.\n    swapaxes : Interchange two axes of an array.\n\n    Examples\n    --------\n    >>> x = np.zeros((3, 4, 5))\n    >>> np.moveaxis(x, 0, -1).shape\n    (4, 5, 3)\n    >>> np.moveaxis(x, -1, 0).shape\n    (5, 3, 4)\n\n    These all achieve the same result:\n\n    >>> np.transpose(x).shape\n    (5, 4, 3)\n    >>> np.swapaxes(x, 0, -1).shape\n    (5, 4, 3)\n    >>> np.moveaxis(x, [0, 1], [-1, -2]).shape\n    (5, 4, 3)\n    >>> np.moveaxis(x, [0, 1, 2], [-1, -2, -3]).shape\n    (5, 4, 3)\n\n    \"\"\"\n    try:\n        # allow duck-array types if they define transpose\n        transpose = a.transpose\n    except AttributeError:\n        a = asarray(a)\n        transpose = a.transpose\n\n    source = normalize_axis_tuple(source, a.ndim, 'source')\n    destination = normalize_axis_tuple(destination, a.ndim, 'destination')\n    if len(source) != len(destination):\n        raise ValueError('`source` and `destination` arguments must have '\n                         'the same number of elements')\n\n    order = [n for n in range(a.ndim) if n not in source]\n\n    for dest, src in sorted(zip(destination, source)):\n        order.insert(dest, src)\n\n    result = transpose(order)\n    return result\n\n\n# fix hack in scipy which imports this function\ndef _move_axis_to_0(a, axis):\n    return moveaxis(a, axis, 0)\n\n\ndef _cross_dispatcher(a, b, axisa=None, axisb=None, axisc=None, axis=None):\n    return (a, b)\n\n\n@array_function_dispatch(_cross_dispatcher)\ndef cross(a, b, axisa=-1, axisb=-1, axisc=-1, axis=None):\n    \"\"\"\n    Return the cross product of two (arrays of) vectors.\n\n    The cross product of `a` and `b` in :math:`R^3` is a vector perpendicular\n    to both `a` and `b`.  If `a` and `b` are arrays of vectors, the vectors\n    are defined by the last axis of `a` and `b` by default, and these axes\n    can have dimensions 2 or 3.  Where the dimension of either `a` or `b` is\n    2, the third component of the input vector is assumed to be zero and the\n    cross product calculated accordingly.  In cases where both input vectors\n    have dimension 2, the z-component of the cross product is returned.\n\n    Parameters\n    ----------\n    a : array_like\n        Components of the first vector(s).\n    b : array_like\n        Components of the second vector(s).\n    axisa : int, optional\n        Axis of `a` that defines the vector(s).  By default, the last axis.\n    axisb : int, optional\n        Axis of `b` that defines the vector(s).  By default, the last axis.\n    axisc : int, optional\n        Axis of `c` containing the cross product vector(s).  Ignored if\n        both input vectors have dimension 2, as the return is scalar.\n        By default, the last axis.\n    axis : int, optional\n        If defined, the axis of `a`, `b` and `c` that defines the vector(s)\n        and cross product(s).  Overrides `axisa`, `axisb` and `axisc`.\n\n    Returns\n    -------\n    c : ndarray\n        Vector cross product(s).\n\n    Raises\n    ------\n    ValueError\n        When the dimension of the vector(s) in `a` and/or `b` does not\n        equal 2 or 3.\n\n    See Also\n    --------\n    inner : Inner product\n    outer : Outer product.\n    ix_ : Construct index arrays.\n\n    Notes\n    -----\n    .. versionadded:: 1.9.0\n\n    Supports full broadcasting of the inputs.\n\n    Examples\n    --------\n    Vector cross-product.\n\n    >>> x = [1, 2, 3]\n    >>> y = [4, 5, 6]\n    >>> np.cross(x, y)\n    array([-3,  6, -3])\n\n    One vector with dimension 2.\n\n    >>> x = [1, 2]\n    >>> y = [4, 5, 6]\n    >>> np.cross(x, y)\n    array([12, -6, -3])\n\n    Equivalently:\n\n    >>> x = [1, 2, 0]\n    >>> y = [4, 5, 6]\n    >>> np.cross(x, y)\n    array([12, -6, -3])\n\n    Both vectors with dimension 2.\n\n    >>> x = [1,2]\n    >>> y = [4,5]\n    >>> np.cross(x, y)\n    array(-3)\n\n    Multiple vector cross-products. Note that the direction of the cross\n    product vector is defined by the `right-hand rule`.\n\n    >>> x = np.array([[1,2,3], [4,5,6]])\n    >>> y = np.array([[4,5,6], [1,2,3]])\n    >>> np.cross(x, y)\n    array([[-3,  6, -3],\n           [ 3, -6,  3]])\n\n    The orientation of `c` can be changed using the `axisc` keyword.\n\n    >>> np.cross(x, y, axisc=0)\n    array([[-3,  3],\n           [ 6, -6],\n           [-3,  3]])\n\n    Change the vector definition of `x` and `y` using `axisa` and `axisb`.\n\n    >>> x = np.array([[1,2,3], [4,5,6], [7, 8, 9]])\n    >>> y = np.array([[7, 8, 9], [4,5,6], [1,2,3]])\n    >>> np.cross(x, y)\n    array([[ -6,  12,  -6],\n           [  0,   0,   0],\n           [  6, -12,   6]])\n    >>> np.cross(x, y, axisa=0, axisb=0)\n    array([[-24,  48, -24],\n           [-30,  60, -30],\n           [-36,  72, -36]])\n\n    \"\"\"\n    if axis is not None:\n        axisa, axisb, axisc = (axis,) * 3\n    a = asarray(a)\n    b = asarray(b)\n    # Check axisa and axisb are within bounds\n    axisa = normalize_axis_index(axisa, a.ndim, msg_prefix='axisa')\n    axisb = normalize_axis_index(axisb, b.ndim, msg_prefix='axisb')\n\n    # Move working axis to the end of the shape\n    a = moveaxis(a, axisa, -1)\n    b = moveaxis(b, axisb, -1)\n    msg = (\"incompatible dimensions for cross product\\n\"\n           \"(dimension must be 2 or 3)\")\n    if a.shape[-1] not in (2, 3) or b.shape[-1] not in (2, 3):\n        raise ValueError(msg)\n\n    # Create the output array\n    shape = broadcast(a[..., 0], b[..., 0]).shape\n    if a.shape[-1] == 3 or b.shape[-1] == 3:\n        shape += (3,)\n        # Check axisc is within bounds\n        axisc = normalize_axis_index(axisc, len(shape), msg_prefix='axisc')\n    dtype = promote_types(a.dtype, b.dtype)\n    cp = empty(shape, dtype)\n\n    # create local aliases for readability\n    a0 = a[..., 0]\n    a1 = a[..., 1]\n    if a.shape[-1] == 3:\n        a2 = a[..., 2]\n    b0 = b[..., 0]\n    b1 = b[..., 1]\n    if b.shape[-1] == 3:\n        b2 = b[..., 2]\n    if cp.ndim != 0 and cp.shape[-1] == 3:\n        cp0 = cp[..., 0]\n        cp1 = cp[..., 1]\n        cp2 = cp[..., 2]\n\n    if a.shape[-1] == 2:\n        if b.shape[-1] == 2:\n            # a0 * b1 - a1 * b0\n            multiply(a0, b1, out=cp)\n            cp -= a1 * b0\n            return cp\n        else:\n            assert b.shape[-1] == 3\n            # cp0 = a1 * b2 - 0  (a2 = 0)\n            # cp1 = 0 - a0 * b2  (a2 = 0)\n            # cp2 = a0 * b1 - a1 * b0\n            multiply(a1, b2, out=cp0)\n            multiply(a0, b2, out=cp1)\n            negative(cp1, out=cp1)\n            multiply(a0, b1, out=cp2)\n            cp2 -= a1 * b0\n    else:\n        assert a.shape[-1] == 3\n        if b.shape[-1] == 3:\n            # cp0 = a1 * b2 - a2 * b1\n            # cp1 = a2 * b0 - a0 * b2\n            # cp2 = a0 * b1 - a1 * b0\n            multiply(a1, b2, out=cp0)\n            tmp = array(a2 * b1)\n            cp0 -= tmp\n            multiply(a2, b0, out=cp1)\n            multiply(a0, b2, out=tmp)\n            cp1 -= tmp\n            multiply(a0, b1, out=cp2)\n            multiply(a1, b0, out=tmp)\n            cp2 -= tmp\n        else:\n            assert b.shape[-1] == 2\n            # cp0 = 0 - a2 * b1  (b2 = 0)\n            # cp1 = a2 * b0 - 0  (b2 = 0)\n            # cp2 = a0 * b1 - a1 * b0\n            multiply(a2, b1, out=cp0)\n            negative(cp0, out=cp0)\n            multiply(a2, b0, out=cp1)\n            multiply(a0, b1, out=cp2)\n            cp2 -= a1 * b0\n\n    return moveaxis(cp, -1, axisc)\n\n\nlittle_endian = (sys.byteorder == 'little')\n\n\n@set_module('numpy')\ndef indices(dimensions, dtype=int, sparse=False):\n    \"\"\"\n    Return an array representing the indices of a grid.\n\n    Compute an array where the subarrays contain index values 0, 1, ...\n    varying only along the corresponding axis.\n\n    Parameters\n    ----------\n    dimensions : sequence of ints\n        The shape of the grid.\n    dtype : dtype, optional\n        Data type of the result.\n    sparse : boolean, optional\n        Return a sparse representation of the grid instead of a dense\n        representation. Default is False.\n\n        .. versionadded:: 1.17\n\n    Returns\n    -------\n    grid : one ndarray or tuple of ndarrays\n        If sparse is False:\n            Returns one array of grid indices,\n            ``grid.shape = (len(dimensions),) + tuple(dimensions)``.\n        If sparse is True:\n            Returns a tuple of arrays, with\n            ``grid[i].shape = (1, ..., 1, dimensions[i], 1, ..., 1)`` with\n            dimensions[i] in the ith place\n\n    See Also\n    --------\n    mgrid, ogrid, meshgrid\n\n    Notes\n    -----\n    The output shape in the dense case is obtained by prepending the number\n    of dimensions in front of the tuple of dimensions, i.e. if `dimensions`\n    is a tuple ``(r0, ..., rN-1)`` of length ``N``, the output shape is\n    ``(N, r0, ..., rN-1)``.\n\n    The subarrays ``grid[k]`` contains the N-D array of indices along the\n    ``k-th`` axis. Explicitly::\n\n        grid[k, i0, i1, ..., iN-1] = ik\n\n    Examples\n    --------\n    >>> grid = np.indices((2, 3))\n    >>> grid.shape\n    (2, 2, 3)\n    >>> grid[0]        # row indices\n    array([[0, 0, 0],\n           [1, 1, 1]])\n    >>> grid[1]        # column indices\n    array([[0, 1, 2],\n           [0, 1, 2]])\n\n    The indices can be used as an index into an array.\n\n    >>> x = np.arange(20).reshape(5, 4)\n    >>> row, col = np.indices((2, 3))\n    >>> x[row, col]\n    array([[0, 1, 2],\n           [4, 5, 6]])\n\n    Note that it would be more straightforward in the above example to\n    extract the required elements directly with ``x[:2, :3]``.\n\n    If sparse is set to true, the grid will be returned in a sparse\n    representation.\n\n    >>> i, j = np.indices((2, 3), sparse=True)\n    >>> i.shape\n    (2, 1)\n    >>> j.shape\n    (1, 3)\n    >>> i        # row indices\n    array([[0],\n           [1]])\n    >>> j        # column indices\n    array([[0, 1, 2]])\n\n    \"\"\"\n    dimensions = tuple(dimensions)\n    N = len(dimensions)\n    shape = (1,)*N\n    if sparse:\n        res = tuple()\n    else:\n        res = empty((N,)+dimensions, dtype=dtype)\n    for i, dim in enumerate(dimensions):\n        idx = arange(dim, dtype=dtype).reshape(\n            shape[:i] + (dim,) + shape[i+1:]\n        )\n        if sparse:\n            res = res + (idx,)\n        else:\n            res[i] = idx\n    return res\n\n\ndef _fromfunction_dispatcher(function, shape, *, dtype=None, like=None, **kwargs):\n    return (like,)\n\n\n@set_array_function_like_doc\n@set_module('numpy')\ndef fromfunction(function, shape, *, dtype=float, like=None, **kwargs):\n    \"\"\"\n    Construct an array by executing a function over each coordinate.\n\n    The resulting array therefore has a value ``fn(x, y, z)`` at\n    coordinate ``(x, y, z)``.\n\n    Parameters\n    ----------\n    function : callable\n        The function is called with N parameters, where N is the rank of\n        `shape`.  Each parameter represents the coordinates of the array\n        varying along a specific axis.  For example, if `shape`\n        were ``(2, 2)``, then the parameters would be\n        ``array([[0, 0], [1, 1]])`` and ``array([[0, 1], [0, 1]])``\n    shape : (N,) tuple of ints\n        Shape of the output array, which also determines the shape of\n        the coordinate arrays passed to `function`.\n    dtype : data-type, optional\n        Data-type of the coordinate arrays passed to `function`.\n        By default, `dtype` is float.\n    ${ARRAY_FUNCTION_LIKE}\n\n        .. versionadded:: 1.20.0\n\n    Returns\n    -------\n    fromfunction : any\n        The result of the call to `function` is passed back directly.\n        Therefore the shape of `fromfunction` is completely determined by\n        `function`.  If `function` returns a scalar value, the shape of\n        `fromfunction` would not match the `shape` parameter.\n\n    See Also\n    --------\n    indices, meshgrid\n\n    Notes\n    -----\n    Keywords other than `dtype` are passed to `function`.\n\n    Examples\n    --------\n    >>> np.fromfunction(lambda i, j: i == j, (3, 3), dtype=int)\n    array([[ True, False, False],\n           [False,  True, False],\n           [False, False,  True]])\n\n    >>> np.fromfunction(lambda i, j: i + j, (3, 3), dtype=int)\n    array([[0, 1, 2],\n           [1, 2, 3],\n           [2, 3, 4]])\n\n    \"\"\"\n    if like is not None:\n        return _fromfunction_with_like(function, shape, dtype=dtype, like=like, **kwargs)\n\n    args = indices(shape, dtype=dtype)\n    return function(*args, **kwargs)\n\n\n_fromfunction_with_like = array_function_dispatch(\n    _fromfunction_dispatcher\n)(fromfunction)\n\n\ndef _frombuffer(buf, dtype, shape, order):\n    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)\n\n\n@set_module('numpy')\ndef isscalar(element):\n    \"\"\"\n    Returns True if the type of `element` is a scalar type.\n\n    Parameters\n    ----------\n    element : any\n        Input argument, can be of any type and shape.\n\n    Returns\n    -------\n    val : bool\n        True if `element` is a scalar type, False if it is not.\n\n    See Also\n    --------\n    ndim : Get the number of dimensions of an array\n\n    Notes\n    -----\n    If you need a stricter way to identify a *numerical* scalar, use\n    ``isinstance(x, numbers.Number)``, as that returns ``False`` for most\n    non-numerical elements such as strings.\n\n    In most cases ``np.ndim(x) == 0`` should be used instead of this function,\n    as that will also return true for 0d arrays. This is how numpy overloads\n    functions in the style of the ``dx`` arguments to `gradient` and the ``bins``\n    argument to `histogram`. Some key differences:\n\n    +--------------------------------------+---------------+-------------------+\n    | x                                    |``isscalar(x)``|``np.ndim(x) == 0``|\n    +======================================+===============+===================+\n    | PEP 3141 numeric objects (including  | ``True``      | ``True``          |\n    | builtins)                            |               |                   |\n    +--------------------------------------+---------------+-------------------+\n    | builtin string and buffer objects    | ``True``      | ``True``          |\n    +--------------------------------------+---------------+-------------------+\n    | other builtin objects, like          | ``False``     | ``True``          |\n    | `pathlib.Path`, `Exception`,         |               |                   |\n    | the result of `re.compile`           |               |                   |\n    +--------------------------------------+---------------+-------------------+\n    | third-party objects like             | ``False``     | ``True``          |\n    | `matplotlib.figure.Figure`           |               |                   |\n    +--------------------------------------+---------------+-------------------+\n    | zero-dimensional numpy arrays        | ``False``     | ``True``          |\n    +--------------------------------------+---------------+-------------------+\n    | other numpy arrays                   | ``False``     | ``False``         |\n    +--------------------------------------+---------------+-------------------+\n    | `list`, `tuple`, and other sequence  | ``False``     | ``False``         |\n    | objects                              |               |                   |\n    +--------------------------------------+---------------+-------------------+\n\n    Examples\n    --------\n    >>> np.isscalar(3.1)\n    True\n    >>> np.isscalar(np.array(3.1))\n    False\n    >>> np.isscalar([3.1])\n    False\n    >>> np.isscalar(False)\n    True\n    >>> np.isscalar('numpy')\n    True\n\n    NumPy supports PEP 3141 numbers:\n\n    >>> from fractions import Fraction\n    >>> np.isscalar(Fraction(5, 17))\n    True\n    >>> from numbers import Number\n    >>> np.isscalar(Number())\n    True\n\n    \"\"\"\n    return (isinstance(element, generic)\n            or type(element) in ScalarType\n            or isinstance(element, numbers.Number))\n\n\n@set_module('numpy')\ndef binary_repr(num, width=None):\n    \"\"\"\n    Return the binary representation of the input number as a string.\n\n    For negative numbers, if width is not given, a minus sign is added to the\n    front. If width is given, the two's complement of the number is\n    returned, with respect to that width.\n\n    In a two's-complement system negative numbers are represented by the two's\n    complement of the absolute value. This is the most common method of\n    representing signed integers on computers [1]_. A N-bit two's-complement\n    system can represent every integer in the range\n    :math:`-2^{N-1}` to :math:`+2^{N-1}-1`.\n\n    Parameters\n    ----------\n    num : int\n        Only an integer decimal number can be used.\n    width : int, optional\n        The length of the returned string if `num` is positive, or the length\n        of the two's complement if `num` is negative, provided that `width` is\n        at least a sufficient number of bits for `num` to be represented in the\n        designated form.\n\n        If the `width` value is insufficient, it will be ignored, and `num` will\n        be returned in binary (`num` > 0) or two's complement (`num` < 0) form\n        with its width equal to the minimum number of bits needed to represent\n        the number in the designated form. This behavior is deprecated and will\n        later raise an error.\n\n        .. deprecated:: 1.12.0\n\n    Returns\n    -------\n    bin : str\n        Binary representation of `num` or two's complement of `num`.\n\n    See Also\n    --------\n    base_repr: Return a string representation of a number in the given base\n               system.\n    bin: Python's built-in binary representation generator of an integer.\n\n    Notes\n    -----\n    `binary_repr` is equivalent to using `base_repr` with base 2, but about 25x\n    faster.\n\n    References\n    ----------\n    .. [1] Wikipedia, \"Two's complement\",\n        https://en.wikipedia.org/wiki/Two's_complement\n\n    Examples\n    --------\n    >>> np.binary_repr(3)\n    '11'\n    >>> np.binary_repr(-3)\n    '-11'\n    >>> np.binary_repr(3, width=4)\n    '0011'\n\n    The two's complement is returned when the input number is negative and\n    width is specified:\n\n    >>> np.binary_repr(-3, width=3)\n    '101'\n    >>> np.binary_repr(-3, width=5)\n    '11101'\n\n    \"\"\"\n    def warn_if_insufficient(width, binwidth):\n        if width is not None and width < binwidth:\n            warnings.warn(\n                \"Insufficient bit width provided. This behavior \"\n                \"will raise an error in the future.\", DeprecationWarning,\n                stacklevel=3)\n\n    # Ensure that num is a Python integer to avoid overflow or unwanted\n    # casts to floating point.\n    num = operator.index(num)\n\n    if num == 0:\n        return '0' * (width or 1)\n\n    elif num > 0:\n        binary = bin(num)[2:]\n        binwidth = len(binary)\n        outwidth = (binwidth if width is None\n                    else max(binwidth, width))\n        warn_if_insufficient(width, binwidth)\n        return binary.zfill(outwidth)\n\n    else:\n        if width is None:\n            return '-' + bin(-num)[2:]\n\n        else:\n            poswidth = len(bin(-num)[2:])\n\n            # See gh-8679: remove extra digit\n            # for numbers at boundaries.\n            if 2**(poswidth - 1) == -num:\n                poswidth -= 1\n\n            twocomp = 2**(poswidth + 1) + num\n            binary = bin(twocomp)[2:]\n            binwidth = len(binary)\n\n            outwidth = max(binwidth, width)\n            warn_if_insufficient(width, binwidth)\n            return '1' * (outwidth - binwidth) + binary\n\n\n@set_module('numpy')\ndef base_repr(number, base=2, padding=0):\n    \"\"\"\n    Return a string representation of a number in the given base system.\n\n    Parameters\n    ----------\n    number : int\n        The value to convert. Positive and negative values are handled.\n    base : int, optional\n        Convert `number` to the `base` number system. The valid range is 2-36,\n        the default value is 2.\n    padding : int, optional\n        Number of zeros padded on the left. Default is 0 (no padding).\n\n    Returns\n    -------\n    out : str\n        String representation of `number` in `base` system.\n\n    See Also\n    --------\n    binary_repr : Faster version of `base_repr` for base 2.\n\n    Examples\n    --------\n    >>> np.base_repr(5)\n    '101'\n    >>> np.base_repr(6, 5)\n    '11'\n    >>> np.base_repr(7, base=5, padding=3)\n    '00012'\n\n    >>> np.base_repr(10, base=16)\n    'A'\n    >>> np.base_repr(32, base=16)\n    '20'\n\n    \"\"\"\n    digits = '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n    if base > len(digits):\n        raise ValueError(\"Bases greater than 36 not handled in base_repr.\")\n    elif base < 2:\n        raise ValueError(\"Bases less than 2 not handled in base_repr.\")\n\n    num = abs(number)\n    res = []\n    while num:\n        res.append(digits[num % base])\n        num //= base\n    if padding:\n        res.append('0' * padding)\n    if number < 0:\n        res.append('-')\n    return ''.join(reversed(res or '0'))\n\n\n# These are all essentially abbreviations\n# These might wind up in a special abbreviations module\n\n\ndef _maketup(descr, val):\n    dt = dtype(descr)\n    # Place val in all scalar tuples:\n    fields = dt.fields\n    if fields is None:\n        return val\n    else:\n        res = [_maketup(fields[name][0], val) for name in dt.names]\n        return tuple(res)\n\n\ndef _identity_dispatcher(n, dtype=None, *, like=None):\n    return (like,)\n\n\n@set_array_function_like_doc\n@set_module('numpy')\ndef identity(n, dtype=None, *, like=None):\n    \"\"\"\n    Return the identity array.\n\n    The identity array is a square array with ones on\n    the main diagonal.\n\n    Parameters\n    ----------\n    n : int\n        Number of rows (and columns) in `n` x `n` output.\n    dtype : data-type, optional\n        Data-type of the output.  Defaults to ``float``.\n    ${ARRAY_FUNCTION_LIKE}\n\n        .. versionadded:: 1.20.0\n\n    Returns\n    -------\n    out : ndarray\n        `n` x `n` array with its main diagonal set to one,\n        and all other elements 0.\n\n    Examples\n    --------\n    >>> np.identity(3)\n    array([[1.,  0.,  0.],\n           [0.,  1.,  0.],\n           [0.,  0.,  1.]])\n\n    \"\"\"\n    if like is not None:\n        return _identity_with_like(n, dtype=dtype, like=like)\n\n    from numpy import eye\n    return eye(n, dtype=dtype, like=like)\n\n\n_identity_with_like = array_function_dispatch(\n    _identity_dispatcher\n)(identity)\n\n\ndef _allclose_dispatcher(a, b, rtol=None, atol=None, equal_nan=None):\n    return (a, b)\n\n\n@array_function_dispatch(_allclose_dispatcher)\ndef allclose(a, b, rtol=1.e-5, atol=1.e-8, equal_nan=False):\n    \"\"\"\n    Returns True if two arrays are element-wise equal within a tolerance.\n\n    The tolerance values are positive, typically very small numbers.  The\n    relative difference (`rtol` * abs(`b`)) and the absolute difference\n    `atol` are added together to compare against the absolute difference\n    between `a` and `b`.\n\n    NaNs are treated as equal if they are in the same place and if\n    ``equal_nan=True``.  Infs are treated as equal if they are in the same\n    place and of the same sign in both arrays.\n\n    Parameters\n    ----------\n    a, b : array_like\n        Input arrays to compare.\n    rtol : float\n        The relative tolerance parameter (see Notes).\n    atol : float\n        The absolute tolerance parameter (see Notes).\n    equal_nan : bool\n        Whether to compare NaN's as equal.  If True, NaN's in `a` will be\n        considered equal to NaN's in `b` in the output array.\n\n        .. versionadded:: 1.10.0\n\n    Returns\n    -------\n    allclose : bool\n        Returns True if the two arrays are equal within the given\n        tolerance; False otherwise.\n\n    See Also\n    --------\n    isclose, all, any, equal\n\n    Notes\n    -----\n    If the following equation is element-wise True, then allclose returns\n    True.\n\n     absolute(`a` - `b`) <= (`atol` + `rtol` * absolute(`b`))\n\n    The above equation is not symmetric in `a` and `b`, so that\n    ``allclose(a, b)`` might be different from ``allclose(b, a)`` in\n    some rare cases.\n\n    The comparison of `a` and `b` uses standard broadcasting, which\n    means that `a` and `b` need not have the same shape in order for\n    ``allclose(a, b)`` to evaluate to True.  The same is true for\n    `equal` but not `array_equal`.\n\n    `allclose` is not defined for non-numeric data types.\n\n    Examples\n    --------\n    >>> np.allclose([1e10,1e-7], [1.00001e10,1e-8])\n    False\n    >>> np.allclose([1e10,1e-8], [1.00001e10,1e-9])\n    True\n    >>> np.allclose([1e10,1e-8], [1.0001e10,1e-9])\n    False\n    >>> np.allclose([1.0, np.nan], [1.0, np.nan])\n    False\n    >>> np.allclose([1.0, np.nan], [1.0, np.nan], equal_nan=True)\n    True\n\n    \"\"\"\n    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))\n    return bool(res)\n\n\ndef _isclose_dispatcher(a, b, rtol=None, atol=None, equal_nan=None):\n    return (a, b)\n\n\n@array_function_dispatch(_isclose_dispatcher)\ndef isclose(a, b, rtol=1.e-5, atol=1.e-8, equal_nan=False):\n    \"\"\"\n    Returns a boolean array where two arrays are element-wise equal within a\n    tolerance.\n\n    The tolerance values are positive, typically very small numbers.  The\n    relative difference (`rtol` * abs(`b`)) and the absolute difference\n    `atol` are added together to compare against the absolute difference\n    between `a` and `b`.\n\n    .. warning:: The default `atol` is not appropriate for comparing numbers\n                 that are much smaller than one (see Notes).\n\n    Parameters\n    ----------\n    a, b : array_like\n        Input arrays to compare.\n    rtol : float\n        The relative tolerance parameter (see Notes).\n    atol : float\n        The absolute tolerance parameter (see Notes).\n    equal_nan : bool\n        Whether to compare NaN's as equal.  If True, NaN's in `a` will be\n        considered equal to NaN's in `b` in the output array.\n\n    Returns\n    -------\n    y : array_like\n        Returns a boolean array of where `a` and `b` are equal within the\n        given tolerance. If both `a` and `b` are scalars, returns a single\n        boolean value.\n\n    See Also\n    --------\n    allclose\n    math.isclose\n\n    Notes\n    -----\n    .. versionadded:: 1.7.0\n\n    For finite values, isclose uses the following equation to test whether\n    two floating point values are equivalent.\n\n     absolute(`a` - `b`) <= (`atol` + `rtol` * absolute(`b`))\n\n    Unlike the built-in `math.isclose`, the above equation is not symmetric\n    in `a` and `b` -- it assumes `b` is the reference value -- so that\n    `isclose(a, b)` might be different from `isclose(b, a)`. Furthermore,\n    the default value of atol is not zero, and is used to determine what\n    small values should be considered close to zero. The default value is\n    appropriate for expected values of order unity: if the expected values\n    are significantly smaller than one, it can result in false positives.\n    `atol` should be carefully selected for the use case at hand. A zero value\n    for `atol` will result in `False` if either `a` or `b` is zero.\n\n    `isclose` is not defined for non-numeric data types.\n\n    Examples\n    --------\n    >>> np.isclose([1e10,1e-7], [1.00001e10,1e-8])\n    array([ True, False])\n    >>> np.isclose([1e10,1e-8], [1.00001e10,1e-9])\n    array([ True, True])\n    >>> np.isclose([1e10,1e-8], [1.0001e10,1e-9])\n    array([False,  True])\n    >>> np.isclose([1.0, np.nan], [1.0, np.nan])\n    array([ True, False])\n    >>> np.isclose([1.0, np.nan], [1.0, np.nan], equal_nan=True)\n    array([ True, True])\n    >>> np.isclose([1e-8, 1e-7], [0.0, 0.0])\n    array([ True, False])\n    >>> np.isclose([1e-100, 1e-7], [0.0, 0.0], atol=0.0)\n    array([False, False])\n    >>> np.isclose([1e-10, 1e-10], [1e-20, 0.0])\n    array([ True,  True])\n    >>> np.isclose([1e-10, 1e-10], [1e-20, 0.999999e-10], atol=0.0)\n    array([False,  True])\n    \"\"\"\n    def within_tol(x, y, atol, rtol):\n        with errstate(invalid='ignore'):\n            return less_equal(abs(x-y), atol + rtol * abs(y))\n\n    x = asanyarray(a)\n    y = asanyarray(b)\n\n    # Make sure y is an inexact type to avoid bad behavior on abs(MIN_INT).\n    # This will cause casting of x later. Also, make sure to allow subclasses\n    # (e.g., for numpy.ma).\n    # NOTE: We explicitly allow timedelta, which used to work. This could\n    #       possibly be deprecated. See also gh-18286.\n    #       timedelta works if `atol` is an integer or also a timedelta.\n    #       Although, the default tolerances are unlikely to be useful\n    if y.dtype.kind != \"m\":\n        dt = multiarray.result_type(y, 1.)\n        y = asanyarray(y, dtype=dt)\n\n    xfin = isfinite(x)\n    yfin = isfinite(y)\n    if all(xfin) and all(yfin):\n        return within_tol(x, y, atol, rtol)\n    else:\n        finite = xfin & yfin\n        cond = zeros_like(finite, subok=True)\n        # Because we're using boolean indexing, x & y must be the same shape.\n        # Ideally, we'd just do x, y = broadcast_arrays(x, y). It's in\n        # lib.stride_tricks, though, so we can't import it here.\n        x = x * ones_like(cond)\n        y = y * ones_like(cond)\n        # Avoid subtraction with infinite/nan values...\n        cond[finite] = within_tol(x[finite], y[finite], atol, rtol)\n        # Check for equality of infinite values...\n        cond[~finite] = (x[~finite] == y[~finite])\n        if equal_nan:\n            # Make NaN == NaN\n            both_nan = isnan(x) & isnan(y)\n\n            # Needed to treat masked arrays correctly. = True would not work.\n            cond[both_nan] = both_nan[both_nan]\n\n        return cond[()]  # Flatten 0d arrays to scalars\n\n\ndef _array_equal_dispatcher(a1, a2, equal_nan=None):\n    return (a1, a2)\n\n\n@array_function_dispatch(_array_equal_dispatcher)\ndef array_equal(a1, a2, equal_nan=False):\n    \"\"\"\n    True if two arrays have the same shape and elements, False otherwise.\n\n    Parameters\n    ----------\n    a1, a2 : array_like\n        Input arrays.\n    equal_nan : bool\n        Whether to compare NaN's as equal. If the dtype of a1 and a2 is\n        complex, values will be considered equal if either the real or the\n        imaginary component of a given value is ``nan``.\n\n        .. versionadded:: 1.19.0\n\n    Returns\n    -------\n    b : bool\n        Returns True if the arrays are equal.\n\n    See Also\n    --------\n    allclose: Returns True if two arrays are element-wise equal within a\n              tolerance.\n    array_equiv: Returns True if input arrays are shape consistent and all\n                 elements equal.\n\n    Examples\n    --------\n    >>> np.array_equal([1, 2], [1, 2])\n    True\n    >>> np.array_equal(np.array([1, 2]), np.array([1, 2]))\n    True\n    >>> np.array_equal([1, 2], [1, 2, 3])\n    False\n    >>> np.array_equal([1, 2], [1, 4])\n    False\n    >>> a = np.array([1, np.nan])\n    >>> np.array_equal(a, a)\n    False\n    >>> np.array_equal(a, a, equal_nan=True)\n    True\n\n    When ``equal_nan`` is True, complex values with nan components are\n    considered equal if either the real *or* the imaginary components are nan.\n\n    >>> a = np.array([1 + 1j])\n    >>> b = a.copy()\n    >>> a.real = np.nan\n    >>> b.imag = np.nan\n    >>> np.array_equal(a, b, equal_nan=True)\n    True\n    \"\"\"\n    try:\n        a1, a2 = asarray(a1), asarray(a2)\n    except Exception:\n        return False\n    if a1.shape != a2.shape:\n        return False\n    if not equal_nan:\n        return bool(asarray(a1 == a2).all())\n    # Handling NaN values if equal_nan is True\n    a1nan, a2nan = isnan(a1), isnan(a2)\n    # NaN's occur at different locations\n    if not (a1nan == a2nan).all():\n        return False\n    # Shapes of a1, a2 and masks are guaranteed to be consistent by this point\n    return bool(asarray(a1[~a1nan] == a2[~a1nan]).all())\n\n\ndef _array_equiv_dispatcher(a1, a2):\n    return (a1, a2)\n\n\n@array_function_dispatch(_array_equiv_dispatcher)\ndef array_equiv(a1, a2):\n    \"\"\"\n    Returns True if input arrays are shape consistent and all elements equal.\n\n    Shape consistent means they are either the same shape, or one input array\n    can be broadcasted to create the same shape as the other one.\n\n    Parameters\n    ----------\n    a1, a2 : array_like\n        Input arrays.\n\n    Returns\n    -------\n    out : bool\n        True if equivalent, False otherwise.\n\n    Examples\n    --------\n    >>> np.array_equiv([1, 2], [1, 2])\n    True\n    >>> np.array_equiv([1, 2], [1, 3])\n    False\n\n    Showing the shape equivalence:\n\n    >>> np.array_equiv([1, 2], [[1, 2], [1, 2]])\n    True\n    >>> np.array_equiv([1, 2], [[1, 2, 1, 2], [1, 2, 1, 2]])\n    False\n\n    >>> np.array_equiv([1, 2], [[1, 2], [1, 3]])\n    False\n\n    \"\"\"\n    try:\n        a1, a2 = asarray(a1), asarray(a2)\n    except Exception:\n        return False\n    try:\n        multiarray.broadcast(a1, a2)\n    except Exception:\n        return False\n\n    return bool(asarray(a1 == a2).all())\n\n\nInf = inf = infty = Infinity = PINF\nnan = NaN = NAN\nFalse_ = bool_(False)\nTrue_ = bool_(True)\n\n\ndef extend_all(module):\n    existing = set(__all__)\n    mall = getattr(module, '__all__')\n    for a in mall:\n        if a not in existing:\n            __all__.append(a)\n\n\nfrom .umath import *\nfrom .numerictypes import *\nfrom . import fromnumeric\nfrom .fromnumeric import *\nfrom . import arrayprint\nfrom .arrayprint import *\nfrom . import _asarray\nfrom ._asarray import *\nfrom . import _ufunc_config\nfrom ._ufunc_config import *\nextend_all(fromnumeric)\nextend_all(umath)\nextend_all(numerictypes)\nextend_all(arrayprint)\nextend_all(_asarray)\nextend_all(_ufunc_config)\n", 2537], "/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py": ["\"\"\"\nThe config module holds package-wide configurables and provides\na uniform API for working with them.\n\nOverview\n========\n\nThis module supports the following requirements:\n- options are referenced using keys in dot.notation, e.g. \"x.y.option - z\".\n- keys are case-insensitive.\n- functions should accept partial/regex keys, when unambiguous.\n- options can be registered by modules at import time.\n- options can be registered at init-time (via core.config_init)\n- options have a default value, and (optionally) a description and\n  validation function associated with them.\n- options can be deprecated, in which case referencing them\n  should produce a warning.\n- deprecated options can optionally be rerouted to a replacement\n  so that accessing a deprecated option reroutes to a differently\n  named option.\n- options can be reset to their default value.\n- all option can be reset to their default value at once.\n- all options in a certain sub - namespace can be reset at once.\n- the user can set / get / reset or ask for the description of an option.\n- a developer can register and mark an option as deprecated.\n- you can register a callback to be invoked when the option value\n  is set or reset. Changing the stored value is considered misuse, but\n  is not verboten.\n\nImplementation\n==============\n\n- Data is stored using nested dictionaries, and should be accessed\n  through the provided API.\n\n- \"Registered options\" and \"Deprecated options\" have metadata associated\n  with them, which are stored in auxiliary dictionaries keyed on the\n  fully-qualified key, e.g. \"x.y.z.option\".\n\n- the config_init module is imported by the package's __init__.py file.\n  placing any register_option() calls there will ensure those options\n  are available as soon as pandas is loaded. If you use register_option\n  in a module, it will only be available after that module is imported,\n  which you should be aware of.\n\n- `config_prefix` is a context_manager (for use with the `with` keyword)\n  which can save developers some typing, see the docstring.\n\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom contextlib import (\n    ContextDecorator,\n    contextmanager,\n)\nimport re\nfrom typing import (\n    Any,\n    Callable,\n    Iterable,\n    NamedTuple,\n    cast,\n)\nimport warnings\n\nfrom pandas._typing import F\n\n\nclass DeprecatedOption(NamedTuple):\n    key: str\n    msg: str | None\n    rkey: str | None\n    removal_ver: str | None\n\n\nclass RegisteredOption(NamedTuple):\n    key: str\n    defval: object\n    doc: str\n    validator: Callable[[object], Any] | None\n    cb: Callable[[str], Any] | None\n\n\n# holds deprecated option metadata\n_deprecated_options: dict[str, DeprecatedOption] = {}\n\n# holds registered option metadata\n_registered_options: dict[str, RegisteredOption] = {}\n\n# holds the current values for registered options\n_global_config: dict[str, Any] = {}\n\n# keys which have a special meaning\n_reserved_keys: list[str] = [\"all\"]\n\n\nclass OptionError(AttributeError, KeyError):\n    \"\"\"\n    Exception for pandas.options, backwards compatible with KeyError\n    checks.\n    \"\"\"\n\n\n#\n# User API\n\n\ndef _get_single_key(pat: str, silent: bool) -> str:\n    keys = _select_options(pat)\n    if len(keys) == 0:\n        if not silent:\n            _warn_if_deprecated(pat)\n        raise OptionError(f\"No such keys(s): {repr(pat)}\")\n    if len(keys) > 1:\n        raise OptionError(\"Pattern matched multiple keys\")\n    key = keys[0]\n\n    if not silent:\n        _warn_if_deprecated(key)\n\n    key = _translate_key(key)\n\n    return key\n\n\ndef _get_option(pat: str, silent: bool = False):\n    key = _get_single_key(pat, silent)\n\n    # walk the nested dict\n    root, k = _get_root(key)\n    return root[k]\n\n\ndef _set_option(*args, **kwargs) -> None:\n    # must at least 1 arg deal with constraints later\n    nargs = len(args)\n    if not nargs or nargs % 2 != 0:\n        raise ValueError(\"Must provide an even number of non-keyword arguments\")\n\n    # default to false\n    silent = kwargs.pop(\"silent\", False)\n\n    if kwargs:\n        kwarg = list(kwargs.keys())[0]\n        raise TypeError(f'_set_option() got an unexpected keyword argument \"{kwarg}\"')\n\n    for k, v in zip(args[::2], args[1::2]):\n        key = _get_single_key(k, silent)\n\n        o = _get_registered_option(key)\n        if o and o.validator:\n            o.validator(v)\n\n        # walk the nested dict\n        root, k = _get_root(key)\n        root[k] = v\n\n        if o.cb:\n            if silent:\n                with warnings.catch_warnings(record=True):\n                    o.cb(key)\n            else:\n                o.cb(key)\n\n\ndef _describe_option(pat: str = \"\", _print_desc: bool = True):\n\n    keys = _select_options(pat)\n    if len(keys) == 0:\n        raise OptionError(\"No such keys(s)\")\n\n    s = \"\\n\".join([_build_option_description(k) for k in keys])\n\n    if _print_desc:\n        print(s)\n    else:\n        return s\n\n\ndef _reset_option(pat: str, silent: bool = False) -> None:\n\n    keys = _select_options(pat)\n\n    if len(keys) == 0:\n        raise OptionError(\"No such keys(s)\")\n\n    if len(keys) > 1 and len(pat) < 4 and pat != \"all\":\n        raise ValueError(\n            \"You must specify at least 4 characters when \"\n            \"resetting multiple keys, use the special keyword \"\n            '\"all\" to reset all the options to their default value'\n        )\n\n    for k in keys:\n        _set_option(k, _registered_options[k].defval, silent=silent)\n\n\ndef get_default_val(pat: str):\n    key = _get_single_key(pat, silent=True)\n    return _get_registered_option(key).defval\n\n\nclass DictWrapper:\n    \"\"\"provide attribute-style access to a nested dict\"\"\"\n\n    def __init__(self, d: dict[str, Any], prefix: str = \"\"):\n        object.__setattr__(self, \"d\", d)\n        object.__setattr__(self, \"prefix\", prefix)\n\n    def __setattr__(self, key: str, val: Any) -> None:\n        prefix = object.__getattribute__(self, \"prefix\")\n        if prefix:\n            prefix += \".\"\n        prefix += key\n        # you can't set new keys\n        # can you can't overwrite subtrees\n        if key in self.d and not isinstance(self.d[key], dict):\n            _set_option(prefix, val)\n        else:\n            raise OptionError(\"You can only set the value of existing options\")\n\n    def __getattr__(self, key: str):\n        prefix = object.__getattribute__(self, \"prefix\")\n        if prefix:\n            prefix += \".\"\n        prefix += key\n        try:\n            v = object.__getattribute__(self, \"d\")[key]\n        except KeyError as err:\n            raise OptionError(\"No such option\") from err\n        if isinstance(v, dict):\n            return DictWrapper(v, prefix)\n        else:\n            return _get_option(prefix)\n\n    def __dir__(self) -> Iterable[str]:\n        return list(self.d.keys())\n\n\n# For user convenience,  we'd like to have the available options described\n# in the docstring. For dev convenience we'd like to generate the docstrings\n# dynamically instead of maintaining them by hand. To this, we use the\n# class below which wraps functions inside a callable, and converts\n# __doc__ into a property function. The doctsrings below are templates\n# using the py2.6+ advanced formatting syntax to plug in a concise list\n# of options, and option descriptions.\n\n\nclass CallableDynamicDoc:\n    def __init__(self, func, doc_tmpl):\n        self.__doc_tmpl__ = doc_tmpl\n        self.__func__ = func\n\n    def __call__(self, *args, **kwds):\n        return self.__func__(*args, **kwds)\n\n    @property\n    def __doc__(self):\n        opts_desc = _describe_option(\"all\", _print_desc=False)\n        opts_list = pp_options_list(list(_registered_options.keys()))\n        return self.__doc_tmpl__.format(opts_desc=opts_desc, opts_list=opts_list)\n\n\n_get_option_tmpl = \"\"\"\nget_option(pat)\n\nRetrieves the value of the specified option.\n\nAvailable options:\n\n{opts_list}\n\nParameters\n----------\npat : str\n    Regexp which should match a single option.\n    Note: partial matches are supported for convenience, but unless you use the\n    full option name (e.g. x.y.z.option_name), your code may break in future\n    versions if new options with similar names are introduced.\n\nReturns\n-------\nresult : the value of the option\n\nRaises\n------\nOptionError : if no such option exists\n\nNotes\n-----\nPlease reference the :ref:`User Guide <options>` for more information.\n\nThe available options with its descriptions:\n\n{opts_desc}\n\"\"\"\n\n_set_option_tmpl = \"\"\"\nset_option(pat, value)\n\nSets the value of the specified option.\n\nAvailable options:\n\n{opts_list}\n\nParameters\n----------\npat : str\n    Regexp which should match a single option.\n    Note: partial matches are supported for convenience, but unless you use the\n    full option name (e.g. x.y.z.option_name), your code may break in future\n    versions if new options with similar names are introduced.\nvalue : object\n    New value of option.\n\nReturns\n-------\nNone\n\nRaises\n------\nOptionError if no such option exists\n\nNotes\n-----\nPlease reference the :ref:`User Guide <options>` for more information.\n\nThe available options with its descriptions:\n\n{opts_desc}\n\"\"\"\n\n_describe_option_tmpl = \"\"\"\ndescribe_option(pat, _print_desc=False)\n\nPrints the description for one or more registered options.\n\nCall with no arguments to get a listing for all registered options.\n\nAvailable options:\n\n{opts_list}\n\nParameters\n----------\npat : str\n    Regexp pattern. All matching keys will have their description displayed.\n_print_desc : bool, default True\n    If True (default) the description(s) will be printed to stdout.\n    Otherwise, the description(s) will be returned as a unicode string\n    (for testing).\n\nReturns\n-------\nNone by default, the description(s) as a unicode string if _print_desc\nis False\n\nNotes\n-----\nPlease reference the :ref:`User Guide <options>` for more information.\n\nThe available options with its descriptions:\n\n{opts_desc}\n\"\"\"\n\n_reset_option_tmpl = \"\"\"\nreset_option(pat)\n\nReset one or more options to their default value.\n\nPass \"all\" as argument to reset all options.\n\nAvailable options:\n\n{opts_list}\n\nParameters\n----------\npat : str/regex\n    If specified only options matching `prefix*` will be reset.\n    Note: partial matches are supported for convenience, but unless you\n    use the full option name (e.g. x.y.z.option_name), your code may break\n    in future versions if new options with similar names are introduced.\n\nReturns\n-------\nNone\n\nNotes\n-----\nPlease reference the :ref:`User Guide <options>` for more information.\n\nThe available options with its descriptions:\n\n{opts_desc}\n\"\"\"\n\n# bind the functions with their docstrings into a Callable\n# and use that as the functions exposed in pd.api\nget_option = CallableDynamicDoc(_get_option, _get_option_tmpl)\nset_option = CallableDynamicDoc(_set_option, _set_option_tmpl)\nreset_option = CallableDynamicDoc(_reset_option, _reset_option_tmpl)\ndescribe_option = CallableDynamicDoc(_describe_option, _describe_option_tmpl)\noptions = DictWrapper(_global_config)\n\n#\n# Functions for use by pandas developers, in addition to User - api\n\n\nclass option_context(ContextDecorator):\n    \"\"\"\n    Context manager to temporarily set options in the `with` statement context.\n\n    You need to invoke as ``option_context(pat, val, [(pat, val), ...])``.\n\n    Examples\n    --------\n    >>> with option_context('display.max_rows', 10, 'display.max_columns', 5):\n    ...     pass\n    \"\"\"\n\n    def __init__(self, *args):\n        if len(args) % 2 != 0 or len(args) < 2:\n            raise ValueError(\n                \"Need to invoke as option_context(pat, val, [(pat, val), ...]).\"\n            )\n\n        self.ops = list(zip(args[::2], args[1::2]))\n\n    def __enter__(self):\n        self.undo = [(pat, _get_option(pat, silent=True)) for pat, val in self.ops]\n\n        for pat, val in self.ops:\n            _set_option(pat, val, silent=True)\n\n    def __exit__(self, *args):\n        if self.undo:\n            for pat, val in self.undo:\n                _set_option(pat, val, silent=True)\n\n\ndef register_option(\n    key: str,\n    defval: object,\n    doc: str = \"\",\n    validator: Callable[[object], Any] | None = None,\n    cb: Callable[[str], Any] | None = None,\n) -> None:\n    \"\"\"\n    Register an option in the package-wide pandas config object\n\n    Parameters\n    ----------\n    key : str\n        Fully-qualified key, e.g. \"x.y.option - z\".\n    defval : object\n        Default value of the option.\n    doc : str\n        Description of the option.\n    validator : Callable, optional\n        Function of a single argument, should raise `ValueError` if\n        called with a value which is not a legal value for the option.\n    cb\n        a function of a single argument \"key\", which is called\n        immediately after an option value is set/reset. key is\n        the full name of the option.\n\n    Raises\n    ------\n    ValueError if `validator` is specified and `defval` is not a valid value.\n\n    \"\"\"\n    import keyword\n    import tokenize\n\n    key = key.lower()\n\n    if key in _registered_options:\n        raise OptionError(f\"Option '{key}' has already been registered\")\n    if key in _reserved_keys:\n        raise OptionError(f\"Option '{key}' is a reserved key\")\n\n    # the default value should be legal\n    if validator:\n        validator(defval)\n\n    # walk the nested dict, creating dicts as needed along the path\n    path = key.split(\".\")\n\n    for k in path:\n        if not re.match(\"^\" + tokenize.Name + \"$\", k):\n            raise ValueError(f\"{k} is not a valid identifier\")\n        if keyword.iskeyword(k):\n            raise ValueError(f\"{k} is a python keyword\")\n\n    cursor = _global_config\n    msg = \"Path prefix to option '{option}' is already an option\"\n\n    for i, p in enumerate(path[:-1]):\n        if not isinstance(cursor, dict):\n            raise OptionError(msg.format(option=\".\".join(path[:i])))\n        if p not in cursor:\n            cursor[p] = {}\n        cursor = cursor[p]\n\n    if not isinstance(cursor, dict):\n        raise OptionError(msg.format(option=\".\".join(path[:-1])))\n\n    cursor[path[-1]] = defval  # initialize\n\n    # save the option metadata\n    _registered_options[key] = RegisteredOption(\n        key=key, defval=defval, doc=doc, validator=validator, cb=cb\n    )\n\n\ndef deprecate_option(\n    key: str,\n    msg: str | None = None,\n    rkey: str | None = None,\n    removal_ver: str | None = None,\n) -> None:\n    \"\"\"\n    Mark option `key` as deprecated, if code attempts to access this option,\n    a warning will be produced, using `msg` if given, or a default message\n    if not.\n    if `rkey` is given, any access to the key will be re-routed to `rkey`.\n\n    Neither the existence of `key` nor that if `rkey` is checked. If they\n    do not exist, any subsequence access will fail as usual, after the\n    deprecation warning is given.\n\n    Parameters\n    ----------\n    key : str\n        Name of the option to be deprecated.\n        must be a fully-qualified option name (e.g \"x.y.z.rkey\").\n    msg : str, optional\n        Warning message to output when the key is referenced.\n        if no message is given a default message will be emitted.\n    rkey : str, optional\n        Name of an option to reroute access to.\n        If specified, any referenced `key` will be\n        re-routed to `rkey` including set/get/reset.\n        rkey must be a fully-qualified option name (e.g \"x.y.z.rkey\").\n        used by the default message if no `msg` is specified.\n    removal_ver : str, optional\n        Specifies the version in which this option will\n        be removed. used by the default message if no `msg` is specified.\n\n    Raises\n    ------\n    OptionError\n        If the specified key has already been deprecated.\n    \"\"\"\n    key = key.lower()\n\n    if key in _deprecated_options:\n        raise OptionError(f\"Option '{key}' has already been defined as deprecated.\")\n\n    _deprecated_options[key] = DeprecatedOption(key, msg, rkey, removal_ver)\n\n\n#\n# functions internal to the module\n\n\ndef _select_options(pat: str) -> list[str]:\n    \"\"\"\n    returns a list of keys matching `pat`\n\n    if pat==\"all\", returns all registered options\n    \"\"\"\n    # short-circuit for exact key\n    if pat in _registered_options:\n        return [pat]\n\n    # else look through all of them\n    keys = sorted(_registered_options.keys())\n    if pat == \"all\":  # reserved key\n        return keys\n\n    return [k for k in keys if re.search(pat, k, re.I)]\n\n\ndef _get_root(key: str) -> tuple[dict[str, Any], str]:\n    path = key.split(\".\")\n    cursor = _global_config\n    for p in path[:-1]:\n        cursor = cursor[p]\n    return cursor, path[-1]\n\n\ndef _is_deprecated(key: str) -> bool:\n    \"\"\"Returns True if the given option has been deprecated\"\"\"\n    key = key.lower()\n    return key in _deprecated_options\n\n\ndef _get_deprecated_option(key: str):\n    \"\"\"\n    Retrieves the metadata for a deprecated option, if `key` is deprecated.\n\n    Returns\n    -------\n    DeprecatedOption (namedtuple) if key is deprecated, None otherwise\n    \"\"\"\n    try:\n        d = _deprecated_options[key]\n    except KeyError:\n        return None\n    else:\n        return d\n\n\ndef _get_registered_option(key: str):\n    \"\"\"\n    Retrieves the option metadata if `key` is a registered option.\n\n    Returns\n    -------\n    RegisteredOption (namedtuple) if key is deprecated, None otherwise\n    \"\"\"\n    return _registered_options.get(key)\n\n\ndef _translate_key(key: str) -> str:\n    \"\"\"\n    if key id deprecated and a replacement key defined, will return the\n    replacement key, otherwise returns `key` as - is\n    \"\"\"\n    d = _get_deprecated_option(key)\n    if d:\n        return d.rkey or key\n    else:\n        return key\n\n\ndef _warn_if_deprecated(key: str) -> bool:\n    \"\"\"\n    Checks if `key` is a deprecated option and if so, prints a warning.\n\n    Returns\n    -------\n    bool - True if `key` is deprecated, False otherwise.\n    \"\"\"\n    d = _get_deprecated_option(key)\n    if d:\n        if d.msg:\n            warnings.warn(d.msg, FutureWarning)\n        else:\n            msg = f\"'{key}' is deprecated\"\n            if d.removal_ver:\n                msg += f\" and will be removed in {d.removal_ver}\"\n            if d.rkey:\n                msg += f\", please use '{d.rkey}' instead.\"\n            else:\n                msg += \", please refrain from using it.\"\n\n            warnings.warn(msg, FutureWarning)\n        return True\n    return False\n\n\ndef _build_option_description(k: str) -> str:\n    \"\"\"Builds a formatted description of a registered option and prints it\"\"\"\n    o = _get_registered_option(k)\n    d = _get_deprecated_option(k)\n\n    s = f\"{k} \"\n\n    if o.doc:\n        s += \"\\n\".join(o.doc.strip().split(\"\\n\"))\n    else:\n        s += \"No description available.\"\n\n    if o:\n        s += f\"\\n    [default: {o.defval}] [currently: {_get_option(k, True)}]\"\n\n    if d:\n        rkey = d.rkey or \"\"\n        s += \"\\n    (Deprecated\"\n        s += f\", use `{rkey}` instead.\"\n        s += \")\"\n\n    return s\n\n\ndef pp_options_list(keys: Iterable[str], width=80, _print: bool = False):\n    \"\"\"Builds a concise listing of available options, grouped by prefix\"\"\"\n    from itertools import groupby\n    from textwrap import wrap\n\n    def pp(name: str, ks: Iterable[str]) -> list[str]:\n        pfx = \"- \" + name + \".[\" if name else \"\"\n        ls = wrap(\n            \", \".join(ks),\n            width,\n            initial_indent=pfx,\n            subsequent_indent=\"  \",\n            break_long_words=False,\n        )\n        if ls and ls[-1] and name:\n            ls[-1] = ls[-1] + \"]\"\n        return ls\n\n    ls: list[str] = []\n    singles = [x for x in sorted(keys) if x.find(\".\") < 0]\n    if singles:\n        ls += pp(\"\", singles)\n    keys = [x for x in keys if x.find(\".\") >= 0]\n\n    for k, g in groupby(sorted(keys), lambda x: x[: x.rfind(\".\")]):\n        ks = [x[len(k) + 1 :] for x in list(g)]\n        ls += pp(k, ks)\n    s = \"\\n\".join(ls)\n    if _print:\n        print(s)\n    else:\n        return s\n\n\n#\n# helpers\n\n\n@contextmanager\ndef config_prefix(prefix):\n    \"\"\"\n    contextmanager for multiple invocations of API with a common prefix\n\n    supported API functions: (register / get / set )__option\n\n    Warning: This is not thread - safe, and won't work properly if you import\n    the API functions into your module using the \"from x import y\" construct.\n\n    Example\n    -------\n    import pandas._config.config as cf\n    with cf.config_prefix(\"display.font\"):\n        cf.register_option(\"color\", \"red\")\n        cf.register_option(\"size\", \" 5 pt\")\n        cf.set_option(size, \" 6 pt\")\n        cf.get_option(size)\n        ...\n\n        etc'\n\n    will register options \"display.font.color\", \"display.font.size\", set the\n    value of \"display.font.size\"... and so on.\n    \"\"\"\n    # Note: reset_option relies on set_option, and on key directly\n    # it does not fit in to this monkey-patching scheme\n\n    global register_option, get_option, set_option, reset_option\n\n    def wrap(func: F) -> F:\n        def inner(key: str, *args, **kwds):\n            pkey = f\"{prefix}.{key}\"\n            return func(pkey, *args, **kwds)\n\n        return cast(F, inner)\n\n    _register_option = register_option\n    _get_option = get_option\n    _set_option = set_option\n    set_option = wrap(set_option)\n    get_option = wrap(get_option)\n    register_option = wrap(register_option)\n    try:\n        yield\n    finally:\n        set_option = _set_option\n        get_option = _get_option\n        register_option = _register_option\n\n\n# These factories and methods are handy for use as the validator\n# arg in register_option\n\n\ndef is_type_factory(_type: type[Any]) -> Callable[[Any], None]:\n    \"\"\"\n\n    Parameters\n    ----------\n    `_type` - a type to be compared against (e.g. type(x) == `_type`)\n\n    Returns\n    -------\n    validator - a function of a single argument x , which raises\n                ValueError if type(x) is not equal to `_type`\n\n    \"\"\"\n\n    def inner(x) -> None:\n        if type(x) != _type:\n            raise ValueError(f\"Value must have type '{_type}'\")\n\n    return inner\n\n\ndef is_instance_factory(_type) -> Callable[[Any], None]:\n    \"\"\"\n\n    Parameters\n    ----------\n    `_type` - the type to be checked against\n\n    Returns\n    -------\n    validator - a function of a single argument x , which raises\n                ValueError if x is not an instance of `_type`\n\n    \"\"\"\n    if isinstance(_type, (tuple, list)):\n        _type = tuple(_type)\n        type_repr = \"|\".join(map(str, _type))\n    else:\n        type_repr = f\"'{_type}'\"\n\n    def inner(x) -> None:\n        if not isinstance(x, _type):\n            raise ValueError(f\"Value must be an instance of {type_repr}\")\n\n    return inner\n\n\ndef is_one_of_factory(legal_values) -> Callable[[Any], None]:\n\n    callables = [c for c in legal_values if callable(c)]\n    legal_values = [c for c in legal_values if not callable(c)]\n\n    def inner(x) -> None:\n        if x not in legal_values:\n\n            if not any(c(x) for c in callables):\n                uvals = [str(lval) for lval in legal_values]\n                pp_values = \"|\".join(uvals)\n                msg = f\"Value must be one of {pp_values}\"\n                if len(callables):\n                    msg += \" or a callable\"\n                raise ValueError(msg)\n\n    return inner\n\n\ndef is_nonnegative_int(value: object) -> None:\n    \"\"\"\n    Verify that value is None or a positive int.\n\n    Parameters\n    ----------\n    value : None or int\n            The `value` to be checked.\n\n    Raises\n    ------\n    ValueError\n        When the value is not None or is a negative integer\n    \"\"\"\n    if value is None:\n        return\n\n    elif isinstance(value, int):\n        if value >= 0:\n            return\n\n    msg = \"Value must be a nonnegative integer or None\"\n    raise ValueError(msg)\n\n\n# common type validators, for convenience\n# usage: register_option(... , validator = is_int)\nis_int = is_type_factory(int)\nis_bool = is_type_factory(bool)\nis_float = is_type_factory(float)\nis_str = is_type_factory(str)\nis_text = is_instance_factory((str, bytes))\n\n\ndef is_callable(obj) -> bool:\n    \"\"\"\n\n    Parameters\n    ----------\n    `obj` - the object to be checked\n\n    Returns\n    -------\n    validator - returns True if object is callable\n        raises ValueError otherwise.\n\n    \"\"\"\n    if not callable(obj):\n        raise ValueError(\"Value must be a callable\")\n    return True\n", 900], "/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/common.py": ["\"\"\"\nCommon type operations.\n\"\"\"\nfrom __future__ import annotations\n\nfrom typing import (\n    Any,\n    Callable,\n)\nimport warnings\n\nimport numpy as np\n\nfrom pandas._libs import (\n    Interval,\n    Period,\n    algos,\n    lib,\n)\nfrom pandas._libs.tslibs import conversion\nfrom pandas._typing import (\n    ArrayLike,\n    DtypeObj,\n)\nfrom pandas.util._exceptions import find_stack_level\n\nfrom pandas.core.dtypes.base import _registry as registry\nfrom pandas.core.dtypes.dtypes import (\n    CategoricalDtype,\n    DatetimeTZDtype,\n    ExtensionDtype,\n    IntervalDtype,\n    PeriodDtype,\n)\nfrom pandas.core.dtypes.generic import (\n    ABCCategorical,\n    ABCIndex,\n)\nfrom pandas.core.dtypes.inference import (  # noqa:F401\n    is_array_like,\n    is_bool,\n    is_complex,\n    is_dataclass,\n    is_decimal,\n    is_dict_like,\n    is_file_like,\n    is_float,\n    is_hashable,\n    is_integer,\n    is_interval,\n    is_iterator,\n    is_list_like,\n    is_named_tuple,\n    is_nested_list_like,\n    is_number,\n    is_re,\n    is_re_compilable,\n    is_scalar,\n    is_sequence,\n)\n\nDT64NS_DTYPE = conversion.DT64NS_DTYPE\nTD64NS_DTYPE = conversion.TD64NS_DTYPE\nINT64_DTYPE = np.dtype(np.int64)\n\n# oh the troubles to reduce import time\n_is_scipy_sparse = None\n\nensure_float64 = algos.ensure_float64\n\n\ndef ensure_float(arr):\n    \"\"\"\n    Ensure that an array object has a float dtype if possible.\n\n    Parameters\n    ----------\n    arr : array-like\n        The array whose data type we want to enforce as float.\n\n    Returns\n    -------\n    float_arr : The original array cast to the float dtype if\n                possible. Otherwise, the original array is returned.\n    \"\"\"\n    if is_extension_array_dtype(arr.dtype):\n        if is_float_dtype(arr.dtype):\n            arr = arr.to_numpy(dtype=arr.dtype.numpy_dtype, na_value=np.nan)\n        else:\n            arr = arr.to_numpy(dtype=\"float64\", na_value=np.nan)\n    elif issubclass(arr.dtype.type, (np.integer, np.bool_)):\n        arr = arr.astype(float)\n    return arr\n\n\nensure_int64 = algos.ensure_int64\nensure_int32 = algos.ensure_int32\nensure_int16 = algos.ensure_int16\nensure_int8 = algos.ensure_int8\nensure_platform_int = algos.ensure_platform_int\nensure_object = algos.ensure_object\n\n\ndef ensure_str(value: bytes | Any) -> str:\n    \"\"\"\n    Ensure that bytes and non-strings get converted into ``str`` objects.\n    \"\"\"\n    if isinstance(value, bytes):\n        value = value.decode(\"utf-8\")\n    elif not isinstance(value, str):\n        value = str(value)\n    return value\n\n\ndef ensure_python_int(value: int | np.integer) -> int:\n    \"\"\"\n    Ensure that a value is a python int.\n\n    Parameters\n    ----------\n    value: int or numpy.integer\n\n    Returns\n    -------\n    int\n\n    Raises\n    ------\n    TypeError: if the value isn't an int or can't be converted to one.\n    \"\"\"\n    if not (is_integer(value) or is_float(value)):\n        if not is_scalar(value):\n            raise TypeError(\n                f\"Value needs to be a scalar value, was type {type(value).__name__}\"\n            )\n        raise TypeError(f\"Wrong type {type(value)} for value {value}\")\n    try:\n        new_value = int(value)\n        assert new_value == value\n    except (TypeError, ValueError, AssertionError) as err:\n        raise TypeError(f\"Wrong type {type(value)} for value {value}\") from err\n    return new_value\n\n\ndef classes(*klasses) -> Callable:\n    \"\"\"Evaluate if the tipo is a subclass of the klasses.\"\"\"\n    return lambda tipo: issubclass(tipo, klasses)\n\n\ndef classes_and_not_datetimelike(*klasses) -> Callable:\n    \"\"\"\n    Evaluate if the tipo is a subclass of the klasses\n    and not a datetimelike.\n    \"\"\"\n    return lambda tipo: (\n        issubclass(tipo, klasses)\n        and not issubclass(tipo, (np.datetime64, np.timedelta64))\n    )\n\n\ndef is_object_dtype(arr_or_dtype) -> bool:\n    \"\"\"\n    Check whether an array-like or dtype is of the object dtype.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like or dtype\n        The array-like or dtype to check.\n\n    Returns\n    -------\n    boolean\n        Whether or not the array-like or dtype is of the object dtype.\n\n    Examples\n    --------\n    >>> is_object_dtype(object)\n    True\n    >>> is_object_dtype(int)\n    False\n    >>> is_object_dtype(np.array([], dtype=object))\n    True\n    >>> is_object_dtype(np.array([], dtype=int))\n    False\n    >>> is_object_dtype([1, 2, 3])\n    False\n    \"\"\"\n    return _is_dtype_type(arr_or_dtype, classes(np.object_))\n\n\ndef is_sparse(arr) -> bool:\n    \"\"\"\n    Check whether an array-like is a 1-D pandas sparse array.\n\n    Check that the one-dimensional array-like is a pandas sparse array.\n    Returns True if it is a pandas sparse array, not another type of\n    sparse array.\n\n    Parameters\n    ----------\n    arr : array-like\n        Array-like to check.\n\n    Returns\n    -------\n    bool\n        Whether or not the array-like is a pandas sparse array.\n\n    Examples\n    --------\n    Returns `True` if the parameter is a 1-D pandas sparse array.\n\n    >>> is_sparse(pd.arrays.SparseArray([0, 0, 1, 0]))\n    True\n    >>> is_sparse(pd.Series(pd.arrays.SparseArray([0, 0, 1, 0])))\n    True\n\n    Returns `False` if the parameter is not sparse.\n\n    >>> is_sparse(np.array([0, 0, 1, 0]))\n    False\n    >>> is_sparse(pd.Series([0, 1, 0, 0]))\n    False\n\n    Returns `False` if the parameter is not a pandas sparse array.\n\n    >>> from scipy.sparse import bsr_matrix\n    >>> is_sparse(bsr_matrix([0, 1, 0, 0]))\n    False\n\n    Returns `False` if the parameter has more than one dimension.\n    \"\"\"\n    from pandas.core.arrays.sparse import SparseDtype\n\n    dtype = getattr(arr, \"dtype\", arr)\n    return isinstance(dtype, SparseDtype)\n\n\ndef is_scipy_sparse(arr) -> bool:\n    \"\"\"\n    Check whether an array-like is a scipy.sparse.spmatrix instance.\n\n    Parameters\n    ----------\n    arr : array-like\n        The array-like to check.\n\n    Returns\n    -------\n    boolean\n        Whether or not the array-like is a scipy.sparse.spmatrix instance.\n\n    Notes\n    -----\n    If scipy is not installed, this function will always return False.\n\n    Examples\n    --------\n    >>> from scipy.sparse import bsr_matrix\n    >>> is_scipy_sparse(bsr_matrix([1, 2, 3]))\n    True\n    >>> is_scipy_sparse(pd.arrays.SparseArray([1, 2, 3]))\n    False\n    \"\"\"\n    global _is_scipy_sparse\n\n    if _is_scipy_sparse is None:\n        try:\n            from scipy.sparse import issparse as _is_scipy_sparse\n        except ImportError:\n            _is_scipy_sparse = lambda _: False\n\n    assert _is_scipy_sparse is not None\n    return _is_scipy_sparse(arr)\n\n\ndef is_categorical(arr) -> bool:\n    \"\"\"\n    Check whether an array-like is a Categorical instance.\n\n    Parameters\n    ----------\n    arr : array-like\n        The array-like to check.\n\n    Returns\n    -------\n    boolean\n        Whether or not the array-like is of a Categorical instance.\n\n    Examples\n    --------\n    >>> is_categorical([1, 2, 3])\n    False\n\n    Categoricals, Series Categoricals, and CategoricalIndex will return True.\n\n    >>> cat = pd.Categorical([1, 2, 3])\n    >>> is_categorical(cat)\n    True\n    >>> is_categorical(pd.Series(cat))\n    True\n    >>> is_categorical(pd.CategoricalIndex([1, 2, 3]))\n    True\n    \"\"\"\n    warnings.warn(\n        \"is_categorical is deprecated and will be removed in a future version. \"\n        \"Use is_categorical_dtype instead.\",\n        FutureWarning,\n        stacklevel=find_stack_level(),\n    )\n    return isinstance(arr, ABCCategorical) or is_categorical_dtype(arr)\n\n\ndef is_datetime64_dtype(arr_or_dtype) -> bool:\n    \"\"\"\n    Check whether an array-like or dtype is of the datetime64 dtype.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like or dtype\n        The array-like or dtype to check.\n\n    Returns\n    -------\n    boolean\n        Whether or not the array-like or dtype is of the datetime64 dtype.\n\n    Examples\n    --------\n    >>> is_datetime64_dtype(object)\n    False\n    >>> is_datetime64_dtype(np.datetime64)\n    True\n    >>> is_datetime64_dtype(np.array([], dtype=int))\n    False\n    >>> is_datetime64_dtype(np.array([], dtype=np.datetime64))\n    True\n    >>> is_datetime64_dtype([1, 2, 3])\n    False\n    \"\"\"\n    if isinstance(arr_or_dtype, np.dtype):\n        # GH#33400 fastpath for dtype object\n        return arr_or_dtype.kind == \"M\"\n    return _is_dtype_type(arr_or_dtype, classes(np.datetime64))\n\n\ndef is_datetime64tz_dtype(arr_or_dtype) -> bool:\n    \"\"\"\n    Check whether an array-like or dtype is of a DatetimeTZDtype dtype.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like or dtype\n        The array-like or dtype to check.\n\n    Returns\n    -------\n    boolean\n        Whether or not the array-like or dtype is of a DatetimeTZDtype dtype.\n\n    Examples\n    --------\n    >>> is_datetime64tz_dtype(object)\n    False\n    >>> is_datetime64tz_dtype([1, 2, 3])\n    False\n    >>> is_datetime64tz_dtype(pd.DatetimeIndex([1, 2, 3]))  # tz-naive\n    False\n    >>> is_datetime64tz_dtype(pd.DatetimeIndex([1, 2, 3], tz=\"US/Eastern\"))\n    True\n\n    >>> dtype = DatetimeTZDtype(\"ns\", tz=\"US/Eastern\")\n    >>> s = pd.Series([], dtype=dtype)\n    >>> is_datetime64tz_dtype(dtype)\n    True\n    >>> is_datetime64tz_dtype(s)\n    True\n    \"\"\"\n    if isinstance(arr_or_dtype, ExtensionDtype):\n        # GH#33400 fastpath for dtype object\n        return arr_or_dtype.kind == \"M\"\n\n    if arr_or_dtype is None:\n        return False\n    return DatetimeTZDtype.is_dtype(arr_or_dtype)\n\n\ndef is_timedelta64_dtype(arr_or_dtype) -> bool:\n    \"\"\"\n    Check whether an array-like or dtype is of the timedelta64 dtype.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like or dtype\n        The array-like or dtype to check.\n\n    Returns\n    -------\n    boolean\n        Whether or not the array-like or dtype is of the timedelta64 dtype.\n\n    Examples\n    --------\n    >>> is_timedelta64_dtype(object)\n    False\n    >>> is_timedelta64_dtype(np.timedelta64)\n    True\n    >>> is_timedelta64_dtype([1, 2, 3])\n    False\n    >>> is_timedelta64_dtype(pd.Series([], dtype=\"timedelta64[ns]\"))\n    True\n    >>> is_timedelta64_dtype('0 days')\n    False\n    \"\"\"\n    if isinstance(arr_or_dtype, np.dtype):\n        # GH#33400 fastpath for dtype object\n        return arr_or_dtype.kind == \"m\"\n\n    return _is_dtype_type(arr_or_dtype, classes(np.timedelta64))\n\n\ndef is_period_dtype(arr_or_dtype) -> bool:\n    \"\"\"\n    Check whether an array-like or dtype is of the Period dtype.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like or dtype\n        The array-like or dtype to check.\n\n    Returns\n    -------\n    boolean\n        Whether or not the array-like or dtype is of the Period dtype.\n\n    Examples\n    --------\n    >>> is_period_dtype(object)\n    False\n    >>> is_period_dtype(PeriodDtype(freq=\"D\"))\n    True\n    >>> is_period_dtype([1, 2, 3])\n    False\n    >>> is_period_dtype(pd.Period(\"2017-01-01\"))\n    False\n    >>> is_period_dtype(pd.PeriodIndex([], freq=\"A\"))\n    True\n    \"\"\"\n    if isinstance(arr_or_dtype, ExtensionDtype):\n        # GH#33400 fastpath for dtype object\n        return arr_or_dtype.type is Period\n\n    if arr_or_dtype is None:\n        return False\n    return PeriodDtype.is_dtype(arr_or_dtype)\n\n\ndef is_interval_dtype(arr_or_dtype) -> bool:\n    \"\"\"\n    Check whether an array-like or dtype is of the Interval dtype.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like or dtype\n        The array-like or dtype to check.\n\n    Returns\n    -------\n    boolean\n        Whether or not the array-like or dtype is of the Interval dtype.\n\n    Examples\n    --------\n    >>> is_interval_dtype(object)\n    False\n    >>> is_interval_dtype(IntervalDtype())\n    True\n    >>> is_interval_dtype([1, 2, 3])\n    False\n    >>>\n    >>> interval = pd.Interval(1, 2, closed=\"right\")\n    >>> is_interval_dtype(interval)\n    False\n    >>> is_interval_dtype(pd.IntervalIndex([interval]))\n    True\n    \"\"\"\n    if isinstance(arr_or_dtype, ExtensionDtype):\n        # GH#33400 fastpath for dtype object\n        return arr_or_dtype.type is Interval\n\n    if arr_or_dtype is None:\n        return False\n    return IntervalDtype.is_dtype(arr_or_dtype)\n\n\ndef is_categorical_dtype(arr_or_dtype) -> bool:\n    \"\"\"\n    Check whether an array-like or dtype is of the Categorical dtype.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like or dtype\n        The array-like or dtype to check.\n\n    Returns\n    -------\n    boolean\n        Whether or not the array-like or dtype is of the Categorical dtype.\n\n    Examples\n    --------\n    >>> is_categorical_dtype(object)\n    False\n    >>> is_categorical_dtype(CategoricalDtype())\n    True\n    >>> is_categorical_dtype([1, 2, 3])\n    False\n    >>> is_categorical_dtype(pd.Categorical([1, 2, 3]))\n    True\n    >>> is_categorical_dtype(pd.CategoricalIndex([1, 2, 3]))\n    True\n    \"\"\"\n    if isinstance(arr_or_dtype, ExtensionDtype):\n        # GH#33400 fastpath for dtype object\n        return arr_or_dtype.name == \"category\"\n\n    if arr_or_dtype is None:\n        return False\n    return CategoricalDtype.is_dtype(arr_or_dtype)\n\n\ndef is_string_or_object_np_dtype(dtype: np.dtype) -> bool:\n    \"\"\"\n    Faster alternative to is_string_dtype, assumes we have a np.dtype object.\n    \"\"\"\n    return dtype == object or dtype.kind in \"SU\"\n\n\ndef is_string_dtype(arr_or_dtype) -> bool:\n    \"\"\"\n    Check whether the provided array or dtype is of the string dtype.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like or dtype\n        The array or dtype to check.\n\n    Returns\n    -------\n    boolean\n        Whether or not the array or dtype is of the string dtype.\n\n    Examples\n    --------\n    >>> is_string_dtype(str)\n    True\n    >>> is_string_dtype(object)\n    True\n    >>> is_string_dtype(int)\n    False\n    >>>\n    >>> is_string_dtype(np.array(['a', 'b']))\n    True\n    >>> is_string_dtype(pd.Series([1, 2]))\n    False\n    \"\"\"\n    # TODO: gh-15585: consider making the checks stricter.\n    def condition(dtype) -> bool:\n        return dtype.kind in (\"O\", \"S\", \"U\") and not is_excluded_dtype(dtype)\n\n    def is_excluded_dtype(dtype) -> bool:\n        \"\"\"\n        These have kind = \"O\" but aren't string dtypes so need to be explicitly excluded\n        \"\"\"\n        return isinstance(dtype, (PeriodDtype, IntervalDtype, CategoricalDtype))\n\n    return _is_dtype(arr_or_dtype, condition)\n\n\ndef is_dtype_equal(source, target) -> bool:\n    \"\"\"\n    Check if two dtypes are equal.\n\n    Parameters\n    ----------\n    source : The first dtype to compare\n    target : The second dtype to compare\n\n    Returns\n    -------\n    boolean\n        Whether or not the two dtypes are equal.\n\n    Examples\n    --------\n    >>> is_dtype_equal(int, float)\n    False\n    >>> is_dtype_equal(\"int\", int)\n    True\n    >>> is_dtype_equal(object, \"category\")\n    False\n    >>> is_dtype_equal(CategoricalDtype(), \"category\")\n    True\n    >>> is_dtype_equal(DatetimeTZDtype(tz=\"UTC\"), \"datetime64\")\n    False\n    \"\"\"\n    if isinstance(target, str):\n        if not isinstance(source, str):\n            # GH#38516 ensure we get the same behavior from\n            #  is_dtype_equal(CDT, \"category\") and CDT == \"category\"\n            try:\n                src = get_dtype(source)\n                if isinstance(src, ExtensionDtype):\n                    return src == target\n            except (TypeError, AttributeError, ImportError):\n                return False\n    elif isinstance(source, str):\n        return is_dtype_equal(target, source)\n\n    try:\n        source = get_dtype(source)\n        target = get_dtype(target)\n        return source == target\n    except (TypeError, AttributeError, ImportError):\n\n        # invalid comparison\n        # object == category will hit this\n        return False\n\n\ndef is_any_int_dtype(arr_or_dtype) -> bool:\n    \"\"\"\n    Check whether the provided array or dtype is of an integer dtype.\n\n    In this function, timedelta64 instances are also considered \"any-integer\"\n    type objects and will return True.\n\n    This function is internal and should not be exposed in the public API.\n\n    The nullable Integer dtypes (e.g. pandas.Int64Dtype) are also considered\n    as integer by this function.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like or dtype\n        The array or dtype to check.\n\n    Returns\n    -------\n    boolean\n        Whether or not the array or dtype is of an integer dtype.\n\n    Examples\n    --------\n    >>> is_any_int_dtype(str)\n    False\n    >>> is_any_int_dtype(int)\n    True\n    >>> is_any_int_dtype(float)\n    False\n    >>> is_any_int_dtype(np.uint64)\n    True\n    >>> is_any_int_dtype(np.datetime64)\n    False\n    >>> is_any_int_dtype(np.timedelta64)\n    True\n    >>> is_any_int_dtype(np.array(['a', 'b']))\n    False\n    >>> is_any_int_dtype(pd.Series([1, 2]))\n    True\n    >>> is_any_int_dtype(np.array([], dtype=np.timedelta64))\n    True\n    >>> is_any_int_dtype(pd.Index([1, 2.]))  # float\n    False\n    \"\"\"\n    return _is_dtype_type(arr_or_dtype, classes(np.integer, np.timedelta64))\n\n\ndef is_integer_dtype(arr_or_dtype) -> bool:\n    \"\"\"\n    Check whether the provided array or dtype is of an integer dtype.\n\n    Unlike in `is_any_int_dtype`, timedelta64 instances will return False.\n\n    The nullable Integer dtypes (e.g. pandas.Int64Dtype) are also considered\n    as integer by this function.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like or dtype\n        The array or dtype to check.\n\n    Returns\n    -------\n    boolean\n        Whether or not the array or dtype is of an integer dtype and\n        not an instance of timedelta64.\n\n    Examples\n    --------\n    >>> is_integer_dtype(str)\n    False\n    >>> is_integer_dtype(int)\n    True\n    >>> is_integer_dtype(float)\n    False\n    >>> is_integer_dtype(np.uint64)\n    True\n    >>> is_integer_dtype('int8')\n    True\n    >>> is_integer_dtype('Int8')\n    True\n    >>> is_integer_dtype(pd.Int8Dtype)\n    True\n    >>> is_integer_dtype(np.datetime64)\n    False\n    >>> is_integer_dtype(np.timedelta64)\n    False\n    >>> is_integer_dtype(np.array(['a', 'b']))\n    False\n    >>> is_integer_dtype(pd.Series([1, 2]))\n    True\n    >>> is_integer_dtype(np.array([], dtype=np.timedelta64))\n    False\n    >>> is_integer_dtype(pd.Index([1, 2.]))  # float\n    False\n    \"\"\"\n    return _is_dtype_type(arr_or_dtype, classes_and_not_datetimelike(np.integer))\n\n\ndef is_signed_integer_dtype(arr_or_dtype) -> bool:\n    \"\"\"\n    Check whether the provided array or dtype is of a signed integer dtype.\n\n    Unlike in `is_any_int_dtype`, timedelta64 instances will return False.\n\n    The nullable Integer dtypes (e.g. pandas.Int64Dtype) are also considered\n    as integer by this function.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like or dtype\n        The array or dtype to check.\n\n    Returns\n    -------\n    boolean\n        Whether or not the array or dtype is of a signed integer dtype\n        and not an instance of timedelta64.\n\n    Examples\n    --------\n    >>> is_signed_integer_dtype(str)\n    False\n    >>> is_signed_integer_dtype(int)\n    True\n    >>> is_signed_integer_dtype(float)\n    False\n    >>> is_signed_integer_dtype(np.uint64)  # unsigned\n    False\n    >>> is_signed_integer_dtype('int8')\n    True\n    >>> is_signed_integer_dtype('Int8')\n    True\n    >>> is_signed_integer_dtype(pd.Int8Dtype)\n    True\n    >>> is_signed_integer_dtype(np.datetime64)\n    False\n    >>> is_signed_integer_dtype(np.timedelta64)\n    False\n    >>> is_signed_integer_dtype(np.array(['a', 'b']))\n    False\n    >>> is_signed_integer_dtype(pd.Series([1, 2]))\n    True\n    >>> is_signed_integer_dtype(np.array([], dtype=np.timedelta64))\n    False\n    >>> is_signed_integer_dtype(pd.Index([1, 2.]))  # float\n    False\n    >>> is_signed_integer_dtype(np.array([1, 2], dtype=np.uint32))  # unsigned\n    False\n    \"\"\"\n    return _is_dtype_type(arr_or_dtype, classes_and_not_datetimelike(np.signedinteger))\n\n\ndef is_unsigned_integer_dtype(arr_or_dtype) -> bool:\n    \"\"\"\n    Check whether the provided array or dtype is of an unsigned integer dtype.\n\n    The nullable Integer dtypes (e.g. pandas.UInt64Dtype) are also\n    considered as integer by this function.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like or dtype\n        The array or dtype to check.\n\n    Returns\n    -------\n    boolean\n        Whether or not the array or dtype is of an unsigned integer dtype.\n\n    Examples\n    --------\n    >>> is_unsigned_integer_dtype(str)\n    False\n    >>> is_unsigned_integer_dtype(int)  # signed\n    False\n    >>> is_unsigned_integer_dtype(float)\n    False\n    >>> is_unsigned_integer_dtype(np.uint64)\n    True\n    >>> is_unsigned_integer_dtype('uint8')\n    True\n    >>> is_unsigned_integer_dtype('UInt8')\n    True\n    >>> is_unsigned_integer_dtype(pd.UInt8Dtype)\n    True\n    >>> is_unsigned_integer_dtype(np.array(['a', 'b']))\n    False\n    >>> is_unsigned_integer_dtype(pd.Series([1, 2]))  # signed\n    False\n    >>> is_unsigned_integer_dtype(pd.Index([1, 2.]))  # float\n    False\n    >>> is_unsigned_integer_dtype(np.array([1, 2], dtype=np.uint32))\n    True\n    \"\"\"\n    return _is_dtype_type(\n        arr_or_dtype, classes_and_not_datetimelike(np.unsignedinteger)\n    )\n\n\ndef is_int64_dtype(arr_or_dtype) -> bool:\n    \"\"\"\n    Check whether the provided array or dtype is of the int64 dtype.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like or dtype\n        The array or dtype to check.\n\n    Returns\n    -------\n    boolean\n        Whether or not the array or dtype is of the int64 dtype.\n\n    Notes\n    -----\n    Depending on system architecture, the return value of `is_int64_dtype(\n    int)` will be True if the OS uses 64-bit integers and False if the OS\n    uses 32-bit integers.\n\n    Examples\n    --------\n    >>> is_int64_dtype(str)\n    False\n    >>> is_int64_dtype(np.int32)\n    False\n    >>> is_int64_dtype(np.int64)\n    True\n    >>> is_int64_dtype('int8')\n    False\n    >>> is_int64_dtype('Int8')\n    False\n    >>> is_int64_dtype(pd.Int64Dtype)\n    True\n    >>> is_int64_dtype(float)\n    False\n    >>> is_int64_dtype(np.uint64)  # unsigned\n    False\n    >>> is_int64_dtype(np.array(['a', 'b']))\n    False\n    >>> is_int64_dtype(np.array([1, 2], dtype=np.int64))\n    True\n    >>> is_int64_dtype(pd.Index([1, 2.]))  # float\n    False\n    >>> is_int64_dtype(np.array([1, 2], dtype=np.uint32))  # unsigned\n    False\n    \"\"\"\n    return _is_dtype_type(arr_or_dtype, classes(np.int64))\n\n\ndef is_datetime64_any_dtype(arr_or_dtype) -> bool:\n    \"\"\"\n    Check whether the provided array or dtype is of the datetime64 dtype.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like or dtype\n        The array or dtype to check.\n\n    Returns\n    -------\n    bool\n        Whether or not the array or dtype is of the datetime64 dtype.\n\n    Examples\n    --------\n    >>> is_datetime64_any_dtype(str)\n    False\n    >>> is_datetime64_any_dtype(int)\n    False\n    >>> is_datetime64_any_dtype(np.datetime64)  # can be tz-naive\n    True\n    >>> is_datetime64_any_dtype(DatetimeTZDtype(\"ns\", \"US/Eastern\"))\n    True\n    >>> is_datetime64_any_dtype(np.array(['a', 'b']))\n    False\n    >>> is_datetime64_any_dtype(np.array([1, 2]))\n    False\n    >>> is_datetime64_any_dtype(np.array([], dtype=\"datetime64[ns]\"))\n    True\n    >>> is_datetime64_any_dtype(pd.DatetimeIndex([1, 2, 3], dtype=\"datetime64[ns]\"))\n    True\n    \"\"\"\n    if isinstance(arr_or_dtype, (np.dtype, ExtensionDtype)):\n        # GH#33400 fastpath for dtype object\n        return arr_or_dtype.kind == \"M\"\n\n    if arr_or_dtype is None:\n        return False\n    return is_datetime64_dtype(arr_or_dtype) or is_datetime64tz_dtype(arr_or_dtype)\n\n\ndef is_datetime64_ns_dtype(arr_or_dtype) -> bool:\n    \"\"\"\n    Check whether the provided array or dtype is of the datetime64[ns] dtype.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like or dtype\n        The array or dtype to check.\n\n    Returns\n    -------\n    bool\n        Whether or not the array or dtype is of the datetime64[ns] dtype.\n\n    Examples\n    --------\n    >>> is_datetime64_ns_dtype(str)\n    False\n    >>> is_datetime64_ns_dtype(int)\n    False\n    >>> is_datetime64_ns_dtype(np.datetime64)  # no unit\n    False\n    >>> is_datetime64_ns_dtype(DatetimeTZDtype(\"ns\", \"US/Eastern\"))\n    True\n    >>> is_datetime64_ns_dtype(np.array(['a', 'b']))\n    False\n    >>> is_datetime64_ns_dtype(np.array([1, 2]))\n    False\n    >>> is_datetime64_ns_dtype(np.array([], dtype=\"datetime64\"))  # no unit\n    False\n    >>> is_datetime64_ns_dtype(np.array([], dtype=\"datetime64[ps]\"))  # wrong unit\n    False\n    >>> is_datetime64_ns_dtype(pd.DatetimeIndex([1, 2, 3], dtype=\"datetime64[ns]\"))\n    True\n    \"\"\"\n    if arr_or_dtype is None:\n        return False\n    try:\n        tipo = get_dtype(arr_or_dtype)\n    except TypeError:\n        if is_datetime64tz_dtype(arr_or_dtype):\n            tipo = get_dtype(arr_or_dtype.dtype)\n        else:\n            return False\n    return tipo == DT64NS_DTYPE or getattr(tipo, \"base\", None) == DT64NS_DTYPE\n\n\ndef is_timedelta64_ns_dtype(arr_or_dtype) -> bool:\n    \"\"\"\n    Check whether the provided array or dtype is of the timedelta64[ns] dtype.\n\n    This is a very specific dtype, so generic ones like `np.timedelta64`\n    will return False if passed into this function.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like or dtype\n        The array or dtype to check.\n\n    Returns\n    -------\n    boolean\n        Whether or not the array or dtype is of the timedelta64[ns] dtype.\n\n    Examples\n    --------\n    >>> is_timedelta64_ns_dtype(np.dtype('m8[ns]'))\n    True\n    >>> is_timedelta64_ns_dtype(np.dtype('m8[ps]'))  # Wrong frequency\n    False\n    >>> is_timedelta64_ns_dtype(np.array([1, 2], dtype='m8[ns]'))\n    True\n    >>> is_timedelta64_ns_dtype(np.array([1, 2], dtype=np.timedelta64))\n    False\n    \"\"\"\n    return _is_dtype(arr_or_dtype, lambda dtype: dtype == TD64NS_DTYPE)\n\n\ndef is_datetime_or_timedelta_dtype(arr_or_dtype) -> bool:\n    \"\"\"\n    Check whether the provided array or dtype is of\n    a timedelta64 or datetime64 dtype.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like or dtype\n        The array or dtype to check.\n\n    Returns\n    -------\n    boolean\n        Whether or not the array or dtype is of a timedelta64,\n        or datetime64 dtype.\n\n    Examples\n    --------\n    >>> is_datetime_or_timedelta_dtype(str)\n    False\n    >>> is_datetime_or_timedelta_dtype(int)\n    False\n    >>> is_datetime_or_timedelta_dtype(np.datetime64)\n    True\n    >>> is_datetime_or_timedelta_dtype(np.timedelta64)\n    True\n    >>> is_datetime_or_timedelta_dtype(np.array(['a', 'b']))\n    False\n    >>> is_datetime_or_timedelta_dtype(pd.Series([1, 2]))\n    False\n    >>> is_datetime_or_timedelta_dtype(np.array([], dtype=np.timedelta64))\n    True\n    >>> is_datetime_or_timedelta_dtype(np.array([], dtype=np.datetime64))\n    True\n    \"\"\"\n    return _is_dtype_type(arr_or_dtype, classes(np.datetime64, np.timedelta64))\n\n\n# This exists to silence numpy deprecation warnings, see GH#29553\ndef is_numeric_v_string_like(a: ArrayLike, b):\n    \"\"\"\n    Check if we are comparing a string-like object to a numeric ndarray.\n    NumPy doesn't like to compare such objects, especially numeric arrays\n    and scalar string-likes.\n\n    Parameters\n    ----------\n    a : array-like, scalar\n        The first object to check.\n    b : array-like, scalar\n        The second object to check.\n\n    Returns\n    -------\n    boolean\n        Whether we return a comparing a string-like object to a numeric array.\n\n    Examples\n    --------\n    >>> is_numeric_v_string_like(np.array([1]), \"foo\")\n    True\n    >>> is_numeric_v_string_like(np.array([1, 2]), np.array([\"foo\"]))\n    True\n    >>> is_numeric_v_string_like(np.array([\"foo\"]), np.array([1, 2]))\n    True\n    >>> is_numeric_v_string_like(np.array([1]), np.array([2]))\n    False\n    >>> is_numeric_v_string_like(np.array([\"foo\"]), np.array([\"foo\"]))\n    False\n    \"\"\"\n    is_a_array = isinstance(a, np.ndarray)\n    is_b_array = isinstance(b, np.ndarray)\n\n    is_a_numeric_array = is_a_array and a.dtype.kind in (\"u\", \"i\", \"f\", \"c\", \"b\")\n    is_b_numeric_array = is_b_array and b.dtype.kind in (\"u\", \"i\", \"f\", \"c\", \"b\")\n    is_a_string_array = is_a_array and a.dtype.kind in (\"S\", \"U\")\n    is_b_string_array = is_b_array and b.dtype.kind in (\"S\", \"U\")\n\n    is_b_scalar_string_like = not is_b_array and isinstance(b, str)\n\n    return (\n        (is_a_numeric_array and is_b_scalar_string_like)\n        or (is_a_numeric_array and is_b_string_array)\n        or (is_b_numeric_array and is_a_string_array)\n    )\n\n\n# This exists to silence numpy deprecation warnings, see GH#29553\ndef is_datetimelike_v_numeric(a, b):\n    \"\"\"\n    Check if we are comparing a datetime-like object to a numeric object.\n    By \"numeric,\" we mean an object that is either of an int or float dtype.\n\n    Parameters\n    ----------\n    a : array-like, scalar\n        The first object to check.\n    b : array-like, scalar\n        The second object to check.\n\n    Returns\n    -------\n    boolean\n        Whether we return a comparing a datetime-like to a numeric object.\n\n    Examples\n    --------\n    >>> from datetime import datetime\n    >>> dt = np.datetime64(datetime(2017, 1, 1))\n    >>>\n    >>> is_datetimelike_v_numeric(1, 1)\n    False\n    >>> is_datetimelike_v_numeric(dt, dt)\n    False\n    >>> is_datetimelike_v_numeric(1, dt)\n    True\n    >>> is_datetimelike_v_numeric(dt, 1)  # symmetric check\n    True\n    >>> is_datetimelike_v_numeric(np.array([dt]), 1)\n    True\n    >>> is_datetimelike_v_numeric(np.array([1]), dt)\n    True\n    >>> is_datetimelike_v_numeric(np.array([dt]), np.array([1]))\n    True\n    >>> is_datetimelike_v_numeric(np.array([1]), np.array([2]))\n    False\n    >>> is_datetimelike_v_numeric(np.array([dt]), np.array([dt]))\n    False\n    \"\"\"\n    if not hasattr(a, \"dtype\"):\n        a = np.asarray(a)\n    if not hasattr(b, \"dtype\"):\n        b = np.asarray(b)\n\n    def is_numeric(x):\n        \"\"\"\n        Check if an object has a numeric dtype (i.e. integer or float).\n        \"\"\"\n        return is_integer_dtype(x) or is_float_dtype(x)\n\n    return (needs_i8_conversion(a) and is_numeric(b)) or (\n        needs_i8_conversion(b) and is_numeric(a)\n    )\n\n\ndef needs_i8_conversion(arr_or_dtype) -> bool:\n    \"\"\"\n    Check whether the array or dtype should be converted to int64.\n\n    An array-like or dtype \"needs\" such a conversion if the array-like\n    or dtype is of a datetime-like dtype\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like or dtype\n        The array or dtype to check.\n\n    Returns\n    -------\n    boolean\n        Whether or not the array or dtype should be converted to int64.\n\n    Examples\n    --------\n    >>> needs_i8_conversion(str)\n    False\n    >>> needs_i8_conversion(np.int64)\n    False\n    >>> needs_i8_conversion(np.datetime64)\n    True\n    >>> needs_i8_conversion(np.array(['a', 'b']))\n    False\n    >>> needs_i8_conversion(pd.Series([1, 2]))\n    False\n    >>> needs_i8_conversion(pd.Series([], dtype=\"timedelta64[ns]\"))\n    True\n    >>> needs_i8_conversion(pd.DatetimeIndex([1, 2, 3], tz=\"US/Eastern\"))\n    True\n    \"\"\"\n    if arr_or_dtype is None:\n        return False\n    if isinstance(arr_or_dtype, (np.dtype, ExtensionDtype)):\n        # fastpath\n        dtype = arr_or_dtype\n        return dtype.kind in [\"m\", \"M\"] or dtype.type is Period\n\n    try:\n        dtype = get_dtype(arr_or_dtype)\n    except (TypeError, ValueError):\n        return False\n    if isinstance(dtype, np.dtype):\n        return dtype.kind in [\"m\", \"M\"]\n    return isinstance(dtype, (PeriodDtype, DatetimeTZDtype))\n\n\ndef is_numeric_dtype(arr_or_dtype) -> bool:\n    \"\"\"\n    Check whether the provided array or dtype is of a numeric dtype.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like or dtype\n        The array or dtype to check.\n\n    Returns\n    -------\n    boolean\n        Whether or not the array or dtype is of a numeric dtype.\n\n    Examples\n    --------\n    >>> is_numeric_dtype(str)\n    False\n    >>> is_numeric_dtype(int)\n    True\n    >>> is_numeric_dtype(float)\n    True\n    >>> is_numeric_dtype(np.uint64)\n    True\n    >>> is_numeric_dtype(np.datetime64)\n    False\n    >>> is_numeric_dtype(np.timedelta64)\n    False\n    >>> is_numeric_dtype(np.array(['a', 'b']))\n    False\n    >>> is_numeric_dtype(pd.Series([1, 2]))\n    True\n    >>> is_numeric_dtype(pd.Index([1, 2.]))\n    True\n    >>> is_numeric_dtype(np.array([], dtype=np.timedelta64))\n    False\n    \"\"\"\n    return _is_dtype_type(\n        arr_or_dtype, classes_and_not_datetimelike(np.number, np.bool_)\n    )\n\n\ndef is_float_dtype(arr_or_dtype) -> bool:\n    \"\"\"\n    Check whether the provided array or dtype is of a float dtype.\n\n    This function is internal and should not be exposed in the public API.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like or dtype\n        The array or dtype to check.\n\n    Returns\n    -------\n    boolean\n        Whether or not the array or dtype is of a float dtype.\n\n    Examples\n    --------\n    >>> is_float_dtype(str)\n    False\n    >>> is_float_dtype(int)\n    False\n    >>> is_float_dtype(float)\n    True\n    >>> is_float_dtype(np.array(['a', 'b']))\n    False\n    >>> is_float_dtype(pd.Series([1, 2]))\n    False\n    >>> is_float_dtype(pd.Index([1, 2.]))\n    True\n    \"\"\"\n    return _is_dtype_type(arr_or_dtype, classes(np.floating))\n\n\ndef is_bool_dtype(arr_or_dtype) -> bool:\n    \"\"\"\n    Check whether the provided array or dtype is of a boolean dtype.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like or dtype\n        The array or dtype to check.\n\n    Returns\n    -------\n    boolean\n        Whether or not the array or dtype is of a boolean dtype.\n\n    Notes\n    -----\n    An ExtensionArray is considered boolean when the ``_is_boolean``\n    attribute is set to True.\n\n    Examples\n    --------\n    >>> is_bool_dtype(str)\n    False\n    >>> is_bool_dtype(int)\n    False\n    >>> is_bool_dtype(bool)\n    True\n    >>> is_bool_dtype(np.bool_)\n    True\n    >>> is_bool_dtype(np.array(['a', 'b']))\n    False\n    >>> is_bool_dtype(pd.Series([1, 2]))\n    False\n    >>> is_bool_dtype(np.array([True, False]))\n    True\n    >>> is_bool_dtype(pd.Categorical([True, False]))\n    True\n    >>> is_bool_dtype(pd.arrays.SparseArray([True, False]))\n    True\n    \"\"\"\n    if arr_or_dtype is None:\n        return False\n    try:\n        dtype = get_dtype(arr_or_dtype)\n    except (TypeError, ValueError):\n        return False\n\n    if isinstance(dtype, CategoricalDtype):\n        arr_or_dtype = dtype.categories\n        # now we use the special definition for Index\n\n    if isinstance(arr_or_dtype, ABCIndex):\n        # Allow Index[object] that is all-bools or Index[\"boolean\"]\n        return arr_or_dtype.inferred_type == \"boolean\"\n    elif isinstance(dtype, ExtensionDtype):\n        return getattr(dtype, \"_is_boolean\", False)\n\n    return issubclass(dtype.type, np.bool_)\n\n\ndef is_extension_type(arr) -> bool:\n    \"\"\"\n    Check whether an array-like is of a pandas extension class instance.\n\n    .. deprecated:: 1.0.0\n        Use ``is_extension_array_dtype`` instead.\n\n    Extension classes include categoricals, pandas sparse objects (i.e.\n    classes represented within the pandas library and not ones external\n    to it like scipy sparse matrices), and datetime-like arrays.\n\n    Parameters\n    ----------\n    arr : array-like, scalar\n        The array-like to check.\n\n    Returns\n    -------\n    boolean\n        Whether or not the array-like is of a pandas extension class instance.\n\n    Examples\n    --------\n    >>> is_extension_type([1, 2, 3])\n    False\n    >>> is_extension_type(np.array([1, 2, 3]))\n    False\n    >>>\n    >>> cat = pd.Categorical([1, 2, 3])\n    >>>\n    >>> is_extension_type(cat)\n    True\n    >>> is_extension_type(pd.Series(cat))\n    True\n    >>> is_extension_type(pd.arrays.SparseArray([1, 2, 3]))\n    True\n    >>> from scipy.sparse import bsr_matrix\n    >>> is_extension_type(bsr_matrix([1, 2, 3]))\n    False\n    >>> is_extension_type(pd.DatetimeIndex([1, 2, 3]))\n    False\n    >>> is_extension_type(pd.DatetimeIndex([1, 2, 3], tz=\"US/Eastern\"))\n    True\n    >>>\n    >>> dtype = DatetimeTZDtype(\"ns\", tz=\"US/Eastern\")\n    >>> s = pd.Series([], dtype=dtype)\n    >>> is_extension_type(s)\n    True\n    \"\"\"\n    warnings.warn(\n        \"'is_extension_type' is deprecated and will be removed in a future \"\n        \"version.  Use 'is_extension_array_dtype' instead.\",\n        FutureWarning,\n        stacklevel=find_stack_level(),\n    )\n\n    if is_categorical_dtype(arr):\n        return True\n    elif is_sparse(arr):\n        return True\n    elif is_datetime64tz_dtype(arr):\n        return True\n    return False\n\n\ndef is_1d_only_ea_obj(obj: Any) -> bool:\n    \"\"\"\n    ExtensionArray that does not support 2D, or more specifically that does\n    not use HybridBlock.\n    \"\"\"\n    from pandas.core.arrays import (\n        DatetimeArray,\n        ExtensionArray,\n        PeriodArray,\n        TimedeltaArray,\n    )\n\n    return isinstance(obj, ExtensionArray) and not isinstance(\n        obj, (DatetimeArray, TimedeltaArray, PeriodArray)\n    )\n\n\ndef is_1d_only_ea_dtype(dtype: DtypeObj | None) -> bool:\n    \"\"\"\n    Analogue to is_extension_array_dtype but excluding DatetimeTZDtype.\n    \"\"\"\n    # Note: if other EA dtypes are ever held in HybridBlock, exclude those\n    #  here too.\n    # NB: need to check DatetimeTZDtype and not is_datetime64tz_dtype\n    #  to exclude ArrowTimestampUSDtype\n    return isinstance(dtype, ExtensionDtype) and not isinstance(\n        dtype, (DatetimeTZDtype, PeriodDtype)\n    )\n\n\ndef is_extension_array_dtype(arr_or_dtype) -> bool:\n    \"\"\"\n    Check if an object is a pandas extension array type.\n\n    See the :ref:`Use Guide <extending.extension-types>` for more.\n\n    Parameters\n    ----------\n    arr_or_dtype : object\n        For array-like input, the ``.dtype`` attribute will\n        be extracted.\n\n    Returns\n    -------\n    bool\n        Whether the `arr_or_dtype` is an extension array type.\n\n    Notes\n    -----\n    This checks whether an object implements the pandas extension\n    array interface. In pandas, this includes:\n\n    * Categorical\n    * Sparse\n    * Interval\n    * Period\n    * DatetimeArray\n    * TimedeltaArray\n\n    Third-party libraries may implement arrays or types satisfying\n    this interface as well.\n\n    Examples\n    --------\n    >>> from pandas.api.types import is_extension_array_dtype\n    >>> arr = pd.Categorical(['a', 'b'])\n    >>> is_extension_array_dtype(arr)\n    True\n    >>> is_extension_array_dtype(arr.dtype)\n    True\n\n    >>> arr = np.array(['a', 'b'])\n    >>> is_extension_array_dtype(arr.dtype)\n    False\n    \"\"\"\n    dtype = getattr(arr_or_dtype, \"dtype\", arr_or_dtype)\n    if isinstance(dtype, ExtensionDtype):\n        return True\n    elif isinstance(dtype, np.dtype):\n        return False\n    else:\n        return registry.find(dtype) is not None\n\n\ndef is_ea_or_datetimelike_dtype(dtype: DtypeObj | None) -> bool:\n    \"\"\"\n    Check for ExtensionDtype, datetime64 dtype, or timedelta64 dtype.\n\n    Notes\n    -----\n    Checks only for dtype objects, not dtype-castable strings or types.\n    \"\"\"\n    return isinstance(dtype, ExtensionDtype) or (\n        isinstance(dtype, np.dtype) and dtype.kind in [\"m\", \"M\"]\n    )\n\n\ndef is_complex_dtype(arr_or_dtype) -> bool:\n    \"\"\"\n    Check whether the provided array or dtype is of a complex dtype.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like or dtype\n        The array or dtype to check.\n\n    Returns\n    -------\n    boolean\n        Whether or not the array or dtype is of a complex dtype.\n\n    Examples\n    --------\n    >>> is_complex_dtype(str)\n    False\n    >>> is_complex_dtype(int)\n    False\n    >>> is_complex_dtype(np.complex_)\n    True\n    >>> is_complex_dtype(np.array(['a', 'b']))\n    False\n    >>> is_complex_dtype(pd.Series([1, 2]))\n    False\n    >>> is_complex_dtype(np.array([1 + 1j, 5]))\n    True\n    \"\"\"\n    return _is_dtype_type(arr_or_dtype, classes(np.complexfloating))\n\n\ndef _is_dtype(arr_or_dtype, condition) -> bool:\n    \"\"\"\n    Return true if the condition is satisfied for the arr_or_dtype.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like, str, np.dtype, or ExtensionArrayType\n        The array-like or dtype object whose dtype we want to extract.\n    condition : callable[Union[np.dtype, ExtensionDtype]]\n\n    Returns\n    -------\n    bool\n\n    \"\"\"\n    if arr_or_dtype is None:\n        return False\n    try:\n        dtype = get_dtype(arr_or_dtype)\n    except (TypeError, ValueError):\n        return False\n    return condition(dtype)\n\n\ndef get_dtype(arr_or_dtype) -> DtypeObj:\n    \"\"\"\n    Get the dtype instance associated with an array\n    or dtype object.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like or dtype\n        The array-like or dtype object whose dtype we want to extract.\n\n    Returns\n    -------\n    obj_dtype : The extract dtype instance from the\n                passed in array or dtype object.\n\n    Raises\n    ------\n    TypeError : The passed in object is None.\n    \"\"\"\n    if arr_or_dtype is None:\n        raise TypeError(\"Cannot deduce dtype from null object\")\n\n    # fastpath\n    elif isinstance(arr_or_dtype, np.dtype):\n        return arr_or_dtype\n    elif isinstance(arr_or_dtype, type):\n        return np.dtype(arr_or_dtype)\n\n    # if we have an array-like\n    elif hasattr(arr_or_dtype, \"dtype\"):\n        arr_or_dtype = arr_or_dtype.dtype\n\n    return pandas_dtype(arr_or_dtype)\n\n\ndef _is_dtype_type(arr_or_dtype, condition) -> bool:\n    \"\"\"\n    Return true if the condition is satisfied for the arr_or_dtype.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like or dtype\n        The array-like or dtype object whose dtype we want to extract.\n    condition : callable[Union[np.dtype, ExtensionDtypeType]]\n\n    Returns\n    -------\n    bool : if the condition is satisfied for the arr_or_dtype\n    \"\"\"\n    if arr_or_dtype is None:\n        return condition(type(None))\n\n    # fastpath\n    if isinstance(arr_or_dtype, np.dtype):\n        return condition(arr_or_dtype.type)\n    elif isinstance(arr_or_dtype, type):\n        if issubclass(arr_or_dtype, ExtensionDtype):\n            arr_or_dtype = arr_or_dtype.type\n        return condition(np.dtype(arr_or_dtype).type)\n\n    # if we have an array-like\n    if hasattr(arr_or_dtype, \"dtype\"):\n        arr_or_dtype = arr_or_dtype.dtype\n\n    # we are not possibly a dtype\n    elif is_list_like(arr_or_dtype):\n        return condition(type(None))\n\n    try:\n        tipo = pandas_dtype(arr_or_dtype).type\n    except (TypeError, ValueError):\n        if is_scalar(arr_or_dtype):\n            return condition(type(None))\n\n        return False\n\n    return condition(tipo)\n\n\ndef infer_dtype_from_object(dtype) -> type:\n    \"\"\"\n    Get a numpy dtype.type-style object for a dtype object.\n\n    This methods also includes handling of the datetime64[ns] and\n    datetime64[ns, TZ] objects.\n\n    If no dtype can be found, we return ``object``.\n\n    Parameters\n    ----------\n    dtype : dtype, type\n        The dtype object whose numpy dtype.type-style\n        object we want to extract.\n\n    Returns\n    -------\n    type\n    \"\"\"\n    if isinstance(dtype, type) and issubclass(dtype, np.generic):\n        # Type object from a dtype\n\n        return dtype\n    elif isinstance(dtype, (np.dtype, ExtensionDtype)):\n        # dtype object\n        try:\n            _validate_date_like_dtype(dtype)\n        except TypeError:\n            # Should still pass if we don't have a date-like\n            pass\n        return dtype.type\n\n    try:\n        dtype = pandas_dtype(dtype)\n    except TypeError:\n        pass\n\n    if is_extension_array_dtype(dtype):\n        return dtype.type\n    elif isinstance(dtype, str):\n\n        # TODO(jreback)\n        # should deprecate these\n        if dtype in [\"datetimetz\", \"datetime64tz\"]:\n            return DatetimeTZDtype.type\n        elif dtype in [\"period\"]:\n            raise NotImplementedError\n\n        if dtype in [\"datetime\", \"timedelta\"]:\n            dtype += \"64\"\n        try:\n            return infer_dtype_from_object(getattr(np, dtype))\n        except (AttributeError, TypeError):\n            # Handles cases like get_dtype(int) i.e.,\n            # Python objects that are valid dtypes\n            # (unlike user-defined types, in general)\n            #\n            # TypeError handles the float16 type code of 'e'\n            # further handle internal types\n            pass\n\n    return infer_dtype_from_object(np.dtype(dtype))\n\n\ndef _validate_date_like_dtype(dtype) -> None:\n    \"\"\"\n    Check whether the dtype is a date-like dtype. Raises an error if invalid.\n\n    Parameters\n    ----------\n    dtype : dtype, type\n        The dtype to check.\n\n    Raises\n    ------\n    TypeError : The dtype could not be casted to a date-like dtype.\n    ValueError : The dtype is an illegal date-like dtype (e.g. the\n                 frequency provided is too specific)\n    \"\"\"\n    try:\n        typ = np.datetime_data(dtype)[0]\n    except ValueError as e:\n        raise TypeError(e) from e\n    if typ not in [\"generic\", \"ns\"]:\n        raise ValueError(\n            f\"{repr(dtype.name)} is too specific of a frequency, \"\n            f\"try passing {repr(dtype.type.__name__)}\"\n        )\n\n\ndef validate_all_hashable(*args, error_name: str | None = None) -> None:\n    \"\"\"\n    Return None if all args are hashable, else raise a TypeError.\n\n    Parameters\n    ----------\n    *args\n        Arguments to validate.\n    error_name : str, optional\n        The name to use if error\n\n    Raises\n    ------\n    TypeError : If an argument is not hashable\n\n    Returns\n    -------\n    None\n    \"\"\"\n    if not all(is_hashable(arg) for arg in args):\n        if error_name:\n            raise TypeError(f\"{error_name} must be a hashable type\")\n        else:\n            raise TypeError(\"All elements must be hashable\")\n\n\ndef pandas_dtype(dtype) -> DtypeObj:\n    \"\"\"\n    Convert input into a pandas only dtype object or a numpy dtype object.\n\n    Parameters\n    ----------\n    dtype : object to be converted\n\n    Returns\n    -------\n    np.dtype or a pandas dtype\n\n    Raises\n    ------\n    TypeError if not a dtype\n    \"\"\"\n    # short-circuit\n    if isinstance(dtype, np.ndarray):\n        return dtype.dtype\n    elif isinstance(dtype, (np.dtype, ExtensionDtype)):\n        return dtype\n\n    # registered extension types\n    result = registry.find(dtype)\n    if result is not None:\n        return result\n\n    # try a numpy dtype\n    # raise a consistent TypeError if failed\n    try:\n        npdtype = np.dtype(dtype)\n    except SyntaxError as err:\n        # np.dtype uses `eval` which can raise SyntaxError\n        raise TypeError(f\"data type '{dtype}' not understood\") from err\n\n    # Any invalid dtype (such as pd.Timestamp) should raise an error.\n    # np.dtype(invalid_type).kind = 0 for such objects. However, this will\n    # also catch some valid dtypes such as object, np.object_ and 'object'\n    # which we safeguard against by catching them earlier and returning\n    # np.dtype(valid_dtype) before this condition is evaluated.\n    if is_hashable(dtype) and dtype in [object, np.object_, \"object\", \"O\"]:\n        # check hashability to avoid errors/DeprecationWarning when we get\n        # here and `dtype` is an array\n        return npdtype\n    elif npdtype.kind == \"O\":\n        raise TypeError(f\"dtype '{dtype}' not understood\")\n\n    return npdtype\n\n\ndef is_all_strings(value: ArrayLike) -> bool:\n    \"\"\"\n    Check if this is an array of strings that we should try parsing.\n\n    Includes object-dtype ndarray containing all-strings, StringArray,\n    and Categorical with all-string categories.\n    Does not include numpy string dtypes.\n    \"\"\"\n    dtype = value.dtype\n\n    if isinstance(dtype, np.dtype):\n        return (\n            dtype == np.dtype(\"object\")\n            and lib.infer_dtype(value, skipna=False) == \"string\"\n        )\n    elif isinstance(dtype, CategoricalDtype):\n        return dtype.categories.inferred_type == \"string\"\n    return dtype == \"string\"\n", 1814], "/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/internals/blocks.py": ["from __future__ import annotations\n\nfrom functools import wraps\nimport re\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Iterable,\n    Sequence,\n    cast,\n    final,\n)\nimport warnings\n\nimport numpy as np\n\nfrom pandas._libs import (\n    Timestamp,\n    algos as libalgos,\n    internals as libinternals,\n    lib,\n    writers,\n)\nfrom pandas._libs.internals import BlockPlacement\nfrom pandas._typing import (\n    ArrayLike,\n    DtypeObj,\n    F,\n    Shape,\n    npt,\n)\nfrom pandas.compat import np_version_under1p20\nfrom pandas.util._decorators import cache_readonly\nfrom pandas.util._exceptions import find_stack_level\nfrom pandas.util._validators import validate_bool_kwarg\n\nfrom pandas.core.dtypes.cast import (\n    astype_array_safe,\n    can_hold_element,\n    find_common_type,\n    infer_dtype_from,\n    maybe_downcast_numeric,\n    maybe_downcast_to_dtype,\n    maybe_upcast,\n    soft_convert_objects,\n)\nfrom pandas.core.dtypes.common import (\n    ensure_platform_int,\n    is_1d_only_ea_dtype,\n    is_1d_only_ea_obj,\n    is_dtype_equal,\n    is_extension_array_dtype,\n    is_interval_dtype,\n    is_list_like,\n    is_string_dtype,\n)\nfrom pandas.core.dtypes.dtypes import (\n    CategoricalDtype,\n    ExtensionDtype,\n    PandasDtype,\n    PeriodDtype,\n)\nfrom pandas.core.dtypes.generic import (\n    ABCDataFrame,\n    ABCIndex,\n    ABCPandasArray,\n    ABCSeries,\n)\nfrom pandas.core.dtypes.inference import is_inferred_bool_dtype\nfrom pandas.core.dtypes.missing import (\n    is_valid_na_for_dtype,\n    isna,\n    na_value_for_dtype,\n)\n\nimport pandas.core.algorithms as algos\nfrom pandas.core.array_algos.putmask import (\n    extract_bool_array,\n    putmask_inplace,\n    putmask_smart,\n    putmask_without_repeat,\n    setitem_datetimelike_compat,\n    validate_putmask,\n)\nfrom pandas.core.array_algos.quantile import quantile_compat\nfrom pandas.core.array_algos.replace import (\n    compare_or_regex_search,\n    replace_regex,\n    should_use_regex,\n)\nfrom pandas.core.array_algos.take import take_nd\nfrom pandas.core.array_algos.transforms import shift\nfrom pandas.core.arrays import (\n    Categorical,\n    DatetimeArray,\n    ExtensionArray,\n    IntervalArray,\n    PandasArray,\n    PeriodArray,\n    TimedeltaArray,\n)\nfrom pandas.core.arrays._mixins import NDArrayBackedExtensionArray\nfrom pandas.core.arrays.sparse import SparseDtype\nfrom pandas.core.base import PandasObject\nimport pandas.core.common as com\nimport pandas.core.computation.expressions as expressions\nfrom pandas.core.construction import (\n    ensure_wrapped_if_datetimelike,\n    extract_array,\n)\nfrom pandas.core.indexers import (\n    check_setitem_lengths,\n    is_empty_indexer,\n    is_scalar_indexer,\n)\nimport pandas.core.missing as missing\n\nif TYPE_CHECKING:\n    from pandas import (\n        Float64Index,\n        Index,\n    )\n\n# comparison is faster than is_object_dtype\n_dtype_obj = np.dtype(\"object\")\n\n\ndef maybe_split(meth: F) -> F:\n    \"\"\"\n    If we have a multi-column block, split and operate block-wise.  Otherwise\n    use the original method.\n    \"\"\"\n\n    @wraps(meth)\n    def newfunc(self, *args, **kwargs) -> list[Block]:\n\n        if self.ndim == 1 or self.shape[0] == 1:\n            return meth(self, *args, **kwargs)\n        else:\n            # Split and operate column-by-column\n            return self.split_and_operate(meth, *args, **kwargs)\n\n    return cast(F, newfunc)\n\n\nclass Block(PandasObject):\n    \"\"\"\n    Canonical n-dimensional unit of homogeneous dtype contained in a pandas\n    data structure\n\n    Index-ignorant; let the container take care of that\n    \"\"\"\n\n    values: np.ndarray | ExtensionArray\n    ndim: int\n    __init__: Callable\n\n    __slots__ = ()\n    is_numeric = False\n    is_object = False\n    is_extension = False\n    _can_consolidate = True\n    _validate_ndim = True\n\n    @final\n    @cache_readonly\n    def _consolidate_key(self):\n        return self._can_consolidate, self.dtype.name\n\n    @property\n    def is_view(self) -> bool:\n        \"\"\"return a boolean if I am possibly a view\"\"\"\n        values = self.values\n        values = cast(np.ndarray, values)\n        return values.base is not None\n\n    @final\n    @cache_readonly\n    def _can_hold_na(self) -> bool:\n        \"\"\"\n        Can we store NA values in this Block?\n        \"\"\"\n        dtype = self.dtype\n        if isinstance(dtype, np.dtype):\n            return dtype.kind not in [\"b\", \"i\", \"u\"]\n        return dtype._can_hold_na\n\n    @final\n    @cache_readonly\n    def is_categorical(self) -> bool:\n        warnings.warn(\n            \"Block.is_categorical is deprecated and will be removed in a \"\n            \"future version.  Use isinstance(block.values, Categorical) \"\n            \"instead. See https://github.com/pandas-dev/pandas/issues/40226\",\n            DeprecationWarning,\n            stacklevel=find_stack_level(),\n        )\n        return isinstance(self.values, Categorical)\n\n    @final\n    @property\n    def is_bool(self) -> bool:\n        \"\"\"\n        We can be bool if a) we are bool dtype or b) object dtype with bool objects.\n        \"\"\"\n        return is_inferred_bool_dtype(self.values)\n\n    @final\n    def external_values(self):\n        return external_values(self.values)\n\n    @property\n    def array_values(self) -> ExtensionArray:\n        \"\"\"\n        The array that Series.array returns. Always an ExtensionArray.\n        \"\"\"\n        # error: Argument 1 to \"PandasArray\" has incompatible type \"Union[ndarray,\n        # ExtensionArray]\"; expected \"Union[ndarray, PandasArray]\"\n        return PandasArray(self.values)  # type: ignore[arg-type]\n\n    def get_values(self, dtype: DtypeObj | None = None) -> np.ndarray:\n        \"\"\"\n        return an internal format, currently just the ndarray\n        this is often overridden to handle to_dense like operations\n        \"\"\"\n        if dtype == _dtype_obj:\n            return self.values.astype(_dtype_obj)\n        # error: Incompatible return value type (got \"Union[ndarray, ExtensionArray]\",\n        # expected \"ndarray\")\n        return self.values  # type: ignore[return-value]\n\n    def values_for_json(self) -> np.ndarray:\n        # Incompatible return value type (got \"Union[ndarray[Any, Any],\n        # ExtensionArray]\", expected \"ndarray[Any, Any]\")\n        return self.values  # type: ignore[return-value]\n\n    @final\n    @cache_readonly\n    def fill_value(self):\n        # Used in reindex_indexer\n        return na_value_for_dtype(self.dtype, compat=False)\n\n    @property\n    def mgr_locs(self) -> BlockPlacement:\n        return self._mgr_locs\n\n    @mgr_locs.setter\n    def mgr_locs(self, new_mgr_locs: BlockPlacement):\n        self._mgr_locs = new_mgr_locs\n\n    @final\n    def make_block(self, values, placement=None) -> Block:\n        \"\"\"\n        Create a new block, with type inference propagate any values that are\n        not specified\n        \"\"\"\n        if placement is None:\n            placement = self._mgr_locs\n        if self.is_extension:\n            values = ensure_block_shape(values, ndim=self.ndim)\n\n        # TODO: perf by not going through new_block\n        # We assume maybe_coerce_values has already been called\n        return new_block(values, placement=placement, ndim=self.ndim)\n\n    @final\n    def make_block_same_class(\n        self, values, placement: BlockPlacement | None = None\n    ) -> Block:\n        \"\"\"Wrap given values in a block of same type as self.\"\"\"\n        if placement is None:\n            placement = self._mgr_locs\n\n        if values.dtype.kind in [\"m\", \"M\"]:\n\n            new_values = ensure_wrapped_if_datetimelike(values)\n            if new_values is not values:\n                # TODO(2.0): remove once fastparquet has stopped relying on it\n                warnings.warn(\n                    \"In a future version, Block.make_block_same_class will \"\n                    \"assume that datetime64 and timedelta64 ndarrays have \"\n                    \"already been cast to DatetimeArray and TimedeltaArray, \"\n                    \"respectively.\",\n                    DeprecationWarning,\n                    stacklevel=find_stack_level(),\n                )\n            values = new_values\n\n        # We assume maybe_coerce_values has already been called\n        return type(self)(values, placement=placement, ndim=self.ndim)\n\n    @final\n    def __repr__(self) -> str:\n        # don't want to print out all of the items here\n        name = type(self).__name__\n        if self.ndim == 1:\n            result = f\"{name}: {len(self)} dtype: {self.dtype}\"\n        else:\n\n            shape = \" x \".join([str(s) for s in self.shape])\n            result = f\"{name}: {self.mgr_locs.indexer}, {shape}, dtype: {self.dtype}\"\n\n        return result\n\n    @final\n    def __len__(self) -> int:\n        return len(self.values)\n\n    def _slice(self, slicer) -> ArrayLike:\n        \"\"\"return a slice of my values\"\"\"\n\n        return self.values[slicer]\n\n    @final\n    def getitem_block(self, slicer: slice | npt.NDArray[np.intp]) -> Block:\n        \"\"\"\n        Perform __getitem__-like, return result as block.\n\n        Only supports slices that preserve dimensionality.\n        \"\"\"\n        axis0_slicer = slicer[0] if isinstance(slicer, tuple) else slicer\n        new_mgr_locs = self._mgr_locs[axis0_slicer]\n\n        new_values = self._slice(slicer)\n\n        if new_values.ndim != self.values.ndim:\n            raise ValueError(\"Only same dim slicing is allowed\")\n\n        return type(self)(new_values, new_mgr_locs, self.ndim)\n\n    @final\n    def getitem_block_columns(\n        self, slicer: slice, new_mgr_locs: BlockPlacement\n    ) -> Block:\n        \"\"\"\n        Perform __getitem__-like, return result as block.\n\n        Only supports slices that preserve dimensionality.\n        \"\"\"\n        new_values = self._slice(slicer)\n\n        if new_values.ndim != self.values.ndim:\n            raise ValueError(\"Only same dim slicing is allowed\")\n\n        return type(self)(new_values, new_mgr_locs, self.ndim)\n\n    # NB: this cannot be made cache_readonly because in libreduction we pin\n    #  new .values that can have different shape GH#42631\n    @property\n    def shape(self) -> Shape:\n        return self.values.shape\n\n    @cache_readonly\n    def dtype(self) -> DtypeObj:\n        return self.values.dtype\n\n    def iget(self, i: int | tuple[int, int] | tuple[slice, int]):\n        # In the case where we have a tuple[slice, int], the slice will always\n        #  be slice(None)\n        # Note: only reached with self.ndim == 2\n        # Invalid index type \"Union[int, Tuple[int, int], Tuple[slice, int]]\"\n        # for \"Union[ndarray[Any, Any], ExtensionArray]\"; expected type\n        # \"Union[int, integer[Any]]\"\n        return self.values[i]  # type: ignore[index]\n\n    def set_inplace(self, locs, values) -> None:\n        \"\"\"\n        Modify block values in-place with new item value.\n\n        Notes\n        -----\n        `set` never creates a new array or new Block, whereas `setitem` _may_\n        create a new array and always creates a new Block.\n        \"\"\"\n        self.values[locs] = values\n\n    def delete(self, loc) -> None:\n        \"\"\"\n        Delete given loc(-s) from block in-place.\n        \"\"\"\n        # Argument 1 to \"delete\" has incompatible type \"Union[ndarray[Any, Any],\n        # ExtensionArray]\"; expected \"Union[_SupportsArray[dtype[Any]],\n        # Sequence[_SupportsArray[dtype[Any]]], Sequence[Sequence\n        # [_SupportsArray[dtype[Any]]]], Sequence[Sequence[Sequence[\n        # _SupportsArray[dtype[Any]]]]], Sequence[Sequence[Sequence[Sequence[\n        # _SupportsArray[dtype[Any]]]]]]]\"  [arg-type]\n        self.values = np.delete(self.values, loc, 0)  # type: ignore[arg-type]\n        self.mgr_locs = self._mgr_locs.delete(loc)\n        try:\n            self._cache.clear()\n        except AttributeError:\n            # _cache not yet initialized\n            pass\n\n    @final\n    def apply(self, func, **kwargs) -> list[Block]:\n        \"\"\"\n        apply the function to my values; return a block if we are not\n        one\n        \"\"\"\n        result = func(self.values, **kwargs)\n\n        return self._split_op_result(result)\n\n    def reduce(self, func, ignore_failures: bool = False) -> list[Block]:\n        # We will apply the function and reshape the result into a single-row\n        #  Block with the same mgr_locs; squeezing will be done at a higher level\n        assert self.ndim == 2\n\n        try:\n            result = func(self.values)\n        except (TypeError, NotImplementedError):\n            if ignore_failures:\n                return []\n            raise\n\n        if self.values.ndim == 1:\n            # TODO(EA2D): special case not needed with 2D EAs\n            res_values = np.array([[result]])\n        else:\n            res_values = result.reshape(-1, 1)\n\n        nb = self.make_block(res_values)\n        return [nb]\n\n    @final\n    def _split_op_result(self, result: ArrayLike) -> list[Block]:\n        # See also: split_and_operate\n        if result.ndim > 1 and isinstance(result.dtype, ExtensionDtype):\n            # TODO(EA2D): unnecessary with 2D EAs\n            # if we get a 2D ExtensionArray, we need to split it into 1D pieces\n            nbs = []\n            for i, loc in enumerate(self._mgr_locs):\n                if not is_1d_only_ea_obj(result):\n                    vals = result[i : i + 1]\n                else:\n                    vals = result[i]\n\n                block = self.make_block(values=vals, placement=loc)\n                nbs.append(block)\n            return nbs\n\n        nb = self.make_block(result)\n\n        return [nb]\n\n    def fillna(\n        self, value, limit=None, inplace: bool = False, downcast=None\n    ) -> list[Block]:\n        \"\"\"\n        fillna on the block with the value. If we fail, then convert to\n        ObjectBlock and try again\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n\n        mask = isna(self.values)\n        mask, noop = validate_putmask(self.values, mask)\n\n        if limit is not None:\n            limit = libalgos.validate_limit(None, limit=limit)\n            mask[mask.cumsum(self.ndim - 1) > limit] = False\n\n        if not self._can_hold_na:\n            if inplace:\n                return [self]\n            else:\n                return [self.copy()]\n\n        if self._can_hold_element(value):\n            nb = self if inplace else self.copy()\n            putmask_inplace(nb.values, mask, value)\n            return nb._maybe_downcast([nb], downcast)\n\n        if noop:\n            # we can't process the value, but nothing to do\n            return [self] if inplace else [self.copy()]\n\n        elif self.ndim == 1 or self.shape[0] == 1:\n            blk = self.coerce_to_target_dtype(value)\n            # bc we have already cast, inplace=True may avoid an extra copy\n            return blk.fillna(value, limit=limit, inplace=True, downcast=None)\n\n        else:\n            # operate column-by-column\n            return self.split_and_operate(\n                type(self).fillna, value, limit=limit, inplace=inplace, downcast=None\n            )\n\n    @final\n    def _split(self) -> list[Block]:\n        \"\"\"\n        Split a block into a list of single-column blocks.\n        \"\"\"\n        assert self.ndim == 2\n\n        new_blocks = []\n        for i, ref_loc in enumerate(self._mgr_locs):\n            vals = self.values[slice(i, i + 1)]\n\n            bp = BlockPlacement(ref_loc)\n            nb = type(self)(vals, placement=bp, ndim=2)\n            new_blocks.append(nb)\n        return new_blocks\n\n    @final\n    def split_and_operate(self, func, *args, **kwargs) -> list[Block]:\n        \"\"\"\n        Split the block and apply func column-by-column.\n\n        Parameters\n        ----------\n        func : Block method\n        *args\n        **kwargs\n\n        Returns\n        -------\n        List[Block]\n        \"\"\"\n        assert self.ndim == 2 and self.shape[0] != 1\n\n        res_blocks = []\n        for nb in self._split():\n            rbs = func(nb, *args, **kwargs)\n            res_blocks.extend(rbs)\n        return res_blocks\n\n    @final\n    def _maybe_downcast(self, blocks: list[Block], downcast=None) -> list[Block]:\n        if downcast is False:\n            return blocks\n\n        if self.dtype == _dtype_obj:\n            # GH#44241 We downcast regardless of the argument;\n            #  respecting 'downcast=None' may be worthwhile at some point,\n            #  but ATM it breaks too much existing code.\n            # split and convert the blocks\n\n            return extend_blocks(\n                [blk.convert(datetime=True, numeric=False) for blk in blocks]\n            )\n\n        if downcast is None:\n            return blocks\n\n        return extend_blocks([b._downcast_2d(downcast) for b in blocks])\n\n    @final\n    @maybe_split\n    def _downcast_2d(self, dtype) -> list[Block]:\n        \"\"\"\n        downcast specialized to 2D case post-validation.\n\n        Refactored to allow use of maybe_split.\n        \"\"\"\n        new_values = maybe_downcast_to_dtype(self.values, dtype=dtype)\n        return [self.make_block(new_values)]\n\n    @final\n    def astype(self, dtype: DtypeObj, copy: bool = False, errors: str = \"raise\"):\n        \"\"\"\n        Coerce to the new dtype.\n\n        Parameters\n        ----------\n        dtype : np.dtype or ExtensionDtype\n        copy : bool, default False\n            copy if indicated\n        errors : str, {'raise', 'ignore'}, default 'raise'\n            - ``raise`` : allow exceptions to be raised\n            - ``ignore`` : suppress exceptions. On error return original object\n\n        Returns\n        -------\n        Block\n        \"\"\"\n        values = self.values\n\n        new_values = astype_array_safe(values, dtype, copy=copy, errors=errors)\n\n        new_values = maybe_coerce_values(new_values)\n        newb = self.make_block(new_values)\n        if newb.shape != self.shape:\n            raise TypeError(\n                f\"cannot set astype for copy = [{copy}] for dtype \"\n                f\"({self.dtype.name} [{self.shape}]) to different shape \"\n                f\"({newb.dtype.name} [{newb.shape}])\"\n            )\n        return newb\n\n    def convert(\n        self,\n        copy: bool = True,\n        datetime: bool = True,\n        numeric: bool = True,\n        timedelta: bool = True,\n    ) -> list[Block]:\n        \"\"\"\n        attempt to coerce any object types to better types return a copy\n        of the block (if copy = True) by definition we are not an ObjectBlock\n        here!\n        \"\"\"\n        return [self.copy()] if copy else [self]\n\n    @final\n    def _can_hold_element(self, element: Any) -> bool:\n        \"\"\"require the same dtype as ourselves\"\"\"\n        element = extract_array(element, extract_numpy=True)\n        return can_hold_element(self.values, element)\n\n    @final\n    def should_store(self, value: ArrayLike) -> bool:\n        \"\"\"\n        Should we set self.values[indexer] = value inplace or do we need to cast?\n\n        Parameters\n        ----------\n        value : np.ndarray or ExtensionArray\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        # faster equivalent to is_dtype_equal(value.dtype, self.dtype)\n        try:\n            return value.dtype == self.dtype\n        except TypeError:\n            return False\n\n    @final\n    def to_native_types(self, na_rep=\"nan\", quoting=None, **kwargs):\n        \"\"\"convert to our native types format\"\"\"\n        result = to_native_types(self.values, na_rep=na_rep, quoting=quoting, **kwargs)\n        return self.make_block(result)\n\n    # block actions #\n    @final\n    def copy(self, deep: bool = True):\n        \"\"\"copy constructor\"\"\"\n        values = self.values\n        if deep:\n            values = values.copy()\n        return type(self)(values, placement=self._mgr_locs, ndim=self.ndim)\n\n    # ---------------------------------------------------------------------\n    # Replace\n\n    @final\n    def replace(\n        self,\n        to_replace,\n        value,\n        inplace: bool = False,\n        # mask may be pre-computed if we're called from replace_list\n        mask: npt.NDArray[np.bool_] | None = None,\n    ) -> list[Block]:\n        \"\"\"\n        replace the to_replace value with value, possible to create new\n        blocks here this is just a call to putmask.\n        \"\"\"\n\n        # Note: the checks we do in NDFrame.replace ensure we never get\n        #  here with listlike to_replace or value, as those cases\n        #  go through replace_list\n\n        values = self.values\n\n        if isinstance(values, Categorical):\n            # TODO: avoid special-casing\n            blk = self if inplace else self.copy()\n            blk.values._replace(to_replace=to_replace, value=value, inplace=True)\n            return [blk]\n\n        if not self._can_hold_element(to_replace):\n            # We cannot hold `to_replace`, so we know immediately that\n            #  replacing it is a no-op.\n            # Note: If to_replace were a list, NDFrame.replace would call\n            #  replace_list instead of replace.\n            return [self] if inplace else [self.copy()]\n\n        if mask is None:\n            mask = missing.mask_missing(values, to_replace)\n        if not mask.any():\n            # Note: we get here with test_replace_extension_other incorrectly\n            #  bc _can_hold_element is incorrect.\n            return [self] if inplace else [self.copy()]\n\n        elif self._can_hold_element(value):\n            blk = self if inplace else self.copy()\n            putmask_inplace(blk.values, mask, value)\n            if not (self.is_object and value is None):\n                # if the user *explicitly* gave None, we keep None, otherwise\n                #  may downcast to NaN\n                blocks = blk.convert(numeric=False, copy=False)\n            else:\n                blocks = [blk]\n            return blocks\n\n        elif self.ndim == 1 or self.shape[0] == 1:\n            blk = self.coerce_to_target_dtype(value)\n            return blk.replace(\n                to_replace=to_replace,\n                value=value,\n                inplace=True,\n                mask=mask,\n            )\n\n        else:\n            # split so that we only upcast where necessary\n            blocks = []\n            for i, nb in enumerate(self._split()):\n                blocks.extend(\n                    type(self).replace(\n                        nb,\n                        to_replace=to_replace,\n                        value=value,\n                        inplace=True,\n                        mask=mask[i : i + 1],\n                    )\n                )\n            return blocks\n\n    @final\n    def _replace_regex(\n        self,\n        to_replace,\n        value,\n        inplace: bool = False,\n        convert: bool = True,\n        mask=None,\n    ) -> list[Block]:\n        \"\"\"\n        Replace elements by the given value.\n\n        Parameters\n        ----------\n        to_replace : object or pattern\n            Scalar to replace or regular expression to match.\n        value : object\n            Replacement object.\n        inplace : bool, default False\n            Perform inplace modification.\n        convert : bool, default True\n            If true, try to coerce any object types to better types.\n        mask : array-like of bool, optional\n            True indicate corresponding element is ignored.\n\n        Returns\n        -------\n        List[Block]\n        \"\"\"\n        if not self._can_hold_element(to_replace):\n            # i.e. only ObjectBlock, but could in principle include a\n            #  String ExtensionBlock\n            return [self] if inplace else [self.copy()]\n\n        rx = re.compile(to_replace)\n\n        new_values = self.values if inplace else self.values.copy()\n        replace_regex(new_values, rx, value, mask)\n\n        block = self.make_block(new_values)\n        return block.convert(numeric=False, copy=False)\n\n    @final\n    def replace_list(\n        self,\n        src_list: Iterable[Any],\n        dest_list: Sequence[Any],\n        inplace: bool = False,\n        regex: bool = False,\n    ) -> list[Block]:\n        \"\"\"\n        See BlockManager.replace_list docstring.\n        \"\"\"\n        values = self.values\n\n        # Exclude anything that we know we won't contain\n        pairs = [\n            (x, y) for x, y in zip(src_list, dest_list) if self._can_hold_element(x)\n        ]\n        if not len(pairs):\n            # shortcut, nothing to replace\n            return [self] if inplace else [self.copy()]\n\n        src_len = len(pairs) - 1\n\n        if is_string_dtype(values.dtype):\n            # Calculate the mask once, prior to the call of comp\n            # in order to avoid repeating the same computations\n            mask = ~isna(values)\n            masks = [\n                compare_or_regex_search(values, s[0], regex=regex, mask=mask)\n                for s in pairs\n            ]\n        else:\n            # GH#38086 faster if we know we dont need to check for regex\n            masks = [missing.mask_missing(values, s[0]) for s in pairs]\n\n        # error: Argument 1 to \"extract_bool_array\" has incompatible type\n        # \"Union[ExtensionArray, ndarray, bool]\"; expected \"Union[ExtensionArray,\n        # ndarray]\"\n        masks = [extract_bool_array(x) for x in masks]  # type: ignore[arg-type]\n\n        rb = [self if inplace else self.copy()]\n        for i, (src, dest) in enumerate(pairs):\n            convert = i == src_len  # only convert once at the end\n            new_rb: list[Block] = []\n\n            # GH-39338: _replace_coerce can split a block into\n            # single-column blocks, so track the index so we know\n            # where to index into the mask\n            for blk_num, blk in enumerate(rb):\n                if len(rb) == 1:\n                    m = masks[i]\n                else:\n                    mib = masks[i]\n                    assert not isinstance(mib, bool)\n                    m = mib[blk_num : blk_num + 1]\n\n                result = blk._replace_coerce(\n                    to_replace=src,\n                    value=dest,\n                    mask=m,\n                    inplace=inplace,\n                    regex=regex,\n                )\n                if convert and blk.is_object and not all(x is None for x in dest_list):\n                    # GH#44498 avoid unwanted cast-back\n                    result = extend_blocks(\n                        [b.convert(numeric=False, copy=True) for b in result]\n                    )\n                new_rb.extend(result)\n            rb = new_rb\n        return rb\n\n    @final\n    def _replace_coerce(\n        self,\n        to_replace,\n        value,\n        mask: np.ndarray,\n        inplace: bool = True,\n        regex: bool = False,\n    ) -> list[Block]:\n        \"\"\"\n        Replace value corresponding to the given boolean array with another\n        value.\n\n        Parameters\n        ----------\n        to_replace : object or pattern\n            Scalar to replace or regular expression to match.\n        value : object\n            Replacement object.\n        mask : np.ndarray[bool]\n            True indicate corresponding element is ignored.\n        inplace : bool, default True\n            Perform inplace modification.\n        regex : bool, default False\n            If true, perform regular expression substitution.\n\n        Returns\n        -------\n        List[Block]\n        \"\"\"\n        if should_use_regex(regex, to_replace):\n            return self._replace_regex(\n                to_replace,\n                value,\n                inplace=inplace,\n                convert=False,\n                mask=mask,\n            )\n        else:\n            if value is None:\n                # gh-45601, gh-45836\n                nb = self.astype(np.dtype(object), copy=False)\n                if nb is self and not inplace:\n                    nb = nb.copy()\n                putmask_inplace(nb.values, mask, value)\n                return [nb]\n            return self.replace(\n                to_replace=to_replace, value=value, inplace=inplace, mask=mask\n            )\n\n    # ---------------------------------------------------------------------\n\n    def _maybe_squeeze_arg(self, arg: np.ndarray) -> np.ndarray:\n        \"\"\"\n        For compatibility with 1D-only ExtensionArrays.\n        \"\"\"\n        return arg\n\n    def setitem(self, indexer, value):\n        \"\"\"\n        Attempt self.values[indexer] = value, possibly creating a new array.\n\n        Parameters\n        ----------\n        indexer : tuple, list-like, array-like, slice, int\n            The subset of self.values to set\n        value : object\n            The value being set\n\n        Returns\n        -------\n        Block\n\n        Notes\n        -----\n        `indexer` is a direct slice/positional indexer. `value` must\n        be a compatible shape.\n        \"\"\"\n        transpose = self.ndim == 2\n\n        if isinstance(indexer, np.ndarray) and indexer.ndim > self.ndim:\n            raise ValueError(f\"Cannot set values with ndim > {self.ndim}\")\n\n        # coerce None values, if appropriate\n        if value is None:\n            if self.is_numeric:\n                value = np.nan\n\n        # coerce if block dtype can store value\n        values = cast(np.ndarray, self.values)\n        if not self._can_hold_element(value):\n            # current dtype cannot store value, coerce to common dtype\n            return self.coerce_to_target_dtype(value).setitem(indexer, value)\n\n        # value must be storable at this moment\n        if is_extension_array_dtype(getattr(value, \"dtype\", None)):\n            # We need to be careful not to allow through strings that\n            #  can be parsed to EADtypes\n            arr_value = value\n        else:\n            arr_value = np.asarray(value)\n\n        if transpose:\n            values = values.T\n\n        # length checking\n        check_setitem_lengths(indexer, value, values)\n\n        if is_empty_indexer(indexer, arr_value):\n            # GH#8669 empty indexers, test_loc_setitem_boolean_mask_allfalse\n            pass\n\n        elif is_scalar_indexer(indexer, self.ndim):\n            # setting a single element for each dim and with a rhs that could\n            #  be e.g. a list; see GH#6043\n            values[indexer] = value\n\n        else:\n            value = setitem_datetimelike_compat(values, len(values[indexer]), value)\n            values[indexer] = value\n\n        return self\n\n    def putmask(self, mask, new) -> list[Block]:\n        \"\"\"\n        putmask the data to the block; it is possible that we may create a\n        new dtype of block\n\n        Return the resulting block(s).\n\n        Parameters\n        ----------\n        mask : np.ndarray[bool], SparseArray[bool], or BooleanArray\n        new : a ndarray/object\n\n        Returns\n        -------\n        List[Block]\n        \"\"\"\n        orig_mask = mask\n        values = cast(np.ndarray, self.values)\n        mask, noop = validate_putmask(values.T, mask)\n        assert not isinstance(new, (ABCIndex, ABCSeries, ABCDataFrame))\n\n        if new is lib.no_default:\n            new = self.fill_value\n\n        # if we are passed a scalar None, convert it here\n        if not self.is_object and is_valid_na_for_dtype(new, self.dtype):\n            new = self.fill_value\n\n        if self._can_hold_element(new):\n            putmask_without_repeat(values.T, mask, new)\n            return [self]\n\n        elif np_version_under1p20 and infer_dtype_from(new)[0].kind in [\"m\", \"M\"]:\n            # using putmask with object dtype will incorrectly cast to object\n            # Having excluded self._can_hold_element, we know we cannot operate\n            #  in-place, so we are safe using `where`\n            return self.where(new, ~mask)\n\n        elif noop:\n            return [self]\n\n        elif self.ndim == 1 or self.shape[0] == 1:\n            # no need to split columns\n\n            if not is_list_like(new):\n                # putmask_smart can't save us the need to cast\n                return self.coerce_to_target_dtype(new).putmask(mask, new)\n\n            # This differs from\n            #  `self.coerce_to_target_dtype(new).putmask(mask, new)`\n            # because putmask_smart will check if new[mask] may be held\n            # by our dtype.\n            nv = putmask_smart(values.T, mask, new).T\n            return [self.make_block(nv)]\n\n        else:\n            is_array = isinstance(new, np.ndarray)\n\n            res_blocks = []\n            nbs = self._split()\n            for i, nb in enumerate(nbs):\n                n = new\n                if is_array:\n                    # we have a different value per-column\n                    n = new[:, i : i + 1]\n\n                submask = orig_mask[:, i : i + 1]\n                rbs = nb.putmask(submask, n)\n                res_blocks.extend(rbs)\n            return res_blocks\n\n    @final\n    def coerce_to_target_dtype(self, other) -> Block:\n        \"\"\"\n        coerce the current block to a dtype compat for other\n        we will return a block, possibly object, and not raise\n\n        we can also safely try to coerce to the same dtype\n        and will receive the same block\n        \"\"\"\n        # if we cannot then coerce to object\n        dtype, _ = infer_dtype_from(other, pandas_dtype=True)\n\n        new_dtype = find_common_type([self.dtype, dtype])\n\n        return self.astype(new_dtype, copy=False)\n\n    def interpolate(\n        self,\n        method: str = \"pad\",\n        axis: int = 0,\n        index: Index | None = None,\n        inplace: bool = False,\n        limit: int | None = None,\n        limit_direction: str = \"forward\",\n        limit_area: str | None = None,\n        fill_value: Any | None = None,\n        coerce: bool = False,\n        downcast: str | None = None,\n        **kwargs,\n    ) -> list[Block]:\n\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n\n        if not self._can_hold_na:\n            # If there are no NAs, then interpolate is a no-op\n            return [self] if inplace else [self.copy()]\n\n        if self.is_object and self.ndim == 2 and self.shape[0] != 1 and axis == 0:\n            # split improves performance in ndarray.copy()\n            return self.split_and_operate(\n                type(self).interpolate,\n                method,\n                axis,\n                index,\n                inplace,\n                limit,\n                limit_direction,\n                limit_area,\n                fill_value,\n                coerce,\n                downcast,\n                **kwargs,\n            )\n\n        try:\n            m = missing.clean_fill_method(method)\n        except ValueError:\n            m = None\n        if m is None and self.dtype.kind != \"f\":\n            # only deal with floats\n            # bc we already checked that can_hold_na, we dont have int dtype here\n            # TODO: make a copy if not inplace?\n            return [self]\n\n        data = self.values if inplace else self.values.copy()\n        data = cast(np.ndarray, data)  # bc overridden by ExtensionBlock\n\n        missing.interpolate_array_2d(\n            data,\n            method=method,\n            axis=axis,\n            index=index,\n            limit=limit,\n            limit_direction=limit_direction,\n            limit_area=limit_area,\n            fill_value=fill_value,\n            **kwargs,\n        )\n\n        nb = self.make_block_same_class(data)\n        return nb._maybe_downcast([nb], downcast)\n\n    def take_nd(\n        self,\n        indexer,\n        axis: int,\n        new_mgr_locs: BlockPlacement | None = None,\n        fill_value=lib.no_default,\n    ) -> Block:\n        \"\"\"\n        Take values according to indexer and return them as a block.bb\n\n        \"\"\"\n        # algos.take_nd dispatches for DatetimeTZBlock, CategoricalBlock\n        # so need to preserve types\n        # sparse is treated like an ndarray, but needs .get_values() shaping\n\n        values = self.values\n\n        if fill_value is lib.no_default:\n            fill_value = self.fill_value\n            allow_fill = False\n        else:\n            allow_fill = True\n\n        new_values = algos.take_nd(\n            values, indexer, axis=axis, allow_fill=allow_fill, fill_value=fill_value\n        )\n\n        # Called from three places in managers, all of which satisfy\n        #  this assertion\n        assert not (axis == 0 and new_mgr_locs is None)\n        if new_mgr_locs is None:\n            new_mgr_locs = self._mgr_locs\n\n        if not is_dtype_equal(new_values.dtype, self.dtype):\n            return self.make_block(new_values, new_mgr_locs)\n        else:\n            return self.make_block_same_class(new_values, new_mgr_locs)\n\n    def diff(self, n: int, axis: int = 1) -> list[Block]:\n        \"\"\"return block for the diff of the values\"\"\"\n        new_values = algos.diff(self.values, n, axis=axis)\n        return [self.make_block(values=new_values)]\n\n    def shift(self, periods: int, axis: int = 0, fill_value: Any = None) -> list[Block]:\n        \"\"\"shift the block by periods, possibly upcast\"\"\"\n        # convert integer to float if necessary. need to do a lot more than\n        # that, handle boolean etc also\n\n        values = cast(np.ndarray, self.values)\n\n        new_values, fill_value = maybe_upcast(values, fill_value)\n\n        new_values = shift(new_values, periods, axis, fill_value)\n\n        return [self.make_block(new_values)]\n\n    def where(self, other, cond) -> list[Block]:\n        \"\"\"\n        evaluate the block; return result block(s) from the result\n\n        Parameters\n        ----------\n        other : a ndarray/object\n        cond : np.ndarray[bool], SparseArray[bool], or BooleanArray\n\n        Returns\n        -------\n        List[Block]\n        \"\"\"\n        assert cond.ndim == self.ndim\n        assert not isinstance(other, (ABCIndex, ABCSeries, ABCDataFrame))\n\n        transpose = self.ndim == 2\n\n        # EABlocks override where\n        values = cast(np.ndarray, self.values)\n        orig_other = other\n        if transpose:\n            values = values.T\n\n        icond, noop = validate_putmask(values, ~cond)\n        if noop:\n            # GH-39595: Always return a copy; short-circuit up/downcasting\n            return self.copy()\n\n        if other is lib.no_default:\n            other = self.fill_value\n\n        if is_valid_na_for_dtype(other, self.dtype) and self.dtype != _dtype_obj:\n            other = self.fill_value\n\n        if not self._can_hold_element(other):\n            # we cannot coerce, return a compat dtype\n            block = self.coerce_to_target_dtype(other)\n            blocks = block.where(orig_other, cond)\n            return self._maybe_downcast(blocks, \"infer\")\n\n        else:\n            alt = setitem_datetimelike_compat(values, icond.sum(), other)\n            if alt is not other:\n                if is_list_like(other) and len(other) < len(values):\n                    # call np.where with other to get the appropriate ValueError\n                    np.where(~icond, values, other)\n                    raise NotImplementedError(\n                        \"This should not be reached; call to np.where above is \"\n                        \"expected to raise ValueError. Please report a bug at \"\n                        \"github.com/pandas-dev/pandas\"\n                    )\n                result = values.copy()\n                np.putmask(result, icond, alt)\n            else:\n                # By the time we get here, we should have all Series/Index\n                #  args extracted to ndarray\n                if (\n                    is_list_like(other)\n                    and not isinstance(other, np.ndarray)\n                    and len(other) == self.shape[-1]\n                ):\n                    # If we don't do this broadcasting here, then expressions.where\n                    #  will broadcast a 1D other to be row-like instead of\n                    #  column-like.\n                    other = np.array(other).reshape(values.shape)\n                    # If lengths don't match (or len(other)==1), we will raise\n                    #  inside expressions.where, see test_series_where\n\n                # Note: expressions.where may upcast.\n                result = expressions.where(~icond, values, other)\n\n        if self._can_hold_na or self.ndim == 1:\n\n            if transpose:\n                result = result.T\n\n            return [self.make_block(result)]\n\n        # might need to separate out blocks\n        cond = ~icond\n        axis = cond.ndim - 1\n        cond = cond.swapaxes(axis, 0)\n        mask = cond.all(axis=1)\n\n        result_blocks: list[Block] = []\n        for m in [mask, ~mask]:\n            if m.any():\n                taken = result.take(m.nonzero()[0], axis=axis)\n                r = maybe_downcast_numeric(taken, self.dtype)\n                if r.dtype != taken.dtype:\n                    warnings.warn(\n                        \"Downcasting integer-dtype results in .where is \"\n                        \"deprecated and will change in a future version. \"\n                        \"To retain the old behavior, explicitly cast the results \"\n                        \"to the desired dtype.\",\n                        FutureWarning,\n                        stacklevel=find_stack_level(),\n                    )\n                nb = self.make_block(r.T, placement=self._mgr_locs[m])\n                result_blocks.append(nb)\n\n        return result_blocks\n\n    def _unstack(\n        self,\n        unstacker,\n        fill_value,\n        new_placement: npt.NDArray[np.intp],\n        needs_masking: npt.NDArray[np.bool_],\n    ):\n        \"\"\"\n        Return a list of unstacked blocks of self\n\n        Parameters\n        ----------\n        unstacker : reshape._Unstacker\n        fill_value : int\n            Only used in ExtensionBlock._unstack\n        new_placement : np.ndarray[np.intp]\n        allow_fill : bool\n        needs_masking : np.ndarray[bool]\n\n        Returns\n        -------\n        blocks : list of Block\n            New blocks of unstacked values.\n        mask : array-like of bool\n            The mask of columns of `blocks` we should keep.\n        \"\"\"\n        new_values, mask = unstacker.get_new_values(\n            self.values.T, fill_value=fill_value\n        )\n\n        mask = mask.any(0)\n        # TODO: in all tests we have mask.all(); can we rely on that?\n\n        # Note: these next two lines ensure that\n        #  mask.sum() == sum(len(nb.mgr_locs) for nb in blocks)\n        #  which the calling function needs in order to pass verify_integrity=False\n        #  to the BlockManager constructor\n        new_values = new_values.T[mask]\n        new_placement = new_placement[mask]\n\n        bp = BlockPlacement(new_placement)\n        blocks = [new_block_2d(new_values, placement=bp)]\n        return blocks, mask\n\n    @final\n    def quantile(\n        self, qs: Float64Index, interpolation=\"linear\", axis: int = 0\n    ) -> Block:\n        \"\"\"\n        compute the quantiles of the\n\n        Parameters\n        ----------\n        qs : Float64Index\n            List of the quantiles to be computed.\n        interpolation : str, default 'linear'\n            Type of interpolation.\n        axis : int, default 0\n            Axis to compute.\n\n        Returns\n        -------\n        Block\n        \"\"\"\n        # We should always have ndim == 2 because Series dispatches to DataFrame\n        assert self.ndim == 2\n        assert axis == 1  # only ever called this way\n        assert is_list_like(qs)  # caller is responsible for this\n\n        result = quantile_compat(self.values, np.asarray(qs._values), interpolation)\n        # ensure_block_shape needed for cases where we start with EA and result\n        #  is ndarray, e.g. IntegerArray, SparseArray\n        result = ensure_block_shape(result, ndim=2)\n        return new_block_2d(result, placement=self._mgr_locs)\n\n\nclass EABackedBlock(Block):\n    \"\"\"\n    Mixin for Block subclasses backed by ExtensionArray.\n    \"\"\"\n\n    values: ExtensionArray\n\n    def where(self, other, cond) -> list[Block]:\n        arr = self.values.T\n\n        cond = extract_bool_array(cond)\n\n        other = self._maybe_squeeze_arg(other)\n        cond = self._maybe_squeeze_arg(cond)\n\n        if other is lib.no_default:\n            other = self.fill_value\n\n        icond, noop = validate_putmask(arr, ~cond)\n        if noop:\n            # GH#44181, GH#45135\n            # Avoid a) raising for Interval/PeriodDtype and b) unnecessary object upcast\n            return self.copy()\n\n        try:\n            res_values = arr._where(cond, other).T\n        except (ValueError, TypeError) as err:\n            _catch_deprecated_value_error(err)\n\n            if is_interval_dtype(self.dtype):\n                # TestSetitemFloatIntervalWithIntIntervalValues\n                blk = self.coerce_to_target_dtype(other)\n                if blk.dtype == _dtype_obj:\n                    # For now at least only support casting e.g.\n                    #  Interval[int64]->Interval[float64]\n                    raise\n                return blk.where(other, cond)\n\n            elif isinstance(self, NDArrayBackedExtensionBlock):\n                # NB: not (yet) the same as\n                #  isinstance(values, NDArrayBackedExtensionArray)\n                if isinstance(self.dtype, PeriodDtype):\n                    # TODO: don't special-case\n                    raise\n                blk = self.coerce_to_target_dtype(other)\n                nbs = blk.where(other, cond)\n                return self._maybe_downcast(nbs, \"infer\")\n\n            else:\n                raise\n\n        nb = self.make_block_same_class(res_values)\n        return [nb]\n\n    def putmask(self, mask, new) -> list[Block]:\n        \"\"\"\n        See Block.putmask.__doc__\n        \"\"\"\n        mask = extract_bool_array(mask)\n\n        values = self.values\n\n        mask = self._maybe_squeeze_arg(mask)\n\n        try:\n            # Caller is responsible for ensuring matching lengths\n            values._putmask(mask, new)\n        except (TypeError, ValueError) as err:\n            _catch_deprecated_value_error(err)\n\n            if is_interval_dtype(self.dtype):\n                # Discussion about what we want to support in the general\n                #  case GH#39584\n                blk = self.coerce_to_target_dtype(new)\n                if blk.dtype == _dtype_obj:\n                    # For now at least, only support casting e.g.\n                    #  Interval[int64]->Interval[float64],\n                    raise\n                return blk.putmask(mask, new)\n\n            elif isinstance(self, NDArrayBackedExtensionBlock):\n                # NB: not (yet) the same as\n                #  isinstance(values, NDArrayBackedExtensionArray)\n                if isinstance(self.dtype, PeriodDtype):\n                    # TODO: don't special-case\n                    raise\n                blk = self.coerce_to_target_dtype(new)\n                return blk.putmask(mask, new)\n\n            else:\n                raise\n\n        return [self]\n\n    def delete(self, loc) -> None:\n        \"\"\"\n        Delete given loc(-s) from block in-place.\n        \"\"\"\n        # This will be unnecessary if/when __array_function__ is implemented\n        self.values = self.values.delete(loc)\n        self.mgr_locs = self._mgr_locs.delete(loc)\n        try:\n            self._cache.clear()\n        except AttributeError:\n            # _cache not yet initialized\n            pass\n\n    @cache_readonly\n    def array_values(self) -> ExtensionArray:\n        return self.values\n\n    def get_values(self, dtype: DtypeObj | None = None) -> np.ndarray:\n        \"\"\"\n        return object dtype as boxed values, such as Timestamps/Timedelta\n        \"\"\"\n        values: ArrayLike = self.values\n        if dtype == _dtype_obj:\n            values = values.astype(object)\n        # TODO(EA2D): reshape not needed with 2D EAs\n        return np.asarray(values).reshape(self.shape)\n\n    def values_for_json(self) -> np.ndarray:\n        return np.asarray(self.values)\n\n    def interpolate(\n        self, method=\"pad\", axis=0, inplace=False, limit=None, fill_value=None, **kwargs\n    ):\n        values = self.values\n        if values.ndim == 2 and axis == 0:\n            # NDArrayBackedExtensionArray.fillna assumes axis=1\n            new_values = values.T.fillna(value=fill_value, method=method, limit=limit).T\n        else:\n            new_values = values.fillna(value=fill_value, method=method, limit=limit)\n        return self.make_block_same_class(new_values)\n\n\nclass ExtensionBlock(libinternals.Block, EABackedBlock):\n    \"\"\"\n    Block for holding extension types.\n\n    Notes\n    -----\n    This holds all 3rd-party extension array types. It's also the immediate\n    parent class for our internal extension types' blocks, CategoricalBlock.\n\n    ExtensionArrays are limited to 1-D.\n    \"\"\"\n\n    _can_consolidate = False\n    _validate_ndim = False\n    is_extension = True\n\n    values: ExtensionArray\n\n    @cache_readonly\n    def shape(self) -> Shape:\n        # TODO(EA2D): override unnecessary with 2D EAs\n        if self.ndim == 1:\n            return (len(self.values),)\n        return len(self._mgr_locs), len(self.values)\n\n    def iget(self, i: int | tuple[int, int] | tuple[slice, int]):\n        # In the case where we have a tuple[slice, int], the slice will always\n        #  be slice(None)\n        # We _could_ make the annotation more specific, but mypy would\n        #  complain about override mismatch:\n        #  Literal[0] | tuple[Literal[0], int] | tuple[slice, int]\n\n        # Note: only reached with self.ndim == 2\n\n        if isinstance(i, tuple):\n            # TODO(EA2D): unnecessary with 2D EAs\n            col, loc = i\n            if not com.is_null_slice(col) and col != 0:\n                raise IndexError(f\"{self} only contains one item\")\n            elif isinstance(col, slice):\n                if col != slice(None):\n                    raise NotImplementedError(col)\n                return self.values[[loc]]\n            return self.values[loc]\n        else:\n            if i != 0:\n                raise IndexError(f\"{self} only contains one item\")\n            return self.values\n\n    def set_inplace(self, locs, values) -> None:\n        # NB: This is a misnomer, is supposed to be inplace but is not,\n        #  see GH#33457\n        # When an ndarray, we should have locs.tolist() == [0]\n        # When a BlockPlacement we should have list(locs) == [0]\n        self.values = values\n        try:\n            # TODO(GH33457) this can be removed\n            self._cache.clear()\n        except AttributeError:\n            # _cache not yet initialized\n            pass\n\n    def _maybe_squeeze_arg(self, arg):\n        \"\"\"\n        If necessary, squeeze a (N, 1) ndarray to (N,)\n        \"\"\"\n        # e.g. if we are passed a 2D mask for putmask\n        if isinstance(arg, np.ndarray) and arg.ndim == self.values.ndim + 1:\n            # TODO(EA2D): unnecessary with 2D EAs\n            assert arg.shape[1] == 1\n            arg = arg[:, 0]\n        return arg\n\n    @property\n    def is_view(self) -> bool:\n        \"\"\"Extension arrays are never treated as views.\"\"\"\n        return False\n\n    @cache_readonly\n    def is_numeric(self):\n        return self.values.dtype._is_numeric\n\n    def setitem(self, indexer, value):\n        \"\"\"\n        Attempt self.values[indexer] = value, possibly creating a new array.\n\n        This differs from Block.setitem by not allowing setitem to change\n        the dtype of the Block.\n\n        Parameters\n        ----------\n        indexer : tuple, list-like, array-like, slice, int\n            The subset of self.values to set\n        value : object\n            The value being set\n\n        Returns\n        -------\n        Block\n\n        Notes\n        -----\n        `indexer` is a direct slice/positional indexer. `value` must\n        be a compatible shape.\n        \"\"\"\n        if not self._can_hold_element(value):\n            # see TestSetitemFloatIntervalWithIntIntervalValues\n            return self.coerce_to_target_dtype(value).setitem(indexer, value)\n\n        if isinstance(indexer, tuple):\n            # TODO(EA2D): not needed with 2D EAs\n            # we are always 1-D\n            indexer = indexer[0]\n            if isinstance(indexer, np.ndarray) and indexer.ndim == 2:\n                # GH#44703\n                if indexer.shape[1] != 1:\n                    raise NotImplementedError(\n                        \"This should not be reached. Please report a bug at \"\n                        \"github.com/pandas-dev/pandas/\"\n                    )\n                indexer = indexer[:, 0]\n\n        # TODO(EA2D): not needed with 2D EAS\n        if isinstance(value, (np.ndarray, ExtensionArray)) and value.ndim == 2:\n            assert value.shape[1] == 1\n            # error: No overload variant of \"__getitem__\" of \"ExtensionArray\"\n            # matches argument type \"Tuple[slice, int]\"\n            value = value[:, 0]  # type: ignore[call-overload]\n        elif isinstance(value, ABCDataFrame):\n            # TODO: should we avoid getting here with DataFrame?\n            assert value.shape[1] == 1\n            value = value._ixs(0, axis=1)._values\n\n        check_setitem_lengths(indexer, value, self.values)\n        self.values[indexer] = value\n        return self\n\n    def take_nd(\n        self,\n        indexer,\n        axis: int = 0,\n        new_mgr_locs: BlockPlacement | None = None,\n        fill_value=lib.no_default,\n    ) -> Block:\n        \"\"\"\n        Take values according to indexer and return them as a block.\n        \"\"\"\n        if fill_value is lib.no_default:\n            fill_value = None\n\n        # TODO(EA2D): special case not needed with 2D EAs\n        # axis doesn't matter; we are really a single-dim object\n        # but are passed the axis depending on the calling routing\n        # if its REALLY axis 0, then this will be a reindex and not a take\n        new_values = self.values.take(indexer, fill_value=fill_value, allow_fill=True)\n\n        # Called from three places in managers, all of which satisfy\n        #  this assertion\n        assert not (self.ndim == 1 and new_mgr_locs is None)\n        if new_mgr_locs is None:\n            new_mgr_locs = self._mgr_locs\n\n        return self.make_block_same_class(new_values, new_mgr_locs)\n\n    def _slice(self, slicer) -> ExtensionArray:\n        \"\"\"\n        Return a slice of my values.\n\n        Parameters\n        ----------\n        slicer : slice, ndarray[int], or a tuple of these\n            Valid (non-reducing) indexer for self.values.\n\n        Returns\n        -------\n        ExtensionArray\n        \"\"\"\n        # return same dims as we currently have\n        if not isinstance(slicer, tuple) and self.ndim == 2:\n            # reached via getitem_block via _slice_take_blocks_ax0\n            # TODO(EA2D): won't be necessary with 2D EAs\n            slicer = (slicer, slice(None))\n\n        if isinstance(slicer, tuple) and len(slicer) == 2:\n            first = slicer[0]\n            if not isinstance(first, slice):\n                raise AssertionError(\n                    \"invalid slicing for a 1-ndim ExtensionArray\", first\n                )\n            # GH#32959 only full-slicers along fake-dim0 are valid\n            # TODO(EA2D): won't be necessary with 2D EAs\n            # range(1) instead of self._mgr_locs to avoid exception on [::-1]\n            #  see test_iloc_getitem_slice_negative_step_ea_block\n            new_locs = range(1)[first]\n            if len(new_locs):\n                # effectively slice(None)\n                slicer = slicer[1]\n            else:\n                raise AssertionError(\n                    \"invalid slicing for a 1-ndim ExtensionArray\", slicer\n                )\n\n        return self.values[slicer]\n\n    @final\n    def getitem_block_index(self, slicer: slice) -> ExtensionBlock:\n        \"\"\"\n        Perform __getitem__-like specialized to slicing along index.\n        \"\"\"\n        # GH#42787 in principle this is equivalent to values[..., slicer], but we don't\n        # require subclasses of ExtensionArray to support that form (for now).\n        new_values = self.values[slicer]\n        return type(self)(new_values, self._mgr_locs, ndim=self.ndim)\n\n    def fillna(\n        self, value, limit=None, inplace: bool = False, downcast=None\n    ) -> list[Block]:\n        values = self.values.fillna(value=value, limit=limit)\n        return [self.make_block_same_class(values=values)]\n\n    def diff(self, n: int, axis: int = 1) -> list[Block]:\n        if axis == 0 and n != 0:\n            # n==0 case will be a no-op so let is fall through\n            # Since we only have one column, the result will be all-NA.\n            #  Create this result by shifting along axis=0 past the length of\n            #  our values.\n            return super().diff(len(self.values), axis=0)\n        if axis == 1:\n            # TODO(EA2D): unnecessary with 2D EAs\n            # we are by definition 1D.\n            axis = 0\n        return super().diff(n, axis)\n\n    def shift(self, periods: int, axis: int = 0, fill_value: Any = None) -> list[Block]:\n        \"\"\"\n        Shift the block by `periods`.\n\n        Dispatches to underlying ExtensionArray and re-boxes in an\n        ExtensionBlock.\n        \"\"\"\n        new_values = self.values.shift(periods=periods, fill_value=fill_value)\n        return [self.make_block_same_class(new_values)]\n\n    def _unstack(\n        self,\n        unstacker,\n        fill_value,\n        new_placement: npt.NDArray[np.intp],\n        needs_masking: npt.NDArray[np.bool_],\n    ):\n        # ExtensionArray-safe unstack.\n        # We override ObjectBlock._unstack, which unstacks directly on the\n        # values of the array. For EA-backed blocks, this would require\n        # converting to a 2-D ndarray of objects.\n        # Instead, we unstack an ndarray of integer positions, followed by\n        # a `take` on the actual values.\n\n        # Caller is responsible for ensuring self.shape[-1] == len(unstacker.index)\n        new_values, mask = unstacker.arange_result\n\n        # Note: these next two lines ensure that\n        #  mask.sum() == sum(len(nb.mgr_locs) for nb in blocks)\n        #  which the calling function needs in order to pass verify_integrity=False\n        #  to the BlockManager constructor\n        new_values = new_values.T[mask]\n        new_placement = new_placement[mask]\n\n        # needs_masking[i] calculated once in BlockManager.unstack tells\n        #  us if there are any -1s in the relevant indices.  When False,\n        #  that allows us to go through a faster path in 'take', among\n        #  other things avoiding e.g. Categorical._validate_scalar.\n        blocks = [\n            # TODO: could cast to object depending on fill_value?\n            type(self)(\n                self.values.take(\n                    indices, allow_fill=needs_masking[i], fill_value=fill_value\n                ),\n                BlockPlacement(place),\n                ndim=2,\n            )\n            for i, (indices, place) in enumerate(zip(new_values, new_placement))\n        ]\n        return blocks, mask\n\n\nclass NumpyBlock(libinternals.NumpyBlock, Block):\n    values: np.ndarray\n\n\nclass NumericBlock(NumpyBlock):\n    __slots__ = ()\n    is_numeric = True\n\n\nclass NDArrayBackedExtensionBlock(libinternals.NDArrayBackedBlock, EABackedBlock):\n    \"\"\"\n    Block backed by an NDArrayBackedExtensionArray\n    \"\"\"\n\n    values: NDArrayBackedExtensionArray\n\n    # error: Signature of \"is_extension\" incompatible with supertype \"Block\"\n    @cache_readonly\n    def is_extension(self) -> bool:  # type: ignore[override]\n        # i.e. datetime64tz, PeriodDtype\n        return not isinstance(self.dtype, np.dtype)\n\n    @property\n    def is_view(self) -> bool:\n        \"\"\"return a boolean if I am possibly a view\"\"\"\n        # check the ndarray values of the DatetimeIndex values\n        return self.values._ndarray.base is not None\n\n    def setitem(self, indexer, value):\n        if not self._can_hold_element(value):\n            return self.coerce_to_target_dtype(value).setitem(indexer, value)\n\n        values = self.values\n        if self.ndim > 1:\n            # Dont transpose with ndim=1 bc we would fail to invalidate\n            #  arr.freq\n            values = values.T\n        values[indexer] = value\n        return self\n\n    def diff(self, n: int, axis: int = 0) -> list[Block]:\n        \"\"\"\n        1st discrete difference.\n\n        Parameters\n        ----------\n        n : int\n            Number of periods to diff.\n        axis : int, default 0\n            Axis to diff upon.\n\n        Returns\n        -------\n        A list with a new Block.\n\n        Notes\n        -----\n        The arguments here are mimicking shift so they are called correctly\n        by apply.\n        \"\"\"\n        values = self.values\n\n        new_values = values - values.shift(n, axis=axis)\n        return [self.make_block(new_values)]\n\n    def shift(self, periods: int, axis: int = 0, fill_value: Any = None) -> list[Block]:\n        values = self.values\n        new_values = values.shift(periods, fill_value=fill_value, axis=axis)\n        return [self.make_block_same_class(new_values)]\n\n    def fillna(\n        self, value, limit=None, inplace: bool = False, downcast=None\n    ) -> list[Block]:\n\n        if not self._can_hold_element(value) and self.dtype.kind != \"m\":\n            # We support filling a DatetimeTZ with a `value` whose timezone\n            #  is different by coercing to object.\n            # TODO: don't special-case td64\n            return self.coerce_to_target_dtype(value).fillna(\n                value, limit, inplace, downcast\n            )\n\n        new_values = self.values.fillna(value=value, limit=limit)\n        return [self.make_block_same_class(values=new_values)]\n\n\ndef _catch_deprecated_value_error(err: Exception) -> None:\n    \"\"\"\n    We catch ValueError for now, but only a specific one raised by DatetimeArray\n    which will no longer be raised in version.2.0.\n    \"\"\"\n    if isinstance(err, ValueError):\n        # TODO(2.0): once DTA._validate_setitem_value deprecation\n        #  is enforced, stop catching ValueError here altogether\n        if \"Timezones don't match\" not in str(err):\n            raise\n\n\nclass DatetimeLikeBlock(NDArrayBackedExtensionBlock):\n    \"\"\"Block for datetime64[ns], timedelta64[ns].\"\"\"\n\n    __slots__ = ()\n    is_numeric = False\n    values: DatetimeArray | TimedeltaArray\n\n    def values_for_json(self) -> np.ndarray:\n        # special casing datetimetz to avoid conversion through\n        #  object dtype\n        return self.values._ndarray\n\n\nclass DatetimeTZBlock(DatetimeLikeBlock):\n    \"\"\"implement a datetime64 block with a tz attribute\"\"\"\n\n    values: DatetimeArray\n\n    __slots__ = ()\n    is_extension = True\n    _validate_ndim = True\n    _can_consolidate = False\n\n\nclass ObjectBlock(NumpyBlock):\n    __slots__ = ()\n    is_object = True\n\n    @maybe_split\n    def reduce(self, func, ignore_failures: bool = False) -> list[Block]:\n        \"\"\"\n        For object-dtype, we operate column-wise.\n        \"\"\"\n        assert self.ndim == 2\n\n        try:\n            res = func(self.values)\n        except TypeError:\n            if not ignore_failures:\n                raise\n            return []\n\n        assert isinstance(res, np.ndarray)\n        assert res.ndim == 1\n        res = res.reshape(1, -1)\n        return [self.make_block_same_class(res)]\n\n    @maybe_split\n    def convert(\n        self,\n        copy: bool = True,\n        datetime: bool = True,\n        numeric: bool = True,\n        timedelta: bool = True,\n    ) -> list[Block]:\n        \"\"\"\n        attempt to cast any object types to better types return a copy of\n        the block (if copy = True) by definition we ARE an ObjectBlock!!!!!\n        \"\"\"\n        values = self.values\n        if values.ndim == 2:\n            # maybe_split ensures we only get here with values.shape[0] == 1,\n            # avoid doing .ravel as that might make a copy\n            values = values[0]\n\n        res_values = soft_convert_objects(\n            values,\n            datetime=datetime,\n            numeric=numeric,\n            timedelta=timedelta,\n            copy=copy,\n        )\n        res_values = ensure_block_shape(res_values, self.ndim)\n        return [self.make_block(res_values)]\n\n\nclass CategoricalBlock(ExtensionBlock):\n    # this Block type is kept for backwards-compatibility\n    __slots__ = ()\n\n    # GH#43232, GH#43334 self.values.dtype can be changed inplace until 2.0,\n    #  so this cannot be cached\n    @property\n    def dtype(self) -> DtypeObj:\n        return self.values.dtype\n\n\n# -----------------------------------------------------------------\n# Constructor Helpers\n\n\ndef maybe_coerce_values(values: ArrayLike) -> ArrayLike:\n    \"\"\"\n    Input validation for values passed to __init__. Ensure that\n    any datetime64/timedelta64 dtypes are in nanoseconds.  Ensure\n    that we do not have string dtypes.\n\n    Parameters\n    ----------\n    values : np.ndarray or ExtensionArray\n\n    Returns\n    -------\n    values : np.ndarray or ExtensionArray\n    \"\"\"\n    # Caller is responsible for ensuring PandasArray is already extracted.\n\n    if isinstance(values, np.ndarray):\n        values = ensure_wrapped_if_datetimelike(values)\n\n        if issubclass(values.dtype.type, str):\n            values = np.array(values, dtype=object)\n\n    if isinstance(values, (DatetimeArray, TimedeltaArray)) and values.freq is not None:\n        # freq is only stored in DatetimeIndex/TimedeltaIndex, not in Series/DataFrame\n        values = values._with_freq(None)\n\n    return values\n\n\ndef get_block_type(dtype: DtypeObj):\n    \"\"\"\n    Find the appropriate Block subclass to use for the given values and dtype.\n\n    Parameters\n    ----------\n    dtype : numpy or pandas dtype\n\n    Returns\n    -------\n    cls : class, subclass of Block\n    \"\"\"\n    # We use vtype and kind checks because they are much more performant\n    #  than is_foo_dtype\n    vtype = dtype.type\n    kind = dtype.kind\n\n    cls: type[Block]\n\n    if isinstance(dtype, SparseDtype):\n        # Need this first(ish) so that Sparse[datetime] is sparse\n        cls = ExtensionBlock\n    elif isinstance(dtype, CategoricalDtype):\n        cls = CategoricalBlock\n    elif vtype is Timestamp:\n        cls = DatetimeTZBlock\n    elif isinstance(dtype, PeriodDtype):\n        cls = NDArrayBackedExtensionBlock\n    elif isinstance(dtype, ExtensionDtype):\n        # Note: need to be sure PandasArray is unwrapped before we get here\n        cls = ExtensionBlock\n\n    elif kind in [\"M\", \"m\"]:\n        cls = DatetimeLikeBlock\n    elif kind in [\"f\", \"c\", \"i\", \"u\", \"b\"]:\n        cls = NumericBlock\n    else:\n        cls = ObjectBlock\n    return cls\n\n\ndef new_block_2d(values: ArrayLike, placement: BlockPlacement):\n    # new_block specialized to case with\n    #  ndim=2\n    #  isinstance(placement, BlockPlacement)\n    #  check_ndim/ensure_block_shape already checked\n    klass = get_block_type(values.dtype)\n\n    values = maybe_coerce_values(values)\n    return klass(values, ndim=2, placement=placement)\n\n\ndef new_block(values, placement, *, ndim: int) -> Block:\n    # caller is responsible for ensuring values is NOT a PandasArray\n\n    if not isinstance(placement, BlockPlacement):\n        placement = BlockPlacement(placement)\n\n    check_ndim(values, placement, ndim)\n\n    klass = get_block_type(values.dtype)\n\n    values = maybe_coerce_values(values)\n    return klass(values, ndim=ndim, placement=placement)\n\n\ndef check_ndim(values, placement: BlockPlacement, ndim: int):\n    \"\"\"\n    ndim inference and validation.\n\n    Validates that values.ndim and ndim are consistent.\n    Validates that len(values) and len(placement) are consistent.\n\n    Parameters\n    ----------\n    values : array-like\n    placement : BlockPlacement\n    ndim : int\n\n    Raises\n    ------\n    ValueError : the number of dimensions do not match\n    \"\"\"\n\n    if values.ndim > ndim:\n        # Check for both np.ndarray and ExtensionArray\n        raise ValueError(\n            \"Wrong number of dimensions. \"\n            f\"values.ndim > ndim [{values.ndim} > {ndim}]\"\n        )\n\n    elif not is_1d_only_ea_dtype(values.dtype):\n        # TODO(EA2D): special case not needed with 2D EAs\n        if values.ndim != ndim:\n            raise ValueError(\n                \"Wrong number of dimensions. \"\n                f\"values.ndim != ndim [{values.ndim} != {ndim}]\"\n            )\n        if len(placement) != len(values):\n            raise ValueError(\n                f\"Wrong number of items passed {len(values)}, \"\n                f\"placement implies {len(placement)}\"\n            )\n    elif ndim == 2 and len(placement) != 1:\n        # TODO(EA2D): special case unnecessary with 2D EAs\n        raise ValueError(\"need to split\")\n\n\ndef extract_pandas_array(\n    values: np.ndarray | ExtensionArray, dtype: DtypeObj | None, ndim: int\n) -> tuple[np.ndarray | ExtensionArray, DtypeObj | None]:\n    \"\"\"\n    Ensure that we don't allow PandasArray / PandasDtype in internals.\n    \"\"\"\n    # For now, blocks should be backed by ndarrays when possible.\n    if isinstance(values, ABCPandasArray):\n        values = values.to_numpy()\n        if ndim and ndim > 1:\n            # TODO(EA2D): special case not needed with 2D EAs\n            values = np.atleast_2d(values)\n\n    if isinstance(dtype, PandasDtype):\n        dtype = dtype.numpy_dtype\n\n    return values, dtype\n\n\n# -----------------------------------------------------------------\n\n\ndef extend_blocks(result, blocks=None) -> list[Block]:\n    \"\"\"return a new extended blocks, given the result\"\"\"\n    if blocks is None:\n        blocks = []\n    if isinstance(result, list):\n        for r in result:\n            if isinstance(r, list):\n                blocks.extend(r)\n            else:\n                blocks.append(r)\n    else:\n        assert isinstance(result, Block), type(result)\n        blocks.append(result)\n    return blocks\n\n\ndef ensure_block_shape(values: ArrayLike, ndim: int = 1) -> ArrayLike:\n    \"\"\"\n    Reshape if possible to have values.ndim == ndim.\n    \"\"\"\n\n    if values.ndim < ndim:\n        if not is_1d_only_ea_dtype(values.dtype):\n            # TODO(EA2D): https://github.com/pandas-dev/pandas/issues/23023\n            # block.shape is incorrect for \"2D\" ExtensionArrays\n            # We can't, and don't need to, reshape.\n            values = cast(\"np.ndarray | DatetimeArray | TimedeltaArray\", values)\n            values = values.reshape(1, -1)\n\n    return values\n\n\ndef to_native_types(\n    values: ArrayLike,\n    *,\n    na_rep=\"nan\",\n    quoting=None,\n    float_format=None,\n    decimal=\".\",\n    **kwargs,\n) -> np.ndarray:\n    \"\"\"convert to our native types format\"\"\"\n    if isinstance(values, Categorical):\n        # GH#40754 Convert categorical datetimes to datetime array\n        values = take_nd(\n            values.categories._values,\n            ensure_platform_int(values._codes),\n            fill_value=na_rep,\n        )\n\n    values = ensure_wrapped_if_datetimelike(values)\n\n    if isinstance(values, (DatetimeArray, TimedeltaArray)):\n        if values.ndim == 1:\n            result = values._format_native_types(na_rep=na_rep, **kwargs)\n            result = result.astype(object, copy=False)\n            return result\n\n        # GH#21734 Process every column separately, they might have different formats\n        results_converted = []\n        for i in range(len(values)):\n            result = values[i, :]._format_native_types(na_rep=na_rep, **kwargs)\n            results_converted.append(result.astype(object, copy=False))\n        return np.vstack(results_converted)\n\n    elif isinstance(values, ExtensionArray):\n        mask = isna(values)\n\n        new_values = np.asarray(values.astype(object))\n        new_values[mask] = na_rep\n        return new_values\n\n    elif values.dtype.kind == \"f\":\n        # see GH#13418: no special formatting is desired at the\n        # output (important for appropriate 'quoting' behaviour),\n        # so do not pass it through the FloatArrayFormatter\n        if float_format is None and decimal == \".\":\n            mask = isna(values)\n\n            if not quoting:\n                values = values.astype(str)\n            else:\n                values = np.array(values, dtype=\"object\")\n\n            values[mask] = na_rep\n            values = values.astype(object, copy=False)\n            return values\n\n        from pandas.io.formats.format import FloatArrayFormatter\n\n        formatter = FloatArrayFormatter(\n            values,\n            na_rep=na_rep,\n            float_format=float_format,\n            decimal=decimal,\n            quoting=quoting,\n            fixed_width=False,\n        )\n        res = formatter.get_result_as_array()\n        res = res.astype(object, copy=False)\n        return res\n\n    else:\n\n        mask = isna(values)\n        itemsize = writers.word_len(na_rep)\n\n        if values.dtype != _dtype_obj and not quoting and itemsize:\n            values = values.astype(str)\n            if values.dtype.itemsize / np.dtype(\"U1\").itemsize < itemsize:\n                # enlarge for the na_rep\n                values = values.astype(f\"<U{itemsize}\")\n        else:\n            values = np.array(values, dtype=\"object\")\n\n        values[mask] = na_rep\n        values = values.astype(object, copy=False)\n        return values\n\n\ndef external_values(values: ArrayLike) -> ArrayLike:\n    \"\"\"\n    The array that Series.values returns (public attribute).\n\n    This has some historical constraints, and is overridden in block\n    subclasses to return the correct array (e.g. period returns\n    object ndarray and datetimetz a datetime64[ns] ndarray instead of\n    proper extension array).\n    \"\"\"\n    if isinstance(values, (PeriodArray, IntervalArray)):\n        return values.astype(object)\n    elif isinstance(values, (DatetimeArray, TimedeltaArray)):\n        # NB: for datetime64tz this is different from np.asarray(values), since\n        #  that returns an object-dtype ndarray of Timestamps.\n        # Avoid FutureWarning in .astype in casting from dt64tz to dt64\n        return values._data\n    else:\n        return values\n", 2256], "/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/internals/managers.py": ["from __future__ import annotations\n\nimport itertools\nfrom typing import (\n    Any,\n    Callable,\n    Hashable,\n    Sequence,\n    TypeVar,\n    cast,\n)\nimport warnings\n\nimport numpy as np\n\nfrom pandas._libs import (\n    internals as libinternals,\n    lib,\n)\nfrom pandas._libs.internals import BlockPlacement\nfrom pandas._typing import (\n    ArrayLike,\n    DtypeObj,\n    Shape,\n    npt,\n    type_t,\n)\nfrom pandas.errors import PerformanceWarning\nfrom pandas.util._decorators import cache_readonly\nfrom pandas.util._exceptions import find_stack_level\nfrom pandas.util._validators import validate_bool_kwarg\n\nfrom pandas.core.dtypes.cast import infer_dtype_from_scalar\nfrom pandas.core.dtypes.common import (\n    ensure_platform_int,\n    is_1d_only_ea_dtype,\n    is_dtype_equal,\n    is_list_like,\n    needs_i8_conversion,\n)\nfrom pandas.core.dtypes.dtypes import ExtensionDtype\nfrom pandas.core.dtypes.generic import (\n    ABCDataFrame,\n    ABCSeries,\n)\nfrom pandas.core.dtypes.missing import (\n    array_equals,\n    isna,\n)\n\nimport pandas.core.algorithms as algos\nfrom pandas.core.arrays._mixins import NDArrayBackedExtensionArray\nfrom pandas.core.arrays.sparse import SparseDtype\nfrom pandas.core.construction import (\n    ensure_wrapped_if_datetimelike,\n    extract_array,\n)\nfrom pandas.core.indexers import maybe_convert_indices\nfrom pandas.core.indexes.api import (\n    Float64Index,\n    Index,\n    ensure_index,\n)\nfrom pandas.core.internals.base import (\n    DataManager,\n    SingleDataManager,\n    interleaved_dtype,\n)\nfrom pandas.core.internals.blocks import (\n    Block,\n    DatetimeTZBlock,\n    NumpyBlock,\n    ensure_block_shape,\n    extend_blocks,\n    get_block_type,\n    new_block,\n    new_block_2d,\n)\nfrom pandas.core.internals.ops import (\n    blockwise_all,\n    operate_blockwise,\n)\n\nT = TypeVar(\"T\", bound=\"BaseBlockManager\")\n\n\nclass BaseBlockManager(DataManager):\n    \"\"\"\n    Core internal data structure to implement DataFrame, Series, etc.\n\n    Manage a bunch of labeled 2D mixed-type ndarrays. Essentially it's a\n    lightweight blocked set of labeled data to be manipulated by the DataFrame\n    public API class\n\n    Attributes\n    ----------\n    shape\n    ndim\n    axes\n    values\n    items\n\n    Methods\n    -------\n    set_axis(axis, new_labels)\n    copy(deep=True)\n\n    get_dtypes\n\n    apply(func, axes, block_filter_fn)\n\n    get_bool_data\n    get_numeric_data\n\n    get_slice(slice_like, axis)\n    get(label)\n    iget(loc)\n\n    take(indexer, axis)\n    reindex_axis(new_labels, axis)\n    reindex_indexer(new_labels, indexer, axis)\n\n    delete(label)\n    insert(loc, label, value)\n    set(label, value)\n\n    Parameters\n    ----------\n    blocks: Sequence of Block\n    axes: Sequence of Index\n    verify_integrity: bool, default True\n\n    Notes\n    -----\n    This is *not* a public API class\n    \"\"\"\n\n    __slots__ = ()\n\n    _blknos: npt.NDArray[np.intp]\n    _blklocs: npt.NDArray[np.intp]\n    blocks: tuple[Block, ...]\n    axes: list[Index]\n\n    ndim: int\n    _known_consolidated: bool\n    _is_consolidated: bool\n\n    def __init__(self, blocks, axes, verify_integrity: bool = True):\n        raise NotImplementedError\n\n    @classmethod\n    def from_blocks(cls: type_t[T], blocks: list[Block], axes: list[Index]) -> T:\n        raise NotImplementedError\n\n    @property\n    def blknos(self) -> npt.NDArray[np.intp]:\n        \"\"\"\n        Suppose we want to find the array corresponding to our i'th column.\n\n        blknos[i] identifies the block from self.blocks that contains this column.\n\n        blklocs[i] identifies the column of interest within\n        self.blocks[self.blknos[i]]\n        \"\"\"\n        if self._blknos is None:\n            # Note: these can be altered by other BlockManager methods.\n            self._rebuild_blknos_and_blklocs()\n\n        return self._blknos\n\n    @property\n    def blklocs(self) -> npt.NDArray[np.intp]:\n        \"\"\"\n        See blknos.__doc__\n        \"\"\"\n        if self._blklocs is None:\n            # Note: these can be altered by other BlockManager methods.\n            self._rebuild_blknos_and_blklocs()\n\n        return self._blklocs\n\n    def make_empty(self: T, axes=None) -> T:\n        \"\"\"return an empty BlockManager with the items axis of len 0\"\"\"\n        if axes is None:\n            axes = [Index([])] + self.axes[1:]\n\n        # preserve dtype if possible\n        if self.ndim == 1:\n            assert isinstance(self, SingleBlockManager)  # for mypy\n            blk = self.blocks[0]\n            arr = blk.values[:0]\n            bp = BlockPlacement(slice(0, 0))\n            nb = blk.make_block_same_class(arr, placement=bp)\n            blocks = [nb]\n        else:\n            blocks = []\n        return type(self).from_blocks(blocks, axes)\n\n    def __nonzero__(self) -> bool:\n        return True\n\n    # Python3 compat\n    __bool__ = __nonzero__\n\n    def _normalize_axis(self, axis: int) -> int:\n        # switch axis to follow BlockManager logic\n        if self.ndim == 2:\n            axis = 1 if axis == 0 else 0\n        return axis\n\n    def set_axis(self, axis: int, new_labels: Index) -> None:\n        # Caller is responsible for ensuring we have an Index object.\n        self._validate_set_axis(axis, new_labels)\n        self.axes[axis] = new_labels\n\n    @property\n    def is_single_block(self) -> bool:\n        # Assumes we are 2D; overridden by SingleBlockManager\n        return len(self.blocks) == 1\n\n    @property\n    def items(self) -> Index:\n        return self.axes[0]\n\n    def get_dtypes(self):\n        dtypes = np.array([blk.dtype for blk in self.blocks])\n        return dtypes.take(self.blknos)\n\n    @property\n    def arrays(self) -> list[ArrayLike]:\n        \"\"\"\n        Quick access to the backing arrays of the Blocks.\n\n        Only for compatibility with ArrayManager for testing convenience.\n        Not to be used in actual code, and return value is not the same as the\n        ArrayManager method (list of 1D arrays vs iterator of 2D ndarrays / 1D EAs).\n        \"\"\"\n        return [blk.values for blk in self.blocks]\n\n    def __repr__(self) -> str:\n        output = type(self).__name__\n        for i, ax in enumerate(self.axes):\n            if i == 0:\n                output += f\"\\nItems: {ax}\"\n            else:\n                output += f\"\\nAxis {i}: {ax}\"\n\n        for block in self.blocks:\n            output += f\"\\n{block}\"\n        return output\n\n    def apply(\n        self: T,\n        f,\n        align_keys: list[str] | None = None,\n        ignore_failures: bool = False,\n        **kwargs,\n    ) -> T:\n        \"\"\"\n        Iterate over the blocks, collect and create a new BlockManager.\n\n        Parameters\n        ----------\n        f : str or callable\n            Name of the Block method to apply.\n        align_keys: List[str] or None, default None\n        ignore_failures: bool, default False\n        **kwargs\n            Keywords to pass to `f`\n\n        Returns\n        -------\n        BlockManager\n        \"\"\"\n        assert \"filter\" not in kwargs\n\n        align_keys = align_keys or []\n        result_blocks: list[Block] = []\n        # fillna: Series/DataFrame is responsible for making sure value is aligned\n\n        aligned_args = {k: kwargs[k] for k in align_keys}\n\n        for b in self.blocks:\n\n            if aligned_args:\n\n                for k, obj in aligned_args.items():\n                    if isinstance(obj, (ABCSeries, ABCDataFrame)):\n                        # The caller is responsible for ensuring that\n                        #  obj.axes[-1].equals(self.items)\n                        if obj.ndim == 1:\n                            kwargs[k] = obj.iloc[b.mgr_locs.indexer]._values\n                        else:\n                            kwargs[k] = obj.iloc[:, b.mgr_locs.indexer]._values\n                    else:\n                        # otherwise we have an ndarray\n                        kwargs[k] = obj[b.mgr_locs.indexer]\n\n            try:\n                if callable(f):\n                    applied = b.apply(f, **kwargs)\n                else:\n                    applied = getattr(b, f)(**kwargs)\n            except (TypeError, NotImplementedError):\n                if not ignore_failures:\n                    raise\n                continue\n            result_blocks = extend_blocks(applied, result_blocks)\n\n        if ignore_failures:\n            return self._combine(result_blocks)\n\n        out = type(self).from_blocks(result_blocks, self.axes)\n        return out\n\n    def where(self: T, other, cond, align: bool) -> T:\n        if align:\n            align_keys = [\"other\", \"cond\"]\n        else:\n            align_keys = [\"cond\"]\n            other = extract_array(other, extract_numpy=True)\n\n        return self.apply(\n            \"where\",\n            align_keys=align_keys,\n            other=other,\n            cond=cond,\n        )\n\n    def setitem(self: T, indexer, value) -> T:\n        \"\"\"\n        Set values with indexer.\n\n        For SingleBlockManager, this backs s[indexer] = value\n        \"\"\"\n        return self.apply(\"setitem\", indexer=indexer, value=value)\n\n    def putmask(self, mask, new, align: bool = True):\n\n        if align:\n            align_keys = [\"new\", \"mask\"]\n        else:\n            align_keys = [\"mask\"]\n            new = extract_array(new, extract_numpy=True)\n\n        return self.apply(\n            \"putmask\",\n            align_keys=align_keys,\n            mask=mask,\n            new=new,\n        )\n\n    def diff(self: T, n: int, axis: int) -> T:\n        axis = self._normalize_axis(axis)\n        return self.apply(\"diff\", n=n, axis=axis)\n\n    def interpolate(self: T, **kwargs) -> T:\n        return self.apply(\"interpolate\", **kwargs)\n\n    def shift(self: T, periods: int, axis: int, fill_value) -> T:\n        axis = self._normalize_axis(axis)\n        if fill_value is lib.no_default:\n            fill_value = None\n\n        if (\n            axis == 0\n            and self.ndim == 2\n            and (\n                self.nblocks > 1\n                or (\n                    # If we only have one block and we know that we can't\n                    #  keep the same dtype (i.e. the _can_hold_element check)\n                    #  then we can go through the reindex_indexer path\n                    #  (and avoid casting logic in the Block method).\n                    #  The exception to this (until 2.0) is datetimelike\n                    #  dtypes with integers, which cast.\n                    not self.blocks[0]._can_hold_element(fill_value)\n                    # TODO(2.0): remove special case for integer-with-datetimelike\n                    #  once deprecation is enforced\n                    and not (\n                        lib.is_integer(fill_value)\n                        and needs_i8_conversion(self.blocks[0].dtype)\n                    )\n                )\n            )\n        ):\n            # GH#35488 we need to watch out for multi-block cases\n            # We only get here with fill_value not-lib.no_default\n            ncols = self.shape[0]\n            nper = abs(periods)\n            nper = min(nper, ncols)\n            if periods > 0:\n                indexer = np.array(\n                    [-1] * nper + list(range(ncols - periods)), dtype=np.intp\n                )\n            else:\n                indexer = np.array(\n                    list(range(nper, ncols)) + [-1] * nper, dtype=np.intp\n                )\n            result = self.reindex_indexer(\n                self.items,\n                indexer,\n                axis=0,\n                fill_value=fill_value,\n                allow_dups=True,\n                consolidate=False,\n            )\n            return result\n\n        return self.apply(\"shift\", periods=periods, axis=axis, fill_value=fill_value)\n\n    def fillna(self: T, value, limit, inplace: bool, downcast) -> T:\n        return self.apply(\n            \"fillna\", value=value, limit=limit, inplace=inplace, downcast=downcast\n        )\n\n    def astype(self: T, dtype, copy: bool = False, errors: str = \"raise\") -> T:\n        return self.apply(\"astype\", dtype=dtype, copy=copy, errors=errors)\n\n    def convert(\n        self: T,\n        copy: bool = True,\n        datetime: bool = True,\n        numeric: bool = True,\n        timedelta: bool = True,\n    ) -> T:\n        return self.apply(\n            \"convert\",\n            copy=copy,\n            datetime=datetime,\n            numeric=numeric,\n            timedelta=timedelta,\n        )\n\n    def replace(self: T, to_replace, value, inplace: bool) -> T:\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        # NDFrame.replace ensures the not-is_list_likes here\n        assert not is_list_like(to_replace)\n        assert not is_list_like(value)\n        return self.apply(\n            \"replace\", to_replace=to_replace, value=value, inplace=inplace\n        )\n\n    def replace_regex(self, **kwargs):\n        return self.apply(\"_replace_regex\", **kwargs)\n\n    def replace_list(\n        self: T,\n        src_list: list[Any],\n        dest_list: list[Any],\n        inplace: bool = False,\n        regex: bool = False,\n    ) -> T:\n        \"\"\"do a list replace\"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n\n        bm = self.apply(\n            \"replace_list\",\n            src_list=src_list,\n            dest_list=dest_list,\n            inplace=inplace,\n            regex=regex,\n        )\n        bm._consolidate_inplace()\n        return bm\n\n    def to_native_types(self: T, **kwargs) -> T:\n        \"\"\"\n        Convert values to native types (strings / python objects) that are used\n        in formatting (repr / csv).\n        \"\"\"\n        return self.apply(\"to_native_types\", **kwargs)\n\n    @property\n    def is_numeric_mixed_type(self) -> bool:\n        return all(block.is_numeric for block in self.blocks)\n\n    @property\n    def any_extension_types(self) -> bool:\n        \"\"\"Whether any of the blocks in this manager are extension blocks\"\"\"\n        return any(block.is_extension for block in self.blocks)\n\n    @property\n    def is_view(self) -> bool:\n        \"\"\"return a boolean if we are a single block and are a view\"\"\"\n        if len(self.blocks) == 1:\n            return self.blocks[0].is_view\n\n        # It is technically possible to figure out which blocks are views\n        # e.g. [ b.values.base is not None for b in self.blocks ]\n        # but then we have the case of possibly some blocks being a view\n        # and some blocks not. setting in theory is possible on the non-view\n        # blocks w/o causing a SettingWithCopy raise/warn. But this is a bit\n        # complicated\n\n        return False\n\n    def _get_data_subset(self: T, predicate: Callable) -> T:\n        blocks = [blk for blk in self.blocks if predicate(blk.values)]\n        return self._combine(blocks, copy=False)\n\n    def get_bool_data(self: T, copy: bool = False) -> T:\n        \"\"\"\n        Select blocks that are bool-dtype and columns from object-dtype blocks\n        that are all-bool.\n\n        Parameters\n        ----------\n        copy : bool, default False\n            Whether to copy the blocks\n        \"\"\"\n\n        new_blocks = []\n\n        for blk in self.blocks:\n            if blk.dtype == bool:\n                new_blocks.append(blk)\n\n            elif blk.is_object:\n                nbs = blk._split()\n                for nb in nbs:\n                    if nb.is_bool:\n                        new_blocks.append(nb)\n\n        return self._combine(new_blocks, copy)\n\n    def get_numeric_data(self: T, copy: bool = False) -> T:\n        \"\"\"\n        Parameters\n        ----------\n        copy : bool, default False\n            Whether to copy the blocks\n        \"\"\"\n        numeric_blocks = [blk for blk in self.blocks if blk.is_numeric]\n        if len(numeric_blocks) == len(self.blocks):\n            # Avoid somewhat expensive _combine\n            if copy:\n                return self.copy(deep=True)\n            return self\n        return self._combine(numeric_blocks, copy)\n\n    def _combine(\n        self: T, blocks: list[Block], copy: bool = True, index: Index | None = None\n    ) -> T:\n        \"\"\"return a new manager with the blocks\"\"\"\n        if len(blocks) == 0:\n            if self.ndim == 2:\n                # retain our own Index dtype\n                if index is not None:\n                    axes = [self.items[:0], index]\n                else:\n                    axes = [self.items[:0]] + self.axes[1:]\n                return self.make_empty(axes)\n            return self.make_empty()\n\n        # FIXME: optimization potential\n        indexer = np.sort(np.concatenate([b.mgr_locs.as_array for b in blocks]))\n        inv_indexer = lib.get_reverse_indexer(indexer, self.shape[0])\n\n        new_blocks: list[Block] = []\n        for b in blocks:\n            b = b.copy(deep=copy)\n            b.mgr_locs = BlockPlacement(inv_indexer[b.mgr_locs.indexer])\n            new_blocks.append(b)\n\n        axes = list(self.axes)\n        if index is not None:\n            axes[-1] = index\n        axes[0] = self.items.take(indexer)\n\n        return type(self).from_blocks(new_blocks, axes)\n\n    @property\n    def nblocks(self) -> int:\n        return len(self.blocks)\n\n    def copy(self: T, deep=True) -> T:\n        \"\"\"\n        Make deep or shallow copy of BlockManager\n\n        Parameters\n        ----------\n        deep : bool or string, default True\n            If False, return shallow copy (do not copy data)\n            If 'all', copy data and a deep copy of the index\n\n        Returns\n        -------\n        BlockManager\n        \"\"\"\n        # this preserves the notion of view copying of axes\n        if deep:\n            # hit in e.g. tests.io.json.test_pandas\n\n            def copy_func(ax):\n                return ax.copy(deep=True) if deep == \"all\" else ax.view()\n\n            new_axes = [copy_func(ax) for ax in self.axes]\n        else:\n            new_axes = list(self.axes)\n\n        res = self.apply(\"copy\", deep=deep)\n\n        res.axes = new_axes\n\n        if self.ndim > 1:\n            # Avoid needing to re-compute these\n            blknos = self._blknos\n            if blknos is not None:\n                res._blknos = blknos.copy()\n                res._blklocs = self._blklocs.copy()\n\n        if deep:\n            res._consolidate_inplace()\n        return res\n\n    def consolidate(self: T) -> T:\n        \"\"\"\n        Join together blocks having same dtype\n\n        Returns\n        -------\n        y : BlockManager\n        \"\"\"\n        if self.is_consolidated():\n            return self\n\n        bm = type(self)(self.blocks, self.axes, verify_integrity=False)\n        bm._is_consolidated = False\n        bm._consolidate_inplace()\n        return bm\n\n    def reindex_indexer(\n        self: T,\n        new_axis: Index,\n        indexer,\n        axis: int,\n        fill_value=None,\n        allow_dups: bool = False,\n        copy: bool = True,\n        consolidate: bool = True,\n        only_slice: bool = False,\n        *,\n        use_na_proxy: bool = False,\n    ) -> T:\n        \"\"\"\n        Parameters\n        ----------\n        new_axis : Index\n        indexer : ndarray of int64 or None\n        axis : int\n        fill_value : object, default None\n        allow_dups : bool, default False\n        copy : bool, default True\n        consolidate: bool, default True\n            Whether to consolidate inplace before reindexing.\n        only_slice : bool, default False\n            Whether to take views, not copies, along columns.\n        use_na_proxy : bool, default False\n            Whether to use a np.void ndarray for newly introduced columns.\n\n        pandas-indexer with -1's only.\n        \"\"\"\n        if indexer is None:\n            if new_axis is self.axes[axis] and not copy:\n                return self\n\n            result = self.copy(deep=copy)\n            result.axes = list(self.axes)\n            result.axes[axis] = new_axis\n            return result\n\n        if consolidate:\n            self._consolidate_inplace()\n\n        # some axes don't allow reindexing with dups\n        if not allow_dups:\n            self.axes[axis]._validate_can_reindex(indexer)\n\n        if axis >= self.ndim:\n            raise IndexError(\"Requested axis not found in manager\")\n\n        if axis == 0:\n            new_blocks = self._slice_take_blocks_ax0(\n                indexer,\n                fill_value=fill_value,\n                only_slice=only_slice,\n                use_na_proxy=use_na_proxy,\n            )\n        else:\n            new_blocks = [\n                blk.take_nd(\n                    indexer,\n                    axis=1,\n                    fill_value=(\n                        fill_value if fill_value is not None else blk.fill_value\n                    ),\n                )\n                for blk in self.blocks\n            ]\n\n        new_axes = list(self.axes)\n        new_axes[axis] = new_axis\n\n        new_mgr = type(self).from_blocks(new_blocks, new_axes)\n        if axis == 1:\n            # We can avoid the need to rebuild these\n            new_mgr._blknos = self.blknos.copy()\n            new_mgr._blklocs = self.blklocs.copy()\n        return new_mgr\n\n    def _slice_take_blocks_ax0(\n        self,\n        slice_or_indexer: slice | np.ndarray,\n        fill_value=lib.no_default,\n        only_slice: bool = False,\n        *,\n        use_na_proxy: bool = False,\n    ) -> list[Block]:\n        \"\"\"\n        Slice/take blocks along axis=0.\n\n        Overloaded for SingleBlock\n\n        Parameters\n        ----------\n        slice_or_indexer : slice or np.ndarray[int64]\n        fill_value : scalar, default lib.no_default\n        only_slice : bool, default False\n            If True, we always return views on existing arrays, never copies.\n            This is used when called from ops.blockwise.operate_blockwise.\n        use_na_proxy : bool, default False\n            Whether to use a np.void ndarray for newly introduced columns.\n\n        Returns\n        -------\n        new_blocks : list of Block\n        \"\"\"\n        allow_fill = fill_value is not lib.no_default\n\n        sl_type, slobj, sllen = _preprocess_slice_or_indexer(\n            slice_or_indexer, self.shape[0], allow_fill=allow_fill\n        )\n\n        if self.is_single_block:\n            blk = self.blocks[0]\n\n            if sl_type == \"slice\":\n                # GH#32959 EABlock would fail since we can't make 0-width\n                # TODO(EA2D): special casing unnecessary with 2D EAs\n                if sllen == 0:\n                    return []\n                bp = BlockPlacement(slice(0, sllen))\n                return [blk.getitem_block_columns(slobj, new_mgr_locs=bp)]\n            elif not allow_fill or self.ndim == 1:\n                if allow_fill and fill_value is None:\n                    fill_value = blk.fill_value\n\n                if not allow_fill and only_slice:\n                    # GH#33597 slice instead of take, so we get\n                    #  views instead of copies\n                    blocks = [\n                        blk.getitem_block_columns(\n                            slice(ml, ml + 1), new_mgr_locs=BlockPlacement(i)\n                        )\n                        for i, ml in enumerate(slobj)\n                    ]\n                    # We have\n                    #  all(np.shares_memory(nb.values, blk.values) for nb in blocks)\n                    return blocks\n                else:\n                    bp = BlockPlacement(slice(0, sllen))\n                    return [\n                        blk.take_nd(\n                            slobj,\n                            axis=0,\n                            new_mgr_locs=bp,\n                            fill_value=fill_value,\n                        )\n                    ]\n\n        if sl_type == \"slice\":\n            blknos = self.blknos[slobj]\n            blklocs = self.blklocs[slobj]\n        else:\n            blknos = algos.take_nd(\n                self.blknos, slobj, fill_value=-1, allow_fill=allow_fill\n            )\n            blklocs = algos.take_nd(\n                self.blklocs, slobj, fill_value=-1, allow_fill=allow_fill\n            )\n\n        # When filling blknos, make sure blknos is updated before appending to\n        # blocks list, that way new blkno is exactly len(blocks).\n        blocks = []\n        group = not only_slice\n        for blkno, mgr_locs in libinternals.get_blkno_placements(blknos, group=group):\n            if blkno == -1:\n                # If we've got here, fill_value was not lib.no_default\n\n                blocks.append(\n                    self._make_na_block(\n                        placement=mgr_locs,\n                        fill_value=fill_value,\n                        use_na_proxy=use_na_proxy,\n                    )\n                )\n            else:\n                blk = self.blocks[blkno]\n\n                # Otherwise, slicing along items axis is necessary.\n                if not blk._can_consolidate and not blk._validate_ndim:\n                    # i.e. we dont go through here for DatetimeTZBlock\n                    # A non-consolidatable block, it's easy, because there's\n                    # only one item and each mgr loc is a copy of that single\n                    # item.\n                    for mgr_loc in mgr_locs:\n                        newblk = blk.copy(deep=False)\n                        newblk.mgr_locs = BlockPlacement(slice(mgr_loc, mgr_loc + 1))\n                        blocks.append(newblk)\n\n                else:\n                    # GH#32779 to avoid the performance penalty of copying,\n                    #  we may try to only slice\n                    taker = blklocs[mgr_locs.indexer]\n                    max_len = max(len(mgr_locs), taker.max() + 1)\n                    if only_slice:\n                        taker = lib.maybe_indices_to_slice(taker, max_len)\n\n                    if isinstance(taker, slice):\n                        nb = blk.getitem_block_columns(taker, new_mgr_locs=mgr_locs)\n                        blocks.append(nb)\n                    elif only_slice:\n                        # GH#33597 slice instead of take, so we get\n                        #  views instead of copies\n                        for i, ml in zip(taker, mgr_locs):\n                            slc = slice(i, i + 1)\n                            bp = BlockPlacement(ml)\n                            nb = blk.getitem_block_columns(slc, new_mgr_locs=bp)\n                            # We have np.shares_memory(nb.values, blk.values)\n                            blocks.append(nb)\n                    else:\n                        nb = blk.take_nd(taker, axis=0, new_mgr_locs=mgr_locs)\n                        blocks.append(nb)\n\n        return blocks\n\n    def _make_na_block(\n        self, placement: BlockPlacement, fill_value=None, use_na_proxy: bool = False\n    ) -> Block:\n        # Note: we only get here with self.ndim == 2\n\n        if use_na_proxy:\n            assert fill_value is None\n            shape = (len(placement), self.shape[1])\n            vals = np.empty(shape, dtype=np.void)\n            nb = NumpyBlock(vals, placement, ndim=2)\n            return nb\n\n        if fill_value is None:\n            fill_value = np.nan\n        block_shape = list(self.shape)\n        block_shape[0] = len(placement)\n\n        dtype, fill_value = infer_dtype_from_scalar(fill_value)\n        # error: Argument \"dtype\" to \"empty\" has incompatible type \"Union[dtype,\n        # ExtensionDtype]\"; expected \"Union[dtype, None, type, _SupportsDtype, str,\n        # Tuple[Any, int], Tuple[Any, Union[int, Sequence[int]]], List[Any], _DtypeDict,\n        # Tuple[Any, Any]]\"\n        block_values = np.empty(block_shape, dtype=dtype)  # type: ignore[arg-type]\n        block_values.fill(fill_value)\n        return new_block_2d(block_values, placement=placement)\n\n    def take(self: T, indexer, axis: int = 1, verify: bool = True) -> T:\n        \"\"\"\n        Take items along any axis.\n\n        indexer : np.ndarray or slice\n        axis : int, default 1\n        verify : bool, default True\n            Check that all entries are between 0 and len(self) - 1, inclusive.\n            Pass verify=False if this check has been done by the caller.\n\n        Returns\n        -------\n        BlockManager\n        \"\"\"\n        # We have 6 tests that get here with a slice\n        indexer = (\n            np.arange(indexer.start, indexer.stop, indexer.step, dtype=np.intp)\n            if isinstance(indexer, slice)\n            else np.asanyarray(indexer, dtype=np.intp)\n        )\n\n        n = self.shape[axis]\n        indexer = maybe_convert_indices(indexer, n, verify=verify)\n\n        new_labels = self.axes[axis].take(indexer)\n        return self.reindex_indexer(\n            new_axis=new_labels,\n            indexer=indexer,\n            axis=axis,\n            allow_dups=True,\n            consolidate=False,\n        )\n\n\nclass BlockManager(libinternals.BlockManager, BaseBlockManager):\n    \"\"\"\n    BaseBlockManager that holds 2D blocks.\n    \"\"\"\n\n    ndim = 2\n\n    # ----------------------------------------------------------------\n    # Constructors\n\n    def __init__(\n        self,\n        blocks: Sequence[Block],\n        axes: Sequence[Index],\n        verify_integrity: bool = True,\n    ):\n\n        if verify_integrity:\n            # Assertion disabled for performance\n            # assert all(isinstance(x, Index) for x in axes)\n\n            for block in blocks:\n                if self.ndim != block.ndim:\n                    raise AssertionError(\n                        f\"Number of Block dimensions ({block.ndim}) must equal \"\n                        f\"number of axes ({self.ndim})\"\n                    )\n                if isinstance(block, DatetimeTZBlock) and block.values.ndim == 1:\n                    # TODO(2.0): remove once fastparquet no longer needs this\n                    warnings.warn(\n                        \"In a future version, the BlockManager constructor \"\n                        \"will assume that a DatetimeTZBlock with block.ndim==2 \"\n                        \"has block.values.ndim == 2.\",\n                        DeprecationWarning,\n                        stacklevel=find_stack_level(),\n                    )\n\n                    # error: Incompatible types in assignment (expression has type\n                    # \"Union[ExtensionArray, ndarray]\", variable has type\n                    # \"DatetimeArray\")\n                    block.values = ensure_block_shape(  # type: ignore[assignment]\n                        block.values, self.ndim\n                    )\n                    try:\n                        block._cache.clear()\n                    except AttributeError:\n                        # _cache not initialized\n                        pass\n\n            self._verify_integrity()\n\n    def _verify_integrity(self) -> None:\n        mgr_shape = self.shape\n        tot_items = sum(len(x.mgr_locs) for x in self.blocks)\n        for block in self.blocks:\n            if block.shape[1:] != mgr_shape[1:]:\n                raise construction_error(tot_items, block.shape[1:], self.axes)\n        if len(self.items) != tot_items:\n            raise AssertionError(\n                \"Number of manager items must equal union of \"\n                f\"block items\\n# manager items: {len(self.items)}, # \"\n                f\"tot_items: {tot_items}\"\n            )\n\n    @classmethod\n    def from_blocks(cls, blocks: list[Block], axes: list[Index]) -> BlockManager:\n        \"\"\"\n        Constructor for BlockManager and SingleBlockManager with same signature.\n        \"\"\"\n        return cls(blocks, axes, verify_integrity=False)\n\n    # ----------------------------------------------------------------\n    # Indexing\n\n    def fast_xs(self, loc: int) -> ArrayLike:\n        \"\"\"\n        Return the array corresponding to `frame.iloc[loc]`.\n\n        Parameters\n        ----------\n        loc : int\n\n        Returns\n        -------\n        np.ndarray or ExtensionArray\n        \"\"\"\n        if len(self.blocks) == 1:\n            return self.blocks[0].iget((slice(None), loc))\n\n        dtype = interleaved_dtype([blk.dtype for blk in self.blocks])\n\n        n = len(self)\n        if isinstance(dtype, ExtensionDtype):\n            cls = dtype.construct_array_type()\n            result = cls._empty((n,), dtype=dtype)\n        else:\n            result = np.empty(n, dtype=dtype)\n            result = ensure_wrapped_if_datetimelike(result)\n\n        for blk in self.blocks:\n            # Such assignment may incorrectly coerce NaT to None\n            # result[blk.mgr_locs] = blk._slice((slice(None), loc))\n            for i, rl in enumerate(blk.mgr_locs):\n                result[rl] = blk.iget((i, loc))\n\n        return result\n\n    def iget(self, i: int) -> SingleBlockManager:\n        \"\"\"\n        Return the data as a SingleBlockManager.\n        \"\"\"\n        block = self.blocks[self.blknos[i]]\n        values = block.iget(self.blklocs[i])\n\n        # shortcut for select a single-dim from a 2-dim BM\n        bp = BlockPlacement(slice(0, len(values)))\n        nb = type(block)(values, placement=bp, ndim=1)\n        return SingleBlockManager(nb, self.axes[1])\n\n    def iget_values(self, i: int) -> ArrayLike:\n        \"\"\"\n        Return the data for column i as the values (ndarray or ExtensionArray).\n        \"\"\"\n        block = self.blocks[self.blknos[i]]\n        values = block.iget(self.blklocs[i])\n        return values\n\n    @property\n    def column_arrays(self) -> list[np.ndarray]:\n        \"\"\"\n        Used in the JSON C code to access column arrays.\n        This optimizes compared to using `iget_values` by converting each\n        \"\"\"\n        # This is an optimized equivalent to\n        #  result = [self.iget_values(i) for i in range(len(self.items))]\n        result: list[np.ndarray | None] = [None] * len(self.items)\n\n        for blk in self.blocks:\n            mgr_locs = blk._mgr_locs\n            values = blk.values_for_json()\n            if values.ndim == 1:\n                # TODO(EA2D): special casing not needed with 2D EAs\n                result[mgr_locs[0]] = values\n\n            else:\n                for i, loc in enumerate(mgr_locs):\n                    result[loc] = values[i]\n\n        # error: Incompatible return value type (got \"List[None]\",\n        # expected \"List[ndarray[Any, Any]]\")\n        return result  # type: ignore[return-value]\n\n    def iset(\n        self, loc: int | slice | np.ndarray, value: ArrayLike, inplace: bool = False\n    ):\n        \"\"\"\n        Set new item in-place. Does not consolidate. Adds new Block if not\n        contained in the current set of items\n        \"\"\"\n\n        # FIXME: refactor, clearly separate broadcasting & zip-like assignment\n        #        can prob also fix the various if tests for sparse/categorical\n        if self._blklocs is None and self.ndim > 1:\n            self._rebuild_blknos_and_blklocs()\n\n        # Note: we exclude DTA/TDA here\n        value_is_extension_type = is_1d_only_ea_dtype(value.dtype)\n        if not value_is_extension_type:\n            if value.ndim == 2:\n                value = value.T\n            else:\n                value = ensure_block_shape(value, ndim=2)\n\n            if value.shape[1:] != self.shape[1:]:\n                raise AssertionError(\n                    \"Shape of new values must be compatible with manager shape\"\n                )\n\n        if lib.is_integer(loc):\n            # We have 6 tests where loc is _not_ an int.\n            # In this case, get_blkno_placements will yield only one tuple,\n            #  containing (self._blknos[loc], BlockPlacement(slice(0, 1, 1)))\n\n            # Check if we can use _iset_single fastpath\n            loc = cast(int, loc)\n            blkno = self.blknos[loc]\n            blk = self.blocks[blkno]\n            if len(blk._mgr_locs) == 1:  # TODO: fastest way to check this?\n                return self._iset_single(\n                    loc,\n                    value,\n                    inplace=inplace,\n                    blkno=blkno,\n                    blk=blk,\n                )\n\n            # error: Incompatible types in assignment (expression has type\n            # \"List[Union[int, slice, ndarray]]\", variable has type \"Union[int,\n            # slice, ndarray]\")\n            loc = [loc]  # type: ignore[assignment]\n\n        # categorical/sparse/datetimetz\n        if value_is_extension_type:\n\n            def value_getitem(placement):\n                return value\n\n        else:\n\n            def value_getitem(placement):\n                return value[placement.indexer]\n\n        # Accessing public blknos ensures the public versions are initialized\n        blknos = self.blknos[loc]\n        blklocs = self.blklocs[loc].copy()\n\n        unfit_mgr_locs = []\n        unfit_val_locs = []\n        removed_blknos = []\n        for blkno_l, val_locs in libinternals.get_blkno_placements(blknos, group=True):\n            blk = self.blocks[blkno_l]\n            blk_locs = blklocs[val_locs.indexer]\n            if inplace and blk.should_store(value):\n                blk.set_inplace(blk_locs, value_getitem(val_locs))\n            else:\n                unfit_mgr_locs.append(blk.mgr_locs.as_array[blk_locs])\n                unfit_val_locs.append(val_locs)\n\n                # If all block items are unfit, schedule the block for removal.\n                if len(val_locs) == len(blk.mgr_locs):\n                    removed_blknos.append(blkno_l)\n                else:\n                    blk.delete(blk_locs)\n                    self._blklocs[blk.mgr_locs.indexer] = np.arange(len(blk))\n\n        if len(removed_blknos):\n            # Remove blocks & update blknos accordingly\n            is_deleted = np.zeros(self.nblocks, dtype=np.bool_)\n            is_deleted[removed_blknos] = True\n\n            new_blknos = np.empty(self.nblocks, dtype=np.intp)\n            new_blknos.fill(-1)\n            new_blknos[~is_deleted] = np.arange(self.nblocks - len(removed_blknos))\n            self._blknos = new_blknos[self._blknos]\n            self.blocks = tuple(\n                blk for i, blk in enumerate(self.blocks) if i not in set(removed_blknos)\n            )\n\n        if unfit_val_locs:\n            unfit_idxr = np.concatenate(unfit_mgr_locs)\n            unfit_count = len(unfit_idxr)\n\n            new_blocks: list[Block] = []\n            if value_is_extension_type:\n                # This code (ab-)uses the fact that EA blocks contain only\n                # one item.\n                # TODO(EA2D): special casing unnecessary with 2D EAs\n                new_blocks.extend(\n                    new_block_2d(\n                        values=value,\n                        placement=BlockPlacement(slice(mgr_loc, mgr_loc + 1)),\n                    )\n                    for mgr_loc in unfit_idxr\n                )\n\n                self._blknos[unfit_idxr] = np.arange(unfit_count) + len(self.blocks)\n                self._blklocs[unfit_idxr] = 0\n\n            else:\n                # unfit_val_locs contains BlockPlacement objects\n                unfit_val_items = unfit_val_locs[0].append(unfit_val_locs[1:])\n\n                new_blocks.append(\n                    new_block_2d(\n                        values=value_getitem(unfit_val_items),\n                        placement=BlockPlacement(unfit_idxr),\n                    )\n                )\n\n                self._blknos[unfit_idxr] = len(self.blocks)\n                self._blklocs[unfit_idxr] = np.arange(unfit_count)\n\n            self.blocks += tuple(new_blocks)\n\n            # Newly created block's dtype may already be present.\n            self._known_consolidated = False\n\n    def _iset_single(\n        self, loc: int, value: ArrayLike, inplace: bool, blkno: int, blk: Block\n    ) -> None:\n        \"\"\"\n        Fastpath for iset when we are only setting a single position and\n        the Block currently in that position is itself single-column.\n\n        In this case we can swap out the entire Block and blklocs and blknos\n        are unaffected.\n        \"\"\"\n        # Caller is responsible for verifying value.shape\n\n        if inplace and blk.should_store(value):\n            iloc = self.blklocs[loc]\n            blk.set_inplace(slice(iloc, iloc + 1), value)\n            return\n\n        nb = new_block_2d(value, placement=blk._mgr_locs)\n        old_blocks = self.blocks\n        new_blocks = old_blocks[:blkno] + (nb,) + old_blocks[blkno + 1 :]\n        self.blocks = new_blocks\n        return\n\n    def insert(self, loc: int, item: Hashable, value: ArrayLike) -> None:\n        \"\"\"\n        Insert item at selected position.\n\n        Parameters\n        ----------\n        loc : int\n        item : hashable\n        value : np.ndarray or ExtensionArray\n        \"\"\"\n        # insert to the axis; this could possibly raise a TypeError\n        new_axis = self.items.insert(loc, item)\n\n        if value.ndim == 2:\n            value = value.T\n            if len(value) > 1:\n                raise ValueError(\n                    f\"Expected a 1D array, got an array with shape {value.T.shape}\"\n                )\n        else:\n            value = ensure_block_shape(value, ndim=self.ndim)\n\n        bp = BlockPlacement(slice(loc, loc + 1))\n        block = new_block_2d(values=value, placement=bp)\n\n        if not len(self.blocks):\n            # Fastpath\n            self._blklocs = np.array([0], dtype=np.intp)\n            self._blknos = np.array([0], dtype=np.intp)\n        else:\n            self._insert_update_mgr_locs(loc)\n            self._insert_update_blklocs_and_blknos(loc)\n\n        self.axes[0] = new_axis\n        self.blocks += (block,)\n\n        self._known_consolidated = False\n\n        if sum(not block.is_extension for block in self.blocks) > 100:\n            warnings.warn(\n                \"DataFrame is highly fragmented.  This is usually the result \"\n                \"of calling `frame.insert` many times, which has poor performance.  \"\n                \"Consider joining all columns at once using pd.concat(axis=1) \"\n                \"instead. To get a de-fragmented frame, use `newframe = frame.copy()`\",\n                PerformanceWarning,\n                stacklevel=find_stack_level(),\n            )\n\n    def _insert_update_mgr_locs(self, loc) -> None:\n        \"\"\"\n        When inserting a new Block at location 'loc', we increment\n        all of the mgr_locs of blocks above that by one.\n        \"\"\"\n        for blkno, count in _fast_count_smallints(self.blknos[loc:]):\n            # .620 this way, .326 of which is in increment_above\n            blk = self.blocks[blkno]\n            blk._mgr_locs = blk._mgr_locs.increment_above(loc)\n\n    def _insert_update_blklocs_and_blknos(self, loc) -> None:\n        \"\"\"\n        When inserting a new Block at location 'loc', we update our\n        _blklocs and _blknos.\n        \"\"\"\n\n        # Accessing public blklocs ensures the public versions are initialized\n        if loc == self.blklocs.shape[0]:\n            # np.append is a lot faster, let's use it if we can.\n            self._blklocs = np.append(self._blklocs, 0)\n            self._blknos = np.append(self._blknos, len(self.blocks))\n        elif loc == 0:\n            # np.append is a lot faster, let's use it if we can.\n            self._blklocs = np.append(self._blklocs[::-1], 0)[::-1]\n            self._blknos = np.append(self._blknos[::-1], len(self.blocks))[::-1]\n        else:\n            new_blklocs, new_blknos = libinternals.update_blklocs_and_blknos(\n                self.blklocs, self.blknos, loc, len(self.blocks)\n            )\n            self._blklocs = new_blklocs\n            self._blknos = new_blknos\n\n    def idelete(self, indexer) -> BlockManager:\n        \"\"\"\n        Delete selected locations, returning a new BlockManager.\n        \"\"\"\n        is_deleted = np.zeros(self.shape[0], dtype=np.bool_)\n        is_deleted[indexer] = True\n        taker = (~is_deleted).nonzero()[0]\n\n        nbs = self._slice_take_blocks_ax0(taker, only_slice=True)\n        new_columns = self.items[~is_deleted]\n        axes = [new_columns, self.axes[1]]\n        return type(self)(tuple(nbs), axes, verify_integrity=False)\n\n    # ----------------------------------------------------------------\n    # Block-wise Operation\n\n    def grouped_reduce(self: T, func: Callable, ignore_failures: bool = False) -> T:\n        \"\"\"\n        Apply grouped reduction function blockwise, returning a new BlockManager.\n\n        Parameters\n        ----------\n        func : grouped reduction function\n        ignore_failures : bool, default False\n            Whether to drop blocks where func raises TypeError.\n\n        Returns\n        -------\n        BlockManager\n        \"\"\"\n        result_blocks: list[Block] = []\n        dropped_any = False\n\n        for blk in self.blocks:\n            if blk.is_object:\n                # split on object-dtype blocks bc some columns may raise\n                #  while others do not.\n                for sb in blk._split():\n                    try:\n                        applied = sb.apply(func)\n                    except (TypeError, NotImplementedError):\n                        if not ignore_failures:\n                            raise\n                        dropped_any = True\n                        continue\n                    result_blocks = extend_blocks(applied, result_blocks)\n            else:\n                try:\n                    applied = blk.apply(func)\n                except (TypeError, NotImplementedError):\n                    if not ignore_failures:\n                        raise\n                    dropped_any = True\n                    continue\n                result_blocks = extend_blocks(applied, result_blocks)\n\n        if len(result_blocks) == 0:\n            index = Index([None])  # placeholder\n        else:\n            index = Index(range(result_blocks[0].values.shape[-1]))\n\n        if dropped_any:\n            # faster to skip _combine if we haven't dropped any blocks\n            return self._combine(result_blocks, copy=False, index=index)\n\n        return type(self).from_blocks(result_blocks, [self.axes[0], index])\n\n    def reduce(\n        self: T, func: Callable, ignore_failures: bool = False\n    ) -> tuple[T, np.ndarray]:\n        \"\"\"\n        Apply reduction function blockwise, returning a single-row BlockManager.\n\n        Parameters\n        ----------\n        func : reduction function\n        ignore_failures : bool, default False\n            Whether to drop blocks where func raises TypeError.\n\n        Returns\n        -------\n        BlockManager\n        np.ndarray\n            Indexer of mgr_locs that are retained.\n        \"\"\"\n        # If 2D, we assume that we're operating column-wise\n        assert self.ndim == 2\n\n        res_blocks: list[Block] = []\n        for blk in self.blocks:\n            nbs = blk.reduce(func, ignore_failures)\n            res_blocks.extend(nbs)\n\n        index = Index([None])  # placeholder\n        if ignore_failures:\n            if res_blocks:\n                indexer = np.concatenate([blk.mgr_locs.as_array for blk in res_blocks])\n                new_mgr = self._combine(res_blocks, copy=False, index=index)\n            else:\n                indexer = []\n                new_mgr = type(self).from_blocks([], [self.items[:0], index])\n        else:\n            indexer = np.arange(self.shape[0])\n            new_mgr = type(self).from_blocks(res_blocks, [self.items, index])\n        return new_mgr, indexer\n\n    def operate_blockwise(self, other: BlockManager, array_op) -> BlockManager:\n        \"\"\"\n        Apply array_op blockwise with another (aligned) BlockManager.\n        \"\"\"\n        return operate_blockwise(self, other, array_op)\n\n    def _equal_values(self: BlockManager, other: BlockManager) -> bool:\n        \"\"\"\n        Used in .equals defined in base class. Only check the column values\n        assuming shape and indexes have already been checked.\n        \"\"\"\n        return blockwise_all(self, other, array_equals)\n\n    def quantile(\n        self: T,\n        *,\n        qs: Float64Index,\n        axis: int = 0,\n        interpolation=\"linear\",\n    ) -> T:\n        \"\"\"\n        Iterate over blocks applying quantile reduction.\n        This routine is intended for reduction type operations and\n        will do inference on the generated blocks.\n\n        Parameters\n        ----------\n        axis: reduction axis, default 0\n        consolidate: bool, default True. Join together blocks having same\n            dtype\n        interpolation : type of interpolation, default 'linear'\n        qs : list of the quantiles to be computed\n\n        Returns\n        -------\n        BlockManager\n        \"\"\"\n        # Series dispatches to DataFrame for quantile, which allows us to\n        #  simplify some of the code here and in the blocks\n        assert self.ndim >= 2\n        assert is_list_like(qs)  # caller is responsible for this\n        assert axis == 1  # only ever called this way\n\n        new_axes = list(self.axes)\n        new_axes[1] = Float64Index(qs)\n\n        blocks = [\n            blk.quantile(axis=axis, qs=qs, interpolation=interpolation)\n            for blk in self.blocks\n        ]\n\n        return type(self)(blocks, new_axes)\n\n    # ----------------------------------------------------------------\n\n    def unstack(self, unstacker, fill_value) -> BlockManager:\n        \"\"\"\n        Return a BlockManager with all blocks unstacked.\n\n        Parameters\n        ----------\n        unstacker : reshape._Unstacker\n        fill_value : Any\n            fill_value for newly introduced missing values.\n\n        Returns\n        -------\n        unstacked : BlockManager\n        \"\"\"\n        new_columns = unstacker.get_new_columns(self.items)\n        new_index = unstacker.new_index\n\n        allow_fill = not unstacker.mask_all\n        if allow_fill:\n            # calculating the full mask once and passing it to Block._unstack is\n            #  faster than letting calculating it in each repeated call\n            new_mask2D = (~unstacker.mask).reshape(*unstacker.full_shape)\n            needs_masking = new_mask2D.any(axis=0)\n        else:\n            needs_masking = np.zeros(unstacker.full_shape[1], dtype=bool)\n\n        new_blocks: list[Block] = []\n        columns_mask: list[np.ndarray] = []\n\n        if len(self.items) == 0:\n            factor = 1\n        else:\n            fac = len(new_columns) / len(self.items)\n            assert fac == int(fac)\n            factor = int(fac)\n\n        for blk in self.blocks:\n            mgr_locs = blk.mgr_locs\n            new_placement = mgr_locs.tile_for_unstack(factor)\n\n            blocks, mask = blk._unstack(\n                unstacker,\n                fill_value,\n                new_placement=new_placement,\n                needs_masking=needs_masking,\n            )\n\n            new_blocks.extend(blocks)\n            columns_mask.extend(mask)\n\n            # Block._unstack should ensure this holds,\n            assert mask.sum() == sum(len(nb._mgr_locs) for nb in blocks)\n            # In turn this ensures that in the BlockManager call below\n            #  we have len(new_columns) == sum(x.shape[0] for x in new_blocks)\n            #  which suffices to allow us to pass verify_inegrity=False\n\n        new_columns = new_columns[columns_mask]\n\n        bm = BlockManager(new_blocks, [new_columns, new_index], verify_integrity=False)\n        return bm\n\n    def to_dict(self, copy: bool = True):\n        \"\"\"\n        Return a dict of str(dtype) -> BlockManager\n\n        Parameters\n        ----------\n        copy : bool, default True\n\n        Returns\n        -------\n        values : a dict of dtype -> BlockManager\n        \"\"\"\n\n        bd: dict[str, list[Block]] = {}\n        for b in self.blocks:\n            bd.setdefault(str(b.dtype), []).append(b)\n\n        # TODO(EA2D): the combine will be unnecessary with 2D EAs\n        return {dtype: self._combine(blocks, copy=copy) for dtype, blocks in bd.items()}\n\n    def as_array(\n        self,\n        dtype: np.dtype | None = None,\n        copy: bool = False,\n        na_value=lib.no_default,\n    ) -> np.ndarray:\n        \"\"\"\n        Convert the blockmanager data into an numpy array.\n\n        Parameters\n        ----------\n        dtype : np.dtype or None, default None\n            Data type of the return array.\n        copy : bool, default False\n            If True then guarantee that a copy is returned. A value of\n            False does not guarantee that the underlying data is not\n            copied.\n        na_value : object, default lib.no_default\n            Value to be used as the missing value sentinel.\n\n        Returns\n        -------\n        arr : ndarray\n        \"\"\"\n        if len(self.blocks) == 0:\n            arr = np.empty(self.shape, dtype=float)\n            return arr.transpose()\n\n        # We want to copy when na_value is provided to avoid\n        # mutating the original object\n        copy = copy or na_value is not lib.no_default\n\n        if self.is_single_block:\n            blk = self.blocks[0]\n            if blk.is_extension:\n                # Avoid implicit conversion of extension blocks to object\n\n                # error: Item \"ndarray\" of \"Union[ndarray, ExtensionArray]\" has no\n                # attribute \"to_numpy\"\n                arr = blk.values.to_numpy(  # type: ignore[union-attr]\n                    dtype=dtype,\n                    na_value=na_value,\n                ).reshape(blk.shape)\n            else:\n                arr = np.asarray(blk.get_values())\n                if dtype:\n                    arr = arr.astype(dtype, copy=False)\n        else:\n            arr = self._interleave(dtype=dtype, na_value=na_value)\n            # The underlying data was copied within _interleave\n            copy = False\n\n        if copy:\n            arr = arr.copy()\n\n        if na_value is not lib.no_default:\n            arr[isna(arr)] = na_value\n\n        return arr.transpose()\n\n    def _interleave(\n        self,\n        dtype: np.dtype | None = None,\n        na_value=lib.no_default,\n    ) -> np.ndarray:\n        \"\"\"\n        Return ndarray from blocks with specified item order\n        Items must be contained in the blocks\n        \"\"\"\n        if not dtype:\n            # Incompatible types in assignment (expression has type\n            # \"Optional[Union[dtype[Any], ExtensionDtype]]\", variable has\n            # type \"Optional[dtype[Any]]\")\n            dtype = interleaved_dtype(  # type: ignore[assignment]\n                [blk.dtype for blk in self.blocks]\n            )\n\n        # TODO: https://github.com/pandas-dev/pandas/issues/22791\n        # Give EAs some input on what happens here. Sparse needs this.\n        if isinstance(dtype, SparseDtype):\n            dtype = dtype.subtype\n            dtype = cast(np.dtype, dtype)\n        elif isinstance(dtype, ExtensionDtype):\n            dtype = np.dtype(\"object\")\n        elif is_dtype_equal(dtype, str):\n            dtype = np.dtype(\"object\")\n\n        result = np.empty(self.shape, dtype=dtype)\n\n        itemmask = np.zeros(self.shape[0])\n\n        if dtype == np.dtype(\"object\") and na_value is lib.no_default:\n            # much more performant than using to_numpy below\n            for blk in self.blocks:\n                rl = blk.mgr_locs\n                arr = blk.get_values(dtype)\n                result[rl.indexer] = arr\n                itemmask[rl.indexer] = 1\n            return result\n\n        for blk in self.blocks:\n            rl = blk.mgr_locs\n            if blk.is_extension:\n                # Avoid implicit conversion of extension blocks to object\n\n                # error: Item \"ndarray\" of \"Union[ndarray, ExtensionArray]\" has no\n                # attribute \"to_numpy\"\n                arr = blk.values.to_numpy(  # type: ignore[union-attr]\n                    dtype=dtype,\n                    na_value=na_value,\n                )\n            else:\n                arr = blk.get_values(dtype)\n            result[rl.indexer] = arr\n            itemmask[rl.indexer] = 1\n\n        if not itemmask.all():\n            raise AssertionError(\"Some items were not contained in blocks\")\n\n        return result\n\n    # ----------------------------------------------------------------\n    # Consolidation\n\n    def is_consolidated(self) -> bool:\n        \"\"\"\n        Return True if more than one block with the same dtype\n        \"\"\"\n        if not self._known_consolidated:\n            self._consolidate_check()\n        return self._is_consolidated\n\n    def _consolidate_check(self) -> None:\n        if len(self.blocks) == 1:\n            # fastpath\n            self._is_consolidated = True\n            self._known_consolidated = True\n            return\n        dtypes = [blk.dtype for blk in self.blocks if blk._can_consolidate]\n        self._is_consolidated = len(dtypes) == len(set(dtypes))\n        self._known_consolidated = True\n\n    def _consolidate_inplace(self) -> None:\n        if not self.is_consolidated():\n            self.blocks = tuple(_consolidate(self.blocks))\n            self._is_consolidated = True\n            self._known_consolidated = True\n            self._rebuild_blknos_and_blklocs()\n\n\nclass SingleBlockManager(BaseBlockManager, SingleDataManager):\n    \"\"\"manage a single block with\"\"\"\n\n    ndim = 1\n    _is_consolidated = True\n    _known_consolidated = True\n    __slots__ = ()\n    is_single_block = True\n\n    def __init__(\n        self,\n        block: Block,\n        axis: Index,\n        verify_integrity: bool = False,\n        fastpath=lib.no_default,\n    ):\n        # Assertions disabled for performance\n        # assert isinstance(block, Block), type(block)\n        # assert isinstance(axis, Index), type(axis)\n\n        if fastpath is not lib.no_default:\n            warnings.warn(\n                \"The `fastpath` keyword is deprecated and will be removed \"\n                \"in a future version.\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n\n        self.axes = [axis]\n        self.blocks = (block,)\n\n    @classmethod\n    def from_blocks(cls, blocks: list[Block], axes: list[Index]) -> SingleBlockManager:\n        \"\"\"\n        Constructor for BlockManager and SingleBlockManager with same signature.\n        \"\"\"\n        assert len(blocks) == 1\n        assert len(axes) == 1\n        return cls(blocks[0], axes[0], verify_integrity=False)\n\n    @classmethod\n    def from_array(cls, array: ArrayLike, index: Index) -> SingleBlockManager:\n        \"\"\"\n        Constructor for if we have an array that is not yet a Block.\n        \"\"\"\n        block = new_block(array, placement=slice(0, len(index)), ndim=1)\n        return cls(block, index)\n\n    def to_2d_mgr(self, columns: Index) -> BlockManager:\n        \"\"\"\n        Manager analogue of Series.to_frame\n        \"\"\"\n        blk = self.blocks[0]\n        arr = ensure_block_shape(blk.values, ndim=2)\n        bp = BlockPlacement(0)\n        new_blk = type(blk)(arr, placement=bp, ndim=2)\n        axes = [columns, self.axes[0]]\n        return BlockManager([new_blk], axes=axes, verify_integrity=False)\n\n    def __getstate__(self):\n        block_values = [b.values for b in self.blocks]\n        block_items = [self.items[b.mgr_locs.indexer] for b in self.blocks]\n        axes_array = list(self.axes)\n\n        extra_state = {\n            \"0.14.1\": {\n                \"axes\": axes_array,\n                \"blocks\": [\n                    {\"values\": b.values, \"mgr_locs\": b.mgr_locs.indexer}\n                    for b in self.blocks\n                ],\n            }\n        }\n\n        # First three elements of the state are to maintain forward\n        # compatibility with 0.13.1.\n        return axes_array, block_values, block_items, extra_state\n\n    def __setstate__(self, state):\n        def unpickle_block(values, mgr_locs, ndim: int) -> Block:\n            # TODO(EA2D): ndim would be unnecessary with 2D EAs\n            # older pickles may store e.g. DatetimeIndex instead of DatetimeArray\n            values = extract_array(values, extract_numpy=True)\n            return new_block(values, placement=mgr_locs, ndim=ndim)\n\n        if isinstance(state, tuple) and len(state) >= 4 and \"0.14.1\" in state[3]:\n            state = state[3][\"0.14.1\"]\n            self.axes = [ensure_index(ax) for ax in state[\"axes\"]]\n            ndim = len(self.axes)\n            self.blocks = tuple(\n                unpickle_block(b[\"values\"], b[\"mgr_locs\"], ndim=ndim)\n                for b in state[\"blocks\"]\n            )\n        else:\n            raise NotImplementedError(\"pre-0.14.1 pickles are no longer supported\")\n\n        self._post_setstate()\n\n    def _post_setstate(self):\n        pass\n\n    @cache_readonly\n    def _block(self) -> Block:\n        return self.blocks[0]\n\n    @property\n    def _blknos(self):\n        \"\"\"compat with BlockManager\"\"\"\n        return None\n\n    @property\n    def _blklocs(self):\n        \"\"\"compat with BlockManager\"\"\"\n        return None\n\n    def getitem_mgr(self, indexer) -> SingleBlockManager:\n        # similar to get_slice, but not restricted to slice indexer\n        blk = self._block\n        array = blk._slice(indexer)\n        if array.ndim > 1:\n            # This will be caught by Series._get_values\n            raise ValueError(\"dimension-expanding indexing not allowed\")\n\n        bp = BlockPlacement(slice(0, len(array)))\n        block = type(blk)(array, placement=bp, ndim=1)\n\n        new_idx = self.index[indexer]\n        return type(self)(block, new_idx)\n\n    def get_slice(self, slobj: slice, axis: int = 0) -> SingleBlockManager:\n        # Assertion disabled for performance\n        # assert isinstance(slobj, slice), type(slobj)\n        if axis >= self.ndim:\n            raise IndexError(\"Requested axis not found in manager\")\n\n        blk = self._block\n        array = blk._slice(slobj)\n        bp = BlockPlacement(slice(0, len(array)))\n        block = type(blk)(array, placement=bp, ndim=1)\n        new_index = self.index._getitem_slice(slobj)\n        return type(self)(block, new_index)\n\n    @property\n    def index(self) -> Index:\n        return self.axes[0]\n\n    @property\n    def dtype(self) -> DtypeObj:\n        return self._block.dtype\n\n    def get_dtypes(self) -> np.ndarray:\n        return np.array([self._block.dtype])\n\n    def external_values(self):\n        \"\"\"The array that Series.values returns\"\"\"\n        return self._block.external_values()\n\n    def internal_values(self):\n        \"\"\"The array that Series._values returns\"\"\"\n        return self._block.values\n\n    def array_values(self):\n        \"\"\"The array that Series.array returns\"\"\"\n        return self._block.array_values\n\n    def get_numeric_data(self, copy: bool = False):\n        if self._block.is_numeric:\n            if copy:\n                return self.copy()\n            return self\n        return self.make_empty()\n\n    @property\n    def _can_hold_na(self) -> bool:\n        return self._block._can_hold_na\n\n    def idelete(self, indexer) -> SingleBlockManager:\n        \"\"\"\n        Delete single location from SingleBlockManager.\n\n        Ensures that self.blocks doesn't become empty.\n        \"\"\"\n        self._block.delete(indexer)\n        self.axes[0] = self.axes[0].delete(indexer)\n        return self\n\n    def fast_xs(self, loc):\n        \"\"\"\n        fast path for getting a cross-section\n        return a view of the data\n        \"\"\"\n        raise NotImplementedError(\"Use series._values[loc] instead\")\n\n    def set_values(self, values: ArrayLike):\n        \"\"\"\n        Set the values of the single block in place.\n\n        Use at your own risk! This does not check if the passed values are\n        valid for the current Block/SingleBlockManager (length, dtype, etc).\n        \"\"\"\n        self.blocks[0].values = values\n        self.blocks[0]._mgr_locs = BlockPlacement(slice(len(values)))\n\n    def _equal_values(self: T, other: T) -> bool:\n        \"\"\"\n        Used in .equals defined in base class. Only check the column values\n        assuming shape and indexes have already been checked.\n        \"\"\"\n        # For SingleBlockManager (i.e.Series)\n        if other.ndim != 1:\n            return False\n        left = self.blocks[0].values\n        right = other.blocks[0].values\n        return array_equals(left, right)\n\n\n# --------------------------------------------------------------------\n# Constructor Helpers\n\n\ndef create_block_manager_from_blocks(\n    blocks: list[Block],\n    axes: list[Index],\n    consolidate: bool = True,\n    verify_integrity: bool = True,\n) -> BlockManager:\n    # If verify_integrity=False, then caller is responsible for checking\n    #  all(x.shape[-1] == len(axes[1]) for x in blocks)\n    #  sum(x.shape[0] for x in blocks) == len(axes[0])\n    #  set(x for for blk in blocks for x in blk.mgr_locs) == set(range(len(axes[0])))\n    #  all(blk.ndim == 2 for blk in blocks)\n    # This allows us to safely pass verify_integrity=False\n\n    try:\n        mgr = BlockManager(blocks, axes, verify_integrity=verify_integrity)\n\n    except ValueError as err:\n        arrays = [blk.values for blk in blocks]\n        tot_items = sum(arr.shape[0] for arr in arrays)\n        raise construction_error(tot_items, arrays[0].shape[1:], axes, err)\n\n    if consolidate:\n        mgr._consolidate_inplace()\n    return mgr\n\n\ndef create_block_manager_from_column_arrays(\n    arrays: list[ArrayLike],\n    axes: list[Index],\n    consolidate: bool = True,\n) -> BlockManager:\n    # Assertions disabled for performance (caller is responsible for verifying)\n    # assert isinstance(axes, list)\n    # assert all(isinstance(x, Index) for x in axes)\n    # assert all(isinstance(x, (np.ndarray, ExtensionArray)) for x in arrays)\n    # assert all(type(x) is not PandasArray for x in arrays)\n    # assert all(x.ndim == 1 for x in arrays)\n    # assert all(len(x) == len(axes[1]) for x in arrays)\n    # assert len(arrays) == len(axes[0])\n    # These last three are sufficient to allow us to safely pass\n    #  verify_integrity=False below.\n\n    try:\n        blocks = _form_blocks(arrays, consolidate)\n        mgr = BlockManager(blocks, axes, verify_integrity=False)\n    except ValueError as e:\n        raise construction_error(len(arrays), arrays[0].shape, axes, e)\n    if consolidate:\n        mgr._consolidate_inplace()\n    return mgr\n\n\ndef construction_error(\n    tot_items: int,\n    block_shape: Shape,\n    axes: list[Index],\n    e: ValueError | None = None,\n):\n    \"\"\"raise a helpful message about our construction\"\"\"\n    passed = tuple(map(int, [tot_items] + list(block_shape)))\n    # Correcting the user facing error message during dataframe construction\n    if len(passed) <= 2:\n        passed = passed[::-1]\n\n    implied = tuple(len(ax) for ax in axes)\n    # Correcting the user facing error message during dataframe construction\n    if len(implied) <= 2:\n        implied = implied[::-1]\n\n    # We return the exception object instead of raising it so that we\n    #  can raise it in the caller; mypy plays better with that\n    if passed == implied and e is not None:\n        return e\n    if block_shape[0] == 0:\n        return ValueError(\"Empty data passed with indices specified.\")\n    return ValueError(f\"Shape of passed values is {passed}, indices imply {implied}\")\n\n\n# -----------------------------------------------------------------------\n\n\ndef _grouping_func(tup: tuple[int, ArrayLike]) -> tuple[int, bool, DtypeObj]:\n    # compat for numpy<1.21, in which comparing a np.dtype with an ExtensionDtype\n    # raises instead of returning False. Once earlier numpy versions are dropped,\n    # this can be simplified to `return tup[1].dtype`\n    dtype = tup[1].dtype\n\n    if is_1d_only_ea_dtype(dtype):\n        # We know these won't be consolidated, so don't need to group these.\n        # This avoids expensive comparisons of CategoricalDtype objects\n        sep = id(dtype)\n    else:\n        sep = 0\n\n    return sep, isinstance(dtype, np.dtype), dtype\n\n\ndef _form_blocks(arrays: list[ArrayLike], consolidate: bool) -> list[Block]:\n    tuples = list(enumerate(arrays))\n\n    if not consolidate:\n        nbs = _tuples_to_blocks_no_consolidate(tuples)\n        return nbs\n\n    # group by dtype\n    grouper = itertools.groupby(tuples, _grouping_func)\n\n    nbs = []\n    for (_, _, dtype), tup_block in grouper:\n        block_type = get_block_type(dtype)\n\n        if isinstance(dtype, np.dtype):\n            is_dtlike = dtype.kind in [\"m\", \"M\"]\n\n            if issubclass(dtype.type, (str, bytes)):\n                dtype = np.dtype(object)\n\n            values, placement = _stack_arrays(list(tup_block), dtype)\n            if is_dtlike:\n                values = ensure_wrapped_if_datetimelike(values)\n            blk = block_type(values, placement=BlockPlacement(placement), ndim=2)\n            nbs.append(blk)\n\n        elif is_1d_only_ea_dtype(dtype):\n            dtype_blocks = [\n                block_type(x[1], placement=BlockPlacement(x[0]), ndim=2)\n                for x in tup_block\n            ]\n            nbs.extend(dtype_blocks)\n\n        else:\n            dtype_blocks = [\n                block_type(\n                    ensure_block_shape(x[1], 2), placement=BlockPlacement(x[0]), ndim=2\n                )\n                for x in tup_block\n            ]\n            nbs.extend(dtype_blocks)\n    return nbs\n\n\ndef _tuples_to_blocks_no_consolidate(tuples) -> list[Block]:\n    # tuples produced within _form_blocks are of the form (placement, array)\n    return [\n        new_block_2d(ensure_block_shape(x[1], ndim=2), placement=BlockPlacement(x[0]))\n        for x in tuples\n    ]\n\n\ndef _stack_arrays(tuples, dtype: np.dtype):\n\n    placement, arrays = zip(*tuples)\n\n    first = arrays[0]\n    shape = (len(arrays),) + first.shape\n\n    stacked = np.empty(shape, dtype=dtype)\n    for i, arr in enumerate(arrays):\n        stacked[i] = arr\n\n    return stacked, placement\n\n\ndef _consolidate(blocks: tuple[Block, ...]) -> list[Block]:\n    \"\"\"\n    Merge blocks having same dtype, exclude non-consolidating blocks\n    \"\"\"\n    # sort by _can_consolidate, dtype\n    gkey = lambda x: x._consolidate_key\n    grouper = itertools.groupby(sorted(blocks, key=gkey), gkey)\n\n    new_blocks: list[Block] = []\n    for (_can_consolidate, dtype), group_blocks in grouper:\n        merged_blocks = _merge_blocks(\n            list(group_blocks), dtype=dtype, can_consolidate=_can_consolidate\n        )\n        new_blocks = extend_blocks(merged_blocks, new_blocks)\n    return new_blocks\n\n\ndef _merge_blocks(\n    blocks: list[Block], dtype: DtypeObj, can_consolidate: bool\n) -> list[Block]:\n\n    if len(blocks) == 1:\n        return blocks\n\n    if can_consolidate:\n\n        # TODO: optimization potential in case all mgrs contain slices and\n        # combination of those slices is a slice, too.\n        new_mgr_locs = np.concatenate([b.mgr_locs.as_array for b in blocks])\n\n        new_values: ArrayLike\n\n        if isinstance(blocks[0].dtype, np.dtype):\n            # error: List comprehension has incompatible type List[Union[ndarray,\n            # ExtensionArray]]; expected List[Union[complex, generic,\n            # Sequence[Union[int, float, complex, str, bytes, generic]],\n            # Sequence[Sequence[Any]], SupportsArray]]\n            new_values = np.vstack([b.values for b in blocks])  # type: ignore[misc]\n        else:\n            bvals = [blk.values for blk in blocks]\n            bvals2 = cast(Sequence[NDArrayBackedExtensionArray], bvals)\n            new_values = bvals2[0]._concat_same_type(bvals2, axis=0)\n\n        argsort = np.argsort(new_mgr_locs)\n        new_values = new_values[argsort]\n        new_mgr_locs = new_mgr_locs[argsort]\n\n        bp = BlockPlacement(new_mgr_locs)\n        return [new_block_2d(new_values, placement=bp)]\n\n    # can't consolidate --> no merge\n    return blocks\n\n\ndef _fast_count_smallints(arr: npt.NDArray[np.intp]):\n    \"\"\"Faster version of set(arr) for sequences of small numbers.\"\"\"\n    counts = np.bincount(arr)\n    nz = counts.nonzero()[0]\n    # Note: list(zip(...) outperforms list(np.c_[nz, counts[nz]]) here,\n    #  in one benchmark by a factor of 11\n    return zip(nz, counts[nz])\n\n\ndef _preprocess_slice_or_indexer(\n    slice_or_indexer: slice | np.ndarray, length: int, allow_fill: bool\n):\n    if isinstance(slice_or_indexer, slice):\n        return (\n            \"slice\",\n            slice_or_indexer,\n            libinternals.slice_len(slice_or_indexer, length),\n        )\n    else:\n        if (\n            not isinstance(slice_or_indexer, np.ndarray)\n            or slice_or_indexer.dtype.kind != \"i\"\n        ):\n            dtype = getattr(slice_or_indexer, \"dtype\", None)\n            raise TypeError(type(slice_or_indexer), dtype)\n\n        indexer = ensure_platform_int(slice_or_indexer)\n        if not allow_fill:\n            indexer = maybe_convert_indices(indexer, length)\n        return \"fancy\", indexer, len(indexer)\n", 2157], "/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/flags.py": ["import weakref\n\n\nclass Flags:\n    \"\"\"\n    Flags that apply to pandas objects.\n\n    .. versionadded:: 1.2.0\n\n    Parameters\n    ----------\n    obj : Series or DataFrame\n        The object these flags are associated with.\n    allows_duplicate_labels : bool, default True\n        Whether to allow duplicate labels in this object. By default,\n        duplicate labels are permitted. Setting this to ``False`` will\n        cause an :class:`errors.DuplicateLabelError` to be raised when\n        `index` (or columns for DataFrame) is not unique, or any\n        subsequent operation on introduces duplicates.\n        See :ref:`duplicates.disallow` for more.\n\n        .. warning::\n\n           This is an experimental feature. Currently, many methods fail to\n           propagate the ``allows_duplicate_labels`` value. In future versions\n           it is expected that every method taking or returning one or more\n           DataFrame or Series objects will propagate ``allows_duplicate_labels``.\n\n    Notes\n    -----\n    Attributes can be set in two ways\n\n    >>> df = pd.DataFrame()\n    >>> df.flags\n    <Flags(allows_duplicate_labels=True)>\n    >>> df.flags.allows_duplicate_labels = False\n    >>> df.flags\n    <Flags(allows_duplicate_labels=False)>\n\n    >>> df.flags['allows_duplicate_labels'] = True\n    >>> df.flags\n    <Flags(allows_duplicate_labels=True)>\n    \"\"\"\n\n    _keys = {\"allows_duplicate_labels\"}\n\n    def __init__(self, obj, *, allows_duplicate_labels):\n        self._allows_duplicate_labels = allows_duplicate_labels\n        self._obj = weakref.ref(obj)\n\n    @property\n    def allows_duplicate_labels(self) -> bool:\n        \"\"\"\n        Whether this object allows duplicate labels.\n\n        Setting ``allows_duplicate_labels=False`` ensures that the\n        index (and columns of a DataFrame) are unique. Most methods\n        that accept and return a Series or DataFrame will propagate\n        the value of ``allows_duplicate_labels``.\n\n        See :ref:`duplicates` for more.\n\n        See Also\n        --------\n        DataFrame.attrs : Set global metadata on this object.\n        DataFrame.set_flags : Set global flags on this object.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({\"A\": [1, 2]}, index=['a', 'a'])\n        >>> df.flags.allows_duplicate_labels\n        True\n        >>> df.flags.allows_duplicate_labels = False\n        Traceback (most recent call last):\n            ...\n        pandas.errors.DuplicateLabelError: Index has duplicates.\n              positions\n        label\n        a        [0, 1]\n        \"\"\"\n        return self._allows_duplicate_labels\n\n    @allows_duplicate_labels.setter\n    def allows_duplicate_labels(self, value: bool):\n        value = bool(value)\n        obj = self._obj()\n        if obj is None:\n            raise ValueError(\"This flag's object has been deleted.\")\n\n        if not value:\n            for ax in obj.axes:\n                ax._maybe_check_unique()\n\n        self._allows_duplicate_labels = value\n\n    def __getitem__(self, key):\n        if key not in self._keys:\n            raise KeyError(key)\n\n        return getattr(self, key)\n\n    def __setitem__(self, key, value):\n        if key not in self._keys:\n            raise ValueError(f\"Unknown flag {key}. Must be one of {self._keys}\")\n        setattr(self, key, value)\n\n    def __repr__(self):\n        return f\"<Flags(allows_duplicate_labels={self.allows_duplicate_labels})>\"\n\n    def __eq__(self, other):\n        if isinstance(other, type(self)):\n            return self.allows_duplicate_labels == other.allows_duplicate_labels\n        return False\n", 113], "/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/generic.py": ["# pyright: reportPropertyTypeMismatch=false\nfrom __future__ import annotations\n\nimport collections\nfrom datetime import timedelta\nimport functools\nimport gc\nimport json\nimport operator\nimport pickle\nimport re\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Hashable,\n    Literal,\n    Mapping,\n    Sequence,\n    Type,\n    cast,\n    final,\n    overload,\n)\nimport warnings\nimport weakref\n\nimport numpy as np\n\nfrom pandas._config import config\n\nfrom pandas._libs import lib\nfrom pandas._libs.tslibs import (\n    Period,\n    Tick,\n    Timestamp,\n    to_offset,\n)\nfrom pandas._typing import (\n    ArrayLike,\n    Axis,\n    CompressionOptions,\n    Dtype,\n    DtypeArg,\n    DtypeObj,\n    FilePath,\n    IndexKeyFunc,\n    IndexLabel,\n    JSONSerializable,\n    Level,\n    Manager,\n    NDFrameT,\n    RandomState,\n    Renamer,\n    StorageOptions,\n    T,\n    TimedeltaConvertibleTypes,\n    TimestampConvertibleTypes,\n    ValueKeyFunc,\n    WriteBuffer,\n    npt,\n)\nfrom pandas.compat._optional import import_optional_dependency\nfrom pandas.compat.numpy import function as nv\nfrom pandas.errors import (\n    AbstractMethodError,\n    InvalidIndexError,\n)\nfrom pandas.util._decorators import (\n    doc,\n    rewrite_axis_style_signature,\n)\nfrom pandas.util._exceptions import find_stack_level\nfrom pandas.util._validators import (\n    validate_ascending,\n    validate_bool_kwarg,\n    validate_fillna_kwargs,\n    validate_inclusive,\n)\n\nfrom pandas.core.dtypes.common import (\n    ensure_object,\n    ensure_platform_int,\n    ensure_str,\n    is_bool,\n    is_bool_dtype,\n    is_datetime64_any_dtype,\n    is_datetime64tz_dtype,\n    is_dict_like,\n    is_dtype_equal,\n    is_extension_array_dtype,\n    is_float,\n    is_list_like,\n    is_number,\n    is_numeric_dtype,\n    is_re_compilable,\n    is_scalar,\n    is_timedelta64_dtype,\n    pandas_dtype,\n)\nfrom pandas.core.dtypes.generic import (\n    ABCDataFrame,\n    ABCSeries,\n)\nfrom pandas.core.dtypes.inference import (\n    is_hashable,\n    is_nested_list_like,\n)\nfrom pandas.core.dtypes.missing import (\n    isna,\n    notna,\n)\n\nfrom pandas.core import (\n    arraylike,\n    indexing,\n    missing,\n    nanops,\n)\nimport pandas.core.algorithms as algos\nfrom pandas.core.array_algos.replace import should_use_regex\nfrom pandas.core.arrays import ExtensionArray\nfrom pandas.core.base import PandasObject\nimport pandas.core.common as com\nfrom pandas.core.construction import (\n    create_series_with_explicit_dtype,\n    extract_array,\n)\nfrom pandas.core.describe import describe_ndframe\nfrom pandas.core.flags import Flags\nfrom pandas.core.indexes.api import (\n    DatetimeIndex,\n    Index,\n    MultiIndex,\n    PeriodIndex,\n    RangeIndex,\n    default_index,\n    ensure_index,\n)\nfrom pandas.core.internals import (\n    ArrayManager,\n    BlockManager,\n    SingleArrayManager,\n)\nfrom pandas.core.internals.construction import mgr_to_mgr\nfrom pandas.core.missing import find_valid_index\nfrom pandas.core.ops import align_method_FRAME\nfrom pandas.core.reshape.concat import concat\nimport pandas.core.sample as sample\nfrom pandas.core.shared_docs import _shared_docs\nfrom pandas.core.sorting import get_indexer_indexer\nfrom pandas.core.window import (\n    Expanding,\n    ExponentialMovingWindow,\n    Rolling,\n    Window,\n)\n\nfrom pandas.io.formats import format as fmt\nfrom pandas.io.formats.format import (\n    DataFrameFormatter,\n    DataFrameRenderer,\n)\nfrom pandas.io.formats.printing import pprint_thing\n\nif TYPE_CHECKING:\n\n    from pandas._libs.tslibs import BaseOffset\n\n    from pandas.core.frame import DataFrame\n    from pandas.core.indexers.objects import BaseIndexer\n    from pandas.core.resample import Resampler\n    from pandas.core.series import Series\n\n# goal is to be able to define the docs close to function, while still being\n# able to share\n_shared_docs = {**_shared_docs}\n_shared_doc_kwargs = {\n    \"axes\": \"keywords for axes\",\n    \"klass\": \"Series/DataFrame\",\n    \"axes_single_arg\": \"int or labels for object\",\n    \"args_transpose\": \"axes to permute (int or label for object)\",\n    \"inplace\": \"\"\"\n    inplace : bool, default False\n        If True, performs operation inplace and returns None.\"\"\",\n    \"optional_by\": \"\"\"\n        by : str or list of str\n            Name or list of names to sort by\"\"\",\n    \"replace_iloc\": \"\"\"\n    This differs from updating with ``.loc`` or ``.iloc``, which require\n    you to specify a location to update with some value.\"\"\",\n}\n\n\nbool_t = bool  # Need alias because NDFrame has def bool:\n\n\nclass NDFrame(PandasObject, indexing.IndexingMixin):\n    \"\"\"\n    N-dimensional analogue of DataFrame. Store multi-dimensional in a\n    size-mutable, labeled data structure\n\n    Parameters\n    ----------\n    data : BlockManager\n    axes : list\n    copy : bool, default False\n    \"\"\"\n\n    _internal_names: list[str] = [\n        \"_mgr\",\n        \"_cacher\",\n        \"_item_cache\",\n        \"_cache\",\n        \"_is_copy\",\n        \"_subtyp\",\n        \"_name\",\n        \"_default_kind\",\n        \"_default_fill_value\",\n        \"_metadata\",\n        \"__array_struct__\",\n        \"__array_interface__\",\n        \"_flags\",\n    ]\n    _internal_names_set: set[str] = set(_internal_names)\n    _accessors: set[str] = set()\n    _hidden_attrs: frozenset[str] = frozenset(\n        [\"_AXIS_NAMES\", \"_AXIS_NUMBERS\", \"get_values\", \"tshift\"]\n    )\n    _metadata: list[str] = []\n    _is_copy: weakref.ReferenceType[NDFrame] | None = None\n    _mgr: Manager\n    _attrs: dict[Hashable, Any]\n    _typ: str\n\n    # ----------------------------------------------------------------------\n    # Constructors\n\n    def __init__(\n        self,\n        data: Manager,\n        copy: bool_t = False,\n        attrs: Mapping[Hashable, Any] | None = None,\n    ):\n        # copy kwarg is retained for mypy compat, is not used\n\n        object.__setattr__(self, \"_is_copy\", None)\n        object.__setattr__(self, \"_mgr\", data)\n        object.__setattr__(self, \"_item_cache\", {})\n        if attrs is None:\n            attrs = {}\n        else:\n            attrs = dict(attrs)\n        object.__setattr__(self, \"_attrs\", attrs)\n        object.__setattr__(self, \"_flags\", Flags(self, allows_duplicate_labels=True))\n\n    @classmethod\n    def _init_mgr(\n        cls,\n        mgr: Manager,\n        axes,\n        dtype: Dtype | None = None,\n        copy: bool_t = False,\n    ) -> Manager:\n        \"\"\"passed a manager and a axes dict\"\"\"\n        for a, axe in axes.items():\n            if axe is not None:\n                axe = ensure_index(axe)\n                bm_axis = cls._get_block_manager_axis(a)\n                mgr = mgr.reindex_axis(axe, axis=bm_axis)\n\n        # make a copy if explicitly requested\n        if copy:\n            mgr = mgr.copy()\n        if dtype is not None:\n            # avoid further copies if we can\n            if (\n                isinstance(mgr, BlockManager)\n                and len(mgr.blocks) == 1\n                and is_dtype_equal(mgr.blocks[0].values.dtype, dtype)\n            ):\n                pass\n            else:\n                mgr = mgr.astype(dtype=dtype)\n        return mgr\n\n    @classmethod\n    def _from_mgr(cls, mgr: Manager):\n        \"\"\"\n        Fastpath to create a new DataFrame/Series from just a BlockManager/ArrayManager.\n\n        Notes\n        -----\n        Skips setting `_flags` attribute; caller is responsible for doing so.\n        \"\"\"\n        obj = cls.__new__(cls)\n        object.__setattr__(obj, \"_is_copy\", None)\n        object.__setattr__(obj, \"_mgr\", mgr)\n        object.__setattr__(obj, \"_item_cache\", {})\n        object.__setattr__(obj, \"_attrs\", {})\n        return obj\n\n    def _as_manager(self: NDFrameT, typ: str, copy: bool_t = True) -> NDFrameT:\n        \"\"\"\n        Private helper function to create a DataFrame with specific manager.\n\n        Parameters\n        ----------\n        typ : {\"block\", \"array\"}\n        copy : bool, default True\n            Only controls whether the conversion from Block->ArrayManager\n            copies the 1D arrays (to ensure proper/contiguous memory layout).\n\n        Returns\n        -------\n        DataFrame\n            New DataFrame using specified manager type. Is not guaranteed\n            to be a copy or not.\n        \"\"\"\n        new_mgr: Manager\n        new_mgr = mgr_to_mgr(self._mgr, typ=typ, copy=copy)\n        # fastpath of passing a manager doesn't check the option/manager class\n        return self._constructor(new_mgr).__finalize__(self)\n\n    # ----------------------------------------------------------------------\n    # attrs and flags\n\n    @property\n    def attrs(self) -> dict[Hashable, Any]:\n        \"\"\"\n        Dictionary of global attributes of this dataset.\n\n        .. warning::\n\n           attrs is experimental and may change without warning.\n\n        See Also\n        --------\n        DataFrame.flags : Global flags applying to this object.\n        \"\"\"\n        if self._attrs is None:\n            self._attrs = {}\n        return self._attrs\n\n    @attrs.setter\n    def attrs(self, value: Mapping[Hashable, Any]) -> None:\n        self._attrs = dict(value)\n\n    @final\n    @property\n    def flags(self) -> Flags:\n        \"\"\"\n        Get the properties associated with this pandas object.\n\n        The available flags are\n\n        * :attr:`Flags.allows_duplicate_labels`\n\n        See Also\n        --------\n        Flags : Flags that apply to pandas objects.\n        DataFrame.attrs : Global metadata applying to this dataset.\n\n        Notes\n        -----\n        \"Flags\" differ from \"metadata\". Flags reflect properties of the\n        pandas object (the Series or DataFrame). Metadata refer to properties\n        of the dataset, and should be stored in :attr:`DataFrame.attrs`.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({\"A\": [1, 2]})\n        >>> df.flags\n        <Flags(allows_duplicate_labels=True)>\n\n        Flags can be get or set using ``.``\n\n        >>> df.flags.allows_duplicate_labels\n        True\n        >>> df.flags.allows_duplicate_labels = False\n\n        Or by slicing with a key\n\n        >>> df.flags[\"allows_duplicate_labels\"]\n        False\n        >>> df.flags[\"allows_duplicate_labels\"] = True\n        \"\"\"\n        return self._flags\n\n    @final\n    def set_flags(\n        self: NDFrameT,\n        *,\n        copy: bool_t = False,\n        allows_duplicate_labels: bool_t | None = None,\n    ) -> NDFrameT:\n        \"\"\"\n        Return a new object with updated flags.\n\n        Parameters\n        ----------\n        allows_duplicate_labels : bool, optional\n            Whether the returned object allows duplicate labels.\n\n        Returns\n        -------\n        Series or DataFrame\n            The same type as the caller.\n\n        See Also\n        --------\n        DataFrame.attrs : Global metadata applying to this dataset.\n        DataFrame.flags : Global flags applying to this object.\n\n        Notes\n        -----\n        This method returns a new object that's a view on the same data\n        as the input. Mutating the input or the output values will be reflected\n        in the other.\n\n        This method is intended to be used in method chains.\n\n        \"Flags\" differ from \"metadata\". Flags reflect properties of the\n        pandas object (the Series or DataFrame). Metadata refer to properties\n        of the dataset, and should be stored in :attr:`DataFrame.attrs`.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({\"A\": [1, 2]})\n        >>> df.flags.allows_duplicate_labels\n        True\n        >>> df2 = df.set_flags(allows_duplicate_labels=False)\n        >>> df2.flags.allows_duplicate_labels\n        False\n        \"\"\"\n        df = self.copy(deep=copy)\n        if allows_duplicate_labels is not None:\n            df.flags[\"allows_duplicate_labels\"] = allows_duplicate_labels\n        return df\n\n    @final\n    @classmethod\n    def _validate_dtype(cls, dtype) -> DtypeObj | None:\n        \"\"\"validate the passed dtype\"\"\"\n        if dtype is not None:\n            dtype = pandas_dtype(dtype)\n\n            # a compound dtype\n            if dtype.kind == \"V\":\n                raise NotImplementedError(\n                    \"compound dtypes are not implemented \"\n                    f\"in the {cls.__name__} constructor\"\n                )\n\n        return dtype\n\n    # ----------------------------------------------------------------------\n    # Construction\n\n    @property\n    def _constructor(self: NDFrameT) -> Callable[..., NDFrameT]:\n        \"\"\"\n        Used when a manipulation result has the same dimensions as the\n        original.\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    # ----------------------------------------------------------------------\n    # Internals\n\n    @final\n    @property\n    def _data(self):\n        # GH#33054 retained because some downstream packages uses this,\n        #  e.g. fastparquet\n        return self._mgr\n\n    # ----------------------------------------------------------------------\n    # Axis\n    _stat_axis_number = 0\n    _stat_axis_name = \"index\"\n    _AXIS_ORDERS: list[str]\n    _AXIS_TO_AXIS_NUMBER: dict[Axis, int] = {0: 0, \"index\": 0, \"rows\": 0}\n    _info_axis_number: int\n    _info_axis_name: str\n    _AXIS_LEN: int\n\n    @property\n    def _AXIS_NUMBERS(self) -> dict[str, int]:\n        \"\"\".. deprecated:: 1.1.0\"\"\"\n        warnings.warn(\n            \"_AXIS_NUMBERS has been deprecated.\",\n            FutureWarning,\n            stacklevel=find_stack_level(),\n        )\n        return {\"index\": 0}\n\n    @property\n    def _AXIS_NAMES(self) -> dict[int, str]:\n        \"\"\".. deprecated:: 1.1.0\"\"\"\n        level = self.ndim + 1\n        warnings.warn(\n            \"_AXIS_NAMES has been deprecated.\", FutureWarning, stacklevel=level\n        )\n        return {0: \"index\"}\n\n    @final\n    def _construct_axes_dict(self, axes=None, **kwargs):\n        \"\"\"Return an axes dictionary for myself.\"\"\"\n        d = {a: self._get_axis(a) for a in (axes or self._AXIS_ORDERS)}\n        d.update(kwargs)\n        return d\n\n    @final\n    @classmethod\n    def _construct_axes_from_arguments(\n        cls, args, kwargs, require_all: bool_t = False, sentinel=None\n    ):\n        \"\"\"\n        Construct and returns axes if supplied in args/kwargs.\n\n        If require_all, raise if all axis arguments are not supplied\n        return a tuple of (axes, kwargs).\n\n        sentinel specifies the default parameter when an axis is not\n        supplied; useful to distinguish when a user explicitly passes None\n        in scenarios where None has special meaning.\n        \"\"\"\n        # construct the args\n        args = list(args)\n        for a in cls._AXIS_ORDERS:\n\n            # look for a argument by position\n            if a not in kwargs:\n                try:\n                    kwargs[a] = args.pop(0)\n                except IndexError as err:\n                    if require_all:\n                        raise TypeError(\n                            \"not enough/duplicate arguments specified!\"\n                        ) from err\n\n        axes = {a: kwargs.pop(a, sentinel) for a in cls._AXIS_ORDERS}\n        return axes, kwargs\n\n    @final\n    @classmethod\n    def _get_axis_number(cls, axis: Axis) -> int:\n        try:\n            return cls._AXIS_TO_AXIS_NUMBER[axis]\n        except KeyError:\n            raise ValueError(f\"No axis named {axis} for object type {cls.__name__}\")\n\n    @final\n    @classmethod\n    def _get_axis_name(cls, axis: Axis) -> str:\n        axis_number = cls._get_axis_number(axis)\n        return cls._AXIS_ORDERS[axis_number]\n\n    @final\n    def _get_axis(self, axis: Axis) -> Index:\n        axis_number = self._get_axis_number(axis)\n        assert axis_number in {0, 1}\n        return self.index if axis_number == 0 else self.columns\n\n    @final\n    @classmethod\n    def _get_block_manager_axis(cls, axis: Axis) -> int:\n        \"\"\"Map the axis to the block_manager axis.\"\"\"\n        axis = cls._get_axis_number(axis)\n        ndim = cls._AXIS_LEN\n        if ndim == 2:\n            # i.e. DataFrame\n            return 1 - axis\n        return axis\n\n    @final\n    def _get_axis_resolvers(self, axis: str) -> dict[str, Series | MultiIndex]:\n        # index or columns\n        axis_index = getattr(self, axis)\n        d = {}\n        prefix = axis[0]\n\n        for i, name in enumerate(axis_index.names):\n            if name is not None:\n                key = level = name\n            else:\n                # prefix with 'i' or 'c' depending on the input axis\n                # e.g., you must do ilevel_0 for the 0th level of an unnamed\n                # multiiindex\n                key = f\"{prefix}level_{i}\"\n                level = i\n\n            level_values = axis_index.get_level_values(level)\n            s = level_values.to_series()\n            s.index = axis_index\n            d[key] = s\n\n        # put the index/columns itself in the dict\n        if isinstance(axis_index, MultiIndex):\n            dindex = axis_index\n        else:\n            dindex = axis_index.to_series()\n\n        d[axis] = dindex\n        return d\n\n    @final\n    def _get_index_resolvers(self) -> dict[Hashable, Series | MultiIndex]:\n        from pandas.core.computation.parsing import clean_column_name\n\n        d: dict[str, Series | MultiIndex] = {}\n        for axis_name in self._AXIS_ORDERS:\n            d.update(self._get_axis_resolvers(axis_name))\n\n        return {clean_column_name(k): v for k, v in d.items() if not isinstance(k, int)}\n\n    @final\n    def _get_cleaned_column_resolvers(self) -> dict[Hashable, Series]:\n        \"\"\"\n        Return the special character free column resolvers of a dataframe.\n\n        Column names with special characters are 'cleaned up' so that they can\n        be referred to by backtick quoting.\n        Used in :meth:`DataFrame.eval`.\n        \"\"\"\n        from pandas.core.computation.parsing import clean_column_name\n\n        if isinstance(self, ABCSeries):\n            return {clean_column_name(self.name): self}\n\n        return {\n            clean_column_name(k): v for k, v in self.items() if not isinstance(k, int)\n        }\n\n    @property\n    def _info_axis(self) -> Index:\n        return getattr(self, self._info_axis_name)\n\n    @property\n    def _stat_axis(self) -> Index:\n        return getattr(self, self._stat_axis_name)\n\n    @property\n    def shape(self) -> tuple[int, ...]:\n        \"\"\"\n        Return a tuple of axis dimensions\n        \"\"\"\n        return tuple(len(self._get_axis(a)) for a in self._AXIS_ORDERS)\n\n    @property\n    def axes(self) -> list[Index]:\n        \"\"\"\n        Return index label(s) of the internal NDFrame\n        \"\"\"\n        # we do it this way because if we have reversed axes, then\n        # the block manager shows then reversed\n        return [self._get_axis(a) for a in self._AXIS_ORDERS]\n\n    @property\n    def ndim(self) -> int:\n        \"\"\"\n        Return an int representing the number of axes / array dimensions.\n\n        Return 1 if Series. Otherwise return 2 if DataFrame.\n\n        See Also\n        --------\n        ndarray.ndim : Number of array dimensions.\n\n        Examples\n        --------\n        >>> s = pd.Series({'a': 1, 'b': 2, 'c': 3})\n        >>> s.ndim\n        1\n\n        >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n        >>> df.ndim\n        2\n        \"\"\"\n        return self._mgr.ndim\n\n    @property\n    def size(self) -> int:\n        \"\"\"\n        Return an int representing the number of elements in this object.\n\n        Return the number of rows if Series. Otherwise return the number of\n        rows times number of columns if DataFrame.\n\n        See Also\n        --------\n        ndarray.size : Number of elements in the array.\n\n        Examples\n        --------\n        >>> s = pd.Series({'a': 1, 'b': 2, 'c': 3})\n        >>> s.size\n        3\n\n        >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n        >>> df.size\n        4\n        \"\"\"\n        return np.prod(self.shape)\n\n    @overload\n    def set_axis(\n        self: NDFrameT, labels, axis: Axis = ..., inplace: Literal[False] = ...\n    ) -> NDFrameT:\n        ...\n\n    @overload\n    def set_axis(self, labels, axis: Axis, inplace: Literal[True]) -> None:\n        ...\n\n    @overload\n    def set_axis(self, labels, *, inplace: Literal[True]) -> None:\n        ...\n\n    @overload\n    def set_axis(\n        self: NDFrameT, labels, axis: Axis = ..., inplace: bool_t = ...\n    ) -> NDFrameT | None:\n        ...\n\n    def set_axis(self, labels, axis: Axis = 0, inplace: bool_t = False):\n        \"\"\"\n        Assign desired index to given axis.\n\n        Indexes for%(extended_summary_sub)s row labels can be changed by assigning\n        a list-like or Index.\n\n        Parameters\n        ----------\n        labels : list-like, Index\n            The values for the new index.\n\n        axis : %(axes_single_arg)s, default 0\n            The axis to update. The value 0 identifies the rows%(axis_description_sub)s.\n\n        inplace : bool, default False\n            Whether to return a new %(klass)s instance.\n\n        Returns\n        -------\n        renamed : %(klass)s or None\n            An object of type %(klass)s or None if ``inplace=True``.\n\n        See Also\n        --------\n        %(klass)s.rename_axis : Alter the name of the index%(see_also_sub)s.\n        \"\"\"\n        self._check_inplace_and_allows_duplicate_labels(inplace)\n        return self._set_axis_nocheck(labels, axis, inplace)\n\n    @final\n    def _set_axis_nocheck(self, labels, axis: Axis, inplace: bool_t):\n        # NDFrame.rename with inplace=False calls set_axis(inplace=True) on a copy.\n        if inplace:\n            setattr(self, self._get_axis_name(axis), labels)\n        else:\n            obj = self.copy()\n            obj.set_axis(labels, axis=axis, inplace=True)\n            return obj\n\n    def _set_axis(self, axis: int, labels: Index) -> None:\n        labels = ensure_index(labels)\n        self._mgr.set_axis(axis, labels)\n        self._clear_item_cache()\n\n    @final\n    def swapaxes(self: NDFrameT, axis1, axis2, copy=True) -> NDFrameT:\n        \"\"\"\n        Interchange axes and swap values axes appropriately.\n\n        Returns\n        -------\n        y : same as input\n        \"\"\"\n        i = self._get_axis_number(axis1)\n        j = self._get_axis_number(axis2)\n\n        if i == j:\n            if copy:\n                return self.copy()\n            return self\n\n        mapping = {i: j, j: i}\n\n        new_axes = (self._get_axis(mapping.get(k, k)) for k in range(self._AXIS_LEN))\n        new_values = self.values.swapaxes(i, j)\n        if copy:\n            new_values = new_values.copy()\n\n        return self._constructor(\n            new_values,\n            *new_axes,\n        ).__finalize__(self, method=\"swapaxes\")\n\n    @final\n    @doc(klass=_shared_doc_kwargs[\"klass\"])\n    def droplevel(self: NDFrameT, level, axis=0) -> NDFrameT:\n        \"\"\"\n        Return {klass} with requested index / column level(s) removed.\n\n        Parameters\n        ----------\n        level : int, str, or list-like\n            If a string is given, must be the name of a level\n            If list-like, elements must be names or positional indexes\n            of levels.\n\n        axis : {{0 or 'index', 1 or 'columns'}}, default 0\n            Axis along which the level(s) is removed:\n\n            * 0 or 'index': remove level(s) in column.\n            * 1 or 'columns': remove level(s) in row.\n\n        Returns\n        -------\n        {klass}\n            {klass} with requested index / column level(s) removed.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([\n        ...     [1, 2, 3, 4],\n        ...     [5, 6, 7, 8],\n        ...     [9, 10, 11, 12]\n        ... ]).set_index([0, 1]).rename_axis(['a', 'b'])\n\n        >>> df.columns = pd.MultiIndex.from_tuples([\n        ...     ('c', 'e'), ('d', 'f')\n        ... ], names=['level_1', 'level_2'])\n\n        >>> df\n        level_1   c   d\n        level_2   e   f\n        a b\n        1 2      3   4\n        5 6      7   8\n        9 10    11  12\n\n        >>> df.droplevel('a')\n        level_1   c   d\n        level_2   e   f\n        b\n        2        3   4\n        6        7   8\n        10      11  12\n\n        >>> df.droplevel('level_2', axis=1)\n        level_1   c   d\n        a b\n        1 2      3   4\n        5 6      7   8\n        9 10    11  12\n        \"\"\"\n        labels = self._get_axis(axis)\n        new_labels = labels.droplevel(level)\n        return self.set_axis(new_labels, axis=axis, inplace=False)\n\n    def pop(self, item: Hashable) -> Series | Any:\n        result = self[item]\n        del self[item]\n\n        return result\n\n    @final\n    def squeeze(self, axis=None):\n        \"\"\"\n        Squeeze 1 dimensional axis objects into scalars.\n\n        Series or DataFrames with a single element are squeezed to a scalar.\n        DataFrames with a single column or a single row are squeezed to a\n        Series. Otherwise the object is unchanged.\n\n        This method is most useful when you don't know if your\n        object is a Series or DataFrame, but you do know it has just a single\n        column. In that case you can safely call `squeeze` to ensure you have a\n        Series.\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns', None}, default None\n            A specific axis to squeeze. By default, all length-1 axes are\n            squeezed.\n\n        Returns\n        -------\n        DataFrame, Series, or scalar\n            The projection after squeezing `axis` or all the axes.\n\n        See Also\n        --------\n        Series.iloc : Integer-location based indexing for selecting scalars.\n        DataFrame.iloc : Integer-location based indexing for selecting Series.\n        Series.to_frame : Inverse of DataFrame.squeeze for a\n            single-column DataFrame.\n\n        Examples\n        --------\n        >>> primes = pd.Series([2, 3, 5, 7])\n\n        Slicing might produce a Series with a single value:\n\n        >>> even_primes = primes[primes % 2 == 0]\n        >>> even_primes\n        0    2\n        dtype: int64\n\n        >>> even_primes.squeeze()\n        2\n\n        Squeezing objects with more than one value in every axis does nothing:\n\n        >>> odd_primes = primes[primes % 2 == 1]\n        >>> odd_primes\n        1    3\n        2    5\n        3    7\n        dtype: int64\n\n        >>> odd_primes.squeeze()\n        1    3\n        2    5\n        3    7\n        dtype: int64\n\n        Squeezing is even more effective when used with DataFrames.\n\n        >>> df = pd.DataFrame([[1, 2], [3, 4]], columns=['a', 'b'])\n        >>> df\n           a  b\n        0  1  2\n        1  3  4\n\n        Slicing a single column will produce a DataFrame with the columns\n        having only one value:\n\n        >>> df_a = df[['a']]\n        >>> df_a\n           a\n        0  1\n        1  3\n\n        So the columns can be squeezed down, resulting in a Series:\n\n        >>> df_a.squeeze('columns')\n        0    1\n        1    3\n        Name: a, dtype: int64\n\n        Slicing a single row from a single column will produce a single\n        scalar DataFrame:\n\n        >>> df_0a = df.loc[df.index < 1, ['a']]\n        >>> df_0a\n           a\n        0  1\n\n        Squeezing the rows produces a single scalar Series:\n\n        >>> df_0a.squeeze('rows')\n        a    1\n        Name: 0, dtype: int64\n\n        Squeezing all axes will project directly into a scalar:\n\n        >>> df_0a.squeeze()\n        1\n        \"\"\"\n        axis = range(self._AXIS_LEN) if axis is None else (self._get_axis_number(axis),)\n        return self.iloc[\n            tuple(\n                0 if i in axis and len(a) == 1 else slice(None)\n                for i, a in enumerate(self.axes)\n            )\n        ]\n\n    # ----------------------------------------------------------------------\n    # Rename\n\n    def _rename(\n        self: NDFrameT,\n        mapper: Renamer | None = None,\n        *,\n        index: Renamer | None = None,\n        columns: Renamer | None = None,\n        axis: Axis | None = None,\n        copy: bool_t = True,\n        inplace: bool_t = False,\n        level: Level | None = None,\n        errors: str = \"ignore\",\n    ) -> NDFrameT | None:\n        \"\"\"\n        Alter axes input function or functions. Function / dict values must be\n        unique (1-to-1). Labels not contained in a dict / Series will be left\n        as-is. Extra labels listed don't throw an error. Alternatively, change\n        ``Series.name`` with a scalar value (Series only).\n\n        Parameters\n        ----------\n        %(axes)s : scalar, list-like, dict-like or function, optional\n            Scalar or list-like will alter the ``Series.name`` attribute,\n            and raise on DataFrame.\n            dict-like or functions are transformations to apply to\n            that axis' values\n        copy : bool, default True\n            Also copy underlying data.\n        inplace : bool, default False\n            Whether to return a new {klass}. If True then value of copy is\n            ignored.\n        level : int or level name, default None\n            In case of a MultiIndex, only rename labels in the specified\n            level.\n        errors : {'ignore', 'raise'}, default 'ignore'\n            If 'raise', raise a `KeyError` when a dict-like `mapper`, `index`,\n            or `columns` contains labels that are not present in the Index\n            being transformed.\n            If 'ignore', existing keys will be renamed and extra keys will be\n            ignored.\n\n        Returns\n        -------\n        renamed : {klass} (new object)\n\n        Raises\n        ------\n        KeyError\n            If any of the labels is not found in the selected axis and\n            \"errors='raise'\".\n\n        See Also\n        --------\n        NDFrame.rename_axis\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3])\n        >>> s\n        0    1\n        1    2\n        2    3\n        dtype: int64\n        >>> s.rename(\"my_name\") # scalar, changes Series.name\n        0    1\n        1    2\n        2    3\n        Name: my_name, dtype: int64\n        >>> s.rename(lambda x: x ** 2)  # function, changes labels\n        0    1\n        1    2\n        4    3\n        dtype: int64\n        >>> s.rename({1: 3, 2: 5})  # mapping, changes labels\n        0    1\n        3    2\n        5    3\n        dtype: int64\n\n        Since ``DataFrame`` doesn't have a ``.name`` attribute,\n        only mapping-type arguments are allowed.\n\n        >>> df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n        >>> df.rename(2)\n        Traceback (most recent call last):\n        ...\n        TypeError: 'int' object is not callable\n\n        ``DataFrame.rename`` supports two calling conventions\n\n        * ``(index=index_mapper, columns=columns_mapper, ...)``\n        * ``(mapper, axis={'index', 'columns'}, ...)``\n\n        We *highly* recommend using keyword arguments to clarify your\n        intent.\n\n        >>> df.rename(index=str, columns={\"A\": \"a\", \"B\": \"c\"})\n           a  c\n        0  1  4\n        1  2  5\n        2  3  6\n\n        >>> df.rename(index=str, columns={\"A\": \"a\", \"C\": \"c\"})\n           a  B\n        0  1  4\n        1  2  5\n        2  3  6\n\n        Using axis-style parameters\n\n        >>> df.rename(str.lower, axis='columns')\n           a  b\n        0  1  4\n        1  2  5\n        2  3  6\n\n        >>> df.rename({1: 2, 2: 4}, axis='index')\n           A  B\n        0  1  4\n        2  2  5\n        4  3  6\n\n        See the :ref:`user guide <basics.rename>` for more.\n        \"\"\"\n        if mapper is None and index is None and columns is None:\n            raise TypeError(\"must pass an index to rename\")\n\n        if index is not None or columns is not None:\n            if axis is not None:\n                raise TypeError(\n                    \"Cannot specify both 'axis' and any of 'index' or 'columns'\"\n                )\n            elif mapper is not None:\n                raise TypeError(\n                    \"Cannot specify both 'mapper' and any of 'index' or 'columns'\"\n                )\n        else:\n            # use the mapper argument\n            if axis and self._get_axis_number(axis) == 1:\n                columns = mapper\n            else:\n                index = mapper\n\n        self._check_inplace_and_allows_duplicate_labels(inplace)\n        result = self if inplace else self.copy(deep=copy)\n\n        for axis_no, replacements in enumerate((index, columns)):\n            if replacements is None:\n                continue\n\n            ax = self._get_axis(axis_no)\n            f = com.get_rename_function(replacements)\n\n            if level is not None:\n                level = ax._get_level_number(level)\n\n            # GH 13473\n            if not callable(replacements):\n                if ax._is_multi and level is not None:\n                    indexer = ax.get_level_values(level).get_indexer_for(replacements)\n                else:\n                    indexer = ax.get_indexer_for(replacements)\n\n                if errors == \"raise\" and len(indexer[indexer == -1]):\n                    missing_labels = [\n                        label\n                        for index, label in enumerate(replacements)\n                        if indexer[index] == -1\n                    ]\n                    raise KeyError(f\"{missing_labels} not found in axis\")\n\n            new_index = ax._transform_index(f, level=level)\n            result._set_axis_nocheck(new_index, axis=axis_no, inplace=True)\n            result._clear_item_cache()\n\n        if inplace:\n            self._update_inplace(result)\n            return None\n        else:\n            return result.__finalize__(self, method=\"rename\")\n\n    @rewrite_axis_style_signature(\"mapper\", [(\"copy\", True), (\"inplace\", False)])\n    def rename_axis(self, mapper=lib.no_default, **kwargs):\n        \"\"\"\n        Set the name of the axis for the index or columns.\n\n        Parameters\n        ----------\n        mapper : scalar, list-like, optional\n            Value to set the axis name attribute.\n        index, columns : scalar, list-like, dict-like or function, optional\n            A scalar, list-like, dict-like or functions transformations to\n            apply to that axis' values.\n            Note that the ``columns`` parameter is not allowed if the\n            object is a Series. This parameter only apply for DataFrame\n            type objects.\n\n            Use either ``mapper`` and ``axis`` to\n            specify the axis to target with ``mapper``, or ``index``\n            and/or ``columns``.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis to rename.\n        copy : bool, default True\n            Also copy underlying data.\n        inplace : bool, default False\n            Modifies the object directly, instead of creating a new Series\n            or DataFrame.\n\n        Returns\n        -------\n        Series, DataFrame, or None\n            The same type as the caller or None if ``inplace=True``.\n\n        See Also\n        --------\n        Series.rename : Alter Series index labels or name.\n        DataFrame.rename : Alter DataFrame index labels or name.\n        Index.rename : Set new names on index.\n\n        Notes\n        -----\n        ``DataFrame.rename_axis`` supports two calling conventions\n\n        * ``(index=index_mapper, columns=columns_mapper, ...)``\n        * ``(mapper, axis={'index', 'columns'}, ...)``\n\n        The first calling convention will only modify the names of\n        the index and/or the names of the Index object that is the columns.\n        In this case, the parameter ``copy`` is ignored.\n\n        The second calling convention will modify the names of the\n        corresponding index if mapper is a list or a scalar.\n        However, if mapper is dict-like or a function, it will use the\n        deprecated behavior of modifying the axis *labels*.\n\n        We *highly* recommend using keyword arguments to clarify your\n        intent.\n\n        Examples\n        --------\n        **Series**\n\n        >>> s = pd.Series([\"dog\", \"cat\", \"monkey\"])\n        >>> s\n        0       dog\n        1       cat\n        2    monkey\n        dtype: object\n        >>> s.rename_axis(\"animal\")\n        animal\n        0    dog\n        1    cat\n        2    monkey\n        dtype: object\n\n        **DataFrame**\n\n        >>> df = pd.DataFrame({\"num_legs\": [4, 4, 2],\n        ...                    \"num_arms\": [0, 0, 2]},\n        ...                   [\"dog\", \"cat\", \"monkey\"])\n        >>> df\n                num_legs  num_arms\n        dog            4         0\n        cat            4         0\n        monkey         2         2\n        >>> df = df.rename_axis(\"animal\")\n        >>> df\n                num_legs  num_arms\n        animal\n        dog            4         0\n        cat            4         0\n        monkey         2         2\n        >>> df = df.rename_axis(\"limbs\", axis=\"columns\")\n        >>> df\n        limbs   num_legs  num_arms\n        animal\n        dog            4         0\n        cat            4         0\n        monkey         2         2\n\n        **MultiIndex**\n\n        >>> df.index = pd.MultiIndex.from_product([['mammal'],\n        ...                                        ['dog', 'cat', 'monkey']],\n        ...                                       names=['type', 'name'])\n        >>> df\n        limbs          num_legs  num_arms\n        type   name\n        mammal dog            4         0\n               cat            4         0\n               monkey         2         2\n\n        >>> df.rename_axis(index={'type': 'class'})\n        limbs          num_legs  num_arms\n        class  name\n        mammal dog            4         0\n               cat            4         0\n               monkey         2         2\n\n        >>> df.rename_axis(columns=str.upper)\n        LIMBS          num_legs  num_arms\n        type   name\n        mammal dog            4         0\n               cat            4         0\n               monkey         2         2\n        \"\"\"\n        axes, kwargs = self._construct_axes_from_arguments(\n            (), kwargs, sentinel=lib.no_default\n        )\n        copy = kwargs.pop(\"copy\", True)\n        inplace = kwargs.pop(\"inplace\", False)\n        axis = kwargs.pop(\"axis\", 0)\n        if axis is not None:\n            axis = self._get_axis_number(axis)\n\n        if kwargs:\n            raise TypeError(\n                \"rename_axis() got an unexpected keyword \"\n                f'argument \"{list(kwargs.keys())[0]}\"'\n            )\n\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n\n        if mapper is not lib.no_default:\n            # Use v0.23 behavior if a scalar or list\n            non_mapper = is_scalar(mapper) or (\n                is_list_like(mapper) and not is_dict_like(mapper)\n            )\n            if non_mapper:\n                return self._set_axis_name(mapper, axis=axis, inplace=inplace)\n            else:\n                raise ValueError(\"Use `.rename` to alter labels with a mapper.\")\n        else:\n            # Use new behavior.  Means that index and/or columns\n            # is specified\n            result = self if inplace else self.copy(deep=copy)\n\n            for axis in range(self._AXIS_LEN):\n                v = axes.get(self._get_axis_name(axis))\n                if v is lib.no_default:\n                    continue\n                non_mapper = is_scalar(v) or (is_list_like(v) and not is_dict_like(v))\n                if non_mapper:\n                    newnames = v\n                else:\n                    f = com.get_rename_function(v)\n                    curnames = self._get_axis(axis).names\n                    newnames = [f(name) for name in curnames]\n                result._set_axis_name(newnames, axis=axis, inplace=True)\n            if not inplace:\n                return result\n\n    @final\n    def _set_axis_name(self, name, axis=0, inplace=False):\n        \"\"\"\n        Set the name(s) of the axis.\n\n        Parameters\n        ----------\n        name : str or list of str\n            Name(s) to set.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis to set the label. The value 0 or 'index' specifies index,\n            and the value 1 or 'columns' specifies columns.\n        inplace : bool, default False\n            If `True`, do operation inplace and return None.\n\n        Returns\n        -------\n        Series, DataFrame, or None\n            The same type as the caller or `None` if `inplace` is `True`.\n\n        See Also\n        --------\n        DataFrame.rename : Alter the axis labels of :class:`DataFrame`.\n        Series.rename : Alter the index labels or set the index name\n            of :class:`Series`.\n        Index.rename : Set the name of :class:`Index` or :class:`MultiIndex`.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({\"num_legs\": [4, 4, 2]},\n        ...                   [\"dog\", \"cat\", \"monkey\"])\n        >>> df\n                num_legs\n        dog            4\n        cat            4\n        monkey         2\n        >>> df._set_axis_name(\"animal\")\n                num_legs\n        animal\n        dog            4\n        cat            4\n        monkey         2\n        >>> df.index = pd.MultiIndex.from_product(\n        ...                [[\"mammal\"], ['dog', 'cat', 'monkey']])\n        >>> df._set_axis_name([\"type\", \"name\"])\n                       num_legs\n        type   name\n        mammal dog        4\n               cat        4\n               monkey     2\n        \"\"\"\n        axis = self._get_axis_number(axis)\n        idx = self._get_axis(axis).set_names(name)\n\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        renamed = self if inplace else self.copy()\n        renamed.set_axis(idx, axis=axis, inplace=True)\n        if not inplace:\n            return renamed\n\n    # ----------------------------------------------------------------------\n    # Comparison Methods\n\n    @final\n    def _indexed_same(self, other) -> bool_t:\n        return all(\n            self._get_axis(a).equals(other._get_axis(a)) for a in self._AXIS_ORDERS\n        )\n\n    @final\n    def equals(self, other: object) -> bool_t:\n        \"\"\"\n        Test whether two objects contain the same elements.\n\n        This function allows two Series or DataFrames to be compared against\n        each other to see if they have the same shape and elements. NaNs in\n        the same location are considered equal.\n\n        The row/column index do not need to have the same type, as long\n        as the values are considered equal. Corresponding columns must be of\n        the same dtype.\n\n        Parameters\n        ----------\n        other : Series or DataFrame\n            The other Series or DataFrame to be compared with the first.\n\n        Returns\n        -------\n        bool\n            True if all elements are the same in both objects, False\n            otherwise.\n\n        See Also\n        --------\n        Series.eq : Compare two Series objects of the same length\n            and return a Series where each element is True if the element\n            in each Series is equal, False otherwise.\n        DataFrame.eq : Compare two DataFrame objects of the same shape and\n            return a DataFrame where each element is True if the respective\n            element in each DataFrame is equal, False otherwise.\n        testing.assert_series_equal : Raises an AssertionError if left and\n            right are not equal. Provides an easy interface to ignore\n            inequality in dtypes, indexes and precision among others.\n        testing.assert_frame_equal : Like assert_series_equal, but targets\n            DataFrames.\n        numpy.array_equal : Return True if two arrays have the same shape\n            and elements, False otherwise.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({1: [10], 2: [20]})\n        >>> df\n            1   2\n        0  10  20\n\n        DataFrames df and exactly_equal have the same types and values for\n        their elements and column labels, which will return True.\n\n        >>> exactly_equal = pd.DataFrame({1: [10], 2: [20]})\n        >>> exactly_equal\n            1   2\n        0  10  20\n        >>> df.equals(exactly_equal)\n        True\n\n        DataFrames df and different_column_type have the same element\n        types and values, but have different types for the column labels,\n        which will still return True.\n\n        >>> different_column_type = pd.DataFrame({1.0: [10], 2.0: [20]})\n        >>> different_column_type\n           1.0  2.0\n        0   10   20\n        >>> df.equals(different_column_type)\n        True\n\n        DataFrames df and different_data_type have different types for the\n        same values for their elements, and will return False even though\n        their column labels are the same values and types.\n\n        >>> different_data_type = pd.DataFrame({1: [10.0], 2: [20.0]})\n        >>> different_data_type\n              1     2\n        0  10.0  20.0\n        >>> df.equals(different_data_type)\n        False\n        \"\"\"\n        if not (isinstance(other, type(self)) or isinstance(self, type(other))):\n            return False\n        other = cast(NDFrame, other)\n        return self._mgr.equals(other._mgr)\n\n    # -------------------------------------------------------------------------\n    # Unary Methods\n\n    @final\n    def __neg__(self):\n        def blk_func(values: ArrayLike):\n            if is_bool_dtype(values.dtype):\n                return operator.inv(values)\n            else:\n                return operator.neg(values)\n\n        new_data = self._mgr.apply(blk_func)\n        res = self._constructor(new_data)\n        return res.__finalize__(self, method=\"__neg__\")\n\n    @final\n    def __pos__(self):\n        def blk_func(values: ArrayLike):\n            if is_bool_dtype(values.dtype):\n                return values.copy()\n            else:\n                return operator.pos(values)\n\n        new_data = self._mgr.apply(blk_func)\n        res = self._constructor(new_data)\n        return res.__finalize__(self, method=\"__pos__\")\n\n    @final\n    def __invert__(self):\n        if not self.size:\n            # inv fails with 0 len\n            return self\n\n        new_data = self._mgr.apply(operator.invert)\n        return self._constructor(new_data).__finalize__(self, method=\"__invert__\")\n\n    @final\n    def __nonzero__(self):\n        raise ValueError(\n            f\"The truth value of a {type(self).__name__} is ambiguous. \"\n            \"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\n        )\n\n    __bool__ = __nonzero__\n\n    @final\n    def bool(self):\n        \"\"\"\n        Return the bool of a single element Series or DataFrame.\n\n        This must be a boolean scalar value, either True or False. It will raise a\n        ValueError if the Series or DataFrame does not have exactly 1 element, or that\n        element is not boolean (integer values 0 and 1 will also raise an exception).\n\n        Returns\n        -------\n        bool\n            The value in the Series or DataFrame.\n\n        See Also\n        --------\n        Series.astype : Change the data type of a Series, including to boolean.\n        DataFrame.astype : Change the data type of a DataFrame, including to boolean.\n        numpy.bool_ : NumPy boolean data type, used by pandas for boolean values.\n\n        Examples\n        --------\n        The method will only work for single element objects with a boolean value:\n\n        >>> pd.Series([True]).bool()\n        True\n        >>> pd.Series([False]).bool()\n        False\n\n        >>> pd.DataFrame({'col': [True]}).bool()\n        True\n        >>> pd.DataFrame({'col': [False]}).bool()\n        False\n        \"\"\"\n        v = self.squeeze()\n        if isinstance(v, (bool, np.bool_)):\n            return bool(v)\n        elif is_scalar(v):\n            raise ValueError(\n                \"bool cannot act on a non-boolean single element \"\n                f\"{type(self).__name__}\"\n            )\n\n        self.__nonzero__()\n\n    @final\n    def abs(self: NDFrameT) -> NDFrameT:\n        \"\"\"\n        Return a Series/DataFrame with absolute numeric value of each element.\n\n        This function only applies to elements that are all numeric.\n\n        Returns\n        -------\n        abs\n            Series/DataFrame containing the absolute value of each element.\n\n        See Also\n        --------\n        numpy.absolute : Calculate the absolute value element-wise.\n\n        Notes\n        -----\n        For ``complex`` inputs, ``1.2 + 1j``, the absolute value is\n        :math:`\\\\sqrt{ a^2 + b^2 }`.\n\n        Examples\n        --------\n        Absolute numeric values in a Series.\n\n        >>> s = pd.Series([-1.10, 2, -3.33, 4])\n        >>> s.abs()\n        0    1.10\n        1    2.00\n        2    3.33\n        3    4.00\n        dtype: float64\n\n        Absolute numeric values in a Series with complex numbers.\n\n        >>> s = pd.Series([1.2 + 1j])\n        >>> s.abs()\n        0    1.56205\n        dtype: float64\n\n        Absolute numeric values in a Series with a Timedelta element.\n\n        >>> s = pd.Series([pd.Timedelta('1 days')])\n        >>> s.abs()\n        0   1 days\n        dtype: timedelta64[ns]\n\n        Select rows with data closest to certain value using argsort (from\n        `StackOverflow <https://stackoverflow.com/a/17758115>`__).\n\n        >>> df = pd.DataFrame({\n        ...     'a': [4, 5, 6, 7],\n        ...     'b': [10, 20, 30, 40],\n        ...     'c': [100, 50, -30, -50]\n        ... })\n        >>> df\n             a    b    c\n        0    4   10  100\n        1    5   20   50\n        2    6   30  -30\n        3    7   40  -50\n        >>> df.loc[(df.c - 43).abs().argsort()]\n             a    b    c\n        1    5   20   50\n        0    4   10  100\n        2    6   30  -30\n        3    7   40  -50\n        \"\"\"\n        res_mgr = self._mgr.apply(np.abs)\n        return self._constructor(res_mgr).__finalize__(self, name=\"abs\")\n\n    @final\n    def __abs__(self: NDFrameT) -> NDFrameT:\n        return self.abs()\n\n    @final\n    def __round__(self: NDFrameT, decimals: int = 0) -> NDFrameT:\n        return self.round(decimals)\n\n    # -------------------------------------------------------------------------\n    # Label or Level Combination Helpers\n    #\n    # A collection of helper methods for DataFrame/Series operations that\n    # accept a combination of column/index labels and levels.  All such\n    # operations should utilize/extend these methods when possible so that we\n    # have consistent precedence and validation logic throughout the library.\n\n    @final\n    def _is_level_reference(self, key, axis=0):\n        \"\"\"\n        Test whether a key is a level reference for a given axis.\n\n        To be considered a level reference, `key` must be a string that:\n          - (axis=0): Matches the name of an index level and does NOT match\n            a column label.\n          - (axis=1): Matches the name of a column level and does NOT match\n            an index label.\n\n        Parameters\n        ----------\n        key : str\n            Potential level name for the given axis\n        axis : int, default 0\n            Axis that levels are associated with (0 for index, 1 for columns)\n\n        Returns\n        -------\n        is_level : bool\n        \"\"\"\n        axis = self._get_axis_number(axis)\n\n        return (\n            key is not None\n            and is_hashable(key)\n            and key in self.axes[axis].names\n            and not self._is_label_reference(key, axis=axis)\n        )\n\n    @final\n    def _is_label_reference(self, key, axis=0) -> bool_t:\n        \"\"\"\n        Test whether a key is a label reference for a given axis.\n\n        To be considered a label reference, `key` must be a string that:\n          - (axis=0): Matches a column label\n          - (axis=1): Matches an index label\n\n        Parameters\n        ----------\n        key : str\n            Potential label name\n        axis : int, default 0\n            Axis perpendicular to the axis that labels are associated with\n            (0 means search for column labels, 1 means search for index labels)\n\n        Returns\n        -------\n        is_label: bool\n        \"\"\"\n        axis = self._get_axis_number(axis)\n        other_axes = (ax for ax in range(self._AXIS_LEN) if ax != axis)\n\n        return (\n            key is not None\n            and is_hashable(key)\n            and any(key in self.axes[ax] for ax in other_axes)\n        )\n\n    @final\n    def _is_label_or_level_reference(self, key: str, axis: int = 0) -> bool_t:\n        \"\"\"\n        Test whether a key is a label or level reference for a given axis.\n\n        To be considered either a label or a level reference, `key` must be a\n        string that:\n          - (axis=0): Matches a column label or an index level\n          - (axis=1): Matches an index label or a column level\n\n        Parameters\n        ----------\n        key : str\n            Potential label or level name\n        axis : int, default 0\n            Axis that levels are associated with (0 for index, 1 for columns)\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        return self._is_level_reference(key, axis=axis) or self._is_label_reference(\n            key, axis=axis\n        )\n\n    @final\n    def _check_label_or_level_ambiguity(self, key, axis: int = 0) -> None:\n        \"\"\"\n        Check whether `key` is ambiguous.\n\n        By ambiguous, we mean that it matches both a level of the input\n        `axis` and a label of the other axis.\n\n        Parameters\n        ----------\n        key : str or object\n            Label or level name.\n        axis : int, default 0\n            Axis that levels are associated with (0 for index, 1 for columns).\n\n        Raises\n        ------\n        ValueError: `key` is ambiguous\n        \"\"\"\n        axis = self._get_axis_number(axis)\n        other_axes = (ax for ax in range(self._AXIS_LEN) if ax != axis)\n\n        if (\n            key is not None\n            and is_hashable(key)\n            and key in self.axes[axis].names\n            and any(key in self.axes[ax] for ax in other_axes)\n        ):\n\n            # Build an informative and grammatical warning\n            level_article, level_type = (\n                (\"an\", \"index\") if axis == 0 else (\"a\", \"column\")\n            )\n\n            label_article, label_type = (\n                (\"a\", \"column\") if axis == 0 else (\"an\", \"index\")\n            )\n\n            msg = (\n                f\"'{key}' is both {level_article} {level_type} level and \"\n                f\"{label_article} {label_type} label, which is ambiguous.\"\n            )\n            raise ValueError(msg)\n\n    @final\n    def _get_label_or_level_values(self, key: str, axis: int = 0) -> np.ndarray:\n        \"\"\"\n        Return a 1-D array of values associated with `key`, a label or level\n        from the given `axis`.\n\n        Retrieval logic:\n          - (axis=0): Return column values if `key` matches a column label.\n            Otherwise return index level values if `key` matches an index\n            level.\n          - (axis=1): Return row values if `key` matches an index label.\n            Otherwise return column level values if 'key' matches a column\n            level\n\n        Parameters\n        ----------\n        key : str\n            Label or level name.\n        axis : int, default 0\n            Axis that levels are associated with (0 for index, 1 for columns)\n\n        Returns\n        -------\n        values : np.ndarray\n\n        Raises\n        ------\n        KeyError\n            if `key` matches neither a label nor a level\n        ValueError\n            if `key` matches multiple labels\n        FutureWarning\n            if `key` is ambiguous. This will become an ambiguity error in a\n            future version\n        \"\"\"\n        axis = self._get_axis_number(axis)\n        other_axes = [ax for ax in range(self._AXIS_LEN) if ax != axis]\n\n        if self._is_label_reference(key, axis=axis):\n            self._check_label_or_level_ambiguity(key, axis=axis)\n            values = self.xs(key, axis=other_axes[0])._values\n        elif self._is_level_reference(key, axis=axis):\n            values = self.axes[axis].get_level_values(key)._values\n        else:\n            raise KeyError(key)\n\n        # Check for duplicates\n        if values.ndim > 1:\n\n            if other_axes and isinstance(self._get_axis(other_axes[0]), MultiIndex):\n                multi_message = (\n                    \"\\n\"\n                    \"For a multi-index, the label must be a \"\n                    \"tuple with elements corresponding to each level.\"\n                )\n            else:\n                multi_message = \"\"\n\n            label_axis_name = \"column\" if axis == 0 else \"index\"\n            raise ValueError(\n                f\"The {label_axis_name} label '{key}' is not unique.{multi_message}\"\n            )\n\n        return values\n\n    @final\n    def _drop_labels_or_levels(self, keys, axis: int = 0):\n        \"\"\"\n        Drop labels and/or levels for the given `axis`.\n\n        For each key in `keys`:\n          - (axis=0): If key matches a column label then drop the column.\n            Otherwise if key matches an index level then drop the level.\n          - (axis=1): If key matches an index label then drop the row.\n            Otherwise if key matches a column level then drop the level.\n\n        Parameters\n        ----------\n        keys : str or list of str\n            labels or levels to drop\n        axis : int, default 0\n            Axis that levels are associated with (0 for index, 1 for columns)\n\n        Returns\n        -------\n        dropped: DataFrame\n\n        Raises\n        ------\n        ValueError\n            if any `keys` match neither a label nor a level\n        \"\"\"\n        axis = self._get_axis_number(axis)\n\n        # Validate keys\n        keys = com.maybe_make_list(keys)\n        invalid_keys = [\n            k for k in keys if not self._is_label_or_level_reference(k, axis=axis)\n        ]\n\n        if invalid_keys:\n            raise ValueError(\n                \"The following keys are not valid labels or \"\n                f\"levels for axis {axis}: {invalid_keys}\"\n            )\n\n        # Compute levels and labels to drop\n        levels_to_drop = [k for k in keys if self._is_level_reference(k, axis=axis)]\n\n        labels_to_drop = [k for k in keys if not self._is_level_reference(k, axis=axis)]\n\n        # Perform copy upfront and then use inplace operations below.\n        # This ensures that we always perform exactly one copy.\n        # ``copy`` and/or ``inplace`` options could be added in the future.\n        dropped = self.copy()\n\n        if axis == 0:\n            # Handle dropping index levels\n            if levels_to_drop:\n                dropped.reset_index(levels_to_drop, drop=True, inplace=True)\n\n            # Handle dropping columns labels\n            if labels_to_drop:\n                dropped.drop(labels_to_drop, axis=1, inplace=True)\n        else:\n            # Handle dropping column levels\n            if levels_to_drop:\n                if isinstance(dropped.columns, MultiIndex):\n                    # Drop the specified levels from the MultiIndex\n                    dropped.columns = dropped.columns.droplevel(levels_to_drop)\n                else:\n                    # Drop the last level of Index by replacing with\n                    # a RangeIndex\n                    dropped.columns = RangeIndex(dropped.columns.size)\n\n            # Handle dropping index labels\n            if labels_to_drop:\n                dropped.drop(labels_to_drop, axis=0, inplace=True)\n\n        return dropped\n\n    # ----------------------------------------------------------------------\n    # Iteration\n\n    # https://github.com/python/typeshed/issues/2148#issuecomment-520783318\n    # Incompatible types in assignment (expression has type \"None\", base class\n    # \"object\" defined the type as \"Callable[[object], int]\")\n    __hash__: None  # type: ignore[assignment]\n\n    def __iter__(self):\n        \"\"\"\n        Iterate over info axis.\n\n        Returns\n        -------\n        iterator\n            Info axis as iterator.\n        \"\"\"\n        return iter(self._info_axis)\n\n    # can we get a better explanation of this?\n    def keys(self):\n        \"\"\"\n        Get the 'info axis' (see Indexing for more).\n\n        This is index for Series, columns for DataFrame.\n\n        Returns\n        -------\n        Index\n            Info axis.\n        \"\"\"\n        return self._info_axis\n\n    def items(self):\n        \"\"\"\n        Iterate over (label, values) on info axis\n\n        This is index for Series and columns for DataFrame.\n\n        Returns\n        -------\n        Generator\n        \"\"\"\n        for h in self._info_axis:\n            yield h, self[h]\n\n    @doc(items)\n    def iteritems(self):\n        return self.items()\n\n    def __len__(self) -> int:\n        \"\"\"Returns length of info axis\"\"\"\n        return len(self._info_axis)\n\n    @final\n    def __contains__(self, key) -> bool_t:\n        \"\"\"True if the key is in the info axis\"\"\"\n        return key in self._info_axis\n\n    @property\n    def empty(self) -> bool_t:\n        \"\"\"\n        Indicator whether Series/DataFrame is empty.\n\n        True if Series/DataFrame is entirely empty (no items), meaning any of the\n        axes are of length 0.\n\n        Returns\n        -------\n        bool\n            If Series/DataFrame is empty, return True, if not return False.\n\n        See Also\n        --------\n        Series.dropna : Return series without null values.\n        DataFrame.dropna : Return DataFrame with labels on given axis omitted\n            where (all or any) data are missing.\n\n        Notes\n        -----\n        If Series/DataFrame contains only NaNs, it is still not considered empty. See\n        the example below.\n\n        Examples\n        --------\n        An example of an actual empty DataFrame. Notice the index is empty:\n\n        >>> df_empty = pd.DataFrame({'A' : []})\n        >>> df_empty\n        Empty DataFrame\n        Columns: [A]\n        Index: []\n        >>> df_empty.empty\n        True\n\n        If we only have NaNs in our DataFrame, it is not considered empty! We\n        will need to drop the NaNs to make the DataFrame empty:\n\n        >>> df = pd.DataFrame({'A' : [np.nan]})\n        >>> df\n            A\n        0 NaN\n        >>> df.empty\n        False\n        >>> df.dropna().empty\n        True\n\n        >>> ser_empty = pd.Series({'A' : []})\n        >>> ser_empty\n        A    []\n        dtype: object\n        >>> ser_empty.empty\n        False\n        >>> ser_empty = pd.Series()\n        >>> ser_empty.empty\n        True\n        \"\"\"\n        return any(len(self._get_axis(a)) == 0 for a in self._AXIS_ORDERS)\n\n    # ----------------------------------------------------------------------\n    # Array Interface\n\n    # This is also set in IndexOpsMixin\n    # GH#23114 Ensure ndarray.__op__(DataFrame) returns NotImplemented\n    __array_priority__ = 1000\n\n    def __array__(self, dtype: npt.DTypeLike | None = None) -> np.ndarray:\n        return np.asarray(self._values, dtype=dtype)\n\n    def __array_wrap__(\n        self,\n        result: np.ndarray,\n        context: tuple[Callable, tuple[Any, ...], int] | None = None,\n    ):\n        \"\"\"\n        Gets called after a ufunc and other functions.\n\n        Parameters\n        ----------\n        result: np.ndarray\n            The result of the ufunc or other function called on the NumPy array\n            returned by __array__\n        context: tuple of (func, tuple, int)\n            This parameter is returned by ufuncs as a 3-element tuple: (name of the\n            ufunc, arguments of the ufunc, domain of the ufunc), but is not set by\n            other numpy functions.q\n\n        Notes\n        -----\n        Series implements __array_ufunc_ so this not called for ufunc on Series.\n        \"\"\"\n        # Note: at time of dask 2022.01.0, this is still used by dask\n        res = lib.item_from_zerodim(result)\n        if is_scalar(res):\n            # e.g. we get here with np.ptp(series)\n            # ptp also requires the item_from_zerodim\n            return res\n        d = self._construct_axes_dict(self._AXIS_ORDERS, copy=False)\n        return self._constructor(res, **d).__finalize__(self, method=\"__array_wrap__\")\n\n    @final\n    def __array_ufunc__(\n        self, ufunc: np.ufunc, method: str, *inputs: Any, **kwargs: Any\n    ):\n        return arraylike.array_ufunc(self, ufunc, method, *inputs, **kwargs)\n\n    # ----------------------------------------------------------------------\n    # Picklability\n\n    @final\n    def __getstate__(self) -> dict[str, Any]:\n        meta = {k: getattr(self, k, None) for k in self._metadata}\n        return {\n            \"_mgr\": self._mgr,\n            \"_typ\": self._typ,\n            \"_metadata\": self._metadata,\n            \"attrs\": self.attrs,\n            \"_flags\": {k: self.flags[k] for k in self.flags._keys},\n            **meta,\n        }\n\n    @final\n    def __setstate__(self, state):\n        if isinstance(state, BlockManager):\n            self._mgr = state\n        elif isinstance(state, dict):\n            if \"_data\" in state and \"_mgr\" not in state:\n                # compat for older pickles\n                state[\"_mgr\"] = state.pop(\"_data\")\n            typ = state.get(\"_typ\")\n            if typ is not None:\n                attrs = state.get(\"_attrs\", {})\n                object.__setattr__(self, \"_attrs\", attrs)\n                flags = state.get(\"_flags\", {\"allows_duplicate_labels\": True})\n                object.__setattr__(self, \"_flags\", Flags(self, **flags))\n\n                # set in the order of internal names\n                # to avoid definitional recursion\n                # e.g. say fill_value needing _mgr to be\n                # defined\n                meta = set(self._internal_names + self._metadata)\n                for k in list(meta):\n                    if k in state and k != \"_flags\":\n                        v = state[k]\n                        object.__setattr__(self, k, v)\n\n                for k, v in state.items():\n                    if k not in meta:\n                        object.__setattr__(self, k, v)\n\n            else:\n                raise NotImplementedError(\"Pre-0.12 pickles are no longer supported\")\n        elif len(state) == 2:\n            raise NotImplementedError(\"Pre-0.12 pickles are no longer supported\")\n\n        self._item_cache = {}\n\n    # ----------------------------------------------------------------------\n    # Rendering Methods\n\n    def __repr__(self) -> str:\n        # string representation based upon iterating over self\n        # (since, by definition, `PandasContainers` are iterable)\n        prepr = f\"[{','.join(map(pprint_thing, self))}]\"\n        return f\"{type(self).__name__}({prepr})\"\n\n    @final\n    def _repr_latex_(self):\n        \"\"\"\n        Returns a LaTeX representation for a particular object.\n        Mainly for use with nbconvert (jupyter notebook conversion to pdf).\n        \"\"\"\n        if config.get_option(\"display.latex.repr\"):\n            return self.to_latex()\n        else:\n            return None\n\n    @final\n    def _repr_data_resource_(self):\n        \"\"\"\n        Not a real Jupyter special repr method, but we use the same\n        naming convention.\n        \"\"\"\n        if config.get_option(\"display.html.table_schema\"):\n            data = self.head(config.get_option(\"display.max_rows\"))\n\n            as_json = data.to_json(orient=\"table\")\n            as_json = cast(str, as_json)\n            return json.loads(as_json, object_pairs_hook=collections.OrderedDict)\n\n    # ----------------------------------------------------------------------\n    # I/O Methods\n\n    @final\n    @doc(klass=\"object\", storage_options=_shared_docs[\"storage_options\"])\n    def to_excel(\n        self,\n        excel_writer,\n        sheet_name: str = \"Sheet1\",\n        na_rep: str = \"\",\n        float_format: str | None = None,\n        columns=None,\n        header=True,\n        index=True,\n        index_label=None,\n        startrow=0,\n        startcol=0,\n        engine=None,\n        merge_cells=True,\n        encoding=None,\n        inf_rep=\"inf\",\n        verbose=True,\n        freeze_panes=None,\n        storage_options: StorageOptions = None,\n    ) -> None:\n        \"\"\"\n        Write {klass} to an Excel sheet.\n\n        To write a single {klass} to an Excel .xlsx file it is only necessary to\n        specify a target file name. To write to multiple sheets it is necessary to\n        create an `ExcelWriter` object with a target file name, and specify a sheet\n        in the file to write to.\n\n        Multiple sheets may be written to by specifying unique `sheet_name`.\n        With all data written to the file it is necessary to save the changes.\n        Note that creating an `ExcelWriter` object with a file name that already\n        exists will result in the contents of the existing file being erased.\n\n        Parameters\n        ----------\n        excel_writer : path-like, file-like, or ExcelWriter object\n            File path or existing ExcelWriter.\n        sheet_name : str, default 'Sheet1'\n            Name of sheet which will contain DataFrame.\n        na_rep : str, default ''\n            Missing data representation.\n        float_format : str, optional\n            Format string for floating point numbers. For example\n            ``float_format=\"%.2f\"`` will format 0.1234 to 0.12.\n        columns : sequence or list of str, optional\n            Columns to write.\n        header : bool or list of str, default True\n            Write out the column names. If a list of string is given it is\n            assumed to be aliases for the column names.\n        index : bool, default True\n            Write row names (index).\n        index_label : str or sequence, optional\n            Column label for index column(s) if desired. If not specified, and\n            `header` and `index` are True, then the index names are used. A\n            sequence should be given if the DataFrame uses MultiIndex.\n        startrow : int, default 0\n            Upper left cell row to dump data frame.\n        startcol : int, default 0\n            Upper left cell column to dump data frame.\n        engine : str, optional\n            Write engine to use, 'openpyxl' or 'xlsxwriter'. You can also set this\n            via the options ``io.excel.xlsx.writer``, ``io.excel.xls.writer``, and\n            ``io.excel.xlsm.writer``.\n\n            .. deprecated:: 1.2.0\n\n                As the `xlwt <https://pypi.org/project/xlwt/>`__ package is no longer\n                maintained, the ``xlwt`` engine will be removed in a future version\n                of pandas.\n\n        merge_cells : bool, default True\n            Write MultiIndex and Hierarchical Rows as merged cells.\n        encoding : str, optional\n            Encoding of the resulting excel file. Only necessary for xlwt,\n            other writers support unicode natively.\n        inf_rep : str, default 'inf'\n            Representation for infinity (there is no native representation for\n            infinity in Excel).\n        verbose : bool, default True\n            Display more information in the error logs.\n        freeze_panes : tuple of int (length 2), optional\n            Specifies the one-based bottommost row and rightmost column that\n            is to be frozen.\n        {storage_options}\n\n            .. versionadded:: 1.2.0\n\n        See Also\n        --------\n        to_csv : Write DataFrame to a comma-separated values (csv) file.\n        ExcelWriter : Class for writing DataFrame objects into excel sheets.\n        read_excel : Read an Excel file into a pandas DataFrame.\n        read_csv : Read a comma-separated values (csv) file into DataFrame.\n\n        Notes\n        -----\n        For compatibility with :meth:`~DataFrame.to_csv`,\n        to_excel serializes lists and dicts to strings before writing.\n\n        Once a workbook has been saved it is not possible to write further\n        data without rewriting the whole workbook.\n\n        Examples\n        --------\n\n        Create, write to and save a workbook:\n\n        >>> df1 = pd.DataFrame([['a', 'b'], ['c', 'd']],\n        ...                    index=['row 1', 'row 2'],\n        ...                    columns=['col 1', 'col 2'])\n        >>> df1.to_excel(\"output.xlsx\")  # doctest: +SKIP\n\n        To specify the sheet name:\n\n        >>> df1.to_excel(\"output.xlsx\",\n        ...              sheet_name='Sheet_name_1')  # doctest: +SKIP\n\n        If you wish to write to more than one sheet in the workbook, it is\n        necessary to specify an ExcelWriter object:\n\n        >>> df2 = df1.copy()\n        >>> with pd.ExcelWriter('output.xlsx') as writer:  # doctest: +SKIP\n        ...     df1.to_excel(writer, sheet_name='Sheet_name_1')\n        ...     df2.to_excel(writer, sheet_name='Sheet_name_2')\n\n        ExcelWriter can also be used to append to an existing Excel file:\n\n        >>> with pd.ExcelWriter('output.xlsx',\n        ...                     mode='a') as writer:  # doctest: +SKIP\n        ...     df.to_excel(writer, sheet_name='Sheet_name_3')\n\n        To set the library that is used to write the Excel file,\n        you can pass the `engine` keyword (the default engine is\n        automatically chosen depending on the file extension):\n\n        >>> df1.to_excel('output1.xlsx', engine='xlsxwriter')  # doctest: +SKIP\n        \"\"\"\n\n        df = self if isinstance(self, ABCDataFrame) else self.to_frame()\n\n        from pandas.io.formats.excel import ExcelFormatter\n\n        formatter = ExcelFormatter(\n            df,\n            na_rep=na_rep,\n            cols=columns,\n            header=header,\n            float_format=float_format,\n            index=index,\n            index_label=index_label,\n            merge_cells=merge_cells,\n            inf_rep=inf_rep,\n        )\n        formatter.write(\n            excel_writer,\n            sheet_name=sheet_name,\n            startrow=startrow,\n            startcol=startcol,\n            freeze_panes=freeze_panes,\n            engine=engine,\n            storage_options=storage_options,\n        )\n\n    @final\n    @doc(\n        storage_options=_shared_docs[\"storage_options\"],\n        compression_options=_shared_docs[\"compression_options\"] % \"path_or_buf\",\n    )\n    def to_json(\n        self,\n        path_or_buf: FilePath | WriteBuffer[bytes] | WriteBuffer[str] | None = None,\n        orient: str | None = None,\n        date_format: str | None = None,\n        double_precision: int = 10,\n        force_ascii: bool_t = True,\n        date_unit: str = \"ms\",\n        default_handler: Callable[[Any], JSONSerializable] | None = None,\n        lines: bool_t = False,\n        compression: CompressionOptions = \"infer\",\n        index: bool_t = True,\n        indent: int | None = None,\n        storage_options: StorageOptions = None,\n    ) -> str | None:\n        \"\"\"\n        Convert the object to a JSON string.\n\n        Note NaN's and None will be converted to null and datetime objects\n        will be converted to UNIX timestamps.\n\n        Parameters\n        ----------\n        path_or_buf : str, path object, file-like object, or None, default None\n            String, path object (implementing os.PathLike[str]), or file-like\n            object implementing a write() function. If None, the result is\n            returned as a string.\n        orient : str\n            Indication of expected JSON string format.\n\n            * Series:\n\n                - default is 'index'\n                - allowed values are: {{'split', 'records', 'index', 'table'}}.\n\n            * DataFrame:\n\n                - default is 'columns'\n                - allowed values are: {{'split', 'records', 'index', 'columns',\n                  'values', 'table'}}.\n\n            * The format of the JSON string:\n\n                - 'split' : dict like {{'index' -> [index], 'columns' -> [columns],\n                  'data' -> [values]}}\n                - 'records' : list like [{{column -> value}}, ... , {{column -> value}}]\n                - 'index' : dict like {{index -> {{column -> value}}}}\n                - 'columns' : dict like {{column -> {{index -> value}}}}\n                - 'values' : just the values array\n                - 'table' : dict like {{'schema': {{schema}}, 'data': {{data}}}}\n\n                Describing the data, where data component is like ``orient='records'``.\n\n        date_format : {{None, 'epoch', 'iso'}}\n            Type of date conversion. 'epoch' = epoch milliseconds,\n            'iso' = ISO8601. The default depends on the `orient`. For\n            ``orient='table'``, the default is 'iso'. For all other orients,\n            the default is 'epoch'.\n        double_precision : int, default 10\n            The number of decimal places to use when encoding\n            floating point values.\n        force_ascii : bool, default True\n            Force encoded string to be ASCII.\n        date_unit : str, default 'ms' (milliseconds)\n            The time unit to encode to, governs timestamp and ISO8601\n            precision.  One of 's', 'ms', 'us', 'ns' for second, millisecond,\n            microsecond, and nanosecond respectively.\n        default_handler : callable, default None\n            Handler to call if object cannot otherwise be converted to a\n            suitable format for JSON. Should receive a single argument which is\n            the object to convert and return a serialisable object.\n        lines : bool, default False\n            If 'orient' is 'records' write out line-delimited json format. Will\n            throw ValueError if incorrect 'orient' since others are not\n            list-like.\n        {compression_options}\n\n            .. versionchanged:: 1.4.0 Zstandard support.\n\n        index : bool, default True\n            Whether to include the index values in the JSON string. Not\n            including the index (``index=False``) is only supported when\n            orient is 'split' or 'table'.\n        indent : int, optional\n           Length of whitespace used to indent each record.\n\n           .. versionadded:: 1.0.0\n\n        {storage_options}\n\n            .. versionadded:: 1.2.0\n\n        Returns\n        -------\n        None or str\n            If path_or_buf is None, returns the resulting json format as a\n            string. Otherwise returns None.\n\n        See Also\n        --------\n        read_json : Convert a JSON string to pandas object.\n\n        Notes\n        -----\n        The behavior of ``indent=0`` varies from the stdlib, which does not\n        indent the output but does insert newlines. Currently, ``indent=0``\n        and the default ``indent=None`` are equivalent in pandas, though this\n        may change in a future release.\n\n        ``orient='table'`` contains a 'pandas_version' field under 'schema'.\n        This stores the version of `pandas` used in the latest revision of the\n        schema.\n\n        Examples\n        --------\n        >>> import json\n        >>> df = pd.DataFrame(\n        ...     [[\"a\", \"b\"], [\"c\", \"d\"]],\n        ...     index=[\"row 1\", \"row 2\"],\n        ...     columns=[\"col 1\", \"col 2\"],\n        ... )\n\n        >>> result = df.to_json(orient=\"split\")\n        >>> parsed = json.loads(result)\n        >>> json.dumps(parsed, indent=4)  # doctest: +SKIP\n        {{\n            \"columns\": [\n                \"col 1\",\n                \"col 2\"\n            ],\n            \"index\": [\n                \"row 1\",\n                \"row 2\"\n            ],\n            \"data\": [\n                [\n                    \"a\",\n                    \"b\"\n                ],\n                [\n                    \"c\",\n                    \"d\"\n                ]\n            ]\n        }}\n\n        Encoding/decoding a Dataframe using ``'records'`` formatted JSON.\n        Note that index labels are not preserved with this encoding.\n\n        >>> result = df.to_json(orient=\"records\")\n        >>> parsed = json.loads(result)\n        >>> json.dumps(parsed, indent=4)  # doctest: +SKIP\n        [\n            {{\n                \"col 1\": \"a\",\n                \"col 2\": \"b\"\n            }},\n            {{\n                \"col 1\": \"c\",\n                \"col 2\": \"d\"\n            }}\n        ]\n\n        Encoding/decoding a Dataframe using ``'index'`` formatted JSON:\n\n        >>> result = df.to_json(orient=\"index\")\n        >>> parsed = json.loads(result)\n        >>> json.dumps(parsed, indent=4)  # doctest: +SKIP\n        {{\n            \"row 1\": {{\n                \"col 1\": \"a\",\n                \"col 2\": \"b\"\n            }},\n            \"row 2\": {{\n                \"col 1\": \"c\",\n                \"col 2\": \"d\"\n            }}\n        }}\n\n        Encoding/decoding a Dataframe using ``'columns'`` formatted JSON:\n\n        >>> result = df.to_json(orient=\"columns\")\n        >>> parsed = json.loads(result)\n        >>> json.dumps(parsed, indent=4)  # doctest: +SKIP\n        {{\n            \"col 1\": {{\n                \"row 1\": \"a\",\n                \"row 2\": \"c\"\n            }},\n            \"col 2\": {{\n                \"row 1\": \"b\",\n                \"row 2\": \"d\"\n            }}\n        }}\n\n        Encoding/decoding a Dataframe using ``'values'`` formatted JSON:\n\n        >>> result = df.to_json(orient=\"values\")\n        >>> parsed = json.loads(result)\n        >>> json.dumps(parsed, indent=4)  # doctest: +SKIP\n        [\n            [\n                \"a\",\n                \"b\"\n            ],\n            [\n                \"c\",\n                \"d\"\n            ]\n        ]\n\n        Encoding with Table Schema:\n\n        >>> result = df.to_json(orient=\"table\")\n        >>> parsed = json.loads(result)\n        >>> json.dumps(parsed, indent=4)  # doctest: +SKIP\n        {{\n            \"schema\": {{\n                \"fields\": [\n                    {{\n                        \"name\": \"index\",\n                        \"type\": \"string\"\n                    }},\n                    {{\n                        \"name\": \"col 1\",\n                        \"type\": \"string\"\n                    }},\n                    {{\n                        \"name\": \"col 2\",\n                        \"type\": \"string\"\n                    }}\n                ],\n                \"primaryKey\": [\n                    \"index\"\n                ],\n                \"pandas_version\": \"1.4.0\"\n            }},\n            \"data\": [\n                {{\n                    \"index\": \"row 1\",\n                    \"col 1\": \"a\",\n                    \"col 2\": \"b\"\n                }},\n                {{\n                    \"index\": \"row 2\",\n                    \"col 1\": \"c\",\n                    \"col 2\": \"d\"\n                }}\n            ]\n        }}\n        \"\"\"\n        from pandas.io import json\n\n        if date_format is None and orient == \"table\":\n            date_format = \"iso\"\n        elif date_format is None:\n            date_format = \"epoch\"\n\n        config.is_nonnegative_int(indent)\n        indent = indent or 0\n\n        return json.to_json(\n            path_or_buf=path_or_buf,\n            obj=self,\n            orient=orient,\n            date_format=date_format,\n            double_precision=double_precision,\n            force_ascii=force_ascii,\n            date_unit=date_unit,\n            default_handler=default_handler,\n            lines=lines,\n            compression=compression,\n            index=index,\n            indent=indent,\n            storage_options=storage_options,\n        )\n\n    @final\n    def to_hdf(\n        self,\n        path_or_buf,\n        key: str,\n        mode: str = \"a\",\n        complevel: int | None = None,\n        complib: str | None = None,\n        append: bool_t = False,\n        format: str | None = None,\n        index: bool_t = True,\n        min_itemsize: int | dict[str, int] | None = None,\n        nan_rep=None,\n        dropna: bool_t | None = None,\n        data_columns: Literal[True] | list[str] | None = None,\n        errors: str = \"strict\",\n        encoding: str = \"UTF-8\",\n    ) -> None:\n        \"\"\"\n        Write the contained data to an HDF5 file using HDFStore.\n\n        Hierarchical Data Format (HDF) is self-describing, allowing an\n        application to interpret the structure and contents of a file with\n        no outside information. One HDF file can hold a mix of related objects\n        which can be accessed as a group or as individual objects.\n\n        In order to add another DataFrame or Series to an existing HDF file\n        please use append mode and a different a key.\n\n        .. warning::\n\n           One can store a subclass of ``DataFrame`` or ``Series`` to HDF5,\n           but the type of the subclass is lost upon storing.\n\n        For more information see the :ref:`user guide <io.hdf5>`.\n\n        Parameters\n        ----------\n        path_or_buf : str or pandas.HDFStore\n            File path or HDFStore object.\n        key : str\n            Identifier for the group in the store.\n        mode : {'a', 'w', 'r+'}, default 'a'\n            Mode to open file:\n\n            - 'w': write, a new file is created (an existing file with\n              the same name would be deleted).\n            - 'a': append, an existing file is opened for reading and\n              writing, and if the file does not exist it is created.\n            - 'r+': similar to 'a', but the file must already exist.\n        complevel : {0-9}, default None\n            Specifies a compression level for data.\n            A value of 0 or None disables compression.\n        complib : {'zlib', 'lzo', 'bzip2', 'blosc'}, default 'zlib'\n            Specifies the compression library to be used.\n            As of v0.20.2 these additional compressors for Blosc are supported\n            (default if no compressor specified: 'blosc:blosclz'):\n            {'blosc:blosclz', 'blosc:lz4', 'blosc:lz4hc', 'blosc:snappy',\n            'blosc:zlib', 'blosc:zstd'}.\n            Specifying a compression library which is not available issues\n            a ValueError.\n        append : bool, default False\n            For Table formats, append the input data to the existing.\n        format : {'fixed', 'table', None}, default 'fixed'\n            Possible values:\n\n            - 'fixed': Fixed format. Fast writing/reading. Not-appendable,\n              nor searchable.\n            - 'table': Table format. Write as a PyTables Table structure\n              which may perform worse but allow more flexible operations\n              like searching / selecting subsets of the data.\n            - If None, pd.get_option('io.hdf.default_format') is checked,\n              followed by fallback to \"fixed\".\n        errors : str, default 'strict'\n            Specifies how encoding and decoding errors are to be handled.\n            See the errors argument for :func:`open` for a full list\n            of options.\n        encoding : str, default \"UTF-8\"\n        min_itemsize : dict or int, optional\n            Map column names to minimum string sizes for columns.\n        nan_rep : Any, optional\n            How to represent null values as str.\n            Not allowed with append=True.\n        data_columns : list of columns or True, optional\n            List of columns to create as indexed data columns for on-disk\n            queries, or True to use all columns. By default only the axes\n            of the object are indexed. See :ref:`io.hdf5-query-data-columns`.\n            Applicable only to format='table'.\n\n        See Also\n        --------\n        read_hdf : Read from HDF file.\n        DataFrame.to_parquet : Write a DataFrame to the binary parquet format.\n        DataFrame.to_sql : Write to a SQL table.\n        DataFrame.to_feather : Write out feather-format for DataFrames.\n        DataFrame.to_csv : Write out to a csv file.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]},\n        ...                   index=['a', 'b', 'c'])  # doctest: +SKIP\n        >>> df.to_hdf('data.h5', key='df', mode='w')  # doctest: +SKIP\n\n        We can add another object to the same file:\n\n        >>> s = pd.Series([1, 2, 3, 4])  # doctest: +SKIP\n        >>> s.to_hdf('data.h5', key='s')  # doctest: +SKIP\n\n        Reading from HDF file:\n\n        >>> pd.read_hdf('data.h5', 'df')  # doctest: +SKIP\n        A  B\n        a  1  4\n        b  2  5\n        c  3  6\n        >>> pd.read_hdf('data.h5', 's')  # doctest: +SKIP\n        0    1\n        1    2\n        2    3\n        3    4\n        dtype: int64\n        \"\"\"\n        from pandas.io import pytables\n\n        # Argument 3 to \"to_hdf\" has incompatible type \"NDFrame\"; expected\n        # \"Union[DataFrame, Series]\" [arg-type]\n        pytables.to_hdf(\n            path_or_buf,\n            key,\n            self,  # type: ignore[arg-type]\n            mode=mode,\n            complevel=complevel,\n            complib=complib,\n            append=append,\n            format=format,\n            index=index,\n            min_itemsize=min_itemsize,\n            nan_rep=nan_rep,\n            dropna=dropna,\n            data_columns=data_columns,\n            errors=errors,\n            encoding=encoding,\n        )\n\n    @final\n    def to_sql(\n        self,\n        name: str,\n        con,\n        schema=None,\n        if_exists: str = \"fail\",\n        index: bool_t = True,\n        index_label=None,\n        chunksize=None,\n        dtype: DtypeArg | None = None,\n        method=None,\n    ) -> int | None:\n        \"\"\"\n        Write records stored in a DataFrame to a SQL database.\n\n        Databases supported by SQLAlchemy [1]_ are supported. Tables can be\n        newly created, appended to, or overwritten.\n\n        Parameters\n        ----------\n        name : str\n            Name of SQL table.\n        con : sqlalchemy.engine.(Engine or Connection) or sqlite3.Connection\n            Using SQLAlchemy makes it possible to use any DB supported by that\n            library. Legacy support is provided for sqlite3.Connection objects. The user\n            is responsible for engine disposal and connection closure for the SQLAlchemy\n            connectable See `here \\\n                <https://docs.sqlalchemy.org/en/13/core/connections.html>`_.\n\n        schema : str, optional\n            Specify the schema (if database flavor supports this). If None, use\n            default schema.\n        if_exists : {'fail', 'replace', 'append'}, default 'fail'\n            How to behave if the table already exists.\n\n            * fail: Raise a ValueError.\n            * replace: Drop the table before inserting new values.\n            * append: Insert new values to the existing table.\n\n        index : bool, default True\n            Write DataFrame index as a column. Uses `index_label` as the column\n            name in the table.\n        index_label : str or sequence, default None\n            Column label for index column(s). If None is given (default) and\n            `index` is True, then the index names are used.\n            A sequence should be given if the DataFrame uses MultiIndex.\n        chunksize : int, optional\n            Specify the number of rows in each batch to be written at a time.\n            By default, all rows will be written at once.\n        dtype : dict or scalar, optional\n            Specifying the datatype for columns. If a dictionary is used, the\n            keys should be the column names and the values should be the\n            SQLAlchemy types or strings for the sqlite3 legacy mode. If a\n            scalar is provided, it will be applied to all columns.\n        method : {None, 'multi', callable}, optional\n            Controls the SQL insertion clause used:\n\n            * None : Uses standard SQL ``INSERT`` clause (one per row).\n            * 'multi': Pass multiple values in a single ``INSERT`` clause.\n            * callable with signature ``(pd_table, conn, keys, data_iter)``.\n\n            Details and a sample callable implementation can be found in the\n            section :ref:`insert method <io.sql.method>`.\n\n        Returns\n        -------\n        None or int\n            Number of rows affected by to_sql. None is returned if the callable\n            passed into ``method`` does not return the number of rows.\n\n            The number of returned rows affected is the sum of the ``rowcount``\n            attribute of ``sqlite3.Cursor`` or SQLAlchemy connectable which may not\n            reflect the exact number of written rows as stipulated in the\n            `sqlite3 <https://docs.python.org/3/library/sqlite3.html#sqlite3.Cursor.rowcount>`__ or\n            `SQLAlchemy <https://docs.sqlalchemy.org/en/14/core/connections.html#sqlalchemy.engine.BaseCursorResult.rowcount>`__.\n\n            .. versionadded:: 1.4.0\n\n        Raises\n        ------\n        ValueError\n            When the table already exists and `if_exists` is 'fail' (the\n            default).\n\n        See Also\n        --------\n        read_sql : Read a DataFrame from a table.\n\n        Notes\n        -----\n        Timezone aware datetime columns will be written as\n        ``Timestamp with timezone`` type with SQLAlchemy if supported by the\n        database. Otherwise, the datetimes will be stored as timezone unaware\n        timestamps local to the original timezone.\n\n        References\n        ----------\n        .. [1] https://docs.sqlalchemy.org\n        .. [2] https://www.python.org/dev/peps/pep-0249/\n\n        Examples\n        --------\n        Create an in-memory SQLite database.\n\n        >>> from sqlalchemy import create_engine\n        >>> engine = create_engine('sqlite://', echo=False)\n\n        Create a table from scratch with 3 rows.\n\n        >>> df = pd.DataFrame({'name' : ['User 1', 'User 2', 'User 3']})\n        >>> df\n             name\n        0  User 1\n        1  User 2\n        2  User 3\n\n        >>> df.to_sql('users', con=engine)\n        3\n        >>> engine.execute(\"SELECT * FROM users\").fetchall()\n        [(0, 'User 1'), (1, 'User 2'), (2, 'User 3')]\n\n        An `sqlalchemy.engine.Connection` can also be passed to `con`:\n\n        >>> with engine.begin() as connection:\n        ...     df1 = pd.DataFrame({'name' : ['User 4', 'User 5']})\n        ...     df1.to_sql('users', con=connection, if_exists='append')\n        2\n\n        This is allowed to support operations that require that the same\n        DBAPI connection is used for the entire operation.\n\n        >>> df2 = pd.DataFrame({'name' : ['User 6', 'User 7']})\n        >>> df2.to_sql('users', con=engine, if_exists='append')\n        2\n        >>> engine.execute(\"SELECT * FROM users\").fetchall()\n        [(0, 'User 1'), (1, 'User 2'), (2, 'User 3'),\n         (0, 'User 4'), (1, 'User 5'), (0, 'User 6'),\n         (1, 'User 7')]\n\n        Overwrite the table with just ``df2``.\n\n        >>> df2.to_sql('users', con=engine, if_exists='replace',\n        ...            index_label='id')\n        2\n        >>> engine.execute(\"SELECT * FROM users\").fetchall()\n        [(0, 'User 6'), (1, 'User 7')]\n\n        Specify the dtype (especially useful for integers with missing values).\n        Notice that while pandas is forced to store the data as floating point,\n        the database supports nullable integers. When fetching the data with\n        Python, we get back integer scalars.\n\n        >>> df = pd.DataFrame({\"A\": [1, None, 2]})\n        >>> df\n             A\n        0  1.0\n        1  NaN\n        2  2.0\n\n        >>> from sqlalchemy.types import Integer\n        >>> df.to_sql('integers', con=engine, index=False,\n        ...           dtype={\"A\": Integer()})\n        3\n\n        >>> engine.execute(\"SELECT * FROM integers\").fetchall()\n        [(1,), (None,), (2,)]\n        \"\"\"  # noqa:E501\n        from pandas.io import sql\n\n        return sql.to_sql(\n            self,\n            name,\n            con,\n            schema=schema,\n            if_exists=if_exists,\n            index=index,\n            index_label=index_label,\n            chunksize=chunksize,\n            dtype=dtype,\n            method=method,\n        )\n\n    @final\n    @doc(\n        storage_options=_shared_docs[\"storage_options\"],\n        compression_options=_shared_docs[\"compression_options\"] % \"path\",\n    )\n    def to_pickle(\n        self,\n        path,\n        compression: CompressionOptions = \"infer\",\n        protocol: int = pickle.HIGHEST_PROTOCOL,\n        storage_options: StorageOptions = None,\n    ) -> None:\n        \"\"\"\n        Pickle (serialize) object to file.\n\n        Parameters\n        ----------\n        path : str\n            File path where the pickled object will be stored.\n        {compression_options}\n        protocol : int\n            Int which indicates which protocol should be used by the pickler,\n            default HIGHEST_PROTOCOL (see [1]_ paragraph 12.1.2). The possible\n            values are 0, 1, 2, 3, 4, 5. A negative value for the protocol\n            parameter is equivalent to setting its value to HIGHEST_PROTOCOL.\n\n            .. [1] https://docs.python.org/3/library/pickle.html.\n\n        {storage_options}\n\n            .. versionadded:: 1.2.0\n\n        See Also\n        --------\n        read_pickle : Load pickled pandas object (or any object) from file.\n        DataFrame.to_hdf : Write DataFrame to an HDF5 file.\n        DataFrame.to_sql : Write DataFrame to a SQL database.\n        DataFrame.to_parquet : Write a DataFrame to the binary parquet format.\n\n        Examples\n        --------\n        >>> original_df = pd.DataFrame({{\"foo\": range(5), \"bar\": range(5, 10)}})  # doctest: +SKIP\n        >>> original_df  # doctest: +SKIP\n           foo  bar\n        0    0    5\n        1    1    6\n        2    2    7\n        3    3    8\n        4    4    9\n        >>> original_df.to_pickle(\"./dummy.pkl\")  # doctest: +SKIP\n\n        >>> unpickled_df = pd.read_pickle(\"./dummy.pkl\")  # doctest: +SKIP\n        >>> unpickled_df  # doctest: +SKIP\n           foo  bar\n        0    0    5\n        1    1    6\n        2    2    7\n        3    3    8\n        4    4    9\n        \"\"\"  # noqa: E501\n        from pandas.io.pickle import to_pickle\n\n        to_pickle(\n            self,\n            path,\n            compression=compression,\n            protocol=protocol,\n            storage_options=storage_options,\n        )\n\n    @final\n    def to_clipboard(\n        self, excel: bool_t = True, sep: str | None = None, **kwargs\n    ) -> None:\n        r\"\"\"\n        Copy object to the system clipboard.\n\n        Write a text representation of object to the system clipboard.\n        This can be pasted into Excel, for example.\n\n        Parameters\n        ----------\n        excel : bool, default True\n            Produce output in a csv format for easy pasting into excel.\n\n            - True, use the provided separator for csv pasting.\n            - False, write a string representation of the object to the clipboard.\n\n        sep : str, default ``'\\t'``\n            Field delimiter.\n        **kwargs\n            These parameters will be passed to DataFrame.to_csv.\n\n        See Also\n        --------\n        DataFrame.to_csv : Write a DataFrame to a comma-separated values\n            (csv) file.\n        read_clipboard : Read text from clipboard and pass to read_csv.\n\n        Notes\n        -----\n        Requirements for your platform.\n\n          - Linux : `xclip`, or `xsel` (with `PyQt4` modules)\n          - Windows : none\n          - macOS : none\n\n        Examples\n        --------\n        Copy the contents of a DataFrame to the clipboard.\n\n        >>> df = pd.DataFrame([[1, 2, 3], [4, 5, 6]], columns=['A', 'B', 'C'])\n\n        >>> df.to_clipboard(sep=',')  # doctest: +SKIP\n        ... # Wrote the following to the system clipboard:\n        ... # ,A,B,C\n        ... # 0,1,2,3\n        ... # 1,4,5,6\n\n        We can omit the index by passing the keyword `index` and setting\n        it to false.\n\n        >>> df.to_clipboard(sep=',', index=False)  # doctest: +SKIP\n        ... # Wrote the following to the system clipboard:\n        ... # A,B,C\n        ... # 1,2,3\n        ... # 4,5,6\n        \"\"\"\n        from pandas.io import clipboards\n\n        clipboards.to_clipboard(self, excel=excel, sep=sep, **kwargs)\n\n    @final\n    def to_xarray(self):\n        \"\"\"\n        Return an xarray object from the pandas object.\n\n        Returns\n        -------\n        xarray.DataArray or xarray.Dataset\n            Data in the pandas structure converted to Dataset if the object is\n            a DataFrame, or a DataArray if the object is a Series.\n\n        See Also\n        --------\n        DataFrame.to_hdf : Write DataFrame to an HDF5 file.\n        DataFrame.to_parquet : Write a DataFrame to the binary parquet format.\n\n        Notes\n        -----\n        See the `xarray docs <https://xarray.pydata.org/en/stable/>`__\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([('falcon', 'bird', 389.0, 2),\n        ...                    ('parrot', 'bird', 24.0, 2),\n        ...                    ('lion', 'mammal', 80.5, 4),\n        ...                    ('monkey', 'mammal', np.nan, 4)],\n        ...                   columns=['name', 'class', 'max_speed',\n        ...                            'num_legs'])\n        >>> df\n             name   class  max_speed  num_legs\n        0  falcon    bird      389.0         2\n        1  parrot    bird       24.0         2\n        2    lion  mammal       80.5         4\n        3  monkey  mammal        NaN         4\n\n        >>> df.to_xarray()\n        <xarray.Dataset>\n        Dimensions:    (index: 4)\n        Coordinates:\n          * index      (index) int64 0 1 2 3\n        Data variables:\n            name       (index) object 'falcon' 'parrot' 'lion' 'monkey'\n            class      (index) object 'bird' 'bird' 'mammal' 'mammal'\n            max_speed  (index) float64 389.0 24.0 80.5 nan\n            num_legs   (index) int64 2 2 4 4\n\n        >>> df['max_speed'].to_xarray()\n        <xarray.DataArray 'max_speed' (index: 4)>\n        array([389. ,  24. ,  80.5,   nan])\n        Coordinates:\n          * index    (index) int64 0 1 2 3\n\n        >>> dates = pd.to_datetime(['2018-01-01', '2018-01-01',\n        ...                         '2018-01-02', '2018-01-02'])\n        >>> df_multiindex = pd.DataFrame({'date': dates,\n        ...                               'animal': ['falcon', 'parrot',\n        ...                                          'falcon', 'parrot'],\n        ...                               'speed': [350, 18, 361, 15]})\n        >>> df_multiindex = df_multiindex.set_index(['date', 'animal'])\n\n        >>> df_multiindex\n                           speed\n        date       animal\n        2018-01-01 falcon    350\n                   parrot     18\n        2018-01-02 falcon    361\n                   parrot     15\n\n        >>> df_multiindex.to_xarray()\n        <xarray.Dataset>\n        Dimensions:  (animal: 2, date: 2)\n        Coordinates:\n          * date     (date) datetime64[ns] 2018-01-01 2018-01-02\n          * animal   (animal) object 'falcon' 'parrot'\n        Data variables:\n            speed    (date, animal) int64 350 18 361 15\n        \"\"\"\n        xarray = import_optional_dependency(\"xarray\")\n\n        if self.ndim == 1:\n            return xarray.DataArray.from_series(self)\n        else:\n            return xarray.Dataset.from_dataframe(self)\n\n    @final\n    @doc(returns=fmt.return_docstring)\n    def to_latex(\n        self,\n        buf=None,\n        columns=None,\n        col_space=None,\n        header=True,\n        index=True,\n        na_rep=\"NaN\",\n        formatters=None,\n        float_format=None,\n        sparsify=None,\n        index_names=True,\n        bold_rows=False,\n        column_format=None,\n        longtable=None,\n        escape=None,\n        encoding=None,\n        decimal=\".\",\n        multicolumn=None,\n        multicolumn_format=None,\n        multirow=None,\n        caption=None,\n        label=None,\n        position=None,\n    ):\n        r\"\"\"\n        Render object to a LaTeX tabular, longtable, or nested table.\n\n        Requires ``\\usepackage{{booktabs}}``.  The output can be copy/pasted\n        into a main LaTeX document or read from an external file\n        with ``\\input{{table.tex}}``.\n\n        .. versionchanged:: 1.0.0\n           Added caption and label arguments.\n\n        .. versionchanged:: 1.2.0\n           Added position argument, changed meaning of caption argument.\n\n        Parameters\n        ----------\n        buf : str, Path or StringIO-like, optional, default None\n            Buffer to write to. If None, the output is returned as a string.\n        columns : list of label, optional\n            The subset of columns to write. Writes all columns by default.\n        col_space : int, optional\n            The minimum width of each column.\n        header : bool or list of str, default True\n            Write out the column names. If a list of strings is given,\n            it is assumed to be aliases for the column names.\n        index : bool, default True\n            Write row names (index).\n        na_rep : str, default 'NaN'\n            Missing data representation.\n        formatters : list of functions or dict of {{str: function}}, optional\n            Formatter functions to apply to columns' elements by position or\n            name. The result of each function must be a unicode string.\n            List must be of length equal to the number of columns.\n        float_format : one-parameter function or str, optional, default None\n            Formatter for floating point numbers. For example\n            ``float_format=\"%.2f\"`` and ``float_format=\"{{:0.2f}}\".format`` will\n            both result in 0.1234 being formatted as 0.12.\n        sparsify : bool, optional\n            Set to False for a DataFrame with a hierarchical index to print\n            every multiindex key at each row. By default, the value will be\n            read from the config module.\n        index_names : bool, default True\n            Prints the names of the indexes.\n        bold_rows : bool, default False\n            Make the row labels bold in the output.\n        column_format : str, optional\n            The columns format as specified in `LaTeX table format\n            <https://en.wikibooks.org/wiki/LaTeX/Tables>`__ e.g. 'rcl' for 3\n            columns. By default, 'l' will be used for all columns except\n            columns of numbers, which default to 'r'.\n        longtable : bool, optional\n            By default, the value will be read from the pandas config\n            module. Use a longtable environment instead of tabular. Requires\n            adding a \\usepackage{{longtable}} to your LaTeX preamble.\n        escape : bool, optional\n            By default, the value will be read from the pandas config\n            module. When set to False prevents from escaping latex special\n            characters in column names.\n        encoding : str, optional\n            A string representing the encoding to use in the output file,\n            defaults to 'utf-8'.\n        decimal : str, default '.'\n            Character recognized as decimal separator, e.g. ',' in Europe.\n        multicolumn : bool, default True\n            Use \\multicolumn to enhance MultiIndex columns.\n            The default will be read from the config module.\n        multicolumn_format : str, default 'l'\n            The alignment for multicolumns, similar to `column_format`\n            The default will be read from the config module.\n        multirow : bool, default False\n            Use \\multirow to enhance MultiIndex rows. Requires adding a\n            \\usepackage{{multirow}} to your LaTeX preamble. Will print\n            centered labels (instead of top-aligned) across the contained\n            rows, separating groups via clines. The default will be read\n            from the pandas config module.\n        caption : str or tuple, optional\n            Tuple (full_caption, short_caption),\n            which results in ``\\caption[short_caption]{{full_caption}}``;\n            if a single string is passed, no short caption will be set.\n\n            .. versionadded:: 1.0.0\n\n            .. versionchanged:: 1.2.0\n               Optionally allow caption to be a tuple ``(full_caption, short_caption)``.\n\n        label : str, optional\n            The LaTeX label to be placed inside ``\\label{{}}`` in the output.\n            This is used with ``\\ref{{}}`` in the main ``.tex`` file.\n\n            .. versionadded:: 1.0.0\n        position : str, optional\n            The LaTeX positional argument for tables, to be placed after\n            ``\\begin{{}}`` in the output.\n\n            .. versionadded:: 1.2.0\n        {returns}\n        See Also\n        --------\n        Styler.to_latex : Render a DataFrame to LaTeX with conditional formatting.\n        DataFrame.to_string : Render a DataFrame to a console-friendly\n            tabular output.\n        DataFrame.to_html : Render a DataFrame as an HTML table.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(dict(name=['Raphael', 'Donatello'],\n        ...                   mask=['red', 'purple'],\n        ...                   weapon=['sai', 'bo staff']))\n        >>> print(df.to_latex(index=False))  # doctest: +SKIP\n        \\begin{{tabular}}{{lll}}\n         \\toprule\n               name &    mask &    weapon \\\\\n         \\midrule\n            Raphael &     red &       sai \\\\\n          Donatello &  purple &  bo staff \\\\\n        \\bottomrule\n        \\end{{tabular}}\n        \"\"\"\n        msg = (\n            \"In future versions `DataFrame.to_latex` is expected to utilise the base \"\n            \"implementation of `Styler.to_latex` for formatting and rendering. \"\n            \"The arguments signature may therefore change. It is recommended instead \"\n            \"to use `DataFrame.style.to_latex` which also contains additional \"\n            \"functionality.\"\n        )\n        warnings.warn(msg, FutureWarning, stacklevel=find_stack_level())\n\n        # Get defaults from the pandas config\n        if self.ndim == 1:\n            self = self.to_frame()\n        if longtable is None:\n            longtable = config.get_option(\"display.latex.longtable\")\n        if escape is None:\n            escape = config.get_option(\"display.latex.escape\")\n        if multicolumn is None:\n            multicolumn = config.get_option(\"display.latex.multicolumn\")\n        if multicolumn_format is None:\n            multicolumn_format = config.get_option(\"display.latex.multicolumn_format\")\n        if multirow is None:\n            multirow = config.get_option(\"display.latex.multirow\")\n\n        self = cast(\"DataFrame\", self)\n        formatter = DataFrameFormatter(\n            self,\n            columns=columns,\n            col_space=col_space,\n            na_rep=na_rep,\n            header=header,\n            index=index,\n            formatters=formatters,\n            float_format=float_format,\n            bold_rows=bold_rows,\n            sparsify=sparsify,\n            index_names=index_names,\n            escape=escape,\n            decimal=decimal,\n        )\n        return DataFrameRenderer(formatter).to_latex(\n            buf=buf,\n            column_format=column_format,\n            longtable=longtable,\n            encoding=encoding,\n            multicolumn=multicolumn,\n            multicolumn_format=multicolumn_format,\n            multirow=multirow,\n            caption=caption,\n            label=label,\n            position=position,\n        )\n\n    @final\n    @doc(\n        storage_options=_shared_docs[\"storage_options\"],\n        compression_options=_shared_docs[\"compression_options\"],\n    )\n    def to_csv(\n        self,\n        path_or_buf: FilePath | WriteBuffer[bytes] | WriteBuffer[str] | None = None,\n        sep: str = \",\",\n        na_rep: str = \"\",\n        float_format: str | None = None,\n        columns: Sequence[Hashable] | None = None,\n        header: bool_t | list[str] = True,\n        index: bool_t = True,\n        index_label: IndexLabel | None = None,\n        mode: str = \"w\",\n        encoding: str | None = None,\n        compression: CompressionOptions = \"infer\",\n        quoting: int | None = None,\n        quotechar: str = '\"',\n        line_terminator: str | None = None,\n        chunksize: int | None = None,\n        date_format: str | None = None,\n        doublequote: bool_t = True,\n        escapechar: str | None = None,\n        decimal: str = \".\",\n        errors: str = \"strict\",\n        storage_options: StorageOptions = None,\n    ) -> str | None:\n        r\"\"\"\n        Write object to a comma-separated values (csv) file.\n\n        Parameters\n        ----------\n        path_or_buf : str, path object, file-like object, or None, default None\n            String, path object (implementing os.PathLike[str]), or file-like\n            object implementing a write() function. If None, the result is\n            returned as a string. If a non-binary file object is passed, it should\n            be opened with `newline=''`, disabling universal newlines. If a binary\n            file object is passed, `mode` might need to contain a `'b'`.\n\n            .. versionchanged:: 1.2.0\n\n               Support for binary file objects was introduced.\n\n        sep : str, default ','\n            String of length 1. Field delimiter for the output file.\n        na_rep : str, default ''\n            Missing data representation.\n        float_format : str, default None\n            Format string for floating point numbers.\n        columns : sequence, optional\n            Columns to write.\n        header : bool or list of str, default True\n            Write out the column names. If a list of strings is given it is\n            assumed to be aliases for the column names.\n        index : bool, default True\n            Write row names (index).\n        index_label : str or sequence, or False, default None\n            Column label for index column(s) if desired. If None is given, and\n            `header` and `index` are True, then the index names are used. A\n            sequence should be given if the object uses MultiIndex. If\n            False do not print fields for index names. Use index_label=False\n            for easier importing in R.\n        mode : str\n            Python write mode, default 'w'.\n        encoding : str, optional\n            A string representing the encoding to use in the output file,\n            defaults to 'utf-8'. `encoding` is not supported if `path_or_buf`\n            is a non-binary file object.\n        {compression_options}\n\n            .. versionchanged:: 1.0.0\n\n               May now be a dict with key 'method' as compression mode\n               and other entries as additional compression options if\n               compression mode is 'zip'.\n\n            .. versionchanged:: 1.1.0\n\n               Passing compression options as keys in dict is\n               supported for compression modes 'gzip', 'bz2', 'zstd', and 'zip'.\n\n            .. versionchanged:: 1.2.0\n\n                Compression is supported for binary file objects.\n\n            .. versionchanged:: 1.2.0\n\n                Previous versions forwarded dict entries for 'gzip' to\n                `gzip.open` instead of `gzip.GzipFile` which prevented\n                setting `mtime`.\n\n        quoting : optional constant from csv module\n            Defaults to csv.QUOTE_MINIMAL. If you have set a `float_format`\n            then floats are converted to strings and thus csv.QUOTE_NONNUMERIC\n            will treat them as non-numeric.\n        quotechar : str, default '\\\"'\n            String of length 1. Character used to quote fields.\n        line_terminator : str, optional\n            The newline character or character sequence to use in the output\n            file. Defaults to `os.linesep`, which depends on the OS in which\n            this method is called ('\\\\n' for linux, '\\\\r\\\\n' for Windows, i.e.).\n        chunksize : int or None\n            Rows to write at a time.\n        date_format : str, default None\n            Format string for datetime objects.\n        doublequote : bool, default True\n            Control quoting of `quotechar` inside a field.\n        escapechar : str, default None\n            String of length 1. Character used to escape `sep` and `quotechar`\n            when appropriate.\n        decimal : str, default '.'\n            Character recognized as decimal separator. E.g. use ',' for\n            European data.\n        errors : str, default 'strict'\n            Specifies how encoding and decoding errors are to be handled.\n            See the errors argument for :func:`open` for a full list\n            of options.\n\n            .. versionadded:: 1.1.0\n\n        {storage_options}\n\n            .. versionadded:: 1.2.0\n\n        Returns\n        -------\n        None or str\n            If path_or_buf is None, returns the resulting csv format as a\n            string. Otherwise returns None.\n\n        See Also\n        --------\n        read_csv : Load a CSV file into a DataFrame.\n        to_excel : Write DataFrame to an Excel file.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({{'name': ['Raphael', 'Donatello'],\n        ...                    'mask': ['red', 'purple'],\n        ...                    'weapon': ['sai', 'bo staff']}})\n        >>> df.to_csv(index=False)\n        'name,mask,weapon\\nRaphael,red,sai\\nDonatello,purple,bo staff\\n'\n\n        Create 'out.zip' containing 'out.csv'\n\n        >>> compression_opts = dict(method='zip',\n        ...                         archive_name='out.csv')  # doctest: +SKIP\n        >>> df.to_csv('out.zip', index=False,\n        ...           compression=compression_opts)  # doctest: +SKIP\n\n        To write a csv file to a new folder or nested folder you will first\n        need to create it using either Pathlib or os:\n\n        >>> from pathlib import Path  # doctest: +SKIP\n        >>> filepath = Path('folder/subfolder/out.csv')  # doctest: +SKIP\n        >>> filepath.parent.mkdir(parents=True, exist_ok=True)  # doctest: +SKIP\n        >>> df.to_csv(filepath)  # doctest: +SKIP\n\n        >>> import os  # doctest: +SKIP\n        >>> os.makedirs('folder/subfolder', exist_ok=True)  # doctest: +SKIP\n        >>> df.to_csv('folder/subfolder/out.csv')  # doctest: +SKIP\n        \"\"\"\n        df = self if isinstance(self, ABCDataFrame) else self.to_frame()\n\n        formatter = DataFrameFormatter(\n            frame=df,\n            header=header,\n            index=index,\n            na_rep=na_rep,\n            float_format=float_format,\n            decimal=decimal,\n        )\n\n        return DataFrameRenderer(formatter).to_csv(\n            path_or_buf,\n            line_terminator=line_terminator,\n            sep=sep,\n            encoding=encoding,\n            errors=errors,\n            compression=compression,\n            quoting=quoting,\n            columns=columns,\n            index_label=index_label,\n            mode=mode,\n            chunksize=chunksize,\n            quotechar=quotechar,\n            date_format=date_format,\n            doublequote=doublequote,\n            escapechar=escapechar,\n            storage_options=storage_options,\n        )\n\n    # ----------------------------------------------------------------------\n    # Lookup Caching\n\n    def _reset_cacher(self) -> None:\n        \"\"\"\n        Reset the cacher.\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    def _maybe_update_cacher(\n        self,\n        clear: bool_t = False,\n        verify_is_copy: bool_t = True,\n        inplace: bool_t = False,\n    ) -> None:\n        \"\"\"\n        See if we need to update our parent cacher if clear, then clear our\n        cache.\n\n        Parameters\n        ----------\n        clear : bool, default False\n            Clear the item cache.\n        verify_is_copy : bool, default True\n            Provide is_copy checks.\n        \"\"\"\n\n        if verify_is_copy:\n            self._check_setitem_copy(t=\"referent\")\n\n        if clear:\n            self._clear_item_cache()\n\n    def _clear_item_cache(self) -> None:\n        raise AbstractMethodError(self)\n\n    # ----------------------------------------------------------------------\n    # Indexing Methods\n\n    def take(\n        self: NDFrameT, indices, axis=0, is_copy: bool_t | None = None, **kwargs\n    ) -> NDFrameT:\n        \"\"\"\n        Return the elements in the given *positional* indices along an axis.\n\n        This means that we are not indexing according to actual values in\n        the index attribute of the object. We are indexing according to the\n        actual position of the element in the object.\n\n        Parameters\n        ----------\n        indices : array-like\n            An array of ints indicating which positions to take.\n        axis : {0 or 'index', 1 or 'columns', None}, default 0\n            The axis on which to select elements. ``0`` means that we are\n            selecting rows, ``1`` means that we are selecting columns.\n        is_copy : bool\n            Before pandas 1.0, ``is_copy=False`` can be specified to ensure\n            that the return value is an actual copy. Starting with pandas 1.0,\n            ``take`` always returns a copy, and the keyword is therefore\n            deprecated.\n\n            .. deprecated:: 1.0.0\n        **kwargs\n            For compatibility with :meth:`numpy.take`. Has no effect on the\n            output.\n\n        Returns\n        -------\n        taken : same type as caller\n            An array-like containing the elements taken from the object.\n\n        See Also\n        --------\n        DataFrame.loc : Select a subset of a DataFrame by labels.\n        DataFrame.iloc : Select a subset of a DataFrame by positions.\n        numpy.take : Take elements from an array along an axis.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([('falcon', 'bird', 389.0),\n        ...                    ('parrot', 'bird', 24.0),\n        ...                    ('lion', 'mammal', 80.5),\n        ...                    ('monkey', 'mammal', np.nan)],\n        ...                   columns=['name', 'class', 'max_speed'],\n        ...                   index=[0, 2, 3, 1])\n        >>> df\n             name   class  max_speed\n        0  falcon    bird      389.0\n        2  parrot    bird       24.0\n        3    lion  mammal       80.5\n        1  monkey  mammal        NaN\n\n        Take elements at positions 0 and 3 along the axis 0 (default).\n\n        Note how the actual indices selected (0 and 1) do not correspond to\n        our selected indices 0 and 3. That's because we are selecting the 0th\n        and 3rd rows, not rows whose indices equal 0 and 3.\n\n        >>> df.take([0, 3])\n             name   class  max_speed\n        0  falcon    bird      389.0\n        1  monkey  mammal        NaN\n\n        Take elements at indices 1 and 2 along the axis 1 (column selection).\n\n        >>> df.take([1, 2], axis=1)\n            class  max_speed\n        0    bird      389.0\n        2    bird       24.0\n        3  mammal       80.5\n        1  mammal        NaN\n\n        We may take elements using negative integers for positive indices,\n        starting from the end of the object, just like with Python lists.\n\n        >>> df.take([-1, -2])\n             name   class  max_speed\n        1  monkey  mammal        NaN\n        3    lion  mammal       80.5\n        \"\"\"\n        if is_copy is not None:\n            warnings.warn(\n                \"is_copy is deprecated and will be removed in a future version. \"\n                \"'take' always returns a copy, so there is no need to specify this.\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n\n        nv.validate_take((), kwargs)\n\n        self._consolidate_inplace()\n\n        new_data = self._mgr.take(\n            indices, axis=self._get_block_manager_axis(axis), verify=True\n        )\n        return self._constructor(new_data).__finalize__(self, method=\"take\")\n\n    def _take_with_is_copy(self: NDFrameT, indices, axis=0) -> NDFrameT:\n        \"\"\"\n        Internal version of the `take` method that sets the `_is_copy`\n        attribute to keep track of the parent dataframe (using in indexing\n        for the SettingWithCopyWarning).\n\n        See the docstring of `take` for full explanation of the parameters.\n        \"\"\"\n        result = self.take(indices=indices, axis=axis)\n        # Maybe set copy if we didn't actually change the index.\n        if not result._get_axis(axis).equals(self._get_axis(axis)):\n            result._set_is_copy(self)\n        return result\n\n    @final\n    def xs(self, key, axis=0, level=None, drop_level: bool_t = True):\n        \"\"\"\n        Return cross-section from the Series/DataFrame.\n\n        This method takes a `key` argument to select data at a particular\n        level of a MultiIndex.\n\n        Parameters\n        ----------\n        key : label or tuple of label\n            Label contained in the index, or partially in a MultiIndex.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Axis to retrieve cross-section on.\n        level : object, defaults to first n levels (n=1 or len(key))\n            In case of a key partially contained in a MultiIndex, indicate\n            which levels are used. Levels can be referred by label or position.\n        drop_level : bool, default True\n            If False, returns object with same levels as self.\n\n        Returns\n        -------\n        Series or DataFrame\n            Cross-section from the original Series or DataFrame\n            corresponding to the selected index levels.\n\n        See Also\n        --------\n        DataFrame.loc : Access a group of rows and columns\n            by label(s) or a boolean array.\n        DataFrame.iloc : Purely integer-location based indexing\n            for selection by position.\n\n        Notes\n        -----\n        `xs` can not be used to set values.\n\n        MultiIndex Slicers is a generic way to get/set values on\n        any level or levels.\n        It is a superset of `xs` functionality, see\n        :ref:`MultiIndex Slicers <advanced.mi_slicers>`.\n\n        Examples\n        --------\n        >>> d = {'num_legs': [4, 4, 2, 2],\n        ...      'num_wings': [0, 0, 2, 2],\n        ...      'class': ['mammal', 'mammal', 'mammal', 'bird'],\n        ...      'animal': ['cat', 'dog', 'bat', 'penguin'],\n        ...      'locomotion': ['walks', 'walks', 'flies', 'walks']}\n        >>> df = pd.DataFrame(data=d)\n        >>> df = df.set_index(['class', 'animal', 'locomotion'])\n        >>> df\n                                   num_legs  num_wings\n        class  animal  locomotion\n        mammal cat     walks              4          0\n               dog     walks              4          0\n               bat     flies              2          2\n        bird   penguin walks              2          2\n\n        Get values at specified index\n\n        >>> df.xs('mammal')\n                           num_legs  num_wings\n        animal locomotion\n        cat    walks              4          0\n        dog    walks              4          0\n        bat    flies              2          2\n\n        Get values at several indexes\n\n        >>> df.xs(('mammal', 'dog'))\n                    num_legs  num_wings\n        locomotion\n        walks              4          0\n\n        Get values at specified index and level\n\n        >>> df.xs('cat', level=1)\n                           num_legs  num_wings\n        class  locomotion\n        mammal walks              4          0\n\n        Get values at several indexes and levels\n\n        >>> df.xs(('bird', 'walks'),\n        ...       level=[0, 'locomotion'])\n                 num_legs  num_wings\n        animal\n        penguin         2          2\n\n        Get values at specified column and axis\n\n        >>> df.xs('num_wings', axis=1)\n        class   animal   locomotion\n        mammal  cat      walks         0\n                dog      walks         0\n                bat      flies         2\n        bird    penguin  walks         2\n        Name: num_wings, dtype: int64\n        \"\"\"\n        axis = self._get_axis_number(axis)\n        labels = self._get_axis(axis)\n\n        if isinstance(key, list):\n            warnings.warn(\n                \"Passing lists as key for xs is deprecated and will be removed in a \"\n                \"future version. Pass key as a tuple instead.\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n\n        if level is not None:\n            if not isinstance(labels, MultiIndex):\n                raise TypeError(\"Index must be a MultiIndex\")\n            loc, new_ax = labels.get_loc_level(key, level=level, drop_level=drop_level)\n\n            # create the tuple of the indexer\n            _indexer = [slice(None)] * self.ndim\n            _indexer[axis] = loc\n            indexer = tuple(_indexer)\n\n            result = self.iloc[indexer]\n            setattr(result, result._get_axis_name(axis), new_ax)\n            return result\n\n        if axis == 1:\n            if drop_level:\n                return self[key]\n            index = self.columns\n        else:\n            index = self.index\n\n        self._consolidate_inplace()\n\n        if isinstance(index, MultiIndex):\n            loc, new_index = index._get_loc_level(key, level=0)\n            if not drop_level:\n                if lib.is_integer(loc):\n                    new_index = index[loc : loc + 1]\n                else:\n                    new_index = index[loc]\n        else:\n            loc = index.get_loc(key)\n\n            if isinstance(loc, np.ndarray):\n                if loc.dtype == np.bool_:\n                    (inds,) = loc.nonzero()\n                    return self._take_with_is_copy(inds, axis=axis)\n                else:\n                    return self._take_with_is_copy(loc, axis=axis)\n\n            if not is_scalar(loc):\n                new_index = index[loc]\n\n        if is_scalar(loc) and axis == 0:\n            # In this case loc should be an integer\n            if self.ndim == 1:\n                # if we encounter an array-like and we only have 1 dim\n                # that means that their are list/ndarrays inside the Series!\n                # so just return them (GH 6394)\n                return self._values[loc]\n\n            new_values = self._mgr.fast_xs(loc)\n\n            result = self._constructor_sliced(\n                new_values,\n                index=self.columns,\n                name=self.index[loc],\n                dtype=new_values.dtype,\n            )\n        elif is_scalar(loc):\n            result = self.iloc[:, slice(loc, loc + 1)]\n        elif axis == 1:\n            result = self.iloc[:, loc]\n        else:\n            result = self.iloc[loc]\n            result.index = new_index\n\n        # this could be a view\n        # but only in a single-dtyped view sliceable case\n        result._set_is_copy(self, copy=not result._is_view)\n        return result\n\n    def __getitem__(self, item):\n        raise AbstractMethodError(self)\n\n    def _slice(self: NDFrameT, slobj: slice, axis=0) -> NDFrameT:\n        \"\"\"\n        Construct a slice of this container.\n\n        Slicing with this method is *always* positional.\n        \"\"\"\n        assert isinstance(slobj, slice), type(slobj)\n        axis = self._get_block_manager_axis(axis)\n        result = self._constructor(self._mgr.get_slice(slobj, axis=axis))\n        result = result.__finalize__(self)\n\n        # this could be a view\n        # but only in a single-dtyped view sliceable case\n        is_copy = axis != 0 or result._is_view\n        result._set_is_copy(self, copy=is_copy)\n        return result\n\n    @final\n    def _set_is_copy(self, ref: NDFrame, copy: bool_t = True) -> None:\n        if not copy:\n            self._is_copy = None\n        else:\n            assert ref is not None\n            self._is_copy = weakref.ref(ref)\n\n    def _check_is_chained_assignment_possible(self) -> bool_t:\n        \"\"\"\n        Check if we are a view, have a cacher, and are of mixed type.\n        If so, then force a setitem_copy check.\n\n        Should be called just near setting a value\n\n        Will return a boolean if it we are a view and are cached, but a\n        single-dtype meaning that the cacher should be updated following\n        setting.\n        \"\"\"\n        if self._is_copy:\n            self._check_setitem_copy(t=\"referent\")\n        return False\n\n    @final\n    def _check_setitem_copy(self, t=\"setting\", force=False):\n        \"\"\"\n\n        Parameters\n        ----------\n        t : str, the type of setting error\n        force : bool, default False\n           If True, then force showing an error.\n\n        validate if we are doing a setitem on a chained copy.\n\n        It is technically possible to figure out that we are setting on\n        a copy even WITH a multi-dtyped pandas object. In other words, some\n        blocks may be views while other are not. Currently _is_view will ALWAYS\n        return False for multi-blocks to avoid having to handle this case.\n\n        df = DataFrame(np.arange(0,9), columns=['count'])\n        df['group'] = 'b'\n\n        # This technically need not raise SettingWithCopy if both are view\n        # (which is not generally guaranteed but is usually True.  However,\n        # this is in general not a good practice and we recommend using .loc.\n        df.iloc[0:5]['group'] = 'a'\n\n        \"\"\"\n        # return early if the check is not needed\n        if not (force or self._is_copy):\n            return\n\n        value = config.get_option(\"mode.chained_assignment\")\n        if value is None:\n            return\n\n        # see if the copy is not actually referred; if so, then dissolve\n        # the copy weakref\n        if self._is_copy is not None and not isinstance(self._is_copy, str):\n            r = self._is_copy()\n            if not gc.get_referents(r) or (r is not None and r.shape == self.shape):\n                self._is_copy = None\n                return\n\n        # a custom message\n        if isinstance(self._is_copy, str):\n            t = self._is_copy\n\n        elif t == \"referent\":\n            t = (\n                \"\\n\"\n                \"A value is trying to be set on a copy of a slice from a \"\n                \"DataFrame\\n\\n\"\n                \"See the caveats in the documentation: \"\n                \"https://pandas.pydata.org/pandas-docs/stable/user_guide/\"\n                \"indexing.html#returning-a-view-versus-a-copy\"\n            )\n\n        else:\n            t = (\n                \"\\n\"\n                \"A value is trying to be set on a copy of a slice from a \"\n                \"DataFrame.\\n\"\n                \"Try using .loc[row_indexer,col_indexer] = value \"\n                \"instead\\n\\nSee the caveats in the documentation: \"\n                \"https://pandas.pydata.org/pandas-docs/stable/user_guide/\"\n                \"indexing.html#returning-a-view-versus-a-copy\"\n            )\n\n        if value == \"raise\":\n            raise com.SettingWithCopyError(t)\n        elif value == \"warn\":\n            warnings.warn(t, com.SettingWithCopyWarning, stacklevel=find_stack_level())\n\n    def __delitem__(self, key) -> None:\n        \"\"\"\n        Delete item\n        \"\"\"\n        deleted = False\n\n        maybe_shortcut = False\n        if self.ndim == 2 and isinstance(self.columns, MultiIndex):\n            try:\n                # By using engine's __contains__ we effectively\n                # restrict to same-length tuples\n                maybe_shortcut = key not in self.columns._engine\n            except TypeError:\n                pass\n\n        if maybe_shortcut:\n            # Allow shorthand to delete all columns whose first len(key)\n            # elements match key:\n            if not isinstance(key, tuple):\n                key = (key,)\n            for col in self.columns:\n                if isinstance(col, tuple) and col[: len(key)] == key:\n                    del self[col]\n                    deleted = True\n        if not deleted:\n            # If the above loop ran and didn't delete anything because\n            # there was no match, this call should raise the appropriate\n            # exception:\n            loc = self.axes[-1].get_loc(key)\n            self._mgr = self._mgr.idelete(loc)\n\n        # delete from the caches\n        try:\n            del self._item_cache[key]\n        except KeyError:\n            pass\n\n    # ----------------------------------------------------------------------\n    # Unsorted\n\n    @final\n    def _check_inplace_and_allows_duplicate_labels(self, inplace):\n        if inplace and not self.flags.allows_duplicate_labels:\n            raise ValueError(\n                \"Cannot specify 'inplace=True' when \"\n                \"'self.flags.allows_duplicate_labels' is False.\"\n            )\n\n    @final\n    def get(self, key, default=None):\n        \"\"\"\n        Get item from object for given key (ex: DataFrame column).\n\n        Returns default value if not found.\n\n        Parameters\n        ----------\n        key : object\n\n        Returns\n        -------\n        value : same type as items contained in object\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(\n        ...     [\n        ...         [24.3, 75.7, \"high\"],\n        ...         [31, 87.8, \"high\"],\n        ...         [22, 71.6, \"medium\"],\n        ...         [35, 95, \"medium\"],\n        ...     ],\n        ...     columns=[\"temp_celsius\", \"temp_fahrenheit\", \"windspeed\"],\n        ...     index=pd.date_range(start=\"2014-02-12\", end=\"2014-02-15\", freq=\"D\"),\n        ... )\n\n        >>> df\n                    temp_celsius  temp_fahrenheit windspeed\n        2014-02-12          24.3             75.7      high\n        2014-02-13          31.0             87.8      high\n        2014-02-14          22.0             71.6    medium\n        2014-02-15          35.0             95.0    medium\n\n        >>> df.get([\"temp_celsius\", \"windspeed\"])\n                    temp_celsius windspeed\n        2014-02-12          24.3      high\n        2014-02-13          31.0      high\n        2014-02-14          22.0    medium\n        2014-02-15          35.0    medium\n\n        If the key isn't found, the default value will be used.\n\n        >>> df.get([\"temp_celsius\", \"temp_kelvin\"], default=\"default_value\")\n        'default_value'\n        \"\"\"\n        try:\n            return self[key]\n        except (KeyError, ValueError, IndexError):\n            return default\n\n    @final\n    @property\n    def _is_view(self) -> bool_t:\n        \"\"\"Return boolean indicating if self is view of another array\"\"\"\n        return self._mgr.is_view\n\n    @final\n    def reindex_like(\n        self: NDFrameT,\n        other,\n        method: str | None = None,\n        copy: bool_t = True,\n        limit=None,\n        tolerance=None,\n    ) -> NDFrameT:\n        \"\"\"\n        Return an object with matching indices as other object.\n\n        Conform the object to the same index on all axes. Optional\n        filling logic, placing NaN in locations having no value\n        in the previous index. A new object is produced unless the\n        new index is equivalent to the current one and copy=False.\n\n        Parameters\n        ----------\n        other : Object of the same data type\n            Its row and column indices are used to define the new indices\n            of this object.\n        method : {None, 'backfill'/'bfill', 'pad'/'ffill', 'nearest'}\n            Method to use for filling holes in reindexed DataFrame.\n            Please note: this is only applicable to DataFrames/Series with a\n            monotonically increasing/decreasing index.\n\n            * None (default): don't fill gaps\n            * pad / ffill: propagate last valid observation forward to next\n              valid\n            * backfill / bfill: use next valid observation to fill gap\n            * nearest: use nearest valid observations to fill gap.\n\n        copy : bool, default True\n            Return a new object, even if the passed indexes are the same.\n        limit : int, default None\n            Maximum number of consecutive labels to fill for inexact matches.\n        tolerance : optional\n            Maximum distance between original and new labels for inexact\n            matches. The values of the index at the matching locations must\n            satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n\n            Tolerance may be a scalar value, which applies the same tolerance\n            to all values, or list-like, which applies variable tolerance per\n            element. List-like includes list, tuple, array, Series, and must be\n            the same size as the index and its dtype must exactly match the\n            index's type.\n\n        Returns\n        -------\n        Series or DataFrame\n            Same type as caller, but with changed indices on each axis.\n\n        See Also\n        --------\n        DataFrame.set_index : Set row labels.\n        DataFrame.reset_index : Remove row labels or move them to new columns.\n        DataFrame.reindex : Change to new indices or expand indices.\n\n        Notes\n        -----\n        Same as calling\n        ``.reindex(index=other.index, columns=other.columns,...)``.\n\n        Examples\n        --------\n        >>> df1 = pd.DataFrame([[24.3, 75.7, 'high'],\n        ...                     [31, 87.8, 'high'],\n        ...                     [22, 71.6, 'medium'],\n        ...                     [35, 95, 'medium']],\n        ...                    columns=['temp_celsius', 'temp_fahrenheit',\n        ...                             'windspeed'],\n        ...                    index=pd.date_range(start='2014-02-12',\n        ...                                        end='2014-02-15', freq='D'))\n\n        >>> df1\n                    temp_celsius  temp_fahrenheit windspeed\n        2014-02-12          24.3             75.7      high\n        2014-02-13          31.0             87.8      high\n        2014-02-14          22.0             71.6    medium\n        2014-02-15          35.0             95.0    medium\n\n        >>> df2 = pd.DataFrame([[28, 'low'],\n        ...                     [30, 'low'],\n        ...                     [35.1, 'medium']],\n        ...                    columns=['temp_celsius', 'windspeed'],\n        ...                    index=pd.DatetimeIndex(['2014-02-12', '2014-02-13',\n        ...                                            '2014-02-15']))\n\n        >>> df2\n                    temp_celsius windspeed\n        2014-02-12          28.0       low\n        2014-02-13          30.0       low\n        2014-02-15          35.1    medium\n\n        >>> df2.reindex_like(df1)\n                    temp_celsius  temp_fahrenheit windspeed\n        2014-02-12          28.0              NaN       low\n        2014-02-13          30.0              NaN       low\n        2014-02-14           NaN              NaN       NaN\n        2014-02-15          35.1              NaN    medium\n        \"\"\"\n        d = other._construct_axes_dict(\n            axes=self._AXIS_ORDERS,\n            method=method,\n            copy=copy,\n            limit=limit,\n            tolerance=tolerance,\n        )\n\n        return self.reindex(**d)\n\n    def drop(\n        self,\n        labels=None,\n        axis=0,\n        index=None,\n        columns=None,\n        level=None,\n        inplace: bool_t = False,\n        errors: str = \"raise\",\n    ):\n\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n\n        if labels is not None:\n            if index is not None or columns is not None:\n                raise ValueError(\"Cannot specify both 'labels' and 'index'/'columns'\")\n            axis_name = self._get_axis_name(axis)\n            axes = {axis_name: labels}\n        elif index is not None or columns is not None:\n            axes, _ = self._construct_axes_from_arguments((index, columns), {})\n        else:\n            raise ValueError(\n                \"Need to specify at least one of 'labels', 'index' or 'columns'\"\n            )\n\n        obj = self\n\n        for axis, labels in axes.items():\n            if labels is not None:\n                obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n\n        if inplace:\n            self._update_inplace(obj)\n        else:\n            return obj\n\n    @final\n    def _drop_axis(\n        self: NDFrameT,\n        labels,\n        axis,\n        level=None,\n        errors: str = \"raise\",\n        consolidate: bool_t = True,\n        only_slice: bool_t = False,\n    ) -> NDFrameT:\n        \"\"\"\n        Drop labels from specified axis. Used in the ``drop`` method\n        internally.\n\n        Parameters\n        ----------\n        labels : single label or list-like\n        axis : int or axis name\n        level : int or level name, default None\n            For MultiIndex\n        errors : {'ignore', 'raise'}, default 'raise'\n            If 'ignore', suppress error and existing labels are dropped.\n        consolidate : bool, default True\n            Whether to call consolidate_inplace in the reindex_indexer call.\n        only_slice : bool, default False\n            Whether indexing along columns should be view-only.\n\n        \"\"\"\n        axis_num = self._get_axis_number(axis)\n        axis = self._get_axis(axis)\n\n        if axis.is_unique:\n            if level is not None:\n                if not isinstance(axis, MultiIndex):\n                    raise AssertionError(\"axis must be a MultiIndex\")\n                new_axis = axis.drop(labels, level=level, errors=errors)\n            else:\n                new_axis = axis.drop(labels, errors=errors)\n            indexer = axis.get_indexer(new_axis)\n\n        # Case for non-unique axis\n        else:\n            is_tuple_labels = is_nested_list_like(labels) or isinstance(labels, tuple)\n            labels = ensure_object(com.index_labels_to_array(labels))\n            if level is not None:\n                if not isinstance(axis, MultiIndex):\n                    raise AssertionError(\"axis must be a MultiIndex\")\n                mask = ~axis.get_level_values(level).isin(labels)\n\n                # GH 18561 MultiIndex.drop should raise if label is absent\n                if errors == \"raise\" and mask.all():\n                    raise KeyError(f\"{labels} not found in axis\")\n            elif (\n                isinstance(axis, MultiIndex)\n                and labels.dtype == \"object\"\n                and not is_tuple_labels\n            ):\n                # Set level to zero in case of MultiIndex and label is string,\n                #  because isin can't handle strings for MultiIndexes GH#36293\n                # In case of tuples we get dtype object but have to use isin GH#42771\n                mask = ~axis.get_level_values(0).isin(labels)\n            else:\n                mask = ~axis.isin(labels)\n                # Check if label doesn't exist along axis\n                labels_missing = (axis.get_indexer_for(labels) == -1).any()\n                if errors == \"raise\" and labels_missing:\n                    raise KeyError(f\"{labels} not found in axis\")\n\n            if is_extension_array_dtype(mask.dtype):\n                # GH#45860\n                mask = mask.to_numpy(dtype=bool)\n\n            indexer = mask.nonzero()[0]\n            new_axis = axis.take(indexer)\n\n        bm_axis = self.ndim - axis_num - 1\n        new_mgr = self._mgr.reindex_indexer(\n            new_axis,\n            indexer,\n            axis=bm_axis,\n            allow_dups=True,\n            consolidate=consolidate,\n            only_slice=only_slice,\n        )\n        result = self._constructor(new_mgr)\n        if self.ndim == 1:\n            result.name = self.name\n\n        return result.__finalize__(self)\n\n    @final\n    def _update_inplace(self, result, verify_is_copy: bool_t = True) -> None:\n        \"\"\"\n        Replace self internals with result.\n\n        Parameters\n        ----------\n        result : same type as self\n        verify_is_copy : bool, default True\n            Provide is_copy checks.\n        \"\"\"\n        # NOTE: This does *not* call __finalize__ and that's an explicit\n        # decision that we may revisit in the future.\n        self._reset_cache()\n        self._clear_item_cache()\n        self._mgr = result._mgr\n        self._maybe_update_cacher(verify_is_copy=verify_is_copy, inplace=True)\n\n    @final\n    def add_prefix(self: NDFrameT, prefix: str) -> NDFrameT:\n        \"\"\"\n        Prefix labels with string `prefix`.\n\n        For Series, the row labels are prefixed.\n        For DataFrame, the column labels are prefixed.\n\n        Parameters\n        ----------\n        prefix : str\n            The string to add before each label.\n\n        Returns\n        -------\n        Series or DataFrame\n            New Series or DataFrame with updated labels.\n\n        See Also\n        --------\n        Series.add_suffix: Suffix row labels with string `suffix`.\n        DataFrame.add_suffix: Suffix column labels with string `suffix`.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3, 4])\n        >>> s\n        0    1\n        1    2\n        2    3\n        3    4\n        dtype: int64\n\n        >>> s.add_prefix('item_')\n        item_0    1\n        item_1    2\n        item_2    3\n        item_3    4\n        dtype: int64\n\n        >>> df = pd.DataFrame({'A': [1, 2, 3, 4], 'B': [3, 4, 5, 6]})\n        >>> df\n           A  B\n        0  1  3\n        1  2  4\n        2  3  5\n        3  4  6\n\n        >>> df.add_prefix('col_')\n             col_A  col_B\n        0       1       3\n        1       2       4\n        2       3       5\n        3       4       6\n        \"\"\"\n        f = functools.partial(\"{prefix}{}\".format, prefix=prefix)\n\n        mapper = {self._info_axis_name: f}\n        # error: Incompatible return value type (got \"Optional[NDFrameT]\",\n        # expected \"NDFrameT\")\n        # error: Argument 1 to \"rename\" of \"NDFrame\" has incompatible type\n        # \"**Dict[str, partial[str]]\"; expected \"Union[str, int, None]\"\n        return self._rename(**mapper)  # type: ignore[return-value, arg-type]\n\n    @final\n    def add_suffix(self: NDFrameT, suffix: str) -> NDFrameT:\n        \"\"\"\n        Suffix labels with string `suffix`.\n\n        For Series, the row labels are suffixed.\n        For DataFrame, the column labels are suffixed.\n\n        Parameters\n        ----------\n        suffix : str\n            The string to add after each label.\n\n        Returns\n        -------\n        Series or DataFrame\n            New Series or DataFrame with updated labels.\n\n        See Also\n        --------\n        Series.add_prefix: Prefix row labels with string `prefix`.\n        DataFrame.add_prefix: Prefix column labels with string `prefix`.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3, 4])\n        >>> s\n        0    1\n        1    2\n        2    3\n        3    4\n        dtype: int64\n\n        >>> s.add_suffix('_item')\n        0_item    1\n        1_item    2\n        2_item    3\n        3_item    4\n        dtype: int64\n\n        >>> df = pd.DataFrame({'A': [1, 2, 3, 4], 'B': [3, 4, 5, 6]})\n        >>> df\n           A  B\n        0  1  3\n        1  2  4\n        2  3  5\n        3  4  6\n\n        >>> df.add_suffix('_col')\n             A_col  B_col\n        0       1       3\n        1       2       4\n        2       3       5\n        3       4       6\n        \"\"\"\n        f = functools.partial(\"{}{suffix}\".format, suffix=suffix)\n\n        mapper = {self._info_axis_name: f}\n        # error: Incompatible return value type (got \"Optional[NDFrameT]\",\n        # expected \"NDFrameT\")\n        # error: Argument 1 to \"rename\" of \"NDFrame\" has incompatible type\n        # \"**Dict[str, partial[str]]\"; expected \"Union[str, int, None]\"\n        return self._rename(**mapper)  # type: ignore[return-value, arg-type]\n\n    def sort_values(\n        self,\n        axis=0,\n        ascending=True,\n        inplace: bool_t = False,\n        kind: str = \"quicksort\",\n        na_position: str = \"last\",\n        ignore_index: bool_t = False,\n        key: ValueKeyFunc = None,\n    ):\n        \"\"\"\n        Sort by the values along either axis.\n\n        Parameters\n        ----------%(optional_by)s\n        axis : %(axes_single_arg)s, default 0\n             Axis to be sorted.\n        ascending : bool or list of bool, default True\n             Sort ascending vs. descending. Specify list for multiple sort\n             orders.  If this is a list of bools, must match the length of\n             the by.\n        inplace : bool, default False\n             If True, perform operation in-place.\n        kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, default 'quicksort'\n             Choice of sorting algorithm. See also :func:`numpy.sort` for more\n             information. `mergesort` and `stable` are the only stable algorithms. For\n             DataFrames, this option is only applied when sorting on a single\n             column or label.\n        na_position : {'first', 'last'}, default 'last'\n             Puts NaNs at the beginning if `first`; `last` puts NaNs at the\n             end.\n        ignore_index : bool, default False\n             If True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\n             .. versionadded:: 1.0.0\n\n        key : callable, optional\n            Apply the key function to the values\n            before sorting. This is similar to the `key` argument in the\n            builtin :meth:`sorted` function, with the notable difference that\n            this `key` function should be *vectorized*. It should expect a\n            ``Series`` and return a Series with the same shape as the input.\n            It will be applied to each column in `by` independently.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        DataFrame or None\n            DataFrame with sorted values or None if ``inplace=True``.\n\n        See Also\n        --------\n        DataFrame.sort_index : Sort a DataFrame by the index.\n        Series.sort_values : Similar method for a Series.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({\n        ...     'col1': ['A', 'A', 'B', np.nan, 'D', 'C'],\n        ...     'col2': [2, 1, 9, 8, 7, 4],\n        ...     'col3': [0, 1, 9, 4, 2, 3],\n        ...     'col4': ['a', 'B', 'c', 'D', 'e', 'F']\n        ... })\n        >>> df\n          col1  col2  col3 col4\n        0    A     2     0    a\n        1    A     1     1    B\n        2    B     9     9    c\n        3  NaN     8     4    D\n        4    D     7     2    e\n        5    C     4     3    F\n\n        Sort by col1\n\n        >>> df.sort_values(by=['col1'])\n          col1  col2  col3 col4\n        0    A     2     0    a\n        1    A     1     1    B\n        2    B     9     9    c\n        5    C     4     3    F\n        4    D     7     2    e\n        3  NaN     8     4    D\n\n        Sort by multiple columns\n\n        >>> df.sort_values(by=['col1', 'col2'])\n          col1  col2  col3 col4\n        1    A     1     1    B\n        0    A     2     0    a\n        2    B     9     9    c\n        5    C     4     3    F\n        4    D     7     2    e\n        3  NaN     8     4    D\n\n        Sort Descending\n\n        >>> df.sort_values(by='col1', ascending=False)\n          col1  col2  col3 col4\n        4    D     7     2    e\n        5    C     4     3    F\n        2    B     9     9    c\n        0    A     2     0    a\n        1    A     1     1    B\n        3  NaN     8     4    D\n\n        Putting NAs first\n\n        >>> df.sort_values(by='col1', ascending=False, na_position='first')\n          col1  col2  col3 col4\n        3  NaN     8     4    D\n        4    D     7     2    e\n        5    C     4     3    F\n        2    B     9     9    c\n        0    A     2     0    a\n        1    A     1     1    B\n\n        Sorting with a key function\n\n        >>> df.sort_values(by='col4', key=lambda col: col.str.lower())\n           col1  col2  col3 col4\n        0    A     2     0    a\n        1    A     1     1    B\n        2    B     9     9    c\n        3  NaN     8     4    D\n        4    D     7     2    e\n        5    C     4     3    F\n\n        Natural sort with the key argument,\n        using the `natsort <https://github.com/SethMMorton/natsort>` package.\n\n        >>> df = pd.DataFrame({\n        ...    \"time\": ['0hr', '128hr', '72hr', '48hr', '96hr'],\n        ...    \"value\": [10, 20, 30, 40, 50]\n        ... })\n        >>> df\n            time  value\n        0    0hr     10\n        1  128hr     20\n        2   72hr     30\n        3   48hr     40\n        4   96hr     50\n        >>> from natsort import index_natsorted\n        >>> df.sort_values(\n        ...    by=\"time\",\n        ...    key=lambda x: np.argsort(index_natsorted(df[\"time\"]))\n        ... )\n            time  value\n        0    0hr     10\n        3   48hr     40\n        2   72hr     30\n        4   96hr     50\n        1  128hr     20\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    def sort_index(\n        self,\n        axis=0,\n        level=None,\n        ascending: bool_t | int | Sequence[bool_t | int] = True,\n        inplace: bool_t = False,\n        kind: str = \"quicksort\",\n        na_position: str = \"last\",\n        sort_remaining: bool_t = True,\n        ignore_index: bool_t = False,\n        key: IndexKeyFunc = None,\n    ):\n\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        axis = self._get_axis_number(axis)\n        ascending = validate_ascending(ascending)\n\n        target = self._get_axis(axis)\n\n        indexer = get_indexer_indexer(\n            target, level, ascending, kind, na_position, sort_remaining, key\n        )\n\n        if indexer is None:\n            if inplace:\n                result = self\n            else:\n                result = self.copy()\n\n            if ignore_index:\n                result.index = default_index(len(self))\n            if inplace:\n                return\n            else:\n                return result\n\n        baxis = self._get_block_manager_axis(axis)\n        new_data = self._mgr.take(indexer, axis=baxis, verify=False)\n\n        # reconstruct axis if needed\n        new_data.set_axis(baxis, new_data.axes[baxis]._sort_levels_monotonic())\n\n        if ignore_index:\n            axis = 1 if isinstance(self, ABCDataFrame) else 0\n            new_data.set_axis(axis, default_index(len(indexer)))\n\n        result = self._constructor(new_data)\n\n        if inplace:\n            return self._update_inplace(result)\n        else:\n            return result.__finalize__(self, method=\"sort_index\")\n\n    @doc(\n        klass=_shared_doc_kwargs[\"klass\"],\n        axes=_shared_doc_kwargs[\"axes\"],\n        optional_labels=\"\",\n        optional_axis=\"\",\n    )\n    def reindex(self: NDFrameT, *args, **kwargs) -> NDFrameT:\n        \"\"\"\n        Conform {klass} to new index with optional filling logic.\n\n        Places NA/NaN in locations having no value in the previous index. A new object\n        is produced unless the new index is equivalent to the current one and\n        ``copy=False``.\n\n        Parameters\n        ----------\n        {optional_labels}\n        {axes} : array-like, optional\n            New labels / index to conform to, should be specified using\n            keywords. Preferably an Index object to avoid duplicating data.\n        {optional_axis}\n        method : {{None, 'backfill'/'bfill', 'pad'/'ffill', 'nearest'}}\n            Method to use for filling holes in reindexed DataFrame.\n            Please note: this is only applicable to DataFrames/Series with a\n            monotonically increasing/decreasing index.\n\n            * None (default): don't fill gaps\n            * pad / ffill: Propagate last valid observation forward to next\n              valid.\n            * backfill / bfill: Use next valid observation to fill gap.\n            * nearest: Use nearest valid observations to fill gap.\n\n        copy : bool, default True\n            Return a new object, even if the passed indexes are the same.\n        level : int or name\n            Broadcast across a level, matching Index values on the\n            passed MultiIndex level.\n        fill_value : scalar, default np.NaN\n            Value to use for missing values. Defaults to NaN, but can be any\n            \"compatible\" value.\n        limit : int, default None\n            Maximum number of consecutive elements to forward or backward fill.\n        tolerance : optional\n            Maximum distance between original and new labels for inexact\n            matches. The values of the index at the matching locations most\n            satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n\n            Tolerance may be a scalar value, which applies the same tolerance\n            to all values, or list-like, which applies variable tolerance per\n            element. List-like includes list, tuple, array, Series, and must be\n            the same size as the index and its dtype must exactly match the\n            index's type.\n\n        Returns\n        -------\n        {klass} with changed index.\n\n        See Also\n        --------\n        DataFrame.set_index : Set row labels.\n        DataFrame.reset_index : Remove row labels or move them to new columns.\n        DataFrame.reindex_like : Change to same indices as other DataFrame.\n\n        Examples\n        --------\n        ``DataFrame.reindex`` supports two calling conventions\n\n        * ``(index=index_labels, columns=column_labels, ...)``\n        * ``(labels, axis={{'index', 'columns'}}, ...)``\n\n        We *highly* recommend using keyword arguments to clarify your\n        intent.\n\n        Create a dataframe with some fictional data.\n\n        >>> index = ['Firefox', 'Chrome', 'Safari', 'IE10', 'Konqueror']\n        >>> df = pd.DataFrame({{'http_status': [200, 200, 404, 404, 301],\n        ...                   'response_time': [0.04, 0.02, 0.07, 0.08, 1.0]}},\n        ...                   index=index)\n        >>> df\n                   http_status  response_time\n        Firefox            200           0.04\n        Chrome             200           0.02\n        Safari             404           0.07\n        IE10               404           0.08\n        Konqueror          301           1.00\n\n        Create a new index and reindex the dataframe. By default\n        values in the new index that do not have corresponding\n        records in the dataframe are assigned ``NaN``.\n\n        >>> new_index = ['Safari', 'Iceweasel', 'Comodo Dragon', 'IE10',\n        ...              'Chrome']\n        >>> df.reindex(new_index)\n                       http_status  response_time\n        Safari               404.0           0.07\n        Iceweasel              NaN            NaN\n        Comodo Dragon          NaN            NaN\n        IE10                 404.0           0.08\n        Chrome               200.0           0.02\n\n        We can fill in the missing values by passing a value to\n        the keyword ``fill_value``. Because the index is not monotonically\n        increasing or decreasing, we cannot use arguments to the keyword\n        ``method`` to fill the ``NaN`` values.\n\n        >>> df.reindex(new_index, fill_value=0)\n                       http_status  response_time\n        Safari                 404           0.07\n        Iceweasel                0           0.00\n        Comodo Dragon            0           0.00\n        IE10                   404           0.08\n        Chrome                 200           0.02\n\n        >>> df.reindex(new_index, fill_value='missing')\n                      http_status response_time\n        Safari                404          0.07\n        Iceweasel         missing       missing\n        Comodo Dragon     missing       missing\n        IE10                  404          0.08\n        Chrome                200          0.02\n\n        We can also reindex the columns.\n\n        >>> df.reindex(columns=['http_status', 'user_agent'])\n                   http_status  user_agent\n        Firefox            200         NaN\n        Chrome             200         NaN\n        Safari             404         NaN\n        IE10               404         NaN\n        Konqueror          301         NaN\n\n        Or we can use \"axis-style\" keyword arguments\n\n        >>> df.reindex(['http_status', 'user_agent'], axis=\"columns\")\n                   http_status  user_agent\n        Firefox            200         NaN\n        Chrome             200         NaN\n        Safari             404         NaN\n        IE10               404         NaN\n        Konqueror          301         NaN\n\n        To further illustrate the filling functionality in\n        ``reindex``, we will create a dataframe with a\n        monotonically increasing index (for example, a sequence\n        of dates).\n\n        >>> date_index = pd.date_range('1/1/2010', periods=6, freq='D')\n        >>> df2 = pd.DataFrame({{\"prices\": [100, 101, np.nan, 100, 89, 88]}},\n        ...                    index=date_index)\n        >>> df2\n                    prices\n        2010-01-01   100.0\n        2010-01-02   101.0\n        2010-01-03     NaN\n        2010-01-04   100.0\n        2010-01-05    89.0\n        2010-01-06    88.0\n\n        Suppose we decide to expand the dataframe to cover a wider\n        date range.\n\n        >>> date_index2 = pd.date_range('12/29/2009', periods=10, freq='D')\n        >>> df2.reindex(date_index2)\n                    prices\n        2009-12-29     NaN\n        2009-12-30     NaN\n        2009-12-31     NaN\n        2010-01-01   100.0\n        2010-01-02   101.0\n        2010-01-03     NaN\n        2010-01-04   100.0\n        2010-01-05    89.0\n        2010-01-06    88.0\n        2010-01-07     NaN\n\n        The index entries that did not have a value in the original data frame\n        (for example, '2009-12-29') are by default filled with ``NaN``.\n        If desired, we can fill in the missing values using one of several\n        options.\n\n        For example, to back-propagate the last valid value to fill the ``NaN``\n        values, pass ``bfill`` as an argument to the ``method`` keyword.\n\n        >>> df2.reindex(date_index2, method='bfill')\n                    prices\n        2009-12-29   100.0\n        2009-12-30   100.0\n        2009-12-31   100.0\n        2010-01-01   100.0\n        2010-01-02   101.0\n        2010-01-03     NaN\n        2010-01-04   100.0\n        2010-01-05    89.0\n        2010-01-06    88.0\n        2010-01-07     NaN\n\n        Please note that the ``NaN`` value present in the original dataframe\n        (at index value 2010-01-03) will not be filled by any of the\n        value propagation schemes. This is because filling while reindexing\n        does not look at dataframe values, but only compares the original and\n        desired indexes. If you do want to fill in the ``NaN`` values present\n        in the original dataframe, use the ``fillna()`` method.\n\n        See the :ref:`user guide <basics.reindexing>` for more.\n        \"\"\"\n        # TODO: Decide if we care about having different examples for different\n        # kinds\n\n        # construct the args\n        axes, kwargs = self._construct_axes_from_arguments(args, kwargs)\n        method = missing.clean_reindex_fill_method(kwargs.pop(\"method\", None))\n        level = kwargs.pop(\"level\", None)\n        copy = kwargs.pop(\"copy\", True)\n        limit = kwargs.pop(\"limit\", None)\n        tolerance = kwargs.pop(\"tolerance\", None)\n        fill_value = kwargs.pop(\"fill_value\", None)\n\n        # Series.reindex doesn't use / need the axis kwarg\n        # We pop and ignore it here, to make writing Series/Frame generic code\n        # easier\n        kwargs.pop(\"axis\", None)\n\n        if kwargs:\n            raise TypeError(\n                \"reindex() got an unexpected keyword \"\n                f'argument \"{list(kwargs.keys())[0]}\"'\n            )\n\n        self._consolidate_inplace()\n\n        # if all axes that are requested to reindex are equal, then only copy\n        # if indicated must have index names equal here as well as values\n        if all(\n            self._get_axis(axis).identical(ax)\n            for axis, ax in axes.items()\n            if ax is not None\n        ):\n            if copy:\n                return self.copy()\n            return self\n\n        # check if we are a multi reindex\n        if self._needs_reindex_multi(axes, method, level):\n            return self._reindex_multi(axes, copy, fill_value)\n\n        # perform the reindex on the axes\n        return self._reindex_axes(\n            axes, level, limit, tolerance, method, fill_value, copy\n        ).__finalize__(self, method=\"reindex\")\n\n    def _reindex_axes(\n        self: NDFrameT, axes, level, limit, tolerance, method, fill_value, copy\n    ) -> NDFrameT:\n        \"\"\"Perform the reindex for all the axes.\"\"\"\n        obj = self\n        for a in self._AXIS_ORDERS:\n            labels = axes[a]\n            if labels is None:\n                continue\n\n            ax = self._get_axis(a)\n            new_index, indexer = ax.reindex(\n                labels, level=level, limit=limit, tolerance=tolerance, method=method\n            )\n\n            axis = self._get_axis_number(a)\n            obj = obj._reindex_with_indexers(\n                {axis: [new_index, indexer]},\n                fill_value=fill_value,\n                copy=copy,\n                allow_dups=False,\n            )\n            # If we've made a copy once, no need to make another one\n            copy = False\n\n        return obj\n\n    def _needs_reindex_multi(self, axes, method, level) -> bool_t:\n        \"\"\"Check if we do need a multi reindex.\"\"\"\n        return (\n            (com.count_not_none(*axes.values()) == self._AXIS_LEN)\n            and method is None\n            and level is None\n            and not self._is_mixed_type\n        )\n\n    def _reindex_multi(self, axes, copy, fill_value):\n        raise AbstractMethodError(self)\n\n    @final\n    def _reindex_with_indexers(\n        self: NDFrameT,\n        reindexers,\n        fill_value=None,\n        copy: bool_t = False,\n        allow_dups: bool_t = False,\n    ) -> NDFrameT:\n        \"\"\"allow_dups indicates an internal call here\"\"\"\n        # reindex doing multiple operations on different axes if indicated\n        new_data = self._mgr\n        for axis in sorted(reindexers.keys()):\n            index, indexer = reindexers[axis]\n            baxis = self._get_block_manager_axis(axis)\n\n            if index is None:\n                continue\n\n            index = ensure_index(index)\n            if indexer is not None:\n                indexer = ensure_platform_int(indexer)\n\n            # TODO: speed up on homogeneous DataFrame objects (see _reindex_multi)\n            new_data = new_data.reindex_indexer(\n                index,\n                indexer,\n                axis=baxis,\n                fill_value=fill_value,\n                allow_dups=allow_dups,\n                copy=copy,\n            )\n            # If we've made a copy once, no need to make another one\n            copy = False\n\n        if copy and new_data is self._mgr:\n            new_data = new_data.copy()\n\n        return self._constructor(new_data).__finalize__(self)\n\n    def filter(\n        self: NDFrameT,\n        items=None,\n        like: str | None = None,\n        regex: str | None = None,\n        axis=None,\n    ) -> NDFrameT:\n        \"\"\"\n        Subset the dataframe rows or columns according to the specified index labels.\n\n        Note that this routine does not filter a dataframe on its\n        contents. The filter is applied to the labels of the index.\n\n        Parameters\n        ----------\n        items : list-like\n            Keep labels from axis which are in items.\n        like : str\n            Keep labels from axis for which \"like in label == True\".\n        regex : str (regular expression)\n            Keep labels from axis for which re.search(regex, label) == True.\n        axis : {0 or \u2018index\u2019, 1 or \u2018columns\u2019, None}, default None\n            The axis to filter on, expressed either as an index (int)\n            or axis name (str). By default this is the info axis,\n            'index' for Series, 'columns' for DataFrame.\n\n        Returns\n        -------\n        same type as input object\n\n        See Also\n        --------\n        DataFrame.loc : Access a group of rows and columns\n            by label(s) or a boolean array.\n\n        Notes\n        -----\n        The ``items``, ``like``, and ``regex`` parameters are\n        enforced to be mutually exclusive.\n\n        ``axis`` defaults to the info axis that is used when indexing\n        with ``[]``.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(np.array(([1, 2, 3], [4, 5, 6])),\n        ...                   index=['mouse', 'rabbit'],\n        ...                   columns=['one', 'two', 'three'])\n        >>> df\n                one  two  three\n        mouse     1    2      3\n        rabbit    4    5      6\n\n        >>> # select columns by name\n        >>> df.filter(items=['one', 'three'])\n                 one  three\n        mouse     1      3\n        rabbit    4      6\n\n        >>> # select columns by regular expression\n        >>> df.filter(regex='e$', axis=1)\n                 one  three\n        mouse     1      3\n        rabbit    4      6\n\n        >>> # select rows containing 'bbi'\n        >>> df.filter(like='bbi', axis=0)\n                 one  two  three\n        rabbit    4    5      6\n        \"\"\"\n        nkw = com.count_not_none(items, like, regex)\n        if nkw > 1:\n            raise TypeError(\n                \"Keyword arguments `items`, `like`, or `regex` \"\n                \"are mutually exclusive\"\n            )\n\n        if axis is None:\n            axis = self._info_axis_name\n        labels = self._get_axis(axis)\n\n        if items is not None:\n            name = self._get_axis_name(axis)\n            return self.reindex(**{name: [r for r in items if r in labels]})\n        elif like:\n\n            def f(x) -> bool_t:\n                assert like is not None  # needed for mypy\n                return like in ensure_str(x)\n\n            values = labels.map(f)\n            return self.loc(axis=axis)[values]\n        elif regex:\n\n            def f(x) -> bool_t:\n                return matcher.search(ensure_str(x)) is not None\n\n            matcher = re.compile(regex)\n            values = labels.map(f)\n            return self.loc(axis=axis)[values]\n        else:\n            raise TypeError(\"Must pass either `items`, `like`, or `regex`\")\n\n    @final\n    def head(self: NDFrameT, n: int = 5) -> NDFrameT:\n        \"\"\"\n        Return the first `n` rows.\n\n        This function returns the first `n` rows for the object based\n        on position. It is useful for quickly testing if your object\n        has the right type of data in it.\n\n        For negative values of `n`, this function returns all rows except\n        the last `n` rows, equivalent to ``df[:-n]``.\n\n        Parameters\n        ----------\n        n : int, default 5\n            Number of rows to select.\n\n        Returns\n        -------\n        same type as caller\n            The first `n` rows of the caller object.\n\n        See Also\n        --------\n        DataFrame.tail: Returns the last `n` rows.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'animal': ['alligator', 'bee', 'falcon', 'lion',\n        ...                    'monkey', 'parrot', 'shark', 'whale', 'zebra']})\n        >>> df\n              animal\n        0  alligator\n        1        bee\n        2     falcon\n        3       lion\n        4     monkey\n        5     parrot\n        6      shark\n        7      whale\n        8      zebra\n\n        Viewing the first 5 lines\n\n        >>> df.head()\n              animal\n        0  alligator\n        1        bee\n        2     falcon\n        3       lion\n        4     monkey\n\n        Viewing the first `n` lines (three in this case)\n\n        >>> df.head(3)\n              animal\n        0  alligator\n        1        bee\n        2     falcon\n\n        For negative values of `n`\n\n        >>> df.head(-3)\n              animal\n        0  alligator\n        1        bee\n        2     falcon\n        3       lion\n        4     monkey\n        5     parrot\n        \"\"\"\n        return self.iloc[:n]\n\n    @final\n    def tail(self: NDFrameT, n: int = 5) -> NDFrameT:\n        \"\"\"\n        Return the last `n` rows.\n\n        This function returns last `n` rows from the object based on\n        position. It is useful for quickly verifying data, for example,\n        after sorting or appending rows.\n\n        For negative values of `n`, this function returns all rows except\n        the first `n` rows, equivalent to ``df[n:]``.\n\n        Parameters\n        ----------\n        n : int, default 5\n            Number of rows to select.\n\n        Returns\n        -------\n        type of caller\n            The last `n` rows of the caller object.\n\n        See Also\n        --------\n        DataFrame.head : The first `n` rows of the caller object.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'animal': ['alligator', 'bee', 'falcon', 'lion',\n        ...                    'monkey', 'parrot', 'shark', 'whale', 'zebra']})\n        >>> df\n              animal\n        0  alligator\n        1        bee\n        2     falcon\n        3       lion\n        4     monkey\n        5     parrot\n        6      shark\n        7      whale\n        8      zebra\n\n        Viewing the last 5 lines\n\n        >>> df.tail()\n           animal\n        4  monkey\n        5  parrot\n        6   shark\n        7   whale\n        8   zebra\n\n        Viewing the last `n` lines (three in this case)\n\n        >>> df.tail(3)\n          animal\n        6  shark\n        7  whale\n        8  zebra\n\n        For negative values of `n`\n\n        >>> df.tail(-3)\n           animal\n        3    lion\n        4  monkey\n        5  parrot\n        6   shark\n        7   whale\n        8   zebra\n        \"\"\"\n        if n == 0:\n            return self.iloc[0:0]\n        return self.iloc[-n:]\n\n    @final\n    def sample(\n        self: NDFrameT,\n        n: int | None = None,\n        frac: float | None = None,\n        replace: bool_t = False,\n        weights=None,\n        random_state: RandomState | None = None,\n        axis: Axis | None = None,\n        ignore_index: bool_t = False,\n    ) -> NDFrameT:\n        \"\"\"\n        Return a random sample of items from an axis of object.\n\n        You can use `random_state` for reproducibility.\n\n        Parameters\n        ----------\n        n : int, optional\n            Number of items from axis to return. Cannot be used with `frac`.\n            Default = 1 if `frac` = None.\n        frac : float, optional\n            Fraction of axis items to return. Cannot be used with `n`.\n        replace : bool, default False\n            Allow or disallow sampling of the same row more than once.\n        weights : str or ndarray-like, optional\n            Default 'None' results in equal probability weighting.\n            If passed a Series, will align with target object on index. Index\n            values in weights not found in sampled object will be ignored and\n            index values in sampled object not in weights will be assigned\n            weights of zero.\n            If called on a DataFrame, will accept the name of a column\n            when axis = 0.\n            Unless weights are a Series, weights must be same length as axis\n            being sampled.\n            If weights do not sum to 1, they will be normalized to sum to 1.\n            Missing values in the weights column will be treated as zero.\n            Infinite values not allowed.\n        random_state : int, array-like, BitGenerator, np.random.RandomState, np.random.Generator, optional\n            If int, array-like, or BitGenerator, seed for random number generator.\n            If np.random.RandomState or np.random.Generator, use as given.\n\n            .. versionchanged:: 1.1.0\n\n                array-like and BitGenerator object now passed to np.random.RandomState()\n                as seed\n\n            .. versionchanged:: 1.4.0\n\n                np.random.Generator objects now accepted\n\n        axis : {0 or \u2018index\u2019, 1 or \u2018columns\u2019, None}, default None\n            Axis to sample. Accepts axis number or name. Default is stat axis\n            for given data type (0 for Series and DataFrames).\n        ignore_index : bool, default False\n            If True, the resulting index will be labeled 0, 1, \u2026, n - 1.\n\n            .. versionadded:: 1.3.0\n\n        Returns\n        -------\n        Series or DataFrame\n            A new object of same type as caller containing `n` items randomly\n            sampled from the caller object.\n\n        See Also\n        --------\n        DataFrameGroupBy.sample: Generates random samples from each group of a\n            DataFrame object.\n        SeriesGroupBy.sample: Generates random samples from each group of a\n            Series object.\n        numpy.random.choice: Generates a random sample from a given 1-D numpy\n            array.\n\n        Notes\n        -----\n        If `frac` > 1, `replacement` should be set to `True`.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'num_legs': [2, 4, 8, 0],\n        ...                    'num_wings': [2, 0, 0, 0],\n        ...                    'num_specimen_seen': [10, 2, 1, 8]},\n        ...                   index=['falcon', 'dog', 'spider', 'fish'])\n        >>> df\n                num_legs  num_wings  num_specimen_seen\n        falcon         2          2                 10\n        dog            4          0                  2\n        spider         8          0                  1\n        fish           0          0                  8\n\n        Extract 3 random elements from the ``Series`` ``df['num_legs']``:\n        Note that we use `random_state` to ensure the reproducibility of\n        the examples.\n\n        >>> df['num_legs'].sample(n=3, random_state=1)\n        fish      0\n        spider    8\n        falcon    2\n        Name: num_legs, dtype: int64\n\n        A random 50% sample of the ``DataFrame`` with replacement:\n\n        >>> df.sample(frac=0.5, replace=True, random_state=1)\n              num_legs  num_wings  num_specimen_seen\n        dog          4          0                  2\n        fish         0          0                  8\n\n        An upsample sample of the ``DataFrame`` with replacement:\n        Note that `replace` parameter has to be `True` for `frac` parameter > 1.\n\n        >>> df.sample(frac=2, replace=True, random_state=1)\n                num_legs  num_wings  num_specimen_seen\n        dog            4          0                  2\n        fish           0          0                  8\n        falcon         2          2                 10\n        falcon         2          2                 10\n        fish           0          0                  8\n        dog            4          0                  2\n        fish           0          0                  8\n        dog            4          0                  2\n\n        Using a DataFrame column as weights. Rows with larger value in the\n        `num_specimen_seen` column are more likely to be sampled.\n\n        >>> df.sample(n=2, weights='num_specimen_seen', random_state=1)\n                num_legs  num_wings  num_specimen_seen\n        falcon         2          2                 10\n        fish           0          0                  8\n        \"\"\"  # noqa:E501\n        if axis is None:\n            axis = self._stat_axis_number\n\n        axis = self._get_axis_number(axis)\n        obj_len = self.shape[axis]\n\n        # Process random_state argument\n        rs = com.random_state(random_state)\n\n        size = sample.process_sampling_size(n, frac, replace)\n        if size is None:\n            assert frac is not None\n            size = round(frac * obj_len)\n\n        if weights is not None:\n            weights = sample.preprocess_weights(self, weights, axis)\n\n        sampled_indices = sample.sample(obj_len, size, replace, weights, rs)\n        result = self.take(sampled_indices, axis=axis)\n\n        if ignore_index:\n            result.index = default_index(len(result))\n\n        return result\n\n    @final\n    @doc(klass=_shared_doc_kwargs[\"klass\"])\n    def pipe(\n        self,\n        func: Callable[..., T] | tuple[Callable[..., T], str],\n        *args,\n        **kwargs,\n    ) -> T:\n        r\"\"\"\n        Apply chainable functions that expect Series or DataFrames.\n\n        Parameters\n        ----------\n        func : function\n            Function to apply to the {klass}.\n            ``args``, and ``kwargs`` are passed into ``func``.\n            Alternatively a ``(callable, data_keyword)`` tuple where\n            ``data_keyword`` is a string indicating the keyword of\n            ``callable`` that expects the {klass}.\n        args : iterable, optional\n            Positional arguments passed into ``func``.\n        kwargs : mapping, optional\n            A dictionary of keyword arguments passed into ``func``.\n\n        Returns\n        -------\n        object : the return type of ``func``.\n\n        See Also\n        --------\n        DataFrame.apply : Apply a function along input axis of DataFrame.\n        DataFrame.applymap : Apply a function elementwise on a whole DataFrame.\n        Series.map : Apply a mapping correspondence on a\n            :class:`~pandas.Series`.\n\n        Notes\n        -----\n        Use ``.pipe`` when chaining together functions that expect\n        Series, DataFrames or GroupBy objects. Instead of writing\n\n        >>> func(g(h(df), arg1=a), arg2=b, arg3=c)  # doctest: +SKIP\n\n        You can write\n\n        >>> (df.pipe(h)\n        ...    .pipe(g, arg1=a)\n        ...    .pipe(func, arg2=b, arg3=c)\n        ... )  # doctest: +SKIP\n\n        If you have a function that takes the data as (say) the second\n        argument, pass a tuple indicating which keyword expects the\n        data. For example, suppose ``f`` takes its data as ``arg2``:\n\n        >>> (df.pipe(h)\n        ...    .pipe(g, arg1=a)\n        ...    .pipe((func, 'arg2'), arg1=a, arg3=c)\n        ...  )  # doctest: +SKIP\n        \"\"\"\n        return com.pipe(self, func, *args, **kwargs)\n\n    # ----------------------------------------------------------------------\n    # Attribute access\n\n    @final\n    def __finalize__(\n        self: NDFrameT, other, method: str | None = None, **kwargs\n    ) -> NDFrameT:\n        \"\"\"\n        Propagate metadata from other to self.\n\n        Parameters\n        ----------\n        other : the object from which to get the attributes that we are going\n            to propagate\n        method : str, optional\n            A passed method name providing context on where ``__finalize__``\n            was called.\n\n            .. warning::\n\n               The value passed as `method` are not currently considered\n               stable across pandas releases.\n        \"\"\"\n        if isinstance(other, NDFrame):\n            for name in other.attrs:\n                self.attrs[name] = other.attrs[name]\n\n            self.flags.allows_duplicate_labels = other.flags.allows_duplicate_labels\n            # For subclasses using _metadata.\n            for name in set(self._metadata) & set(other._metadata):\n                assert isinstance(name, str)\n                object.__setattr__(self, name, getattr(other, name, None))\n\n        if method == \"concat\":\n            attrs = other.objs[0].attrs\n            check_attrs = all(objs.attrs == attrs for objs in other.objs[1:])\n            if check_attrs:\n                for name in attrs:\n                    self.attrs[name] = attrs[name]\n\n            allows_duplicate_labels = all(\n                x.flags.allows_duplicate_labels for x in other.objs\n            )\n            self.flags.allows_duplicate_labels = allows_duplicate_labels\n\n        return self\n\n    def __getattr__(self, name: str):\n        \"\"\"\n        After regular attribute access, try looking up the name\n        This allows simpler access to columns for interactive use.\n        \"\"\"\n        # Note: obj.x will always call obj.__getattribute__('x') prior to\n        # calling obj.__getattr__('x').\n        if (\n            name not in self._internal_names_set\n            and name not in self._metadata\n            and name not in self._accessors\n            and self._info_axis._can_hold_identifiers_and_holds_name(name)\n        ):\n            return self[name]\n        return object.__getattribute__(self, name)\n\n    def __setattr__(self, name: str, value) -> None:\n        \"\"\"\n        After regular attribute access, try setting the name\n        This allows simpler access to columns for interactive use.\n        \"\"\"\n        # first try regular attribute access via __getattribute__, so that\n        # e.g. ``obj.x`` and ``obj.x = 4`` will always reference/modify\n        # the same attribute.\n\n        try:\n            object.__getattribute__(self, name)\n            return object.__setattr__(self, name, value)\n        except AttributeError:\n            pass\n\n        # if this fails, go on to more involved attribute setting\n        # (note that this matches __getattr__, above).\n        if name in self._internal_names_set:\n            object.__setattr__(self, name, value)\n        elif name in self._metadata:\n            object.__setattr__(self, name, value)\n        else:\n            try:\n                existing = getattr(self, name)\n                if isinstance(existing, Index):\n                    object.__setattr__(self, name, value)\n                elif name in self._info_axis:\n                    self[name] = value\n                else:\n                    object.__setattr__(self, name, value)\n            except (AttributeError, TypeError):\n                if isinstance(self, ABCDataFrame) and (is_list_like(value)):\n                    warnings.warn(\n                        \"Pandas doesn't allow columns to be \"\n                        \"created via a new attribute name - see \"\n                        \"https://pandas.pydata.org/pandas-docs/\"\n                        \"stable/indexing.html#attribute-access\",\n                        stacklevel=find_stack_level(),\n                    )\n                object.__setattr__(self, name, value)\n\n    @final\n    def _dir_additions(self) -> set[str]:\n        \"\"\"\n        add the string-like attributes from the info_axis.\n        If info_axis is a MultiIndex, its first level values are used.\n        \"\"\"\n        additions = super()._dir_additions()\n        if self._info_axis._can_hold_strings:\n            additions.update(self._info_axis._dir_additions_for_owner)\n        return additions\n\n    # ----------------------------------------------------------------------\n    # Consolidation of internals\n\n    @final\n    def _protect_consolidate(self, f):\n        \"\"\"\n        Consolidate _mgr -- if the blocks have changed, then clear the\n        cache\n        \"\"\"\n        if isinstance(self._mgr, (ArrayManager, SingleArrayManager)):\n            return f()\n        blocks_before = len(self._mgr.blocks)\n        result = f()\n        if len(self._mgr.blocks) != blocks_before:\n            self._clear_item_cache()\n        return result\n\n    @final\n    def _consolidate_inplace(self) -> None:\n        \"\"\"Consolidate data in place and return None\"\"\"\n\n        def f():\n            self._mgr = self._mgr.consolidate()\n\n        self._protect_consolidate(f)\n\n    @final\n    def _consolidate(self):\n        \"\"\"\n        Compute NDFrame with \"consolidated\" internals (data of each dtype\n        grouped together in a single ndarray).\n\n        Returns\n        -------\n        consolidated : same type as caller\n        \"\"\"\n        f = lambda: self._mgr.consolidate()\n        cons_data = self._protect_consolidate(f)\n        return self._constructor(cons_data).__finalize__(self)\n\n    @final\n    @property\n    def _is_mixed_type(self) -> bool_t:\n        if self._mgr.is_single_block:\n            return False\n\n        if self._mgr.any_extension_types:\n            # Even if they have the same dtype, we can't consolidate them,\n            #  so we pretend this is \"mixed'\"\n            return True\n\n        return self.dtypes.nunique() > 1\n\n    @final\n    def _check_inplace_setting(self, value) -> bool_t:\n        \"\"\"check whether we allow in-place setting with this type of value\"\"\"\n        if self._is_mixed_type and not self._mgr.is_numeric_mixed_type:\n\n            # allow an actual np.nan thru\n            if is_float(value) and np.isnan(value):\n                return True\n\n            raise TypeError(\n                \"Cannot do inplace boolean setting on \"\n                \"mixed-types with a non np.nan value\"\n            )\n\n        return True\n\n    @final\n    def _get_numeric_data(self):\n        return self._constructor(self._mgr.get_numeric_data()).__finalize__(self)\n\n    @final\n    def _get_bool_data(self):\n        return self._constructor(self._mgr.get_bool_data()).__finalize__(self)\n\n    # ----------------------------------------------------------------------\n    # Internal Interface Methods\n\n    @property\n    def values(self) -> np.ndarray:\n        raise AbstractMethodError(self)\n\n    @property\n    def _values(self) -> np.ndarray:\n        \"\"\"internal implementation\"\"\"\n        raise AbstractMethodError(self)\n\n    @property\n    def dtypes(self):\n        \"\"\"\n        Return the dtypes in the DataFrame.\n\n        This returns a Series with the data type of each column.\n        The result's index is the original DataFrame's columns. Columns\n        with mixed types are stored with the ``object`` dtype. See\n        :ref:`the User Guide <basics.dtypes>` for more.\n\n        Returns\n        -------\n        pandas.Series\n            The data type of each column.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'float': [1.0],\n        ...                    'int': [1],\n        ...                    'datetime': [pd.Timestamp('20180310')],\n        ...                    'string': ['foo']})\n        >>> df.dtypes\n        float              float64\n        int                  int64\n        datetime    datetime64[ns]\n        string              object\n        dtype: object\n        \"\"\"\n        data = self._mgr.get_dtypes()\n        return self._constructor_sliced(data, index=self._info_axis, dtype=np.object_)\n\n    def astype(\n        self: NDFrameT, dtype, copy: bool_t = True, errors: str = \"raise\"\n    ) -> NDFrameT:\n        \"\"\"\n        Cast a pandas object to a specified dtype ``dtype``.\n\n        Parameters\n        ----------\n        dtype : data type, or dict of column name -> data type\n            Use a numpy.dtype or Python type to cast entire pandas object to\n            the same type. Alternatively, use {col: dtype, ...}, where col is a\n            column label and dtype is a numpy.dtype or Python type to cast one\n            or more of the DataFrame's columns to column-specific types.\n        copy : bool, default True\n            Return a copy when ``copy=True`` (be very careful setting\n            ``copy=False`` as changes to values then may propagate to other\n            pandas objects).\n        errors : {'raise', 'ignore'}, default 'raise'\n            Control raising of exceptions on invalid data for provided dtype.\n\n            - ``raise`` : allow exceptions to be raised\n            - ``ignore`` : suppress exceptions. On error return original object.\n\n        Returns\n        -------\n        casted : same type as caller\n\n        See Also\n        --------\n        to_datetime : Convert argument to datetime.\n        to_timedelta : Convert argument to timedelta.\n        to_numeric : Convert argument to a numeric type.\n        numpy.ndarray.astype : Cast a numpy array to a specified type.\n\n        Notes\n        -----\n        .. deprecated:: 1.3.0\n\n            Using ``astype`` to convert from timezone-naive dtype to\n            timezone-aware dtype is deprecated and will raise in a\n            future version.  Use :meth:`Series.dt.tz_localize` instead.\n\n        Examples\n        --------\n        Create a DataFrame:\n\n        >>> d = {'col1': [1, 2], 'col2': [3, 4]}\n        >>> df = pd.DataFrame(data=d)\n        >>> df.dtypes\n        col1    int64\n        col2    int64\n        dtype: object\n\n        Cast all columns to int32:\n\n        >>> df.astype('int32').dtypes\n        col1    int32\n        col2    int32\n        dtype: object\n\n        Cast col1 to int32 using a dictionary:\n\n        >>> df.astype({'col1': 'int32'}).dtypes\n        col1    int32\n        col2    int64\n        dtype: object\n\n        Create a series:\n\n        >>> ser = pd.Series([1, 2], dtype='int32')\n        >>> ser\n        0    1\n        1    2\n        dtype: int32\n        >>> ser.astype('int64')\n        0    1\n        1    2\n        dtype: int64\n\n        Convert to categorical type:\n\n        >>> ser.astype('category')\n        0    1\n        1    2\n        dtype: category\n        Categories (2, int64): [1, 2]\n\n        Convert to ordered categorical type with custom ordering:\n\n        >>> from pandas.api.types import CategoricalDtype\n        >>> cat_dtype = CategoricalDtype(\n        ...     categories=[2, 1], ordered=True)\n        >>> ser.astype(cat_dtype)\n        0    1\n        1    2\n        dtype: category\n        Categories (2, int64): [2 < 1]\n\n        Note that using ``copy=False`` and changing data on a new\n        pandas object may propagate changes:\n\n        >>> s1 = pd.Series([1, 2])\n        >>> s2 = s1.astype('int64', copy=False)\n        >>> s2[0] = 10\n        >>> s1  # note that s1[0] has changed too\n        0    10\n        1     2\n        dtype: int64\n\n        Create a series of dates:\n\n        >>> ser_date = pd.Series(pd.date_range('20200101', periods=3))\n        >>> ser_date\n        0   2020-01-01\n        1   2020-01-02\n        2   2020-01-03\n        dtype: datetime64[ns]\n        \"\"\"\n        if is_dict_like(dtype):\n            if self.ndim == 1:  # i.e. Series\n                if len(dtype) > 1 or self.name not in dtype:\n                    raise KeyError(\n                        \"Only the Series name can be used for \"\n                        \"the key in Series dtype mappings.\"\n                    )\n                new_type = dtype[self.name]\n                return self.astype(new_type, copy, errors)\n\n            for col_name in dtype.keys():\n                if col_name not in self:\n                    raise KeyError(\n                        \"Only a column name can be used for the \"\n                        \"key in a dtype mappings argument. \"\n                        f\"'{col_name}' not found in columns.\"\n                    )\n\n            # GH#44417 cast to Series so we can use .iat below, which will be\n            #  robust in case we\n            from pandas import Series\n\n            dtype_ser = Series(dtype, dtype=object)\n            dtype_ser = dtype_ser.reindex(self.columns, fill_value=None, copy=False)\n\n            results = []\n            for i, (col_name, col) in enumerate(self.items()):\n                cdt = dtype_ser.iat[i]\n                if isna(cdt):\n                    res_col = col.copy() if copy else col\n                else:\n                    res_col = col.astype(dtype=cdt, copy=copy, errors=errors)\n                results.append(res_col)\n\n        elif is_extension_array_dtype(dtype) and self.ndim > 1:\n            # GH 18099/22869: columnwise conversion to extension dtype\n            # GH 24704: use iloc to handle duplicate column names\n            # TODO(EA2D): special case not needed with 2D EAs\n            results = [\n                self.iloc[:, i].astype(dtype, copy=copy)\n                for i in range(len(self.columns))\n            ]\n\n        else:\n            # else, only a single dtype is given\n            new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)\n            return self._constructor(new_data).__finalize__(self, method=\"astype\")\n\n        # GH 33113: handle empty frame or series\n        if not results:\n            return self.copy()\n\n        # GH 19920: retain column metadata after concat\n        result = concat(results, axis=1, copy=False)\n        result.columns = self.columns\n        result = result.__finalize__(self, method=\"astype\")\n        # https://github.com/python/mypy/issues/8354\n        return cast(NDFrameT, result)\n\n    @final\n    def copy(self: NDFrameT, deep: bool_t = True) -> NDFrameT:\n        \"\"\"\n        Make a copy of this object's indices and data.\n\n        When ``deep=True`` (default), a new object will be created with a\n        copy of the calling object's data and indices. Modifications to\n        the data or indices of the copy will not be reflected in the\n        original object (see notes below).\n\n        When ``deep=False``, a new object will be created without copying\n        the calling object's data or index (only references to the data\n        and index are copied). Any changes to the data of the original\n        will be reflected in the shallow copy (and vice versa).\n\n        Parameters\n        ----------\n        deep : bool, default True\n            Make a deep copy, including a copy of the data and the indices.\n            With ``deep=False`` neither the indices nor the data are copied.\n\n        Returns\n        -------\n        copy : Series or DataFrame\n            Object type matches caller.\n\n        Notes\n        -----\n        When ``deep=True``, data is copied but actual Python objects\n        will not be copied recursively, only the reference to the object.\n        This is in contrast to `copy.deepcopy` in the Standard Library,\n        which recursively copies object data (see examples below).\n\n        While ``Index`` objects are copied when ``deep=True``, the underlying\n        numpy array is not copied for performance reasons. Since ``Index`` is\n        immutable, the underlying data can be safely shared and a copy\n        is not needed.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2], index=[\"a\", \"b\"])\n        >>> s\n        a    1\n        b    2\n        dtype: int64\n\n        >>> s_copy = s.copy()\n        >>> s_copy\n        a    1\n        b    2\n        dtype: int64\n\n        **Shallow copy versus default (deep) copy:**\n\n        >>> s = pd.Series([1, 2], index=[\"a\", \"b\"])\n        >>> deep = s.copy()\n        >>> shallow = s.copy(deep=False)\n\n        Shallow copy shares data and index with original.\n\n        >>> s is shallow\n        False\n        >>> s.values is shallow.values and s.index is shallow.index\n        True\n\n        Deep copy has own copy of data and index.\n\n        >>> s is deep\n        False\n        >>> s.values is deep.values or s.index is deep.index\n        False\n\n        Updates to the data shared by shallow copy and original is reflected\n        in both; deep copy remains unchanged.\n\n        >>> s[0] = 3\n        >>> shallow[1] = 4\n        >>> s\n        a    3\n        b    4\n        dtype: int64\n        >>> shallow\n        a    3\n        b    4\n        dtype: int64\n        >>> deep\n        a    1\n        b    2\n        dtype: int64\n\n        Note that when copying an object containing Python objects, a deep copy\n        will copy the data, but will not do so recursively. Updating a nested\n        data object will be reflected in the deep copy.\n\n        >>> s = pd.Series([[1, 2], [3, 4]])\n        >>> deep = s.copy()\n        >>> s[0][0] = 10\n        >>> s\n        0    [10, 2]\n        1     [3, 4]\n        dtype: object\n        >>> deep\n        0    [10, 2]\n        1     [3, 4]\n        dtype: object\n        \"\"\"\n        data = self._mgr.copy(deep=deep)\n        self._clear_item_cache()\n        return self._constructor(data).__finalize__(self, method=\"copy\")\n\n    @final\n    def __copy__(self: NDFrameT, deep: bool_t = True) -> NDFrameT:\n        return self.copy(deep=deep)\n\n    @final\n    def __deepcopy__(self: NDFrameT, memo=None) -> NDFrameT:\n        \"\"\"\n        Parameters\n        ----------\n        memo, default None\n            Standard signature. Unused\n        \"\"\"\n        return self.copy(deep=True)\n\n    @final\n    def _convert(\n        self: NDFrameT,\n        datetime: bool_t = False,\n        numeric: bool_t = False,\n        timedelta: bool_t = False,\n    ) -> NDFrameT:\n        \"\"\"\n        Attempt to infer better dtype for object columns.\n\n        Parameters\n        ----------\n        datetime : bool, default False\n            If True, convert to date where possible.\n        numeric : bool, default False\n            If True, attempt to convert to numbers (including strings), with\n            unconvertible values becoming NaN.\n        timedelta : bool, default False\n            If True, convert to timedelta where possible.\n\n        Returns\n        -------\n        converted : same as input object\n        \"\"\"\n        validate_bool_kwarg(datetime, \"datetime\")\n        validate_bool_kwarg(numeric, \"numeric\")\n        validate_bool_kwarg(timedelta, \"timedelta\")\n        return self._constructor(\n            self._mgr.convert(\n                datetime=datetime,\n                numeric=numeric,\n                timedelta=timedelta,\n                copy=True,\n            )\n        ).__finalize__(self)\n\n    @final\n    def infer_objects(self: NDFrameT) -> NDFrameT:\n        \"\"\"\n        Attempt to infer better dtypes for object columns.\n\n        Attempts soft conversion of object-dtyped\n        columns, leaving non-object and unconvertible\n        columns unchanged. The inference rules are the\n        same as during normal Series/DataFrame construction.\n\n        Returns\n        -------\n        converted : same type as input object\n\n        See Also\n        --------\n        to_datetime : Convert argument to datetime.\n        to_timedelta : Convert argument to timedelta.\n        to_numeric : Convert argument to numeric type.\n        convert_dtypes : Convert argument to best possible dtype.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({\"A\": [\"a\", 1, 2, 3]})\n        >>> df = df.iloc[1:]\n        >>> df\n           A\n        1  1\n        2  2\n        3  3\n\n        >>> df.dtypes\n        A    object\n        dtype: object\n\n        >>> df.infer_objects().dtypes\n        A    int64\n        dtype: object\n        \"\"\"\n        # numeric=False necessary to only soft convert;\n        # python objects will still be converted to\n        # native numpy numeric types\n        return self._constructor(\n            self._mgr.convert(datetime=True, numeric=False, timedelta=True, copy=True)\n        ).__finalize__(self, method=\"infer_objects\")\n\n    @final\n    def convert_dtypes(\n        self: NDFrameT,\n        infer_objects: bool_t = True,\n        convert_string: bool_t = True,\n        convert_integer: bool_t = True,\n        convert_boolean: bool_t = True,\n        convert_floating: bool_t = True,\n    ) -> NDFrameT:\n        \"\"\"\n        Convert columns to best possible dtypes using dtypes supporting ``pd.NA``.\n\n        .. versionadded:: 1.0.0\n\n        Parameters\n        ----------\n        infer_objects : bool, default True\n            Whether object dtypes should be converted to the best possible types.\n        convert_string : bool, default True\n            Whether object dtypes should be converted to ``StringDtype()``.\n        convert_integer : bool, default True\n            Whether, if possible, conversion can be done to integer extension types.\n        convert_boolean : bool, defaults True\n            Whether object dtypes should be converted to ``BooleanDtypes()``.\n        convert_floating : bool, defaults True\n            Whether, if possible, conversion can be done to floating extension types.\n            If `convert_integer` is also True, preference will be give to integer\n            dtypes if the floats can be faithfully casted to integers.\n\n            .. versionadded:: 1.2.0\n\n        Returns\n        -------\n        Series or DataFrame\n            Copy of input object with new dtype.\n\n        See Also\n        --------\n        infer_objects : Infer dtypes of objects.\n        to_datetime : Convert argument to datetime.\n        to_timedelta : Convert argument to timedelta.\n        to_numeric : Convert argument to a numeric type.\n\n        Notes\n        -----\n        By default, ``convert_dtypes`` will attempt to convert a Series (or each\n        Series in a DataFrame) to dtypes that support ``pd.NA``. By using the options\n        ``convert_string``, ``convert_integer``, ``convert_boolean`` and\n        ``convert_boolean``, it is possible to turn off individual conversions\n        to ``StringDtype``, the integer extension types, ``BooleanDtype``\n        or floating extension types, respectively.\n\n        For object-dtyped columns, if ``infer_objects`` is ``True``, use the inference\n        rules as during normal Series/DataFrame construction.  Then, if possible,\n        convert to ``StringDtype``, ``BooleanDtype`` or an appropriate integer\n        or floating extension type, otherwise leave as ``object``.\n\n        If the dtype is integer, convert to an appropriate integer extension type.\n\n        If the dtype is numeric, and consists of all integers, convert to an\n        appropriate integer extension type. Otherwise, convert to an\n        appropriate floating extension type.\n\n        .. versionchanged:: 1.2\n            Starting with pandas 1.2, this method also converts float columns\n            to the nullable floating extension type.\n\n        In the future, as new dtypes are added that support ``pd.NA``, the results\n        of this method will change to support those new dtypes.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(\n        ...     {\n        ...         \"a\": pd.Series([1, 2, 3], dtype=np.dtype(\"int32\")),\n        ...         \"b\": pd.Series([\"x\", \"y\", \"z\"], dtype=np.dtype(\"O\")),\n        ...         \"c\": pd.Series([True, False, np.nan], dtype=np.dtype(\"O\")),\n        ...         \"d\": pd.Series([\"h\", \"i\", np.nan], dtype=np.dtype(\"O\")),\n        ...         \"e\": pd.Series([10, np.nan, 20], dtype=np.dtype(\"float\")),\n        ...         \"f\": pd.Series([np.nan, 100.5, 200], dtype=np.dtype(\"float\")),\n        ...     }\n        ... )\n\n        Start with a DataFrame with default dtypes.\n\n        >>> df\n           a  b      c    d     e      f\n        0  1  x   True    h  10.0    NaN\n        1  2  y  False    i   NaN  100.5\n        2  3  z    NaN  NaN  20.0  200.0\n\n        >>> df.dtypes\n        a      int32\n        b     object\n        c     object\n        d     object\n        e    float64\n        f    float64\n        dtype: object\n\n        Convert the DataFrame to use best possible dtypes.\n\n        >>> dfn = df.convert_dtypes()\n        >>> dfn\n           a  b      c     d     e      f\n        0  1  x   True     h    10   <NA>\n        1  2  y  False     i  <NA>  100.5\n        2  3  z   <NA>  <NA>    20  200.0\n\n        >>> dfn.dtypes\n        a      Int32\n        b     string\n        c    boolean\n        d     string\n        e      Int64\n        f    Float64\n        dtype: object\n\n        Start with a Series of strings and missing data represented by ``np.nan``.\n\n        >>> s = pd.Series([\"a\", \"b\", np.nan])\n        >>> s\n        0      a\n        1      b\n        2    NaN\n        dtype: object\n\n        Obtain a Series with dtype ``StringDtype``.\n\n        >>> s.convert_dtypes()\n        0       a\n        1       b\n        2    <NA>\n        dtype: string\n        \"\"\"\n        if self.ndim == 1:\n            return self._convert_dtypes(\n                infer_objects,\n                convert_string,\n                convert_integer,\n                convert_boolean,\n                convert_floating,\n            )\n        else:\n            results = [\n                col._convert_dtypes(\n                    infer_objects,\n                    convert_string,\n                    convert_integer,\n                    convert_boolean,\n                    convert_floating,\n                )\n                for col_name, col in self.items()\n            ]\n            if len(results) > 0:\n                result = concat(results, axis=1, copy=False, keys=self.columns)\n                cons = cast(Type[\"DataFrame\"], self._constructor)\n                result = cons(result)\n                result = result.__finalize__(self, method=\"convert_dtypes\")\n                # https://github.com/python/mypy/issues/8354\n                return cast(NDFrameT, result)\n            else:\n                return self.copy()\n\n    # ----------------------------------------------------------------------\n    # Filling NA's\n\n    @doc(**_shared_doc_kwargs)\n    def fillna(\n        self: NDFrameT,\n        value=None,\n        method=None,\n        axis=None,\n        inplace: bool_t = False,\n        limit=None,\n        downcast=None,\n    ) -> NDFrameT | None:\n        \"\"\"\n        Fill NA/NaN values using the specified method.\n\n        Parameters\n        ----------\n        value : scalar, dict, Series, or DataFrame\n            Value to use to fill holes (e.g. 0), alternately a\n            dict/Series/DataFrame of values specifying which value to use for\n            each index (for a Series) or column (for a DataFrame).  Values not\n            in the dict/Series/DataFrame will not be filled. This value cannot\n            be a list.\n        method : {{'backfill', 'bfill', 'pad', 'ffill', None}}, default None\n            Method to use for filling holes in reindexed Series\n            pad / ffill: propagate last valid observation forward to next valid\n            backfill / bfill: use next valid observation to fill gap.\n        axis : {axes_single_arg}\n            Axis along which to fill missing values.\n        inplace : bool, default False\n            If True, fill in-place. Note: this will modify any\n            other views on this object (e.g., a no-copy slice for a column in a\n            DataFrame).\n        limit : int, default None\n            If method is specified, this is the maximum number of consecutive\n            NaN values to forward/backward fill. In other words, if there is\n            a gap with more than this number of consecutive NaNs, it will only\n            be partially filled. If method is not specified, this is the\n            maximum number of entries along the entire axis where NaNs will be\n            filled. Must be greater than 0 if not None.\n        downcast : dict, default is None\n            A dict of item->dtype of what to downcast if possible,\n            or the string 'infer' which will try to downcast to an appropriate\n            equal type (e.g. float64 to int64 if possible).\n\n        Returns\n        -------\n        {klass} or None\n            Object with missing values filled or None if ``inplace=True``.\n\n        See Also\n        --------\n        interpolate : Fill NaN values using interpolation.\n        reindex : Conform object to new index.\n        asfreq : Convert TimeSeries to specified frequency.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([[np.nan, 2, np.nan, 0],\n        ...                    [3, 4, np.nan, 1],\n        ...                    [np.nan, np.nan, np.nan, np.nan],\n        ...                    [np.nan, 3, np.nan, 4]],\n        ...                   columns=list(\"ABCD\"))\n        >>> df\n             A    B   C    D\n        0  NaN  2.0 NaN  0.0\n        1  3.0  4.0 NaN  1.0\n        2  NaN  NaN NaN  NaN\n        3  NaN  3.0 NaN  4.0\n\n        Replace all NaN elements with 0s.\n\n        >>> df.fillna(0)\n             A    B    C    D\n        0  0.0  2.0  0.0  0.0\n        1  3.0  4.0  0.0  1.0\n        2  0.0  0.0  0.0  0.0\n        3  0.0  3.0  0.0  4.0\n\n        We can also propagate non-null values forward or backward.\n\n        >>> df.fillna(method=\"ffill\")\n             A    B   C    D\n        0  NaN  2.0 NaN  0.0\n        1  3.0  4.0 NaN  1.0\n        2  3.0  4.0 NaN  1.0\n        3  3.0  3.0 NaN  4.0\n\n        Replace all NaN elements in column 'A', 'B', 'C', and 'D', with 0, 1,\n        2, and 3 respectively.\n\n        >>> values = {{\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}}\n        >>> df.fillna(value=values)\n             A    B    C    D\n        0  0.0  2.0  2.0  0.0\n        1  3.0  4.0  2.0  1.0\n        2  0.0  1.0  2.0  3.0\n        3  0.0  3.0  2.0  4.0\n\n        Only replace the first NaN element.\n\n        >>> df.fillna(value=values, limit=1)\n             A    B    C    D\n        0  0.0  2.0  2.0  0.0\n        1  3.0  4.0  NaN  1.0\n        2  NaN  1.0  NaN  3.0\n        3  NaN  3.0  NaN  4.0\n\n        When filling using a DataFrame, replacement happens along\n        the same column names and same indices\n\n        >>> df2 = pd.DataFrame(np.zeros((4, 4)), columns=list(\"ABCE\"))\n        >>> df.fillna(df2)\n             A    B    C    D\n        0  0.0  2.0  0.0  0.0\n        1  3.0  4.0  0.0  1.0\n        2  0.0  0.0  0.0  NaN\n        3  0.0  3.0  0.0  4.0\n\n        Note that column D is not affected since it is not present in df2.\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        value, method = validate_fillna_kwargs(value, method)\n\n        self._consolidate_inplace()\n\n        # set the default here, so functions examining the signaure\n        # can detect if something was set (e.g. in groupby) (GH9221)\n        if axis is None:\n            axis = 0\n        axis = self._get_axis_number(axis)\n\n        if value is None:\n            if not self._mgr.is_single_block and axis == 1:\n                if inplace:\n                    raise NotImplementedError()\n                result = self.T.fillna(method=method, limit=limit).T\n\n                return result\n\n            new_data = self._mgr.interpolate(\n                method=method,\n                axis=axis,\n                limit=limit,\n                inplace=inplace,\n                coerce=True,\n                downcast=downcast,\n            )\n        else:\n            if self.ndim == 1:\n                if isinstance(value, (dict, ABCSeries)):\n                    if not len(value):\n                        # test_fillna_nonscalar\n                        if inplace:\n                            return None\n                        return self.copy()\n                    value = create_series_with_explicit_dtype(\n                        value, dtype_if_empty=object\n                    )\n                    value = value.reindex(self.index, copy=False)\n                    value = value._values\n                elif not is_list_like(value):\n                    pass\n                else:\n                    raise TypeError(\n                        '\"value\" parameter must be a scalar, dict '\n                        \"or Series, but you passed a \"\n                        f'\"{type(value).__name__}\"'\n                    )\n\n                new_data = self._mgr.fillna(\n                    value=value, limit=limit, inplace=inplace, downcast=downcast\n                )\n\n            elif isinstance(value, (dict, ABCSeries)):\n                if axis == 1:\n                    raise NotImplementedError(\n                        \"Currently only can fill \"\n                        \"with dict/Series column \"\n                        \"by column\"\n                    )\n\n                result = self if inplace else self.copy()\n                is_dict = isinstance(downcast, dict)\n                for k, v in value.items():\n                    if k not in result:\n                        continue\n                    downcast_k = downcast if not is_dict else downcast.get(k)\n                    result[k] = result[k].fillna(v, limit=limit, downcast=downcast_k)\n                return result if not inplace else None\n\n            elif not is_list_like(value):\n                if not self._mgr.is_single_block and axis == 1:\n\n                    result = self.T.fillna(value=value, limit=limit).T\n\n                    new_data = result\n                else:\n\n                    new_data = self._mgr.fillna(\n                        value=value, limit=limit, inplace=inplace, downcast=downcast\n                    )\n            elif isinstance(value, ABCDataFrame) and self.ndim == 2:\n\n                new_data = self.where(self.notna(), value)._mgr\n            else:\n                raise ValueError(f\"invalid fill value with a {type(value)}\")\n\n        result = self._constructor(new_data)\n        if inplace:\n            return self._update_inplace(result)\n        else:\n            return result.__finalize__(self, method=\"fillna\")\n\n    @doc(klass=_shared_doc_kwargs[\"klass\"])\n    def ffill(\n        self: NDFrameT,\n        axis: None | Axis = None,\n        inplace: bool_t = False,\n        limit: None | int = None,\n        downcast=None,\n    ) -> NDFrameT | None:\n        \"\"\"\n        Synonym for :meth:`DataFrame.fillna` with ``method='ffill'``.\n\n        Returns\n        -------\n        {klass} or None\n            Object with missing values filled or None if ``inplace=True``.\n        \"\"\"\n        return self.fillna(\n            method=\"ffill\", axis=axis, inplace=inplace, limit=limit, downcast=downcast\n        )\n\n    pad = ffill\n\n    @doc(klass=_shared_doc_kwargs[\"klass\"])\n    def bfill(\n        self: NDFrameT,\n        axis: None | Axis = None,\n        inplace: bool_t = False,\n        limit: None | int = None,\n        downcast=None,\n    ) -> NDFrameT | None:\n        \"\"\"\n        Synonym for :meth:`DataFrame.fillna` with ``method='bfill'``.\n\n        Returns\n        -------\n        {klass} or None\n            Object with missing values filled or None if ``inplace=True``.\n        \"\"\"\n        return self.fillna(\n            method=\"bfill\", axis=axis, inplace=inplace, limit=limit, downcast=downcast\n        )\n\n    backfill = bfill\n\n    @doc(\n        _shared_docs[\"replace\"],\n        klass=_shared_doc_kwargs[\"klass\"],\n        inplace=_shared_doc_kwargs[\"inplace\"],\n        replace_iloc=_shared_doc_kwargs[\"replace_iloc\"],\n    )\n    def replace(\n        self,\n        to_replace=None,\n        value=lib.no_default,\n        inplace: bool_t = False,\n        limit: int | None = None,\n        regex=False,\n        method=lib.no_default,\n    ):\n        if not (\n            is_scalar(to_replace)\n            or is_re_compilable(to_replace)\n            or is_list_like(to_replace)\n        ):\n            raise TypeError(\n                \"Expecting 'to_replace' to be either a scalar, array-like, \"\n                \"dict or None, got invalid type \"\n                f\"{repr(type(to_replace).__name__)}\"\n            )\n\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        if not is_bool(regex) and to_replace is not None:\n            raise ValueError(\"'to_replace' must be 'None' if 'regex' is not a bool\")\n\n        self._consolidate_inplace()\n\n        if value is lib.no_default or method is not lib.no_default:\n            # GH#36984 if the user explicitly passes value=None we want to\n            #  respect that. We have the corner case where the user explicitly\n            #  passes value=None *and* a method, which we interpret as meaning\n            #  they want the (documented) default behavior.\n            if method is lib.no_default:\n                # TODO: get this to show up as the default in the docs?\n                method = \"pad\"\n\n            # passing a single value that is scalar like\n            # when value is None (GH5319), for compat\n            if not is_dict_like(to_replace) and not is_dict_like(regex):\n                to_replace = [to_replace]\n\n            if isinstance(to_replace, (tuple, list)):\n                if isinstance(self, ABCDataFrame):\n                    from pandas import Series\n\n                    result = self.apply(\n                        Series._replace_single,\n                        args=(to_replace, method, inplace, limit),\n                    )\n                    if inplace:\n                        return\n                    return result\n                self = cast(\"Series\", self)\n                return self._replace_single(to_replace, method, inplace, limit)\n\n            if not is_dict_like(to_replace):\n                if not is_dict_like(regex):\n                    raise TypeError(\n                        'If \"to_replace\" and \"value\" are both None '\n                        'and \"to_replace\" is not a list, then '\n                        \"regex must be a mapping\"\n                    )\n                to_replace = regex\n                regex = True\n\n            items = list(to_replace.items())\n            if items:\n                keys, values = zip(*items)\n            else:\n                keys, values = ([], [])\n\n            are_mappings = [is_dict_like(v) for v in values]\n\n            if any(are_mappings):\n                if not all(are_mappings):\n                    raise TypeError(\n                        \"If a nested mapping is passed, all values \"\n                        \"of the top level mapping must be mappings\"\n                    )\n                # passed a nested dict/Series\n                to_rep_dict = {}\n                value_dict = {}\n\n                for k, v in items:\n                    keys, values = list(zip(*v.items())) or ([], [])\n\n                    to_rep_dict[k] = list(keys)\n                    value_dict[k] = list(values)\n\n                to_replace, value = to_rep_dict, value_dict\n            else:\n                to_replace, value = keys, values\n\n            return self.replace(\n                to_replace, value, inplace=inplace, limit=limit, regex=regex\n            )\n        else:\n\n            # need a non-zero len on all axes\n            if not self.size:\n                if inplace:\n                    return\n                return self.copy()\n\n            if is_dict_like(to_replace):\n                if is_dict_like(value):  # {'A' : NA} -> {'A' : 0}\n                    # Note: Checking below for `in foo.keys()` instead of\n                    #  `in foo` is needed for when we have a Series and not dict\n                    mapping = {\n                        col: (to_replace[col], value[col])\n                        for col in to_replace.keys()\n                        if col in value.keys() and col in self\n                    }\n                    return self._replace_columnwise(mapping, inplace, regex)\n\n                # {'A': NA} -> 0\n                elif not is_list_like(value):\n                    # Operate column-wise\n                    if self.ndim == 1:\n                        raise ValueError(\n                            \"Series.replace cannot use dict-like to_replace \"\n                            \"and non-None value\"\n                        )\n                    mapping = {\n                        col: (to_rep, value) for col, to_rep in to_replace.items()\n                    }\n                    return self._replace_columnwise(mapping, inplace, regex)\n                else:\n                    raise TypeError(\"value argument must be scalar, dict, or Series\")\n\n            elif is_list_like(to_replace):\n                if not is_list_like(value):\n                    # e.g. to_replace = [NA, ''] and value is 0,\n                    #  so we replace NA with 0 and then replace '' with 0\n                    value = [value] * len(to_replace)\n\n                # e.g. we have to_replace = [NA, ''] and value = [0, 'missing']\n                if len(to_replace) != len(value):\n                    raise ValueError(\n                        f\"Replacement lists must match in length. \"\n                        f\"Expecting {len(to_replace)} got {len(value)} \"\n                    )\n                new_data = self._mgr.replace_list(\n                    src_list=to_replace,\n                    dest_list=value,\n                    inplace=inplace,\n                    regex=regex,\n                )\n\n            elif to_replace is None:\n                if not (\n                    is_re_compilable(regex)\n                    or is_list_like(regex)\n                    or is_dict_like(regex)\n                ):\n                    raise TypeError(\n                        f\"'regex' must be a string or a compiled regular expression \"\n                        f\"or a list or dict of strings or regular expressions, \"\n                        f\"you passed a {repr(type(regex).__name__)}\"\n                    )\n                return self.replace(\n                    regex, value, inplace=inplace, limit=limit, regex=True\n                )\n            else:\n\n                # dest iterable dict-like\n                if is_dict_like(value):  # NA -> {'A' : 0, 'B' : -1}\n                    # Operate column-wise\n                    if self.ndim == 1:\n                        raise ValueError(\n                            \"Series.replace cannot use dict-value and \"\n                            \"non-None to_replace\"\n                        )\n                    mapping = {col: (to_replace, val) for col, val in value.items()}\n                    return self._replace_columnwise(mapping, inplace, regex)\n\n                elif not is_list_like(value):  # NA -> 0\n                    regex = should_use_regex(regex, to_replace)\n                    if regex:\n                        new_data = self._mgr.replace_regex(\n                            to_replace=to_replace,\n                            value=value,\n                            inplace=inplace,\n                        )\n                    else:\n                        new_data = self._mgr.replace(\n                            to_replace=to_replace, value=value, inplace=inplace\n                        )\n                else:\n                    raise TypeError(\n                        f'Invalid \"to_replace\" type: {repr(type(to_replace).__name__)}'\n                    )\n\n        result = self._constructor(new_data)\n        if inplace:\n            return self._update_inplace(result)\n        else:\n            return result.__finalize__(self, method=\"replace\")\n\n    def interpolate(\n        self: NDFrameT,\n        method: str = \"linear\",\n        axis: Axis = 0,\n        limit: int | None = None,\n        inplace: bool_t = False,\n        limit_direction: str | None = None,\n        limit_area: str | None = None,\n        downcast: str | None = None,\n        **kwargs,\n    ) -> NDFrameT | None:\n        \"\"\"\n        Fill NaN values using an interpolation method.\n\n        Please note that only ``method='linear'`` is supported for\n        DataFrame/Series with a MultiIndex.\n\n        Parameters\n        ----------\n        method : str, default 'linear'\n            Interpolation technique to use. One of:\n\n            * 'linear': Ignore the index and treat the values as equally\n              spaced. This is the only method supported on MultiIndexes.\n            * 'time': Works on daily and higher resolution data to interpolate\n              given length of interval.\n            * 'index', 'values': use the actual numerical values of the index.\n            * 'pad': Fill in NaNs using existing values.\n            * 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'spline',\n              'barycentric', 'polynomial': Passed to\n              `scipy.interpolate.interp1d`. These methods use the numerical\n              values of the index.  Both 'polynomial' and 'spline' require that\n              you also specify an `order` (int), e.g.\n              ``df.interpolate(method='polynomial', order=5)``.\n            * 'krogh', 'piecewise_polynomial', 'spline', 'pchip', 'akima',\n              'cubicspline': Wrappers around the SciPy interpolation methods of\n              similar names. See `Notes`.\n            * 'from_derivatives': Refers to\n              `scipy.interpolate.BPoly.from_derivatives` which\n              replaces 'piecewise_polynomial' interpolation method in\n              scipy 0.18.\n\n        axis : {{0 or 'index', 1 or 'columns', None}}, default None\n            Axis to interpolate along.\n        limit : int, optional\n            Maximum number of consecutive NaNs to fill. Must be greater than\n            0.\n        inplace : bool, default False\n            Update the data in place if possible.\n        limit_direction : {{'forward', 'backward', 'both'}}, Optional\n            Consecutive NaNs will be filled in this direction.\n\n            If limit is specified:\n                * If 'method' is 'pad' or 'ffill', 'limit_direction' must be 'forward'.\n                * If 'method' is 'backfill' or 'bfill', 'limit_direction' must be\n                  'backwards'.\n\n            If 'limit' is not specified:\n                * If 'method' is 'backfill' or 'bfill', the default is 'backward'\n                * else the default is 'forward'\n\n            .. versionchanged:: 1.1.0\n                raises ValueError if `limit_direction` is 'forward' or 'both' and\n                    method is 'backfill' or 'bfill'.\n                raises ValueError if `limit_direction` is 'backward' or 'both' and\n                    method is 'pad' or 'ffill'.\n\n        limit_area : {{`None`, 'inside', 'outside'}}, default None\n            If limit is specified, consecutive NaNs will be filled with this\n            restriction.\n\n            * ``None``: No fill restriction.\n            * 'inside': Only fill NaNs surrounded by valid values\n              (interpolate).\n            * 'outside': Only fill NaNs outside valid values (extrapolate).\n\n        downcast : optional, 'infer' or None, defaults to None\n            Downcast dtypes if possible.\n        ``**kwargs`` : optional\n            Keyword arguments to pass on to the interpolating function.\n\n        Returns\n        -------\n        Series or DataFrame or None\n            Returns the same object type as the caller, interpolated at\n            some or all ``NaN`` values or None if ``inplace=True``.\n\n        See Also\n        --------\n        fillna : Fill missing values using different methods.\n        scipy.interpolate.Akima1DInterpolator : Piecewise cubic polynomials\n            (Akima interpolator).\n        scipy.interpolate.BPoly.from_derivatives : Piecewise polynomial in the\n            Bernstein basis.\n        scipy.interpolate.interp1d : Interpolate a 1-D function.\n        scipy.interpolate.KroghInterpolator : Interpolate polynomial (Krogh\n            interpolator).\n        scipy.interpolate.PchipInterpolator : PCHIP 1-d monotonic cubic\n            interpolation.\n        scipy.interpolate.CubicSpline : Cubic spline data interpolator.\n\n        Notes\n        -----\n        The 'krogh', 'piecewise_polynomial', 'spline', 'pchip' and 'akima'\n        methods are wrappers around the respective SciPy implementations of\n        similar names. These use the actual numerical values of the index.\n        For more information on their behavior, see the\n        `SciPy documentation\n        <https://docs.scipy.org/doc/scipy/reference/interpolate.html#univariate-interpolation>`__\n        and `SciPy tutorial\n        <https://docs.scipy.org/doc/scipy/reference/tutorial/interpolate.html>`__.\n\n        Examples\n        --------\n        Filling in ``NaN`` in a :class:`~pandas.Series` via linear\n        interpolation.\n\n        >>> s = pd.Series([0, 1, np.nan, 3])\n        >>> s\n        0    0.0\n        1    1.0\n        2    NaN\n        3    3.0\n        dtype: float64\n        >>> s.interpolate()\n        0    0.0\n        1    1.0\n        2    2.0\n        3    3.0\n        dtype: float64\n\n        Filling in ``NaN`` in a Series by padding, but filling at most two\n        consecutive ``NaN`` at a time.\n\n        >>> s = pd.Series([np.nan, \"single_one\", np.nan,\n        ...                \"fill_two_more\", np.nan, np.nan, np.nan,\n        ...                4.71, np.nan])\n        >>> s\n        0              NaN\n        1       single_one\n        2              NaN\n        3    fill_two_more\n        4              NaN\n        5              NaN\n        6              NaN\n        7             4.71\n        8              NaN\n        dtype: object\n        >>> s.interpolate(method='pad', limit=2)\n        0              NaN\n        1       single_one\n        2       single_one\n        3    fill_two_more\n        4    fill_two_more\n        5    fill_two_more\n        6              NaN\n        7             4.71\n        8             4.71\n        dtype: object\n\n        Filling in ``NaN`` in a Series via polynomial interpolation or splines:\n        Both 'polynomial' and 'spline' methods require that you also specify\n        an ``order`` (int).\n\n        >>> s = pd.Series([0, 2, np.nan, 8])\n        >>> s.interpolate(method='polynomial', order=2)\n        0    0.000000\n        1    2.000000\n        2    4.666667\n        3    8.000000\n        dtype: float64\n\n        Fill the DataFrame forward (that is, going down) along each column\n        using linear interpolation.\n\n        Note how the last entry in column 'a' is interpolated differently,\n        because there is no entry after it to use for interpolation.\n        Note how the first entry in column 'b' remains ``NaN``, because there\n        is no entry before it to use for interpolation.\n\n        >>> df = pd.DataFrame([(0.0, np.nan, -1.0, 1.0),\n        ...                    (np.nan, 2.0, np.nan, np.nan),\n        ...                    (2.0, 3.0, np.nan, 9.0),\n        ...                    (np.nan, 4.0, -4.0, 16.0)],\n        ...                   columns=list('abcd'))\n        >>> df\n             a    b    c     d\n        0  0.0  NaN -1.0   1.0\n        1  NaN  2.0  NaN   NaN\n        2  2.0  3.0  NaN   9.0\n        3  NaN  4.0 -4.0  16.0\n        >>> df.interpolate(method='linear', limit_direction='forward', axis=0)\n             a    b    c     d\n        0  0.0  NaN -1.0   1.0\n        1  1.0  2.0 -2.0   5.0\n        2  2.0  3.0 -3.0   9.0\n        3  2.0  4.0 -4.0  16.0\n\n        Using polynomial interpolation.\n\n        >>> df['d'].interpolate(method='polynomial', order=2)\n        0     1.0\n        1     4.0\n        2     9.0\n        3    16.0\n        Name: d, dtype: float64\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n\n        axis = self._get_axis_number(axis)\n\n        fillna_methods = [\"ffill\", \"bfill\", \"pad\", \"backfill\"]\n        should_transpose = axis == 1 and method not in fillna_methods\n\n        obj = self.T if should_transpose else self\n\n        if obj.empty:\n            return self.copy()\n\n        if method not in fillna_methods:\n            axis = self._info_axis_number\n\n        if isinstance(obj.index, MultiIndex) and method != \"linear\":\n            raise ValueError(\n                \"Only `method=linear` interpolation is supported on MultiIndexes.\"\n            )\n\n        # Set `limit_direction` depending on `method`\n        if limit_direction is None:\n            limit_direction = (\n                \"backward\" if method in (\"backfill\", \"bfill\") else \"forward\"\n            )\n        else:\n            if method in (\"pad\", \"ffill\") and limit_direction != \"forward\":\n                raise ValueError(\n                    f\"`limit_direction` must be 'forward' for method `{method}`\"\n                )\n            if method in (\"backfill\", \"bfill\") and limit_direction != \"backward\":\n                raise ValueError(\n                    f\"`limit_direction` must be 'backward' for method `{method}`\"\n                )\n\n        if obj.ndim == 2 and np.all(obj.dtypes == np.dtype(\"object\")):\n            raise TypeError(\n                \"Cannot interpolate with all object-dtype columns \"\n                \"in the DataFrame. Try setting at least one \"\n                \"column to a numeric dtype.\"\n            )\n\n        # create/use the index\n        if method == \"linear\":\n            # prior default\n            index = Index(np.arange(len(obj.index)))\n        else:\n            index = obj.index\n            methods = {\"index\", \"values\", \"nearest\", \"time\"}\n            is_numeric_or_datetime = (\n                is_numeric_dtype(index.dtype)\n                or is_datetime64_any_dtype(index.dtype)\n                or is_timedelta64_dtype(index.dtype)\n            )\n            if method not in methods and not is_numeric_or_datetime:\n                raise ValueError(\n                    \"Index column must be numeric or datetime type when \"\n                    f\"using {method} method other than linear. \"\n                    \"Try setting a numeric or datetime index column before \"\n                    \"interpolating.\"\n                )\n\n        if isna(index).any():\n            raise NotImplementedError(\n                \"Interpolation with NaNs in the index \"\n                \"has not been implemented. Try filling \"\n                \"those NaNs before interpolating.\"\n            )\n        new_data = obj._mgr.interpolate(\n            method=method,\n            axis=axis,\n            index=index,\n            limit=limit,\n            limit_direction=limit_direction,\n            limit_area=limit_area,\n            inplace=inplace,\n            downcast=downcast,\n            **kwargs,\n        )\n\n        result = self._constructor(new_data)\n        if should_transpose:\n            result = result.T\n        if inplace:\n            return self._update_inplace(result)\n        else:\n            return result.__finalize__(self, method=\"interpolate\")\n\n    # ----------------------------------------------------------------------\n    # Timeseries methods Methods\n\n    @final\n    def asof(self, where, subset=None):\n        \"\"\"\n        Return the last row(s) without any NaNs before `where`.\n\n        The last row (for each element in `where`, if list) without any\n        NaN is taken.\n        In case of a :class:`~pandas.DataFrame`, the last row without NaN\n        considering only the subset of columns (if not `None`)\n\n        If there is no good value, NaN is returned for a Series or\n        a Series of NaN values for a DataFrame\n\n        Parameters\n        ----------\n        where : date or array-like of dates\n            Date(s) before which the last row(s) are returned.\n        subset : str or array-like of str, default `None`\n            For DataFrame, if not `None`, only use these columns to\n            check for NaNs.\n\n        Returns\n        -------\n        scalar, Series, or DataFrame\n\n            The return can be:\n\n            * scalar : when `self` is a Series and `where` is a scalar\n            * Series: when `self` is a Series and `where` is an array-like,\n              or when `self` is a DataFrame and `where` is a scalar\n            * DataFrame : when `self` is a DataFrame and `where` is an\n              array-like\n\n            Return scalar, Series, or DataFrame.\n\n        See Also\n        --------\n        merge_asof : Perform an asof merge. Similar to left join.\n\n        Notes\n        -----\n        Dates are assumed to be sorted. Raises if this is not the case.\n\n        Examples\n        --------\n        A Series and a scalar `where`.\n\n        >>> s = pd.Series([1, 2, np.nan, 4], index=[10, 20, 30, 40])\n        >>> s\n        10    1.0\n        20    2.0\n        30    NaN\n        40    4.0\n        dtype: float64\n\n        >>> s.asof(20)\n        2.0\n\n        For a sequence `where`, a Series is returned. The first value is\n        NaN, because the first element of `where` is before the first\n        index value.\n\n        >>> s.asof([5, 20])\n        5     NaN\n        20    2.0\n        dtype: float64\n\n        Missing values are not considered. The following is ``2.0``, not\n        NaN, even though NaN is at the index location for ``30``.\n\n        >>> s.asof(30)\n        2.0\n\n        Take all columns into consideration\n\n        >>> df = pd.DataFrame({'a': [10, 20, 30, 40, 50],\n        ...                    'b': [None, None, None, None, 500]},\n        ...                   index=pd.DatetimeIndex(['2018-02-27 09:01:00',\n        ...                                           '2018-02-27 09:02:00',\n        ...                                           '2018-02-27 09:03:00',\n        ...                                           '2018-02-27 09:04:00',\n        ...                                           '2018-02-27 09:05:00']))\n        >>> df.asof(pd.DatetimeIndex(['2018-02-27 09:03:30',\n        ...                           '2018-02-27 09:04:30']))\n                              a   b\n        2018-02-27 09:03:30 NaN NaN\n        2018-02-27 09:04:30 NaN NaN\n\n        Take a single column into consideration\n\n        >>> df.asof(pd.DatetimeIndex(['2018-02-27 09:03:30',\n        ...                           '2018-02-27 09:04:30']),\n        ...         subset=['a'])\n                                 a   b\n        2018-02-27 09:03:30   30.0 NaN\n        2018-02-27 09:04:30   40.0 NaN\n        \"\"\"\n        if isinstance(where, str):\n            where = Timestamp(where)\n\n        if not self.index.is_monotonic:\n            raise ValueError(\"asof requires a sorted index\")\n\n        is_series = isinstance(self, ABCSeries)\n        if is_series:\n            if subset is not None:\n                raise ValueError(\"subset is not valid for Series\")\n        else:\n            if subset is None:\n                subset = self.columns\n            if not is_list_like(subset):\n                subset = [subset]\n\n        is_list = is_list_like(where)\n        if not is_list:\n            start = self.index[0]\n            if isinstance(self.index, PeriodIndex):\n                where = Period(where, freq=self.index.freq)\n\n            if where < start:\n                if not is_series:\n                    return self._constructor_sliced(\n                        index=self.columns, name=where, dtype=np.float64\n                    )\n                return np.nan\n\n            # It's always much faster to use a *while* loop here for\n            # Series than pre-computing all the NAs. However a\n            # *while* loop is extremely expensive for DataFrame\n            # so we later pre-compute all the NAs and use the same\n            # code path whether *where* is a scalar or list.\n            # See PR: https://github.com/pandas-dev/pandas/pull/14476\n            if is_series:\n                loc = self.index.searchsorted(where, side=\"right\")\n                if loc > 0:\n                    loc -= 1\n\n                values = self._values\n                while loc > 0 and isna(values[loc]):\n                    loc -= 1\n                return values[loc]\n\n        if not isinstance(where, Index):\n            where = Index(where) if is_list else Index([where])\n\n        nulls = self.isna() if is_series else self[subset].isna().any(1)\n        if nulls.all():\n            if is_series:\n                self = cast(\"Series\", self)\n                return self._constructor(np.nan, index=where, name=self.name)\n            elif is_list:\n                self = cast(\"DataFrame\", self)\n                return self._constructor(np.nan, index=where, columns=self.columns)\n            else:\n                self = cast(\"DataFrame\", self)\n                return self._constructor_sliced(\n                    np.nan, index=self.columns, name=where[0]\n                )\n\n        locs = self.index.asof_locs(where, ~(nulls._values))\n\n        # mask the missing\n        missing = locs == -1\n        data = self.take(locs)\n        data.index = where\n        data.loc[missing] = np.nan\n        return data if is_list else data.iloc[-1]\n\n    # ----------------------------------------------------------------------\n    # Action Methods\n\n    @doc(klass=_shared_doc_kwargs[\"klass\"])\n    def isna(self: NDFrameT) -> NDFrameT:\n        \"\"\"\n        Detect missing values.\n\n        Return a boolean same-sized object indicating if the values are NA.\n        NA values, such as None or :attr:`numpy.NaN`, gets mapped to True\n        values.\n        Everything else gets mapped to False values. Characters such as empty\n        strings ``''`` or :attr:`numpy.inf` are not considered NA values\n        (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n\n        Returns\n        -------\n        {klass}\n            Mask of bool values for each element in {klass} that\n            indicates whether an element is an NA value.\n\n        See Also\n        --------\n        {klass}.isnull : Alias of isna.\n        {klass}.notna : Boolean inverse of isna.\n        {klass}.dropna : Omit axes labels with missing values.\n        isna : Top-level isna.\n\n        Examples\n        --------\n        Show which entries in a DataFrame are NA.\n\n        >>> df = pd.DataFrame(dict(age=[5, 6, np.NaN],\n        ...                    born=[pd.NaT, pd.Timestamp('1939-05-27'),\n        ...                          pd.Timestamp('1940-04-25')],\n        ...                    name=['Alfred', 'Batman', ''],\n        ...                    toy=[None, 'Batmobile', 'Joker']))\n        >>> df\n           age       born    name        toy\n        0  5.0        NaT  Alfred       None\n        1  6.0 1939-05-27  Batman  Batmobile\n        2  NaN 1940-04-25              Joker\n\n        >>> df.isna()\n             age   born   name    toy\n        0  False   True  False   True\n        1  False  False  False  False\n        2   True  False  False  False\n\n        Show which entries in a Series are NA.\n\n        >>> ser = pd.Series([5, 6, np.NaN])\n        >>> ser\n        0    5.0\n        1    6.0\n        2    NaN\n        dtype: float64\n\n        >>> ser.isna()\n        0    False\n        1    False\n        2     True\n        dtype: bool\n        \"\"\"\n        return isna(self).__finalize__(self, method=\"isna\")\n\n    @doc(isna, klass=_shared_doc_kwargs[\"klass\"])\n    def isnull(self: NDFrameT) -> NDFrameT:\n        return isna(self).__finalize__(self, method=\"isnull\")\n\n    @doc(klass=_shared_doc_kwargs[\"klass\"])\n    def notna(self: NDFrameT) -> NDFrameT:\n        \"\"\"\n        Detect existing (non-missing) values.\n\n        Return a boolean same-sized object indicating if the values are not NA.\n        Non-missing values get mapped to True. Characters such as empty\n        strings ``''`` or :attr:`numpy.inf` are not considered NA values\n        (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n        NA values, such as None or :attr:`numpy.NaN`, get mapped to False\n        values.\n\n        Returns\n        -------\n        {klass}\n            Mask of bool values for each element in {klass} that\n            indicates whether an element is not an NA value.\n\n        See Also\n        --------\n        {klass}.notnull : Alias of notna.\n        {klass}.isna : Boolean inverse of notna.\n        {klass}.dropna : Omit axes labels with missing values.\n        notna : Top-level notna.\n\n        Examples\n        --------\n        Show which entries in a DataFrame are not NA.\n\n        >>> df = pd.DataFrame(dict(age=[5, 6, np.NaN],\n        ...                    born=[pd.NaT, pd.Timestamp('1939-05-27'),\n        ...                          pd.Timestamp('1940-04-25')],\n        ...                    name=['Alfred', 'Batman', ''],\n        ...                    toy=[None, 'Batmobile', 'Joker']))\n        >>> df\n           age       born    name        toy\n        0  5.0        NaT  Alfred       None\n        1  6.0 1939-05-27  Batman  Batmobile\n        2  NaN 1940-04-25              Joker\n\n        >>> df.notna()\n             age   born  name    toy\n        0   True  False  True  False\n        1   True   True  True   True\n        2  False   True  True   True\n\n        Show which entries in a Series are not NA.\n\n        >>> ser = pd.Series([5, 6, np.NaN])\n        >>> ser\n        0    5.0\n        1    6.0\n        2    NaN\n        dtype: float64\n\n        >>> ser.notna()\n        0     True\n        1     True\n        2    False\n        dtype: bool\n        \"\"\"\n        return notna(self).__finalize__(self, method=\"notna\")\n\n    @doc(notna, klass=_shared_doc_kwargs[\"klass\"])\n    def notnull(self: NDFrameT) -> NDFrameT:\n        return notna(self).__finalize__(self, method=\"notnull\")\n\n    @final\n    def _clip_with_scalar(self, lower, upper, inplace: bool_t = False):\n        if (lower is not None and np.any(isna(lower))) or (\n            upper is not None and np.any(isna(upper))\n        ):\n            raise ValueError(\"Cannot use an NA value as a clip threshold\")\n\n        result = self\n        mask = isna(self._values)\n\n        with np.errstate(all=\"ignore\"):\n            if upper is not None:\n                subset = self <= upper\n                result = result.where(subset, upper, axis=None, inplace=False)\n            if lower is not None:\n                subset = self >= lower\n                result = result.where(subset, lower, axis=None, inplace=False)\n\n        if np.any(mask):\n            result[mask] = np.nan\n\n        if inplace:\n            return self._update_inplace(result)\n        else:\n            return result\n\n    @final\n    def _clip_with_one_bound(self, threshold, method, axis, inplace):\n\n        if axis is not None:\n            axis = self._get_axis_number(axis)\n\n        # method is self.le for upper bound and self.ge for lower bound\n        if is_scalar(threshold) and is_number(threshold):\n            if method.__name__ == \"le\":\n                return self._clip_with_scalar(None, threshold, inplace=inplace)\n            return self._clip_with_scalar(threshold, None, inplace=inplace)\n\n        # GH #15390\n        # In order for where method to work, the threshold must\n        # be transformed to NDFrame from other array like structure.\n        if (not isinstance(threshold, ABCSeries)) and is_list_like(threshold):\n            if isinstance(self, ABCSeries):\n                threshold = self._constructor(threshold, index=self.index)\n            else:\n                threshold = align_method_FRAME(self, threshold, axis, flex=None)[1]\n\n        # GH 40420\n        # Treat missing thresholds as no bounds, not clipping the values\n        if is_list_like(threshold):\n            fill_value = np.inf if method.__name__ == \"le\" else -np.inf\n            threshold_inf = threshold.fillna(fill_value)\n        else:\n            threshold_inf = threshold\n\n        subset = method(threshold_inf, axis=axis) | isna(self)\n\n        # GH 40420\n        return self.where(subset, threshold, axis=axis, inplace=inplace)\n\n    def clip(\n        self: NDFrameT,\n        lower=None,\n        upper=None,\n        axis: Axis | None = None,\n        inplace: bool_t = False,\n        *args,\n        **kwargs,\n    ) -> NDFrameT | None:\n        \"\"\"\n        Trim values at input threshold(s).\n\n        Assigns values outside boundary to boundary values. Thresholds\n        can be singular values or array like, and in the latter case\n        the clipping is performed element-wise in the specified axis.\n\n        Parameters\n        ----------\n        lower : float or array-like, default None\n            Minimum threshold value. All values below this\n            threshold will be set to it. A missing\n            threshold (e.g `NA`) will not clip the value.\n        upper : float or array-like, default None\n            Maximum threshold value. All values above this\n            threshold will be set to it. A missing\n            threshold (e.g `NA`) will not clip the value.\n        axis : int or str axis name, optional\n            Align object with lower and upper along the given axis.\n        inplace : bool, default False\n            Whether to perform the operation in place on the data.\n        *args, **kwargs\n            Additional keywords have no effect but might be accepted\n            for compatibility with numpy.\n\n        Returns\n        -------\n        Series or DataFrame or None\n            Same type as calling object with the values outside the\n            clip boundaries replaced or None if ``inplace=True``.\n\n        See Also\n        --------\n        Series.clip : Trim values at input threshold in series.\n        DataFrame.clip : Trim values at input threshold in dataframe.\n        numpy.clip : Clip (limit) the values in an array.\n\n        Examples\n        --------\n        >>> data = {'col_0': [9, -3, 0, -1, 5], 'col_1': [-2, -7, 6, 8, -5]}\n        >>> df = pd.DataFrame(data)\n        >>> df\n           col_0  col_1\n        0      9     -2\n        1     -3     -7\n        2      0      6\n        3     -1      8\n        4      5     -5\n\n        Clips per column using lower and upper thresholds:\n\n        >>> df.clip(-4, 6)\n           col_0  col_1\n        0      6     -2\n        1     -3     -4\n        2      0      6\n        3     -1      6\n        4      5     -4\n\n        Clips using specific lower and upper thresholds per column element:\n\n        >>> t = pd.Series([2, -4, -1, 6, 3])\n        >>> t\n        0    2\n        1   -4\n        2   -1\n        3    6\n        4    3\n        dtype: int64\n\n        >>> df.clip(t, t + 4, axis=0)\n           col_0  col_1\n        0      6      2\n        1     -3     -4\n        2      0      3\n        3      6      8\n        4      5      3\n\n        Clips using specific lower threshold per column element, with missing values:\n\n        >>> t = pd.Series([2, -4, np.NaN, 6, 3])\n        >>> t\n        0    2.0\n        1   -4.0\n        2    NaN\n        3    6.0\n        4    3.0\n        dtype: float64\n\n        >>> df.clip(t, axis=0)\n        col_0  col_1\n        0      9      2\n        1     -3     -4\n        2      0      6\n        3      6      8\n        4      5      3\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n\n        axis = nv.validate_clip_with_axis(axis, args, kwargs)\n        if axis is not None:\n            axis = self._get_axis_number(axis)\n\n        # GH 17276\n        # numpy doesn't like NaN as a clip value\n        # so ignore\n        # GH 19992\n        # numpy doesn't drop a list-like bound containing NaN\n        isna_lower = isna(lower)\n        if not is_list_like(lower):\n            if np.any(isna_lower):\n                lower = None\n        elif np.all(isna_lower):\n            lower = None\n        isna_upper = isna(upper)\n        if not is_list_like(upper):\n            if np.any(isna_upper):\n                upper = None\n        elif np.all(isna_upper):\n            upper = None\n\n        # GH 2747 (arguments were reversed)\n        if (\n            lower is not None\n            and upper is not None\n            and is_scalar(lower)\n            and is_scalar(upper)\n        ):\n            lower, upper = min(lower, upper), max(lower, upper)\n\n        # fast-path for scalars\n        if (lower is None or (is_scalar(lower) and is_number(lower))) and (\n            upper is None or (is_scalar(upper) and is_number(upper))\n        ):\n            return self._clip_with_scalar(lower, upper, inplace=inplace)\n\n        result = self\n        if lower is not None:\n            result = result._clip_with_one_bound(\n                lower, method=self.ge, axis=axis, inplace=inplace\n            )\n        if upper is not None:\n            if inplace:\n                result = self\n            result = result._clip_with_one_bound(\n                upper, method=self.le, axis=axis, inplace=inplace\n            )\n\n        return result\n\n    @doc(**_shared_doc_kwargs)\n    def asfreq(\n        self: NDFrameT,\n        freq,\n        method=None,\n        how: str | None = None,\n        normalize: bool_t = False,\n        fill_value=None,\n    ) -> NDFrameT:\n        \"\"\"\n        Convert time series to specified frequency.\n\n        Returns the original data conformed to a new index with the specified\n        frequency.\n\n        If the index of this {klass} is a :class:`~pandas.PeriodIndex`, the new index\n        is the result of transforming the original index with\n        :meth:`PeriodIndex.asfreq <pandas.PeriodIndex.asfreq>` (so the original index\n        will map one-to-one to the new index).\n\n        Otherwise, the new index will be equivalent to ``pd.date_range(start, end,\n        freq=freq)`` where ``start`` and ``end`` are, respectively, the first and\n        last entries in the original index (see :func:`pandas.date_range`). The\n        values corresponding to any timesteps in the new index which were not present\n        in the original index will be null (``NaN``), unless a method for filling\n        such unknowns is provided (see the ``method`` parameter below).\n\n        The :meth:`resample` method is more appropriate if an operation on each group of\n        timesteps (such as an aggregate) is necessary to represent the data at the new\n        frequency.\n\n        Parameters\n        ----------\n        freq : DateOffset or str\n            Frequency DateOffset or string.\n        method : {{'backfill'/'bfill', 'pad'/'ffill'}}, default None\n            Method to use for filling holes in reindexed Series (note this\n            does not fill NaNs that already were present):\n\n            * 'pad' / 'ffill': propagate last valid observation forward to next\n              valid\n            * 'backfill' / 'bfill': use NEXT valid observation to fill.\n        how : {{'start', 'end'}}, default end\n            For PeriodIndex only (see PeriodIndex.asfreq).\n        normalize : bool, default False\n            Whether to reset output index to midnight.\n        fill_value : scalar, optional\n            Value to use for missing values, applied during upsampling (note\n            this does not fill NaNs that already were present).\n\n        Returns\n        -------\n        {klass}\n            {klass} object reindexed to the specified frequency.\n\n        See Also\n        --------\n        reindex : Conform DataFrame to new index with optional filling logic.\n\n        Notes\n        -----\n        To learn more about the frequency strings, please see `this link\n        <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`__.\n\n        Examples\n        --------\n        Start by creating a series with 4 one minute timestamps.\n\n        >>> index = pd.date_range('1/1/2000', periods=4, freq='T')\n        >>> series = pd.Series([0.0, None, 2.0, 3.0], index=index)\n        >>> df = pd.DataFrame({{'s': series}})\n        >>> df\n                               s\n        2000-01-01 00:00:00    0.0\n        2000-01-01 00:01:00    NaN\n        2000-01-01 00:02:00    2.0\n        2000-01-01 00:03:00    3.0\n\n        Upsample the series into 30 second bins.\n\n        >>> df.asfreq(freq='30S')\n                               s\n        2000-01-01 00:00:00    0.0\n        2000-01-01 00:00:30    NaN\n        2000-01-01 00:01:00    NaN\n        2000-01-01 00:01:30    NaN\n        2000-01-01 00:02:00    2.0\n        2000-01-01 00:02:30    NaN\n        2000-01-01 00:03:00    3.0\n\n        Upsample again, providing a ``fill value``.\n\n        >>> df.asfreq(freq='30S', fill_value=9.0)\n                               s\n        2000-01-01 00:00:00    0.0\n        2000-01-01 00:00:30    9.0\n        2000-01-01 00:01:00    NaN\n        2000-01-01 00:01:30    9.0\n        2000-01-01 00:02:00    2.0\n        2000-01-01 00:02:30    9.0\n        2000-01-01 00:03:00    3.0\n\n        Upsample again, providing a ``method``.\n\n        >>> df.asfreq(freq='30S', method='bfill')\n                               s\n        2000-01-01 00:00:00    0.0\n        2000-01-01 00:00:30    NaN\n        2000-01-01 00:01:00    NaN\n        2000-01-01 00:01:30    2.0\n        2000-01-01 00:02:00    2.0\n        2000-01-01 00:02:30    3.0\n        2000-01-01 00:03:00    3.0\n        \"\"\"\n        from pandas.core.resample import asfreq\n\n        return asfreq(\n            self,\n            freq,\n            method=method,\n            how=how,\n            normalize=normalize,\n            fill_value=fill_value,\n        )\n\n    @final\n    def at_time(self: NDFrameT, time, asof: bool_t = False, axis=None) -> NDFrameT:\n        \"\"\"\n        Select values at particular time of day (e.g., 9:30AM).\n\n        Parameters\n        ----------\n        time : datetime.time or str\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n\n        Returns\n        -------\n        Series or DataFrame\n\n        Raises\n        ------\n        TypeError\n            If the index is not  a :class:`DatetimeIndex`\n\n        See Also\n        --------\n        between_time : Select values between particular times of the day.\n        first : Select initial periods of time series based on a date offset.\n        last : Select final periods of time series based on a date offset.\n        DatetimeIndex.indexer_at_time : Get just the index locations for\n            values at particular time of the day.\n\n        Examples\n        --------\n        >>> i = pd.date_range('2018-04-09', periods=4, freq='12H')\n        >>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i)\n        >>> ts\n                             A\n        2018-04-09 00:00:00  1\n        2018-04-09 12:00:00  2\n        2018-04-10 00:00:00  3\n        2018-04-10 12:00:00  4\n\n        >>> ts.at_time('12:00')\n                             A\n        2018-04-09 12:00:00  2\n        2018-04-10 12:00:00  4\n        \"\"\"\n        if axis is None:\n            axis = self._stat_axis_number\n        axis = self._get_axis_number(axis)\n\n        index = self._get_axis(axis)\n\n        if not isinstance(index, DatetimeIndex):\n            raise TypeError(\"Index must be DatetimeIndex\")\n\n        indexer = index.indexer_at_time(time, asof=asof)\n        return self._take_with_is_copy(indexer, axis=axis)\n\n    @final\n    def between_time(\n        self: NDFrameT,\n        start_time,\n        end_time,\n        include_start: bool_t | lib.NoDefault = lib.no_default,\n        include_end: bool_t | lib.NoDefault = lib.no_default,\n        inclusive: str | None = None,\n        axis=None,\n    ) -> NDFrameT:\n        \"\"\"\n        Select values between particular times of the day (e.g., 9:00-9:30 AM).\n\n        By setting ``start_time`` to be later than ``end_time``,\n        you can get the times that are *not* between the two times.\n\n        Parameters\n        ----------\n        start_time : datetime.time or str\n            Initial time as a time filter limit.\n        end_time : datetime.time or str\n            End time as a time filter limit.\n        include_start : bool, default True\n            Whether the start time needs to be included in the result.\n\n            .. deprecated:: 1.4.0\n               Arguments `include_start` and `include_end` have been deprecated\n               to standardize boundary inputs. Use `inclusive` instead, to set\n               each bound as closed or open.\n        include_end : bool, default True\n            Whether the end time needs to be included in the result.\n\n            .. deprecated:: 1.4.0\n               Arguments `include_start` and `include_end` have been deprecated\n               to standardize boundary inputs. Use `inclusive` instead, to set\n               each bound as closed or open.\n        inclusive : {\"both\", \"neither\", \"left\", \"right\"}, default \"both\"\n            Include boundaries; whether to set each bound as closed or open.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Determine range time on index or columns value.\n\n        Returns\n        -------\n        Series or DataFrame\n            Data from the original object filtered to the specified dates range.\n\n        Raises\n        ------\n        TypeError\n            If the index is not  a :class:`DatetimeIndex`\n\n        See Also\n        --------\n        at_time : Select values at a particular time of the day.\n        first : Select initial periods of time series based on a date offset.\n        last : Select final periods of time series based on a date offset.\n        DatetimeIndex.indexer_between_time : Get just the index locations for\n            values between particular times of the day.\n\n        Examples\n        --------\n        >>> i = pd.date_range('2018-04-09', periods=4, freq='1D20min')\n        >>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i)\n        >>> ts\n                             A\n        2018-04-09 00:00:00  1\n        2018-04-10 00:20:00  2\n        2018-04-11 00:40:00  3\n        2018-04-12 01:00:00  4\n\n        >>> ts.between_time('0:15', '0:45')\n                             A\n        2018-04-10 00:20:00  2\n        2018-04-11 00:40:00  3\n\n        You get the times that are *not* between two times by setting\n        ``start_time`` later than ``end_time``:\n\n        >>> ts.between_time('0:45', '0:15')\n                             A\n        2018-04-09 00:00:00  1\n        2018-04-12 01:00:00  4\n        \"\"\"\n        if axis is None:\n            axis = self._stat_axis_number\n        axis = self._get_axis_number(axis)\n\n        index = self._get_axis(axis)\n        if not isinstance(index, DatetimeIndex):\n            raise TypeError(\"Index must be DatetimeIndex\")\n\n        old_include_arg_used = (include_start != lib.no_default) or (\n            include_end != lib.no_default\n        )\n\n        if old_include_arg_used and inclusive is not None:\n            raise ValueError(\n                \"Deprecated arguments `include_start` and `include_end` \"\n                \"cannot be passed if `inclusive` has been given.\"\n            )\n        # If any of the deprecated arguments ('include_start', 'include_end')\n        # have been passed\n        elif old_include_arg_used:\n            warnings.warn(\n                \"`include_start` and `include_end` are deprecated in \"\n                \"favour of `inclusive`.\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n            left = True if isinstance(include_start, lib.NoDefault) else include_start\n            right = True if isinstance(include_end, lib.NoDefault) else include_end\n\n            inc_dict = {\n                (True, True): \"both\",\n                (True, False): \"left\",\n                (False, True): \"right\",\n                (False, False): \"neither\",\n            }\n            inclusive = inc_dict[(left, right)]\n        elif inclusive is None:\n            # On arg removal inclusive can default to \"both\"\n            inclusive = \"both\"\n        left_inclusive, right_inclusive = validate_inclusive(inclusive)\n        indexer = index.indexer_between_time(\n            start_time,\n            end_time,\n            include_start=left_inclusive,\n            include_end=right_inclusive,\n        )\n        return self._take_with_is_copy(indexer, axis=axis)\n\n    @doc(**_shared_doc_kwargs)\n    def resample(\n        self,\n        rule,\n        axis=0,\n        closed: str | None = None,\n        label: str | None = None,\n        convention: str = \"start\",\n        kind: str | None = None,\n        loffset=None,\n        base: int | None = None,\n        on=None,\n        level=None,\n        origin: str | TimestampConvertibleTypes = \"start_day\",\n        offset: TimedeltaConvertibleTypes | None = None,\n    ) -> Resampler:\n        \"\"\"\n        Resample time-series data.\n\n        Convenience method for frequency conversion and resampling of time series.\n        The object must have a datetime-like index (`DatetimeIndex`, `PeriodIndex`,\n        or `TimedeltaIndex`), or the caller must pass the label of a datetime-like\n        series/index to the ``on``/``level`` keyword parameter.\n\n        Parameters\n        ----------\n        rule : DateOffset, Timedelta or str\n            The offset string or object representing target conversion.\n        axis : {{0 or 'index', 1 or 'columns'}}, default 0\n            Which axis to use for up- or down-sampling. For `Series` this\n            will default to 0, i.e. along the rows. Must be\n            `DatetimeIndex`, `TimedeltaIndex` or `PeriodIndex`.\n        closed : {{'right', 'left'}}, default None\n            Which side of bin interval is closed. The default is 'left'\n            for all frequency offsets except for 'M', 'A', 'Q', 'BM',\n            'BA', 'BQ', and 'W' which all have a default of 'right'.\n        label : {{'right', 'left'}}, default None\n            Which bin edge label to label bucket with. The default is 'left'\n            for all frequency offsets except for 'M', 'A', 'Q', 'BM',\n            'BA', 'BQ', and 'W' which all have a default of 'right'.\n        convention : {{'start', 'end', 's', 'e'}}, default 'start'\n            For `PeriodIndex` only, controls whether to use the start or\n            end of `rule`.\n        kind : {{'timestamp', 'period'}}, optional, default None\n            Pass 'timestamp' to convert the resulting index to a\n            `DateTimeIndex` or 'period' to convert it to a `PeriodIndex`.\n            By default the input representation is retained.\n        loffset : timedelta, default None\n            Adjust the resampled time labels.\n\n            .. deprecated:: 1.1.0\n                You should add the loffset to the `df.index` after the resample.\n                See below.\n\n        base : int, default 0\n            For frequencies that evenly subdivide 1 day, the \"origin\" of the\n            aggregated intervals. For example, for '5min' frequency, base could\n            range from 0 through 4. Defaults to 0.\n\n            .. deprecated:: 1.1.0\n                The new arguments that you should use are 'offset' or 'origin'.\n\n        on : str, optional\n            For a DataFrame, column to use instead of index for resampling.\n            Column must be datetime-like.\n        level : str or int, optional\n            For a MultiIndex, level (name or number) to use for\n            resampling. `level` must be datetime-like.\n        origin : Timestamp or str, default 'start_day'\n            The timestamp on which to adjust the grouping. The timezone of origin\n            must match the timezone of the index.\n            If string, must be one of the following:\n\n            - 'epoch': `origin` is 1970-01-01\n            - 'start': `origin` is the first value of the timeseries\n            - 'start_day': `origin` is the first day at midnight of the timeseries\n\n            .. versionadded:: 1.1.0\n\n            - 'end': `origin` is the last value of the timeseries\n            - 'end_day': `origin` is the ceiling midnight of the last day\n\n            .. versionadded:: 1.3.0\n\n        offset : Timedelta or str, default is None\n            An offset timedelta added to the origin.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        pandas.core.Resampler\n            :class:`~pandas.core.Resampler` object.\n\n        See Also\n        --------\n        Series.resample : Resample a Series.\n        DataFrame.resample : Resample a DataFrame.\n        groupby : Group {klass} by mapping, function, label, or list of labels.\n        asfreq : Reindex a {klass} with the given frequency without grouping.\n\n        Notes\n        -----\n        See the `user guide\n        <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#resampling>`__\n        for more.\n\n        To learn more about the offset strings, please see `this link\n        <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects>`__.\n\n        Examples\n        --------\n        Start by creating a series with 9 one minute timestamps.\n\n        >>> index = pd.date_range('1/1/2000', periods=9, freq='T')\n        >>> series = pd.Series(range(9), index=index)\n        >>> series\n        2000-01-01 00:00:00    0\n        2000-01-01 00:01:00    1\n        2000-01-01 00:02:00    2\n        2000-01-01 00:03:00    3\n        2000-01-01 00:04:00    4\n        2000-01-01 00:05:00    5\n        2000-01-01 00:06:00    6\n        2000-01-01 00:07:00    7\n        2000-01-01 00:08:00    8\n        Freq: T, dtype: int64\n\n        Downsample the series into 3 minute bins and sum the values\n        of the timestamps falling into a bin.\n\n        >>> series.resample('3T').sum()\n        2000-01-01 00:00:00     3\n        2000-01-01 00:03:00    12\n        2000-01-01 00:06:00    21\n        Freq: 3T, dtype: int64\n\n        Downsample the series into 3 minute bins as above, but label each\n        bin using the right edge instead of the left. Please note that the\n        value in the bucket used as the label is not included in the bucket,\n        which it labels. For example, in the original series the\n        bucket ``2000-01-01 00:03:00`` contains the value 3, but the summed\n        value in the resampled bucket with the label ``2000-01-01 00:03:00``\n        does not include 3 (if it did, the summed value would be 6, not 3).\n        To include this value close the right side of the bin interval as\n        illustrated in the example below this one.\n\n        >>> series.resample('3T', label='right').sum()\n        2000-01-01 00:03:00     3\n        2000-01-01 00:06:00    12\n        2000-01-01 00:09:00    21\n        Freq: 3T, dtype: int64\n\n        Downsample the series into 3 minute bins as above, but close the right\n        side of the bin interval.\n\n        >>> series.resample('3T', label='right', closed='right').sum()\n        2000-01-01 00:00:00     0\n        2000-01-01 00:03:00     6\n        2000-01-01 00:06:00    15\n        2000-01-01 00:09:00    15\n        Freq: 3T, dtype: int64\n\n        Upsample the series into 30 second bins.\n\n        >>> series.resample('30S').asfreq()[0:5]   # Select first 5 rows\n        2000-01-01 00:00:00   0.0\n        2000-01-01 00:00:30   NaN\n        2000-01-01 00:01:00   1.0\n        2000-01-01 00:01:30   NaN\n        2000-01-01 00:02:00   2.0\n        Freq: 30S, dtype: float64\n\n        Upsample the series into 30 second bins and fill the ``NaN``\n        values using the ``pad`` method.\n\n        >>> series.resample('30S').pad()[0:5]\n        2000-01-01 00:00:00    0\n        2000-01-01 00:00:30    0\n        2000-01-01 00:01:00    1\n        2000-01-01 00:01:30    1\n        2000-01-01 00:02:00    2\n        Freq: 30S, dtype: int64\n\n        Upsample the series into 30 second bins and fill the\n        ``NaN`` values using the ``bfill`` method.\n\n        >>> series.resample('30S').bfill()[0:5]\n        2000-01-01 00:00:00    0\n        2000-01-01 00:00:30    1\n        2000-01-01 00:01:00    1\n        2000-01-01 00:01:30    2\n        2000-01-01 00:02:00    2\n        Freq: 30S, dtype: int64\n\n        Pass a custom function via ``apply``\n\n        >>> def custom_resampler(arraylike):\n        ...     return np.sum(arraylike) + 5\n        ...\n        >>> series.resample('3T').apply(custom_resampler)\n        2000-01-01 00:00:00     8\n        2000-01-01 00:03:00    17\n        2000-01-01 00:06:00    26\n        Freq: 3T, dtype: int64\n\n        For a Series with a PeriodIndex, the keyword `convention` can be\n        used to control whether to use the start or end of `rule`.\n\n        Resample a year by quarter using 'start' `convention`. Values are\n        assigned to the first quarter of the period.\n\n        >>> s = pd.Series([1, 2], index=pd.period_range('2012-01-01',\n        ...                                             freq='A',\n        ...                                             periods=2))\n        >>> s\n        2012    1\n        2013    2\n        Freq: A-DEC, dtype: int64\n        >>> s.resample('Q', convention='start').asfreq()\n        2012Q1    1.0\n        2012Q2    NaN\n        2012Q3    NaN\n        2012Q4    NaN\n        2013Q1    2.0\n        2013Q2    NaN\n        2013Q3    NaN\n        2013Q4    NaN\n        Freq: Q-DEC, dtype: float64\n\n        Resample quarters by month using 'end' `convention`. Values are\n        assigned to the last month of the period.\n\n        >>> q = pd.Series([1, 2, 3, 4], index=pd.period_range('2018-01-01',\n        ...                                                   freq='Q',\n        ...                                                   periods=4))\n        >>> q\n        2018Q1    1\n        2018Q2    2\n        2018Q3    3\n        2018Q4    4\n        Freq: Q-DEC, dtype: int64\n        >>> q.resample('M', convention='end').asfreq()\n        2018-03    1.0\n        2018-04    NaN\n        2018-05    NaN\n        2018-06    2.0\n        2018-07    NaN\n        2018-08    NaN\n        2018-09    3.0\n        2018-10    NaN\n        2018-11    NaN\n        2018-12    4.0\n        Freq: M, dtype: float64\n\n        For DataFrame objects, the keyword `on` can be used to specify the\n        column instead of the index for resampling.\n\n        >>> d = {{'price': [10, 11, 9, 13, 14, 18, 17, 19],\n        ...      'volume': [50, 60, 40, 100, 50, 100, 40, 50]}}\n        >>> df = pd.DataFrame(d)\n        >>> df['week_starting'] = pd.date_range('01/01/2018',\n        ...                                     periods=8,\n        ...                                     freq='W')\n        >>> df\n           price  volume week_starting\n        0     10      50    2018-01-07\n        1     11      60    2018-01-14\n        2      9      40    2018-01-21\n        3     13     100    2018-01-28\n        4     14      50    2018-02-04\n        5     18     100    2018-02-11\n        6     17      40    2018-02-18\n        7     19      50    2018-02-25\n        >>> df.resample('M', on='week_starting').mean()\n                       price  volume\n        week_starting\n        2018-01-31     10.75    62.5\n        2018-02-28     17.00    60.0\n\n        For a DataFrame with MultiIndex, the keyword `level` can be used to\n        specify on which level the resampling needs to take place.\n\n        >>> days = pd.date_range('1/1/2000', periods=4, freq='D')\n        >>> d2 = {{'price': [10, 11, 9, 13, 14, 18, 17, 19],\n        ...       'volume': [50, 60, 40, 100, 50, 100, 40, 50]}}\n        >>> df2 = pd.DataFrame(\n        ...     d2,\n        ...     index=pd.MultiIndex.from_product(\n        ...         [days, ['morning', 'afternoon']]\n        ...     )\n        ... )\n        >>> df2\n                              price  volume\n        2000-01-01 morning       10      50\n                   afternoon     11      60\n        2000-01-02 morning        9      40\n                   afternoon     13     100\n        2000-01-03 morning       14      50\n                   afternoon     18     100\n        2000-01-04 morning       17      40\n                   afternoon     19      50\n        >>> df2.resample('D', level=0).sum()\n                    price  volume\n        2000-01-01     21     110\n        2000-01-02     22     140\n        2000-01-03     32     150\n        2000-01-04     36      90\n\n        If you want to adjust the start of the bins based on a fixed timestamp:\n\n        >>> start, end = '2000-10-01 23:30:00', '2000-10-02 00:30:00'\n        >>> rng = pd.date_range(start, end, freq='7min')\n        >>> ts = pd.Series(np.arange(len(rng)) * 3, index=rng)\n        >>> ts\n        2000-10-01 23:30:00     0\n        2000-10-01 23:37:00     3\n        2000-10-01 23:44:00     6\n        2000-10-01 23:51:00     9\n        2000-10-01 23:58:00    12\n        2000-10-02 00:05:00    15\n        2000-10-02 00:12:00    18\n        2000-10-02 00:19:00    21\n        2000-10-02 00:26:00    24\n        Freq: 7T, dtype: int64\n\n        >>> ts.resample('17min').sum()\n        2000-10-01 23:14:00     0\n        2000-10-01 23:31:00     9\n        2000-10-01 23:48:00    21\n        2000-10-02 00:05:00    54\n        2000-10-02 00:22:00    24\n        Freq: 17T, dtype: int64\n\n        >>> ts.resample('17min', origin='epoch').sum()\n        2000-10-01 23:18:00     0\n        2000-10-01 23:35:00    18\n        2000-10-01 23:52:00    27\n        2000-10-02 00:09:00    39\n        2000-10-02 00:26:00    24\n        Freq: 17T, dtype: int64\n\n        >>> ts.resample('17min', origin='2000-01-01').sum()\n        2000-10-01 23:24:00     3\n        2000-10-01 23:41:00    15\n        2000-10-01 23:58:00    45\n        2000-10-02 00:15:00    45\n        Freq: 17T, dtype: int64\n\n        If you want to adjust the start of the bins with an `offset` Timedelta, the two\n        following lines are equivalent:\n\n        >>> ts.resample('17min', origin='start').sum()\n        2000-10-01 23:30:00     9\n        2000-10-01 23:47:00    21\n        2000-10-02 00:04:00    54\n        2000-10-02 00:21:00    24\n        Freq: 17T, dtype: int64\n\n        >>> ts.resample('17min', offset='23h30min').sum()\n        2000-10-01 23:30:00     9\n        2000-10-01 23:47:00    21\n        2000-10-02 00:04:00    54\n        2000-10-02 00:21:00    24\n        Freq: 17T, dtype: int64\n\n        If you want to take the largest Timestamp as the end of the bins:\n\n        >>> ts.resample('17min', origin='end').sum()\n        2000-10-01 23:35:00     0\n        2000-10-01 23:52:00    18\n        2000-10-02 00:09:00    27\n        2000-10-02 00:26:00    63\n        Freq: 17T, dtype: int64\n\n        In contrast with the `start_day`, you can use `end_day` to take the ceiling\n        midnight of the largest Timestamp as the end of the bins and drop the bins\n        not containing data:\n\n        >>> ts.resample('17min', origin='end_day').sum()\n        2000-10-01 23:38:00     3\n        2000-10-01 23:55:00    15\n        2000-10-02 00:12:00    45\n        2000-10-02 00:29:00    45\n        Freq: 17T, dtype: int64\n\n        To replace the use of the deprecated `base` argument, you can now use `offset`,\n        in this example it is equivalent to have `base=2`:\n\n        >>> ts.resample('17min', offset='2min').sum()\n        2000-10-01 23:16:00     0\n        2000-10-01 23:33:00     9\n        2000-10-01 23:50:00    36\n        2000-10-02 00:07:00    39\n        2000-10-02 00:24:00    24\n        Freq: 17T, dtype: int64\n\n        To replace the use of the deprecated `loffset` argument:\n\n        >>> from pandas.tseries.frequencies import to_offset\n        >>> loffset = '19min'\n        >>> ts_out = ts.resample('17min').sum()\n        >>> ts_out.index = ts_out.index + to_offset(loffset)\n        >>> ts_out\n        2000-10-01 23:33:00     0\n        2000-10-01 23:50:00     9\n        2000-10-02 00:07:00    21\n        2000-10-02 00:24:00    54\n        2000-10-02 00:41:00    24\n        Freq: 17T, dtype: int64\n        \"\"\"\n        from pandas.core.resample import get_resampler\n\n        axis = self._get_axis_number(axis)\n        return get_resampler(\n            self,\n            freq=rule,\n            label=label,\n            closed=closed,\n            axis=axis,\n            kind=kind,\n            loffset=loffset,\n            convention=convention,\n            base=base,\n            key=on,\n            level=level,\n            origin=origin,\n            offset=offset,\n        )\n\n    @final\n    def first(self: NDFrameT, offset) -> NDFrameT:\n        \"\"\"\n        Select initial periods of time series data based on a date offset.\n\n        When having a DataFrame with dates as index, this function can\n        select the first few rows based on a date offset.\n\n        Parameters\n        ----------\n        offset : str, DateOffset or dateutil.relativedelta\n            The offset length of the data that will be selected. For instance,\n            '1M' will display all the rows having their index within the first month.\n\n        Returns\n        -------\n        Series or DataFrame\n            A subset of the caller.\n\n        Raises\n        ------\n        TypeError\n            If the index is not  a :class:`DatetimeIndex`\n\n        See Also\n        --------\n        last : Select final periods of time series based on a date offset.\n        at_time : Select values at a particular time of the day.\n        between_time : Select values between particular times of the day.\n\n        Examples\n        --------\n        >>> i = pd.date_range('2018-04-09', periods=4, freq='2D')\n        >>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i)\n        >>> ts\n                    A\n        2018-04-09  1\n        2018-04-11  2\n        2018-04-13  3\n        2018-04-15  4\n\n        Get the rows for the first 3 days:\n\n        >>> ts.first('3D')\n                    A\n        2018-04-09  1\n        2018-04-11  2\n\n        Notice the data for 3 first calendar days were returned, not the first\n        3 days observed in the dataset, and therefore data for 2018-04-13 was\n        not returned.\n        \"\"\"\n        if not isinstance(self.index, DatetimeIndex):\n            raise TypeError(\"'first' only supports a DatetimeIndex index\")\n\n        if len(self.index) == 0:\n            return self\n\n        offset = to_offset(offset)\n        if not isinstance(offset, Tick) and offset.is_on_offset(self.index[0]):\n            # GH#29623 if first value is end of period, remove offset with n = 1\n            #  before adding the real offset\n            end_date = end = self.index[0] - offset.base + offset\n        else:\n            end_date = end = self.index[0] + offset\n\n        # Tick-like, e.g. 3 weeks\n        if isinstance(offset, Tick) and end_date in self.index:\n            end = self.index.searchsorted(end_date, side=\"left\")\n            return self.iloc[:end]\n\n        return self.loc[:end]\n\n    @final\n    def last(self: NDFrameT, offset) -> NDFrameT:\n        \"\"\"\n        Select final periods of time series data based on a date offset.\n\n        For a DataFrame with a sorted DatetimeIndex, this function\n        selects the last few rows based on a date offset.\n\n        Parameters\n        ----------\n        offset : str, DateOffset, dateutil.relativedelta\n            The offset length of the data that will be selected. For instance,\n            '3D' will display all the rows having their index within the last 3 days.\n\n        Returns\n        -------\n        Series or DataFrame\n            A subset of the caller.\n\n        Raises\n        ------\n        TypeError\n            If the index is not  a :class:`DatetimeIndex`\n\n        See Also\n        --------\n        first : Select initial periods of time series based on a date offset.\n        at_time : Select values at a particular time of the day.\n        between_time : Select values between particular times of the day.\n\n        Examples\n        --------\n        >>> i = pd.date_range('2018-04-09', periods=4, freq='2D')\n        >>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i)\n        >>> ts\n                    A\n        2018-04-09  1\n        2018-04-11  2\n        2018-04-13  3\n        2018-04-15  4\n\n        Get the rows for the last 3 days:\n\n        >>> ts.last('3D')\n                    A\n        2018-04-13  3\n        2018-04-15  4\n\n        Notice the data for 3 last calendar days were returned, not the last\n        3 observed days in the dataset, and therefore data for 2018-04-11 was\n        not returned.\n        \"\"\"\n        if not isinstance(self.index, DatetimeIndex):\n            raise TypeError(\"'last' only supports a DatetimeIndex index\")\n\n        if len(self.index) == 0:\n            return self\n\n        offset = to_offset(offset)\n\n        start_date = self.index[-1] - offset\n        start = self.index.searchsorted(start_date, side=\"right\")\n        return self.iloc[start:]\n\n    @final\n    def rank(\n        self: NDFrameT,\n        axis=0,\n        method: str = \"average\",\n        numeric_only: bool_t | None | lib.NoDefault = lib.no_default,\n        na_option: str = \"keep\",\n        ascending: bool_t = True,\n        pct: bool_t = False,\n    ) -> NDFrameT:\n        \"\"\"\n        Compute numerical data ranks (1 through n) along axis.\n\n        By default, equal values are assigned a rank that is the average of the\n        ranks of those values.\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Index to direct ranking.\n        method : {'average', 'min', 'max', 'first', 'dense'}, default 'average'\n            How to rank the group of records that have the same value (i.e. ties):\n\n            * average: average rank of the group\n            * min: lowest rank in the group\n            * max: highest rank in the group\n            * first: ranks assigned in order they appear in the array\n            * dense: like 'min', but rank always increases by 1 between groups.\n\n        numeric_only : bool, optional\n            For DataFrame objects, rank only numeric columns if set to True.\n        na_option : {'keep', 'top', 'bottom'}, default 'keep'\n            How to rank NaN values:\n\n            * keep: assign NaN rank to NaN values\n            * top: assign lowest rank to NaN values\n            * bottom: assign highest rank to NaN values\n\n        ascending : bool, default True\n            Whether or not the elements should be ranked in ascending order.\n        pct : bool, default False\n            Whether or not to display the returned rankings in percentile\n            form.\n\n        Returns\n        -------\n        same type as caller\n            Return a Series or DataFrame with data ranks as values.\n\n        See Also\n        --------\n        core.groupby.GroupBy.rank : Rank of values within each group.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(data={'Animal': ['cat', 'penguin', 'dog',\n        ...                                    'spider', 'snake'],\n        ...                         'Number_legs': [4, 2, 4, 8, np.nan]})\n        >>> df\n            Animal  Number_legs\n        0      cat          4.0\n        1  penguin          2.0\n        2      dog          4.0\n        3   spider          8.0\n        4    snake          NaN\n\n        The following example shows how the method behaves with the above\n        parameters:\n\n        * default_rank: this is the default behaviour obtained without using\n          any parameter.\n        * max_rank: setting ``method = 'max'`` the records that have the\n          same values are ranked using the highest rank (e.g.: since 'cat'\n          and 'dog' are both in the 2nd and 3rd position, rank 3 is assigned.)\n        * NA_bottom: choosing ``na_option = 'bottom'``, if there are records\n          with NaN values they are placed at the bottom of the ranking.\n        * pct_rank: when setting ``pct = True``, the ranking is expressed as\n          percentile rank.\n\n        >>> df['default_rank'] = df['Number_legs'].rank()\n        >>> df['max_rank'] = df['Number_legs'].rank(method='max')\n        >>> df['NA_bottom'] = df['Number_legs'].rank(na_option='bottom')\n        >>> df['pct_rank'] = df['Number_legs'].rank(pct=True)\n        >>> df\n            Animal  Number_legs  default_rank  max_rank  NA_bottom  pct_rank\n        0      cat          4.0           2.5       3.0        2.5     0.625\n        1  penguin          2.0           1.0       1.0        1.0     0.250\n        2      dog          4.0           2.5       3.0        2.5     0.625\n        3   spider          8.0           4.0       4.0        4.0     1.000\n        4    snake          NaN           NaN       NaN        5.0       NaN\n        \"\"\"\n        warned = False\n        if numeric_only is None:\n            # GH#45036\n            warnings.warn(\n                f\"'numeric_only=None' in {type(self).__name__}.rank is deprecated \"\n                \"and will raise in a future version. Pass either 'True' or \"\n                \"'False'. 'False' will be the default.\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n            warned = True\n        elif numeric_only is lib.no_default:\n            numeric_only = None\n\n        axis = self._get_axis_number(axis)\n\n        if na_option not in {\"keep\", \"top\", \"bottom\"}:\n            msg = \"na_option must be one of 'keep', 'top', or 'bottom'\"\n            raise ValueError(msg)\n\n        def ranker(data):\n            if data.ndim == 2:\n                # i.e. DataFrame, we cast to ndarray\n                values = data.values\n            else:\n                # i.e. Series, can dispatch to EA\n                values = data._values\n\n            if isinstance(values, ExtensionArray):\n                ranks = values._rank(\n                    axis=axis,\n                    method=method,\n                    ascending=ascending,\n                    na_option=na_option,\n                    pct=pct,\n                )\n            else:\n                ranks = algos.rank(\n                    values,\n                    axis=axis,\n                    method=method,\n                    ascending=ascending,\n                    na_option=na_option,\n                    pct=pct,\n                )\n\n            ranks_obj = self._constructor(ranks, **data._construct_axes_dict())\n            return ranks_obj.__finalize__(self, method=\"rank\")\n\n        # if numeric_only is None, and we can't get anything, we try with\n        # numeric_only=True\n        if numeric_only is None:\n            try:\n                return ranker(self)\n            except TypeError:\n                numeric_only = True\n                if not warned:\n                    # Only warn here if we didn't already issue a warning above\n                    # GH#45036\n                    warnings.warn(\n                        f\"Dropping of nuisance columns in {type(self).__name__}.rank \"\n                        \"is deprecated; in a future version this will raise TypeError. \"\n                        \"Select only valid columns before calling rank.\",\n                        FutureWarning,\n                        stacklevel=find_stack_level(),\n                    )\n\n        if numeric_only:\n            data = self._get_numeric_data()\n        else:\n            data = self\n\n        return ranker(data)\n\n    @doc(_shared_docs[\"compare\"], klass=_shared_doc_kwargs[\"klass\"])\n    def compare(\n        self,\n        other,\n        align_axis: Axis = 1,\n        keep_shape: bool_t = False,\n        keep_equal: bool_t = False,\n    ):\n        from pandas.core.reshape.concat import concat\n\n        if type(self) is not type(other):\n            cls_self, cls_other = type(self).__name__, type(other).__name__\n            raise TypeError(\n                f\"can only compare '{cls_self}' (not '{cls_other}') with '{cls_self}'\"\n            )\n\n        mask = ~((self == other) | (self.isna() & other.isna()))\n        keys = [\"self\", \"other\"]\n\n        if not keep_equal:\n            self = self.where(mask)\n            other = other.where(mask)\n\n        if not keep_shape:\n            if isinstance(self, ABCDataFrame):\n                cmask = mask.any()\n                rmask = mask.any(axis=1)\n                self = self.loc[rmask, cmask]\n                other = other.loc[rmask, cmask]\n            else:\n                self = self[mask]\n                other = other[mask]\n\n        if align_axis in (1, \"columns\"):  # This is needed for Series\n            axis = 1\n        else:\n            axis = self._get_axis_number(align_axis)\n\n        diff = concat([self, other], axis=axis, keys=keys)\n\n        if axis >= self.ndim:\n            # No need to reorganize data if stacking on new axis\n            # This currently applies for stacking two Series on columns\n            return diff\n\n        ax = diff._get_axis(axis)\n        ax_names = np.array(ax.names)\n\n        # set index names to positions to avoid confusion\n        ax.names = np.arange(len(ax_names))\n\n        # bring self-other to inner level\n        order = list(range(1, ax.nlevels)) + [0]\n        if isinstance(diff, ABCDataFrame):\n            diff = diff.reorder_levels(order, axis=axis)\n        else:\n            diff = diff.reorder_levels(order)\n\n        # restore the index names in order\n        diff._get_axis(axis=axis).names = ax_names[order]\n\n        # reorder axis to keep things organized\n        indices = (\n            np.arange(diff.shape[axis]).reshape([2, diff.shape[axis] // 2]).T.flatten()\n        )\n        diff = diff.take(indices, axis=axis)\n\n        return diff\n\n    @doc(**_shared_doc_kwargs)\n    def align(\n        self,\n        other,\n        join=\"outer\",\n        axis=None,\n        level=None,\n        copy=True,\n        fill_value=None,\n        method=None,\n        limit=None,\n        fill_axis=0,\n        broadcast_axis=None,\n    ):\n        \"\"\"\n        Align two objects on their axes with the specified join method.\n\n        Join method is specified for each axis Index.\n\n        Parameters\n        ----------\n        other : DataFrame or Series\n        join : {{'outer', 'inner', 'left', 'right'}}, default 'outer'\n        axis : allowed axis of the other object, default None\n            Align on index (0), columns (1), or both (None).\n        level : int or level name, default None\n            Broadcast across a level, matching Index values on the\n            passed MultiIndex level.\n        copy : bool, default True\n            Always returns new objects. If copy=False and no reindexing is\n            required then original objects are returned.\n        fill_value : scalar, default np.NaN\n            Value to use for missing values. Defaults to NaN, but can be any\n            \"compatible\" value.\n        method : {{'backfill', 'bfill', 'pad', 'ffill', None}}, default None\n            Method to use for filling holes in reindexed Series:\n\n            - pad / ffill: propagate last valid observation forward to next valid.\n            - backfill / bfill: use NEXT valid observation to fill gap.\n\n        limit : int, default None\n            If method is specified, this is the maximum number of consecutive\n            NaN values to forward/backward fill. In other words, if there is\n            a gap with more than this number of consecutive NaNs, it will only\n            be partially filled. If method is not specified, this is the\n            maximum number of entries along the entire axis where NaNs will be\n            filled. Must be greater than 0 if not None.\n        fill_axis : {axes_single_arg}, default 0\n            Filling axis, method and limit.\n        broadcast_axis : {axes_single_arg}, default None\n            Broadcast values along this axis, if aligning two objects of\n            different dimensions.\n\n        Returns\n        -------\n        (left, right) : ({klass}, type of other)\n            Aligned objects.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(\n        ...     [[1, 2, 3, 4], [6, 7, 8, 9]], columns=[\"D\", \"B\", \"E\", \"A\"], index=[1, 2]\n        ... )\n        >>> other = pd.DataFrame(\n        ...     [[10, 20, 30, 40], [60, 70, 80, 90], [600, 700, 800, 900]],\n        ...     columns=[\"A\", \"B\", \"C\", \"D\"],\n        ...     index=[2, 3, 4],\n        ... )\n        >>> df\n           D  B  E  A\n        1  1  2  3  4\n        2  6  7  8  9\n        >>> other\n            A    B    C    D\n        2   10   20   30   40\n        3   60   70   80   90\n        4  600  700  800  900\n\n        Align on columns:\n\n        >>> left, right = df.align(other, join=\"outer\", axis=1)\n        >>> left\n           A  B   C  D  E\n        1  4  2 NaN  1  3\n        2  9  7 NaN  6  8\n        >>> right\n            A    B    C    D   E\n        2   10   20   30   40 NaN\n        3   60   70   80   90 NaN\n        4  600  700  800  900 NaN\n\n        We can also align on the index:\n\n        >>> left, right = df.align(other, join=\"outer\", axis=0)\n        >>> left\n            D    B    E    A\n        1  1.0  2.0  3.0  4.0\n        2  6.0  7.0  8.0  9.0\n        3  NaN  NaN  NaN  NaN\n        4  NaN  NaN  NaN  NaN\n        >>> right\n            A      B      C      D\n        1    NaN    NaN    NaN    NaN\n        2   10.0   20.0   30.0   40.0\n        3   60.0   70.0   80.0   90.0\n        4  600.0  700.0  800.0  900.0\n\n        Finally, the default `axis=None` will align on both index and columns:\n\n        >>> left, right = df.align(other, join=\"outer\", axis=None)\n        >>> left\n             A    B   C    D    E\n        1  4.0  2.0 NaN  1.0  3.0\n        2  9.0  7.0 NaN  6.0  8.0\n        3  NaN  NaN NaN  NaN  NaN\n        4  NaN  NaN NaN  NaN  NaN\n        >>> right\n               A      B      C      D   E\n        1    NaN    NaN    NaN    NaN NaN\n        2   10.0   20.0   30.0   40.0 NaN\n        3   60.0   70.0   80.0   90.0 NaN\n        4  600.0  700.0  800.0  900.0 NaN\n        \"\"\"\n\n        method = missing.clean_fill_method(method)\n\n        if broadcast_axis == 1 and self.ndim != other.ndim:\n            if isinstance(self, ABCSeries):\n                # this means other is a DataFrame, and we need to broadcast\n                # self\n                cons = self._constructor_expanddim\n                df = cons(\n                    {c: self for c in other.columns}, **other._construct_axes_dict()\n                )\n                return df._align_frame(\n                    other,\n                    join=join,\n                    axis=axis,\n                    level=level,\n                    copy=copy,\n                    fill_value=fill_value,\n                    method=method,\n                    limit=limit,\n                    fill_axis=fill_axis,\n                )\n            elif isinstance(other, ABCSeries):\n                # this means self is a DataFrame, and we need to broadcast\n                # other\n                cons = other._constructor_expanddim\n                df = cons(\n                    {c: other for c in self.columns}, **self._construct_axes_dict()\n                )\n                return self._align_frame(\n                    df,\n                    join=join,\n                    axis=axis,\n                    level=level,\n                    copy=copy,\n                    fill_value=fill_value,\n                    method=method,\n                    limit=limit,\n                    fill_axis=fill_axis,\n                )\n\n        if axis is not None:\n            axis = self._get_axis_number(axis)\n        if isinstance(other, ABCDataFrame):\n            return self._align_frame(\n                other,\n                join=join,\n                axis=axis,\n                level=level,\n                copy=copy,\n                fill_value=fill_value,\n                method=method,\n                limit=limit,\n                fill_axis=fill_axis,\n            )\n        elif isinstance(other, ABCSeries):\n            return self._align_series(\n                other,\n                join=join,\n                axis=axis,\n                level=level,\n                copy=copy,\n                fill_value=fill_value,\n                method=method,\n                limit=limit,\n                fill_axis=fill_axis,\n            )\n        else:  # pragma: no cover\n            raise TypeError(f\"unsupported type: {type(other)}\")\n\n    @final\n    def _align_frame(\n        self,\n        other,\n        join=\"outer\",\n        axis=None,\n        level=None,\n        copy: bool_t = True,\n        fill_value=None,\n        method=None,\n        limit=None,\n        fill_axis=0,\n    ):\n        # defaults\n        join_index, join_columns = None, None\n        ilidx, iridx = None, None\n        clidx, cridx = None, None\n\n        is_series = isinstance(self, ABCSeries)\n\n        if (axis is None or axis == 0) and not self.index.equals(other.index):\n            join_index, ilidx, iridx = self.index.join(\n                other.index, how=join, level=level, return_indexers=True\n            )\n\n        if (\n            (axis is None or axis == 1)\n            and not is_series\n            and not self.columns.equals(other.columns)\n        ):\n            join_columns, clidx, cridx = self.columns.join(\n                other.columns, how=join, level=level, return_indexers=True\n            )\n\n        if is_series:\n            reindexers = {0: [join_index, ilidx]}\n        else:\n            reindexers = {0: [join_index, ilidx], 1: [join_columns, clidx]}\n\n        left = self._reindex_with_indexers(\n            reindexers, copy=copy, fill_value=fill_value, allow_dups=True\n        )\n        # other must be always DataFrame\n        right = other._reindex_with_indexers(\n            {0: [join_index, iridx], 1: [join_columns, cridx]},\n            copy=copy,\n            fill_value=fill_value,\n            allow_dups=True,\n        )\n\n        if method is not None:\n            _left = left.fillna(method=method, axis=fill_axis, limit=limit)\n            assert _left is not None  # needed for mypy\n            left = _left\n            right = right.fillna(method=method, axis=fill_axis, limit=limit)\n\n        # if DatetimeIndex have different tz, convert to UTC\n        left, right = _align_as_utc(left, right, join_index)\n\n        return (\n            left.__finalize__(self),\n            right.__finalize__(other),\n        )\n\n    @final\n    def _align_series(\n        self,\n        other,\n        join=\"outer\",\n        axis=None,\n        level=None,\n        copy: bool_t = True,\n        fill_value=None,\n        method=None,\n        limit=None,\n        fill_axis=0,\n    ):\n\n        is_series = isinstance(self, ABCSeries)\n\n        # series/series compat, other must always be a Series\n        if is_series:\n            if axis:\n                raise ValueError(\"cannot align series to a series other than axis 0\")\n\n            # equal\n            if self.index.equals(other.index):\n                join_index, lidx, ridx = None, None, None\n            else:\n                join_index, lidx, ridx = self.index.join(\n                    other.index, how=join, level=level, return_indexers=True\n                )\n\n            left = self._reindex_indexer(join_index, lidx, copy)\n            right = other._reindex_indexer(join_index, ridx, copy)\n\n        else:\n            # one has > 1 ndim\n            fdata = self._mgr\n            if axis in [0, 1]:\n                join_index = self.axes[axis]\n                lidx, ridx = None, None\n                if not join_index.equals(other.index):\n                    join_index, lidx, ridx = join_index.join(\n                        other.index, how=join, level=level, return_indexers=True\n                    )\n\n                if lidx is not None:\n                    bm_axis = self._get_block_manager_axis(axis)\n                    fdata = fdata.reindex_indexer(join_index, lidx, axis=bm_axis)\n\n            else:\n                raise ValueError(\"Must specify axis=0 or 1\")\n\n            if copy and fdata is self._mgr:\n                fdata = fdata.copy()\n\n            left = self._constructor(fdata)\n\n            if ridx is None:\n                right = other\n            else:\n                right = other.reindex(join_index, level=level)\n\n        # fill\n        fill_na = notna(fill_value) or (method is not None)\n        if fill_na:\n            left = left.fillna(fill_value, method=method, limit=limit, axis=fill_axis)\n            right = right.fillna(fill_value, method=method, limit=limit)\n\n        # if DatetimeIndex have different tz, convert to UTC\n        if is_series or (not is_series and axis == 0):\n            left, right = _align_as_utc(left, right, join_index)\n\n        return (\n            left.__finalize__(self),\n            right.__finalize__(other),\n        )\n\n    @final\n    def _where(\n        self,\n        cond,\n        other=lib.no_default,\n        inplace=False,\n        axis=None,\n        level=None,\n        errors=\"raise\",\n    ):\n        \"\"\"\n        Equivalent to public method `where`, except that `other` is not\n        applied as a function even if callable. Used in __setitem__.\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n\n        if axis is not None:\n            axis = self._get_axis_number(axis)\n\n        # align the cond to same shape as myself\n        cond = com.apply_if_callable(cond, self)\n        if isinstance(cond, NDFrame):\n            cond, _ = cond.align(self, join=\"right\", broadcast_axis=1, copy=False)\n        else:\n            if not hasattr(cond, \"shape\"):\n                cond = np.asanyarray(cond)\n            if cond.shape != self.shape:\n                raise ValueError(\"Array conditional must be same shape as self\")\n            cond = self._constructor(cond, **self._construct_axes_dict())\n\n        # make sure we are boolean\n        fill_value = bool(inplace)\n        cond = cond.fillna(fill_value)\n\n        msg = \"Boolean array expected for the condition, not {dtype}\"\n\n        if not cond.empty:\n            if not isinstance(cond, ABCDataFrame):\n                # This is a single-dimensional object.\n                if not is_bool_dtype(cond):\n                    raise ValueError(msg.format(dtype=cond.dtype))\n            else:\n                for dt in cond.dtypes:\n                    if not is_bool_dtype(dt):\n                        raise ValueError(msg.format(dtype=dt))\n        else:\n            # GH#21947 we have an empty DataFrame/Series, could be object-dtype\n            cond = cond.astype(bool)\n\n        cond = -cond if inplace else cond\n        cond = cond.reindex(self._info_axis, axis=self._info_axis_number, copy=False)\n\n        # try to align with other\n        if isinstance(other, NDFrame):\n\n            # align with me\n            if other.ndim <= self.ndim:\n\n                _, other = self.align(\n                    other,\n                    join=\"left\",\n                    axis=axis,\n                    level=level,\n                    fill_value=None,\n                    copy=False,\n                )\n\n                # if we are NOT aligned, raise as we cannot where index\n                if axis is None and not other._indexed_same(self):\n                    raise InvalidIndexError\n\n                elif other.ndim < self.ndim:\n                    # TODO(EA2D): avoid object-dtype cast in EA case GH#38729\n                    other = other._values\n                    if axis == 0:\n                        other = np.reshape(other, (-1, 1))\n                    elif axis == 1:\n                        other = np.reshape(other, (1, -1))\n\n                    other = np.broadcast_to(other, self.shape)\n\n            # slice me out of the other\n            else:\n                raise NotImplementedError(\n                    \"cannot align with a higher dimensional NDFrame\"\n                )\n\n        elif not isinstance(other, (MultiIndex, NDFrame)):\n            # mainly just catching Index here\n            other = extract_array(other, extract_numpy=True)\n\n        if isinstance(other, (np.ndarray, ExtensionArray)):\n\n            if other.shape != self.shape:\n                if self.ndim != 1:\n                    # In the ndim == 1 case we may have\n                    #  other length 1, which we treat as scalar (GH#2745, GH#4192)\n                    #  or len(other) == icond.sum(), which we treat like\n                    #  __setitem__ (GH#3235)\n                    raise ValueError(\n                        \"other must be the same shape as self when an ndarray\"\n                    )\n\n            # we are the same shape, so create an actual object for alignment\n            else:\n                other = self._constructor(other, **self._construct_axes_dict())\n\n        if axis is None:\n            axis = 0\n\n        if self.ndim == getattr(other, \"ndim\", 0):\n            align = True\n        else:\n            align = self._get_axis_number(axis) == 1\n\n        if inplace:\n            # we may have different type blocks come out of putmask, so\n            # reconstruct the block manager\n\n            self._check_inplace_setting(other)\n            new_data = self._mgr.putmask(mask=cond, new=other, align=align)\n            result = self._constructor(new_data)\n            return self._update_inplace(result)\n\n        else:\n            new_data = self._mgr.where(\n                other=other,\n                cond=cond,\n                align=align,\n            )\n            result = self._constructor(new_data)\n            return result.__finalize__(self)\n\n    @doc(\n        klass=_shared_doc_kwargs[\"klass\"],\n        cond=\"True\",\n        cond_rev=\"False\",\n        name=\"where\",\n        name_other=\"mask\",\n    )\n    def where(\n        self,\n        cond,\n        other=np.nan,\n        inplace=False,\n        axis=None,\n        level=None,\n        errors=\"raise\",\n        try_cast=lib.no_default,\n    ):\n        \"\"\"\n        Replace values where the condition is {cond_rev}.\n\n        Parameters\n        ----------\n        cond : bool {klass}, array-like, or callable\n            Where `cond` is {cond}, keep the original value. Where\n            {cond_rev}, replace with corresponding value from `other`.\n            If `cond` is callable, it is computed on the {klass} and\n            should return boolean {klass} or array. The callable must\n            not change input {klass} (though pandas doesn't check it).\n        other : scalar, {klass}, or callable\n            Entries where `cond` is {cond_rev} are replaced with\n            corresponding value from `other`.\n            If other is callable, it is computed on the {klass} and\n            should return scalar or {klass}. The callable must not\n            change input {klass} (though pandas doesn't check it).\n        inplace : bool, default False\n            Whether to perform the operation in place on the data.\n        axis : int, default None\n            Alignment axis if needed.\n        level : int, default None\n            Alignment level if needed.\n        errors : str, {{'raise', 'ignore'}}, default 'raise'\n            Note that currently this parameter won't affect\n            the results and will always coerce to a suitable dtype.\n\n            - 'raise' : allow exceptions to be raised.\n            - 'ignore' : suppress exceptions. On error return original object.\n\n        try_cast : bool, default None\n            Try to cast the result back to the input type (if possible).\n\n            .. deprecated:: 1.3.0\n                Manually cast back if necessary.\n\n        Returns\n        -------\n        Same type as caller or None if ``inplace=True``.\n\n        See Also\n        --------\n        :func:`DataFrame.{name_other}` : Return an object of same shape as\n            self.\n\n        Notes\n        -----\n        The {name} method is an application of the if-then idiom. For each\n        element in the calling DataFrame, if ``cond`` is ``{cond}`` the\n        element is used; otherwise the corresponding element from the DataFrame\n        ``other`` is used.\n\n        The signature for :func:`DataFrame.where` differs from\n        :func:`numpy.where`. Roughly ``df1.where(m, df2)`` is equivalent to\n        ``np.where(m, df1, df2)``.\n\n        For further details and examples see the ``{name}`` documentation in\n        :ref:`indexing <indexing.where_mask>`.\n\n        Examples\n        --------\n        >>> s = pd.Series(range(5))\n        >>> s.where(s > 0)\n        0    NaN\n        1    1.0\n        2    2.0\n        3    3.0\n        4    4.0\n        dtype: float64\n        >>> s.mask(s > 0)\n        0    0.0\n        1    NaN\n        2    NaN\n        3    NaN\n        4    NaN\n        dtype: float64\n\n        >>> s.where(s > 1, 10)\n        0    10\n        1    10\n        2    2\n        3    3\n        4    4\n        dtype: int64\n        >>> s.mask(s > 1, 10)\n        0     0\n        1     1\n        2    10\n        3    10\n        4    10\n        dtype: int64\n\n        >>> df = pd.DataFrame(np.arange(10).reshape(-1, 2), columns=['A', 'B'])\n        >>> df\n           A  B\n        0  0  1\n        1  2  3\n        2  4  5\n        3  6  7\n        4  8  9\n        >>> m = df % 3 == 0\n        >>> df.where(m, -df)\n           A  B\n        0  0 -1\n        1 -2  3\n        2 -4 -5\n        3  6 -7\n        4 -8  9\n        >>> df.where(m, -df) == np.where(m, df, -df)\n              A     B\n        0  True  True\n        1  True  True\n        2  True  True\n        3  True  True\n        4  True  True\n        >>> df.where(m, -df) == df.mask(~m, -df)\n              A     B\n        0  True  True\n        1  True  True\n        2  True  True\n        3  True  True\n        4  True  True\n        \"\"\"\n        other = com.apply_if_callable(other, self)\n\n        if try_cast is not lib.no_default:\n            warnings.warn(\n                \"try_cast keyword is deprecated and will be removed in a \"\n                \"future version.\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n\n        return self._where(cond, other, inplace, axis, level, errors=errors)\n\n    @doc(\n        where,\n        klass=_shared_doc_kwargs[\"klass\"],\n        cond=\"False\",\n        cond_rev=\"True\",\n        name=\"mask\",\n        name_other=\"where\",\n    )\n    def mask(\n        self,\n        cond,\n        other=np.nan,\n        inplace=False,\n        axis=None,\n        level=None,\n        errors=\"raise\",\n        try_cast=lib.no_default,\n    ):\n\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        cond = com.apply_if_callable(cond, self)\n\n        if try_cast is not lib.no_default:\n            warnings.warn(\n                \"try_cast keyword is deprecated and will be removed in a \"\n                \"future version.\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n\n        # see gh-21891\n        if not hasattr(cond, \"__invert__\"):\n            cond = np.array(cond)\n\n        return self.where(\n            ~cond,\n            other=other,\n            inplace=inplace,\n            axis=axis,\n            level=level,\n            errors=errors,\n        )\n\n    @doc(klass=_shared_doc_kwargs[\"klass\"])\n    def shift(\n        self: NDFrameT, periods=1, freq=None, axis=0, fill_value=None\n    ) -> NDFrameT:\n        \"\"\"\n        Shift index by desired number of periods with an optional time `freq`.\n\n        When `freq` is not passed, shift the index without realigning the data.\n        If `freq` is passed (in this case, the index must be date or datetime,\n        or it will raise a `NotImplementedError`), the index will be\n        increased using the periods and the `freq`. `freq` can be inferred\n        when specified as \"infer\" as long as either freq or inferred_freq\n        attribute is set in the index.\n\n        Parameters\n        ----------\n        periods : int\n            Number of periods to shift. Can be positive or negative.\n        freq : DateOffset, tseries.offsets, timedelta, or str, optional\n            Offset to use from the tseries module or time rule (e.g. 'EOM').\n            If `freq` is specified then the index values are shifted but the\n            data is not realigned. That is, use `freq` if you would like to\n            extend the index when shifting and preserve the original data.\n            If `freq` is specified as \"infer\" then it will be inferred from\n            the freq or inferred_freq attributes of the index. If neither of\n            those attributes exist, a ValueError is thrown.\n        axis : {{0 or 'index', 1 or 'columns', None}}, default None\n            Shift direction.\n        fill_value : object, optional\n            The scalar value to use for newly introduced missing values.\n            the default depends on the dtype of `self`.\n            For numeric data, ``np.nan`` is used.\n            For datetime, timedelta, or period data, etc. :attr:`NaT` is used.\n            For extension dtypes, ``self.dtype.na_value`` is used.\n\n            .. versionchanged:: 1.1.0\n\n        Returns\n        -------\n        {klass}\n            Copy of input object, shifted.\n\n        See Also\n        --------\n        Index.shift : Shift values of Index.\n        DatetimeIndex.shift : Shift values of DatetimeIndex.\n        PeriodIndex.shift : Shift values of PeriodIndex.\n        tshift : Shift the time index, using the index's frequency if\n            available.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({{\"Col1\": [10, 20, 15, 30, 45],\n        ...                    \"Col2\": [13, 23, 18, 33, 48],\n        ...                    \"Col3\": [17, 27, 22, 37, 52]}},\n        ...                   index=pd.date_range(\"2020-01-01\", \"2020-01-05\"))\n        >>> df\n                    Col1  Col2  Col3\n        2020-01-01    10    13    17\n        2020-01-02    20    23    27\n        2020-01-03    15    18    22\n        2020-01-04    30    33    37\n        2020-01-05    45    48    52\n\n        >>> df.shift(periods=3)\n                    Col1  Col2  Col3\n        2020-01-01   NaN   NaN   NaN\n        2020-01-02   NaN   NaN   NaN\n        2020-01-03   NaN   NaN   NaN\n        2020-01-04  10.0  13.0  17.0\n        2020-01-05  20.0  23.0  27.0\n\n        >>> df.shift(periods=1, axis=\"columns\")\n                    Col1  Col2  Col3\n        2020-01-01   NaN    10    13\n        2020-01-02   NaN    20    23\n        2020-01-03   NaN    15    18\n        2020-01-04   NaN    30    33\n        2020-01-05   NaN    45    48\n\n        >>> df.shift(periods=3, fill_value=0)\n                    Col1  Col2  Col3\n        2020-01-01     0     0     0\n        2020-01-02     0     0     0\n        2020-01-03     0     0     0\n        2020-01-04    10    13    17\n        2020-01-05    20    23    27\n\n        >>> df.shift(periods=3, freq=\"D\")\n                    Col1  Col2  Col3\n        2020-01-04    10    13    17\n        2020-01-05    20    23    27\n        2020-01-06    15    18    22\n        2020-01-07    30    33    37\n        2020-01-08    45    48    52\n\n        >>> df.shift(periods=3, freq=\"infer\")\n                    Col1  Col2  Col3\n        2020-01-04    10    13    17\n        2020-01-05    20    23    27\n        2020-01-06    15    18    22\n        2020-01-07    30    33    37\n        2020-01-08    45    48    52\n        \"\"\"\n        if periods == 0:\n            return self.copy()\n\n        if freq is None:\n            # when freq is None, data is shifted, index is not\n            axis = self._get_axis_number(axis)\n            new_data = self._mgr.shift(\n                periods=periods, axis=axis, fill_value=fill_value\n            )\n            return self._constructor(new_data).__finalize__(self, method=\"shift\")\n\n        # when freq is given, index is shifted, data is not\n        index = self._get_axis(axis)\n\n        if freq == \"infer\":\n            freq = getattr(index, \"freq\", None)\n\n            if freq is None:\n                freq = getattr(index, \"inferred_freq\", None)\n\n            if freq is None:\n                msg = \"Freq was not set in the index hence cannot be inferred\"\n                raise ValueError(msg)\n\n        elif isinstance(freq, str):\n            freq = to_offset(freq)\n\n        if isinstance(index, PeriodIndex):\n            orig_freq = to_offset(index.freq)\n            if freq != orig_freq:\n                assert orig_freq is not None  # for mypy\n                raise ValueError(\n                    f\"Given freq {freq.rule_code} does not match \"\n                    f\"PeriodIndex freq {orig_freq.rule_code}\"\n                )\n            new_ax = index.shift(periods)\n        else:\n            new_ax = index.shift(periods, freq)\n\n        result = self.set_axis(new_ax, axis=axis)\n        return result.__finalize__(self, method=\"shift\")\n\n    @final\n    def slice_shift(self: NDFrameT, periods: int = 1, axis=0) -> NDFrameT:\n        \"\"\"\n        Equivalent to `shift` without copying data.\n        The shifted data will not include the dropped periods and the\n        shifted axis will be smaller than the original.\n\n        .. deprecated:: 1.2.0\n            slice_shift is deprecated,\n            use DataFrame/Series.shift instead.\n\n        Parameters\n        ----------\n        periods : int\n            Number of periods to move, can be positive or negative.\n\n        Returns\n        -------\n        shifted : same type as caller\n\n        Notes\n        -----\n        While the `slice_shift` is faster than `shift`, you may pay for it\n        later during alignment.\n        \"\"\"\n\n        msg = (\n            \"The 'slice_shift' method is deprecated \"\n            \"and will be removed in a future version. \"\n            \"You can use DataFrame/Series.shift instead.\"\n        )\n        warnings.warn(msg, FutureWarning, stacklevel=find_stack_level())\n\n        if periods == 0:\n            return self\n\n        if periods > 0:\n            vslicer = slice(None, -periods)\n            islicer = slice(periods, None)\n        else:\n            vslicer = slice(-periods, None)\n            islicer = slice(None, periods)\n\n        new_obj = self._slice(vslicer, axis=axis)\n        shifted_axis = self._get_axis(axis)[islicer]\n        new_obj.set_axis(shifted_axis, axis=axis, inplace=True)\n\n        return new_obj.__finalize__(self, method=\"slice_shift\")\n\n    @final\n    def tshift(self: NDFrameT, periods: int = 1, freq=None, axis: Axis = 0) -> NDFrameT:\n        \"\"\"\n        Shift the time index, using the index's frequency if available.\n\n        .. deprecated:: 1.1.0\n            Use `shift` instead.\n\n        Parameters\n        ----------\n        periods : int\n            Number of periods to move, can be positive or negative.\n        freq : DateOffset, timedelta, or str, default None\n            Increment to use from the tseries module\n            or time rule expressed as a string (e.g. 'EOM').\n        axis : {0 or \u2018index\u2019, 1 or \u2018columns\u2019, None}, default 0\n            Corresponds to the axis that contains the Index.\n\n        Returns\n        -------\n        shifted : Series/DataFrame\n\n        Notes\n        -----\n        If freq is not specified then tries to use the freq or inferred_freq\n        attributes of the index. If neither of those attributes exist, a\n        ValueError is thrown\n        \"\"\"\n        warnings.warn(\n            (\n                \"tshift is deprecated and will be removed in a future version. \"\n                \"Please use shift instead.\"\n            ),\n            FutureWarning,\n            stacklevel=find_stack_level(),\n        )\n\n        if freq is None:\n            freq = \"infer\"\n\n        return self.shift(periods, freq, axis)\n\n    def truncate(\n        self: NDFrameT, before=None, after=None, axis=None, copy: bool_t = True\n    ) -> NDFrameT:\n        \"\"\"\n        Truncate a Series or DataFrame before and after some index value.\n\n        This is a useful shorthand for boolean indexing based on index\n        values above or below certain thresholds.\n\n        Parameters\n        ----------\n        before : date, str, int\n            Truncate all rows before this index value.\n        after : date, str, int\n            Truncate all rows after this index value.\n        axis : {0 or 'index', 1 or 'columns'}, optional\n            Axis to truncate. Truncates the index (rows) by default.\n        copy : bool, default is True,\n            Return a copy of the truncated section.\n\n        Returns\n        -------\n        type of caller\n            The truncated Series or DataFrame.\n\n        See Also\n        --------\n        DataFrame.loc : Select a subset of a DataFrame by label.\n        DataFrame.iloc : Select a subset of a DataFrame by position.\n\n        Notes\n        -----\n        If the index being truncated contains only datetime values,\n        `before` and `after` may be specified as strings instead of\n        Timestamps.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': ['a', 'b', 'c', 'd', 'e'],\n        ...                    'B': ['f', 'g', 'h', 'i', 'j'],\n        ...                    'C': ['k', 'l', 'm', 'n', 'o']},\n        ...                   index=[1, 2, 3, 4, 5])\n        >>> df\n           A  B  C\n        1  a  f  k\n        2  b  g  l\n        3  c  h  m\n        4  d  i  n\n        5  e  j  o\n\n        >>> df.truncate(before=2, after=4)\n           A  B  C\n        2  b  g  l\n        3  c  h  m\n        4  d  i  n\n\n        The columns of a DataFrame can be truncated.\n\n        >>> df.truncate(before=\"A\", after=\"B\", axis=\"columns\")\n           A  B\n        1  a  f\n        2  b  g\n        3  c  h\n        4  d  i\n        5  e  j\n\n        For Series, only rows can be truncated.\n\n        >>> df['A'].truncate(before=2, after=4)\n        2    b\n        3    c\n        4    d\n        Name: A, dtype: object\n\n        The index values in ``truncate`` can be datetimes or string\n        dates.\n\n        >>> dates = pd.date_range('2016-01-01', '2016-02-01', freq='s')\n        >>> df = pd.DataFrame(index=dates, data={'A': 1})\n        >>> df.tail()\n                             A\n        2016-01-31 23:59:56  1\n        2016-01-31 23:59:57  1\n        2016-01-31 23:59:58  1\n        2016-01-31 23:59:59  1\n        2016-02-01 00:00:00  1\n\n        >>> df.truncate(before=pd.Timestamp('2016-01-05'),\n        ...             after=pd.Timestamp('2016-01-10')).tail()\n                             A\n        2016-01-09 23:59:56  1\n        2016-01-09 23:59:57  1\n        2016-01-09 23:59:58  1\n        2016-01-09 23:59:59  1\n        2016-01-10 00:00:00  1\n\n        Because the index is a DatetimeIndex containing only dates, we can\n        specify `before` and `after` as strings. They will be coerced to\n        Timestamps before truncation.\n\n        >>> df.truncate('2016-01-05', '2016-01-10').tail()\n                             A\n        2016-01-09 23:59:56  1\n        2016-01-09 23:59:57  1\n        2016-01-09 23:59:58  1\n        2016-01-09 23:59:59  1\n        2016-01-10 00:00:00  1\n\n        Note that ``truncate`` assumes a 0 value for any unspecified time\n        component (midnight). This differs from partial string slicing, which\n        returns any partially matching dates.\n\n        >>> df.loc['2016-01-05':'2016-01-10', :].tail()\n                             A\n        2016-01-10 23:59:55  1\n        2016-01-10 23:59:56  1\n        2016-01-10 23:59:57  1\n        2016-01-10 23:59:58  1\n        2016-01-10 23:59:59  1\n        \"\"\"\n        if axis is None:\n            axis = self._stat_axis_number\n        axis = self._get_axis_number(axis)\n        ax = self._get_axis(axis)\n\n        # GH 17935\n        # Check that index is sorted\n        if not ax.is_monotonic_increasing and not ax.is_monotonic_decreasing:\n            raise ValueError(\"truncate requires a sorted index\")\n\n        # if we have a date index, convert to dates, otherwise\n        # treat like a slice\n        if ax._is_all_dates:\n            from pandas.core.tools.datetimes import to_datetime\n\n            before = to_datetime(before)\n            after = to_datetime(after)\n\n        if before is not None and after is not None and before > after:\n            raise ValueError(f\"Truncate: {after} must be after {before}\")\n\n        if len(ax) > 1 and ax.is_monotonic_decreasing and ax.nunique() > 1:\n            before, after = after, before\n\n        slicer = [slice(None, None)] * self._AXIS_LEN\n        slicer[axis] = slice(before, after)\n        result = self.loc[tuple(slicer)]\n\n        if isinstance(ax, MultiIndex):\n            setattr(result, self._get_axis_name(axis), ax.truncate(before, after))\n\n        if copy:\n            result = result.copy()\n\n        return result\n\n    @final\n    def tz_convert(\n        self: NDFrameT, tz, axis=0, level=None, copy: bool_t = True\n    ) -> NDFrameT:\n        \"\"\"\n        Convert tz-aware axis to target time zone.\n\n        Parameters\n        ----------\n        tz : str or tzinfo object\n        axis : the axis to convert\n        level : int, str, default None\n            If axis is a MultiIndex, convert a specific level. Otherwise\n            must be None.\n        copy : bool, default True\n            Also make a copy of the underlying data.\n\n        Returns\n        -------\n        {klass}\n            Object with time zone converted axis.\n\n        Raises\n        ------\n        TypeError\n            If the axis is tz-naive.\n        \"\"\"\n        axis = self._get_axis_number(axis)\n        ax = self._get_axis(axis)\n\n        def _tz_convert(ax, tz):\n            if not hasattr(ax, \"tz_convert\"):\n                if len(ax) > 0:\n                    ax_name = self._get_axis_name(axis)\n                    raise TypeError(\n                        f\"{ax_name} is not a valid DatetimeIndex or PeriodIndex\"\n                    )\n                else:\n                    ax = DatetimeIndex([], tz=tz)\n            else:\n                ax = ax.tz_convert(tz)\n            return ax\n\n        # if a level is given it must be a MultiIndex level or\n        # equivalent to the axis name\n        if isinstance(ax, MultiIndex):\n            level = ax._get_level_number(level)\n            new_level = _tz_convert(ax.levels[level], tz)\n            ax = ax.set_levels(new_level, level=level)\n        else:\n            if level not in (None, 0, ax.name):\n                raise ValueError(f\"The level {level} is not valid\")\n            ax = _tz_convert(ax, tz)\n\n        result = self.copy(deep=copy)\n        result = result.set_axis(ax, axis=axis, inplace=False)\n        return result.__finalize__(self, method=\"tz_convert\")\n\n    @final\n    def tz_localize(\n        self: NDFrameT,\n        tz,\n        axis=0,\n        level=None,\n        copy: bool_t = True,\n        ambiguous=\"raise\",\n        nonexistent: str = \"raise\",\n    ) -> NDFrameT:\n        \"\"\"\n        Localize tz-naive index of a Series or DataFrame to target time zone.\n\n        This operation localizes the Index. To localize the values in a\n        timezone-naive Series, use :meth:`Series.dt.tz_localize`.\n\n        Parameters\n        ----------\n        tz : str or tzinfo\n        axis : the axis to localize\n        level : int, str, default None\n            If axis ia a MultiIndex, localize a specific level. Otherwise\n            must be None.\n        copy : bool, default True\n            Also make a copy of the underlying data.\n        ambiguous : 'infer', bool-ndarray, 'NaT', default 'raise'\n            When clocks moved backward due to DST, ambiguous times may arise.\n            For example in Central European Time (UTC+01), when going from\n            03:00 DST to 02:00 non-DST, 02:30:00 local time occurs both at\n            00:30:00 UTC and at 01:30:00 UTC. In such a situation, the\n            `ambiguous` parameter dictates how ambiguous times should be\n            handled.\n\n            - 'infer' will attempt to infer fall dst-transition hours based on\n              order\n            - bool-ndarray where True signifies a DST time, False designates\n              a non-DST time (note that this flag is only applicable for\n              ambiguous times)\n            - 'NaT' will return NaT where there are ambiguous times\n            - 'raise' will raise an AmbiguousTimeError if there are ambiguous\n              times.\n        nonexistent : str, default 'raise'\n            A nonexistent time does not exist in a particular timezone\n            where clocks moved forward due to DST. Valid values are:\n\n            - 'shift_forward' will shift the nonexistent time forward to the\n              closest existing time\n            - 'shift_backward' will shift the nonexistent time backward to the\n              closest existing time\n            - 'NaT' will return NaT where there are nonexistent times\n            - timedelta objects will shift nonexistent times by the timedelta\n            - 'raise' will raise an NonExistentTimeError if there are\n              nonexistent times.\n\n        Returns\n        -------\n        Series or DataFrame\n            Same type as the input.\n\n        Raises\n        ------\n        TypeError\n            If the TimeSeries is tz-aware and tz is not None.\n\n        Examples\n        --------\n        Localize local times:\n\n        >>> s = pd.Series([1],\n        ...               index=pd.DatetimeIndex(['2018-09-15 01:30:00']))\n        >>> s.tz_localize('CET')\n        2018-09-15 01:30:00+02:00    1\n        dtype: int64\n\n        Be careful with DST changes. When there is sequential data, pandas\n        can infer the DST time:\n\n        >>> s = pd.Series(range(7),\n        ...               index=pd.DatetimeIndex(['2018-10-28 01:30:00',\n        ...                                       '2018-10-28 02:00:00',\n        ...                                       '2018-10-28 02:30:00',\n        ...                                       '2018-10-28 02:00:00',\n        ...                                       '2018-10-28 02:30:00',\n        ...                                       '2018-10-28 03:00:00',\n        ...                                       '2018-10-28 03:30:00']))\n        >>> s.tz_localize('CET', ambiguous='infer')\n        2018-10-28 01:30:00+02:00    0\n        2018-10-28 02:00:00+02:00    1\n        2018-10-28 02:30:00+02:00    2\n        2018-10-28 02:00:00+01:00    3\n        2018-10-28 02:30:00+01:00    4\n        2018-10-28 03:00:00+01:00    5\n        2018-10-28 03:30:00+01:00    6\n        dtype: int64\n\n        In some cases, inferring the DST is impossible. In such cases, you can\n        pass an ndarray to the ambiguous parameter to set the DST explicitly\n\n        >>> s = pd.Series(range(3),\n        ...               index=pd.DatetimeIndex(['2018-10-28 01:20:00',\n        ...                                       '2018-10-28 02:36:00',\n        ...                                       '2018-10-28 03:46:00']))\n        >>> s.tz_localize('CET', ambiguous=np.array([True, True, False]))\n        2018-10-28 01:20:00+02:00    0\n        2018-10-28 02:36:00+02:00    1\n        2018-10-28 03:46:00+01:00    2\n        dtype: int64\n\n        If the DST transition causes nonexistent times, you can shift these\n        dates forward or backward with a timedelta object or `'shift_forward'`\n        or `'shift_backward'`.\n\n        >>> s = pd.Series(range(2),\n        ...               index=pd.DatetimeIndex(['2015-03-29 02:30:00',\n        ...                                       '2015-03-29 03:30:00']))\n        >>> s.tz_localize('Europe/Warsaw', nonexistent='shift_forward')\n        2015-03-29 03:00:00+02:00    0\n        2015-03-29 03:30:00+02:00    1\n        dtype: int64\n        >>> s.tz_localize('Europe/Warsaw', nonexistent='shift_backward')\n        2015-03-29 01:59:59.999999999+01:00    0\n        2015-03-29 03:30:00+02:00              1\n        dtype: int64\n        >>> s.tz_localize('Europe/Warsaw', nonexistent=pd.Timedelta('1H'))\n        2015-03-29 03:30:00+02:00    0\n        2015-03-29 03:30:00+02:00    1\n        dtype: int64\n        \"\"\"\n        nonexistent_options = (\"raise\", \"NaT\", \"shift_forward\", \"shift_backward\")\n        if nonexistent not in nonexistent_options and not isinstance(\n            nonexistent, timedelta\n        ):\n            raise ValueError(\n                \"The nonexistent argument must be one of 'raise', \"\n                \"'NaT', 'shift_forward', 'shift_backward' or \"\n                \"a timedelta object\"\n            )\n\n        axis = self._get_axis_number(axis)\n        ax = self._get_axis(axis)\n\n        def _tz_localize(ax, tz, ambiguous, nonexistent):\n            if not hasattr(ax, \"tz_localize\"):\n                if len(ax) > 0:\n                    ax_name = self._get_axis_name(axis)\n                    raise TypeError(\n                        f\"{ax_name} is not a valid DatetimeIndex or PeriodIndex\"\n                    )\n                else:\n                    ax = DatetimeIndex([], tz=tz)\n            else:\n                ax = ax.tz_localize(tz, ambiguous=ambiguous, nonexistent=nonexistent)\n            return ax\n\n        # if a level is given it must be a MultiIndex level or\n        # equivalent to the axis name\n        if isinstance(ax, MultiIndex):\n            level = ax._get_level_number(level)\n            new_level = _tz_localize(ax.levels[level], tz, ambiguous, nonexistent)\n            ax = ax.set_levels(new_level, level=level)\n        else:\n            if level not in (None, 0, ax.name):\n                raise ValueError(f\"The level {level} is not valid\")\n            ax = _tz_localize(ax, tz, ambiguous, nonexistent)\n\n        result = self.copy(deep=copy)\n        result = result.set_axis(ax, axis=axis, inplace=False)\n        return result.__finalize__(self, method=\"tz_localize\")\n\n    # ----------------------------------------------------------------------\n    # Numeric Methods\n\n    @final\n    def describe(\n        self: NDFrameT,\n        percentiles=None,\n        include=None,\n        exclude=None,\n        datetime_is_numeric=False,\n    ) -> NDFrameT:\n        \"\"\"\n        Generate descriptive statistics.\n\n        Descriptive statistics include those that summarize the central\n        tendency, dispersion and shape of a\n        dataset's distribution, excluding ``NaN`` values.\n\n        Analyzes both numeric and object series, as well\n        as ``DataFrame`` column sets of mixed data types. The output\n        will vary depending on what is provided. Refer to the notes\n        below for more detail.\n\n        Parameters\n        ----------\n        percentiles : list-like of numbers, optional\n            The percentiles to include in the output. All should\n            fall between 0 and 1. The default is\n            ``[.25, .5, .75]``, which returns the 25th, 50th, and\n            75th percentiles.\n        include : 'all', list-like of dtypes or None (default), optional\n            A white list of data types to include in the result. Ignored\n            for ``Series``. Here are the options:\n\n            - 'all' : All columns of the input will be included in the output.\n            - A list-like of dtypes : Limits the results to the\n              provided data types.\n              To limit the result to numeric types submit\n              ``numpy.number``. To limit it instead to object columns submit\n              the ``numpy.object`` data type. Strings\n              can also be used in the style of\n              ``select_dtypes`` (e.g. ``df.describe(include=['O'])``). To\n              select pandas categorical columns, use ``'category'``\n            - None (default) : The result will include all numeric columns.\n        exclude : list-like of dtypes or None (default), optional,\n            A black list of data types to omit from the result. Ignored\n            for ``Series``. Here are the options:\n\n            - A list-like of dtypes : Excludes the provided data types\n              from the result. To exclude numeric types submit\n              ``numpy.number``. To exclude object columns submit the data\n              type ``numpy.object``. Strings can also be used in the style of\n              ``select_dtypes`` (e.g. ``df.describe(exclude=['O'])``). To\n              exclude pandas categorical columns, use ``'category'``\n            - None (default) : The result will exclude nothing.\n        datetime_is_numeric : bool, default False\n            Whether to treat datetime dtypes as numeric. This affects statistics\n            calculated for the column. For DataFrame input, this also\n            controls whether datetime columns are included by default.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        Series or DataFrame\n            Summary statistics of the Series or Dataframe provided.\n\n        See Also\n        --------\n        DataFrame.count: Count number of non-NA/null observations.\n        DataFrame.max: Maximum of the values in the object.\n        DataFrame.min: Minimum of the values in the object.\n        DataFrame.mean: Mean of the values.\n        DataFrame.std: Standard deviation of the observations.\n        DataFrame.select_dtypes: Subset of a DataFrame including/excluding\n            columns based on their dtype.\n\n        Notes\n        -----\n        For numeric data, the result's index will include ``count``,\n        ``mean``, ``std``, ``min``, ``max`` as well as lower, ``50`` and\n        upper percentiles. By default the lower percentile is ``25`` and the\n        upper percentile is ``75``. The ``50`` percentile is the\n        same as the median.\n\n        For object data (e.g. strings or timestamps), the result's index\n        will include ``count``, ``unique``, ``top``, and ``freq``. The ``top``\n        is the most common value. The ``freq`` is the most common value's\n        frequency. Timestamps also include the ``first`` and ``last`` items.\n\n        If multiple object values have the highest count, then the\n        ``count`` and ``top`` results will be arbitrarily chosen from\n        among those with the highest count.\n\n        For mixed data types provided via a ``DataFrame``, the default is to\n        return only an analysis of numeric columns. If the dataframe consists\n        only of object and categorical data without any numeric columns, the\n        default is to return an analysis of both the object and categorical\n        columns. If ``include='all'`` is provided as an option, the result\n        will include a union of attributes of each type.\n\n        The `include` and `exclude` parameters can be used to limit\n        which columns in a ``DataFrame`` are analyzed for the output.\n        The parameters are ignored when analyzing a ``Series``.\n\n        Examples\n        --------\n        Describing a numeric ``Series``.\n\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.describe()\n        count    3.0\n        mean     2.0\n        std      1.0\n        min      1.0\n        25%      1.5\n        50%      2.0\n        75%      2.5\n        max      3.0\n        dtype: float64\n\n        Describing a categorical ``Series``.\n\n        >>> s = pd.Series(['a', 'a', 'b', 'c'])\n        >>> s.describe()\n        count     4\n        unique    3\n        top       a\n        freq      2\n        dtype: object\n\n        Describing a timestamp ``Series``.\n\n        >>> s = pd.Series([\n        ...   np.datetime64(\"2000-01-01\"),\n        ...   np.datetime64(\"2010-01-01\"),\n        ...   np.datetime64(\"2010-01-01\")\n        ... ])\n        >>> s.describe(datetime_is_numeric=True)\n        count                      3\n        mean     2006-09-01 08:00:00\n        min      2000-01-01 00:00:00\n        25%      2004-12-31 12:00:00\n        50%      2010-01-01 00:00:00\n        75%      2010-01-01 00:00:00\n        max      2010-01-01 00:00:00\n        dtype: object\n\n        Describing a ``DataFrame``. By default only numeric fields\n        are returned.\n\n        >>> df = pd.DataFrame({'categorical': pd.Categorical(['d','e','f']),\n        ...                    'numeric': [1, 2, 3],\n        ...                    'object': ['a', 'b', 'c']\n        ...                   })\n        >>> df.describe()\n               numeric\n        count      3.0\n        mean       2.0\n        std        1.0\n        min        1.0\n        25%        1.5\n        50%        2.0\n        75%        2.5\n        max        3.0\n\n        Describing all columns of a ``DataFrame`` regardless of data type.\n\n        >>> df.describe(include='all')  # doctest: +SKIP\n               categorical  numeric object\n        count            3      3.0      3\n        unique           3      NaN      3\n        top              f      NaN      a\n        freq             1      NaN      1\n        mean           NaN      2.0    NaN\n        std            NaN      1.0    NaN\n        min            NaN      1.0    NaN\n        25%            NaN      1.5    NaN\n        50%            NaN      2.0    NaN\n        75%            NaN      2.5    NaN\n        max            NaN      3.0    NaN\n\n        Describing a column from a ``DataFrame`` by accessing it as\n        an attribute.\n\n        >>> df.numeric.describe()\n        count    3.0\n        mean     2.0\n        std      1.0\n        min      1.0\n        25%      1.5\n        50%      2.0\n        75%      2.5\n        max      3.0\n        Name: numeric, dtype: float64\n\n        Including only numeric columns in a ``DataFrame`` description.\n\n        >>> df.describe(include=[np.number])\n               numeric\n        count      3.0\n        mean       2.0\n        std        1.0\n        min        1.0\n        25%        1.5\n        50%        2.0\n        75%        2.5\n        max        3.0\n\n        Including only string columns in a ``DataFrame`` description.\n\n        >>> df.describe(include=[object])  # doctest: +SKIP\n               object\n        count       3\n        unique      3\n        top         a\n        freq        1\n\n        Including only categorical columns from a ``DataFrame`` description.\n\n        >>> df.describe(include=['category'])\n               categorical\n        count            3\n        unique           3\n        top              d\n        freq             1\n\n        Excluding numeric columns from a ``DataFrame`` description.\n\n        >>> df.describe(exclude=[np.number])  # doctest: +SKIP\n               categorical object\n        count            3      3\n        unique           3      3\n        top              f      a\n        freq             1      1\n\n        Excluding object columns from a ``DataFrame`` description.\n\n        >>> df.describe(exclude=[object])  # doctest: +SKIP\n               categorical  numeric\n        count            3      3.0\n        unique           3      NaN\n        top              f      NaN\n        freq             1      NaN\n        mean           NaN      2.0\n        std            NaN      1.0\n        min            NaN      1.0\n        25%            NaN      1.5\n        50%            NaN      2.0\n        75%            NaN      2.5\n        max            NaN      3.0\n        \"\"\"\n        return describe_ndframe(\n            obj=self,\n            include=include,\n            exclude=exclude,\n            datetime_is_numeric=datetime_is_numeric,\n            percentiles=percentiles,\n        )\n\n    @final\n    def pct_change(\n        self: NDFrameT,\n        periods=1,\n        fill_method=\"pad\",\n        limit=None,\n        freq=None,\n        **kwargs,\n    ) -> NDFrameT:\n        \"\"\"\n        Percentage change between the current and a prior element.\n\n        Computes the percentage change from the immediately previous row by\n        default. This is useful in comparing the percentage of change in a time\n        series of elements.\n\n        Parameters\n        ----------\n        periods : int, default 1\n            Periods to shift for forming percent change.\n        fill_method : str, default 'pad'\n            How to handle NAs before computing percent changes.\n        limit : int, default None\n            The number of consecutive NAs to fill before stopping.\n        freq : DateOffset, timedelta, or str, optional\n            Increment to use from time series API (e.g. 'M' or BDay()).\n        **kwargs\n            Additional keyword arguments are passed into\n            `DataFrame.shift` or `Series.shift`.\n\n        Returns\n        -------\n        chg : Series or DataFrame\n            The same type as the calling object.\n\n        See Also\n        --------\n        Series.diff : Compute the difference of two elements in a Series.\n        DataFrame.diff : Compute the difference of two elements in a DataFrame.\n        Series.shift : Shift the index by some number of periods.\n        DataFrame.shift : Shift the index by some number of periods.\n\n        Examples\n        --------\n        **Series**\n\n        >>> s = pd.Series([90, 91, 85])\n        >>> s\n        0    90\n        1    91\n        2    85\n        dtype: int64\n\n        >>> s.pct_change()\n        0         NaN\n        1    0.011111\n        2   -0.065934\n        dtype: float64\n\n        >>> s.pct_change(periods=2)\n        0         NaN\n        1         NaN\n        2   -0.055556\n        dtype: float64\n\n        See the percentage change in a Series where filling NAs with last\n        valid observation forward to next valid.\n\n        >>> s = pd.Series([90, 91, None, 85])\n        >>> s\n        0    90.0\n        1    91.0\n        2     NaN\n        3    85.0\n        dtype: float64\n\n        >>> s.pct_change(fill_method='ffill')\n        0         NaN\n        1    0.011111\n        2    0.000000\n        3   -0.065934\n        dtype: float64\n\n        **DataFrame**\n\n        Percentage change in French franc, Deutsche Mark, and Italian lira from\n        1980-01-01 to 1980-03-01.\n\n        >>> df = pd.DataFrame({\n        ...     'FR': [4.0405, 4.0963, 4.3149],\n        ...     'GR': [1.7246, 1.7482, 1.8519],\n        ...     'IT': [804.74, 810.01, 860.13]},\n        ...     index=['1980-01-01', '1980-02-01', '1980-03-01'])\n        >>> df\n                        FR      GR      IT\n        1980-01-01  4.0405  1.7246  804.74\n        1980-02-01  4.0963  1.7482  810.01\n        1980-03-01  4.3149  1.8519  860.13\n\n        >>> df.pct_change()\n                          FR        GR        IT\n        1980-01-01       NaN       NaN       NaN\n        1980-02-01  0.013810  0.013684  0.006549\n        1980-03-01  0.053365  0.059318  0.061876\n\n        Percentage of change in GOOG and APPL stock volume. Shows computing\n        the percentage change between columns.\n\n        >>> df = pd.DataFrame({\n        ...     '2016': [1769950, 30586265],\n        ...     '2015': [1500923, 40912316],\n        ...     '2014': [1371819, 41403351]},\n        ...     index=['GOOG', 'APPL'])\n        >>> df\n                  2016      2015      2014\n        GOOG   1769950   1500923   1371819\n        APPL  30586265  40912316  41403351\n\n        >>> df.pct_change(axis='columns', periods=-1)\n                  2016      2015  2014\n        GOOG  0.179241  0.094112   NaN\n        APPL -0.252395 -0.011860   NaN\n        \"\"\"\n        axis = self._get_axis_number(kwargs.pop(\"axis\", self._stat_axis_name))\n        if fill_method is None:\n            data = self\n        else:\n            _data = self.fillna(method=fill_method, axis=axis, limit=limit)\n            assert _data is not None  # needed for mypy\n            data = _data\n\n        shifted = data.shift(periods=periods, freq=freq, axis=axis, **kwargs)\n        # Unsupported left operand type for / (\"NDFrameT\")\n        rs = data / shifted - 1  # type: ignore[operator]\n        if freq is not None:\n            # Shift method is implemented differently when freq is not None\n            # We want to restore the original index\n            rs = rs.loc[~rs.index.duplicated()]\n            rs = rs.reindex_like(data)\n        return rs\n\n    @final\n    def _agg_by_level(\n        self,\n        name: str,\n        axis: Axis = 0,\n        level: Level = 0,\n        skipna: bool_t = True,\n        **kwargs,\n    ):\n        if axis is None:\n            raise ValueError(\"Must specify 'axis' when aggregating by level.\")\n        grouped = self.groupby(level=level, axis=axis, sort=False)\n        if hasattr(grouped, name) and skipna:\n            return getattr(grouped, name)(**kwargs)\n        axis = self._get_axis_number(axis)\n        method = getattr(type(self), name)\n        applyf = lambda x: method(x, axis=axis, skipna=skipna, **kwargs)\n        return grouped.aggregate(applyf)\n\n    @final\n    def _logical_func(\n        self,\n        name: str,\n        func,\n        axis: Axis = 0,\n        bool_only: bool_t | None = None,\n        skipna: bool_t = True,\n        level: Level | None = None,\n        **kwargs,\n    ) -> Series | bool_t:\n        nv.validate_logical_func((), kwargs, fname=name)\n        validate_bool_kwarg(skipna, \"skipna\", none_allowed=False)\n        if level is not None:\n            warnings.warn(\n                \"Using the level keyword in DataFrame and Series aggregations is \"\n                \"deprecated and will be removed in a future version. Use groupby \"\n                \"instead. df.any(level=1) should use df.groupby(level=1).any()\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n            if bool_only is not None:\n                raise NotImplementedError(\n                    \"Option bool_only is not implemented with option level.\"\n                )\n            return self._agg_by_level(name, axis=axis, level=level, skipna=skipna)\n\n        if self.ndim > 1 and axis is None:\n            # Reduce along one dimension then the other, to simplify DataFrame._reduce\n            res = self._logical_func(\n                name, func, axis=0, bool_only=bool_only, skipna=skipna, **kwargs\n            )\n            return res._logical_func(name, func, skipna=skipna, **kwargs)\n\n        if (\n            self.ndim > 1\n            and axis == 1\n            and len(self._mgr.arrays) > 1\n            # TODO(EA2D): special-case not needed\n            and all(x.ndim == 2 for x in self._mgr.arrays)\n            and bool_only is not None\n            and not kwargs\n        ):\n            # Fastpath avoiding potentially expensive transpose\n            obj = self\n            if bool_only:\n                obj = self._get_bool_data()\n            return obj._reduce_axis1(name, func, skipna=skipna)\n\n        return self._reduce(\n            func,\n            name=name,\n            axis=axis,\n            skipna=skipna,\n            numeric_only=bool_only,\n            filter_type=\"bool\",\n        )\n\n    def any(\n        self,\n        axis: Axis = 0,\n        bool_only: bool_t | None = None,\n        skipna: bool_t = True,\n        level: Level | None = None,\n        **kwargs,\n    ) -> Series | bool_t:\n        return self._logical_func(\n            \"any\", nanops.nanany, axis, bool_only, skipna, level, **kwargs\n        )\n\n    def all(\n        self,\n        axis: Axis = 0,\n        bool_only: bool_t | None = None,\n        skipna: bool_t = True,\n        level: Level | None = None,\n        **kwargs,\n    ) -> Series | bool_t:\n        return self._logical_func(\n            \"all\", nanops.nanall, axis, bool_only, skipna, level, **kwargs\n        )\n\n    @final\n    def _accum_func(\n        self,\n        name: str,\n        func,\n        axis: Axis | None = None,\n        skipna: bool_t = True,\n        *args,\n        **kwargs,\n    ):\n        skipna = nv.validate_cum_func_with_skipna(skipna, args, kwargs, name)\n        if axis is None:\n            axis = self._stat_axis_number\n        else:\n            axis = self._get_axis_number(axis)\n\n        if axis == 1:\n            return self.T._accum_func(\n                name, func, axis=0, skipna=skipna, *args, **kwargs\n            ).T\n\n        def block_accum_func(blk_values):\n            values = blk_values.T if hasattr(blk_values, \"T\") else blk_values\n\n            result = nanops.na_accum_func(values, func, skipna=skipna)\n\n            result = result.T if hasattr(result, \"T\") else result\n            return result\n\n        result = self._mgr.apply(block_accum_func)\n\n        return self._constructor(result).__finalize__(self, method=name)\n\n    def cummax(self, axis: Axis | None = None, skipna: bool_t = True, *args, **kwargs):\n        return self._accum_func(\n            \"cummax\", np.maximum.accumulate, axis, skipna, *args, **kwargs\n        )\n\n    def cummin(self, axis: Axis | None = None, skipna: bool_t = True, *args, **kwargs):\n        return self._accum_func(\n            \"cummin\", np.minimum.accumulate, axis, skipna, *args, **kwargs\n        )\n\n    def cumsum(self, axis: Axis | None = None, skipna: bool_t = True, *args, **kwargs):\n        return self._accum_func(\"cumsum\", np.cumsum, axis, skipna, *args, **kwargs)\n\n    def cumprod(self, axis: Axis | None = None, skipna: bool_t = True, *args, **kwargs):\n        return self._accum_func(\"cumprod\", np.cumprod, axis, skipna, *args, **kwargs)\n\n    @final\n    def _stat_function_ddof(\n        self,\n        name: str,\n        func,\n        axis: Axis | None = None,\n        skipna: bool_t = True,\n        level: Level | None = None,\n        ddof: int = 1,\n        numeric_only: bool_t | None = None,\n        **kwargs,\n    ) -> Series | float:\n        nv.validate_stat_ddof_func((), kwargs, fname=name)\n        validate_bool_kwarg(skipna, \"skipna\", none_allowed=False)\n        if axis is None:\n            axis = self._stat_axis_number\n        if level is not None:\n            warnings.warn(\n                \"Using the level keyword in DataFrame and Series aggregations is \"\n                \"deprecated and will be removed in a future version. Use groupby \"\n                \"instead. df.var(level=1) should use df.groupby(level=1).var().\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n            return self._agg_by_level(\n                name, axis=axis, level=level, skipna=skipna, ddof=ddof\n            )\n        return self._reduce(\n            func, name, axis=axis, numeric_only=numeric_only, skipna=skipna, ddof=ddof\n        )\n\n    def sem(\n        self,\n        axis: Axis | None = None,\n        skipna: bool_t = True,\n        level: Level | None = None,\n        ddof: int = 1,\n        numeric_only: bool_t | None = None,\n        **kwargs,\n    ) -> Series | float:\n        return self._stat_function_ddof(\n            \"sem\", nanops.nansem, axis, skipna, level, ddof, numeric_only, **kwargs\n        )\n\n    def var(\n        self,\n        axis: Axis | None = None,\n        skipna: bool_t = True,\n        level: Level | None = None,\n        ddof: int = 1,\n        numeric_only: bool_t | None = None,\n        **kwargs,\n    ) -> Series | float:\n        return self._stat_function_ddof(\n            \"var\", nanops.nanvar, axis, skipna, level, ddof, numeric_only, **kwargs\n        )\n\n    def std(\n        self,\n        axis: Axis | None = None,\n        skipna: bool_t = True,\n        level: Level | None = None,\n        ddof: int = 1,\n        numeric_only: bool_t | None = None,\n        **kwargs,\n    ) -> Series | float:\n        return self._stat_function_ddof(\n            \"std\", nanops.nanstd, axis, skipna, level, ddof, numeric_only, **kwargs\n        )\n\n    @final\n    def _stat_function(\n        self,\n        name: str,\n        func,\n        axis: Axis | None | lib.NoDefault = None,\n        skipna: bool_t = True,\n        level: Level | None = None,\n        numeric_only: bool_t | None = None,\n        **kwargs,\n    ):\n        if name == \"median\":\n            nv.validate_median((), kwargs)\n        else:\n            nv.validate_stat_func((), kwargs, fname=name)\n\n        validate_bool_kwarg(skipna, \"skipna\", none_allowed=False)\n\n        if axis is None and level is None and self.ndim > 1:\n            # user must have explicitly passed axis=None\n            # GH#21597\n            warnings.warn(\n                f\"In a future version, DataFrame.{name}(axis=None) will return a \"\n                f\"scalar {name} over the entire DataFrame. To retain the old \"\n                f\"behavior, use 'frame.{name}(axis=0)' or just 'frame.{name}()'\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n        if axis is lib.no_default:\n            axis = None\n\n        if axis is None:\n            axis = self._stat_axis_number\n        axis = cast(Axis, axis)\n        if level is not None:\n            warnings.warn(\n                \"Using the level keyword in DataFrame and Series aggregations is \"\n                \"deprecated and will be removed in a future version. Use groupby \"\n                \"instead. df.median(level=1) should use df.groupby(level=1).median().\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n            return self._agg_by_level(\n                name, axis=axis, level=level, skipna=skipna, numeric_only=numeric_only\n            )\n        return self._reduce(\n            func, name=name, axis=axis, skipna=skipna, numeric_only=numeric_only\n        )\n\n    def min(\n        self,\n        axis: Axis | None | lib.NoDefault = lib.no_default,\n        skipna: bool_t = True,\n        level: Level | None = None,\n        numeric_only: bool_t | None = None,\n        **kwargs,\n    ):\n        return self._stat_function(\n            \"min\",\n            nanops.nanmin,\n            axis,\n            skipna,\n            level,\n            numeric_only,\n            **kwargs,\n        )\n\n    def max(\n        self,\n        axis: Axis | None | lib.NoDefault = lib.no_default,\n        skipna: bool_t = True,\n        level: Level | None = None,\n        numeric_only: bool_t | None = None,\n        **kwargs,\n    ):\n        return self._stat_function(\n            \"max\",\n            nanops.nanmax,\n            axis,\n            skipna,\n            level,\n            numeric_only,\n            **kwargs,\n        )\n\n    def mean(\n        self,\n        axis: Axis | None | lib.NoDefault = lib.no_default,\n        skipna: bool_t = True,\n        level: Level | None = None,\n        numeric_only: bool_t | None = None,\n        **kwargs,\n    ) -> Series | float:\n        return self._stat_function(\n            \"mean\", nanops.nanmean, axis, skipna, level, numeric_only, **kwargs\n        )\n\n    def median(\n        self,\n        axis: Axis | None | lib.NoDefault = lib.no_default,\n        skipna: bool_t = True,\n        level: Level | None = None,\n        numeric_only: bool_t | None = None,\n        **kwargs,\n    ) -> Series | float:\n        return self._stat_function(\n            \"median\", nanops.nanmedian, axis, skipna, level, numeric_only, **kwargs\n        )\n\n    def skew(\n        self,\n        axis: Axis | None | lib.NoDefault = lib.no_default,\n        skipna: bool_t = True,\n        level: Level | None = None,\n        numeric_only: bool_t | None = None,\n        **kwargs,\n    ) -> Series | float:\n        return self._stat_function(\n            \"skew\", nanops.nanskew, axis, skipna, level, numeric_only, **kwargs\n        )\n\n    def kurt(\n        self,\n        axis: Axis | None | lib.NoDefault = lib.no_default,\n        skipna: bool_t = True,\n        level: Level | None = None,\n        numeric_only: bool_t | None = None,\n        **kwargs,\n    ) -> Series | float:\n        return self._stat_function(\n            \"kurt\", nanops.nankurt, axis, skipna, level, numeric_only, **kwargs\n        )\n\n    kurtosis = kurt\n\n    @final\n    def _min_count_stat_function(\n        self,\n        name: str,\n        func,\n        axis: Axis | None = None,\n        skipna: bool_t = True,\n        level: Level | None = None,\n        numeric_only: bool_t | None = None,\n        min_count: int = 0,\n        **kwargs,\n    ):\n        if name == \"sum\":\n            nv.validate_sum((), kwargs)\n        elif name == \"prod\":\n            nv.validate_prod((), kwargs)\n        else:\n            nv.validate_stat_func((), kwargs, fname=name)\n\n        validate_bool_kwarg(skipna, \"skipna\", none_allowed=False)\n\n        if axis is None:\n            axis = self._stat_axis_number\n        if level is not None:\n            warnings.warn(\n                \"Using the level keyword in DataFrame and Series aggregations is \"\n                \"deprecated and will be removed in a future version. Use groupby \"\n                \"instead. df.sum(level=1) should use df.groupby(level=1).sum().\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n            return self._agg_by_level(\n                name,\n                axis=axis,\n                level=level,\n                skipna=skipna,\n                min_count=min_count,\n                numeric_only=numeric_only,\n            )\n\n        return self._reduce(\n            func,\n            name=name,\n            axis=axis,\n            skipna=skipna,\n            numeric_only=numeric_only,\n            min_count=min_count,\n        )\n\n    def sum(\n        self,\n        axis: Axis | None = None,\n        skipna: bool_t = True,\n        level: Level | None = None,\n        numeric_only: bool_t | None = None,\n        min_count=0,\n        **kwargs,\n    ):\n        return self._min_count_stat_function(\n            \"sum\", nanops.nansum, axis, skipna, level, numeric_only, min_count, **kwargs\n        )\n\n    def prod(\n        self,\n        axis: Axis | None = None,\n        skipna: bool_t = True,\n        level: Level | None = None,\n        numeric_only: bool_t | None = None,\n        min_count: int = 0,\n        **kwargs,\n    ):\n        return self._min_count_stat_function(\n            \"prod\",\n            nanops.nanprod,\n            axis,\n            skipna,\n            level,\n            numeric_only,\n            min_count,\n            **kwargs,\n        )\n\n    product = prod\n\n    def mad(\n        self,\n        axis: Axis | None = None,\n        skipna: bool_t = True,\n        level: Level | None = None,\n    ) -> Series | float:\n        \"\"\"\n        {desc}\n\n        Parameters\n        ----------\n        axis : {axis_descr}\n            Axis for the function to be applied on.\n        skipna : bool, default True\n            Exclude NA/null values when computing the result.\n        level : int or level name, default None\n            If the axis is a MultiIndex (hierarchical), count along a\n            particular level, collapsing into a {name1}.\n\n        Returns\n        -------\n        {name1} or {name2} (if level specified)\\\n        {see_also}\\\n        {examples}\n        \"\"\"\n        if not is_bool(skipna):\n            warnings.warn(\n                \"Passing None for skipna is deprecated and will raise in a future\"\n                \"version. Pass True instead. Only boolean values will be allowed \"\n                \"in the future.\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n            skipna = True\n        if axis is None:\n            axis = self._stat_axis_number\n        if level is not None:\n            warnings.warn(\n                \"Using the level keyword in DataFrame and Series aggregations is \"\n                \"deprecated and will be removed in a future version. Use groupby \"\n                \"instead. df.mad(level=1) should use df.groupby(level=1).mad()\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n            return self._agg_by_level(\"mad\", axis=axis, level=level, skipna=skipna)\n\n        data = self._get_numeric_data()\n        if axis == 0:\n            demeaned = data - data.mean(axis=0)\n        else:\n            demeaned = data.sub(data.mean(axis=1), axis=0)\n        return np.abs(demeaned).mean(axis=axis, skipna=skipna)\n\n    @classmethod\n    def _add_numeric_operations(cls):\n        \"\"\"\n        Add the operations to the cls; evaluate the doc strings again\n        \"\"\"\n        axis_descr, name1, name2 = _doc_params(cls)\n\n        @doc(\n            _bool_doc,\n            desc=_any_desc,\n            name1=name1,\n            name2=name2,\n            axis_descr=axis_descr,\n            see_also=_any_see_also,\n            examples=_any_examples,\n            empty_value=False,\n        )\n        def any(self, axis=0, bool_only=None, skipna=True, level=None, **kwargs):\n            return NDFrame.any(self, axis, bool_only, skipna, level, **kwargs)\n\n        setattr(cls, \"any\", any)\n\n        @doc(\n            _bool_doc,\n            desc=_all_desc,\n            name1=name1,\n            name2=name2,\n            axis_descr=axis_descr,\n            see_also=_all_see_also,\n            examples=_all_examples,\n            empty_value=True,\n        )\n        def all(self, axis=0, bool_only=None, skipna=True, level=None, **kwargs):\n            return NDFrame.all(self, axis, bool_only, skipna, level, **kwargs)\n\n        setattr(cls, \"all\", all)\n\n        # error: Argument 1 to \"doc\" has incompatible type \"Optional[str]\"; expected\n        # \"Union[str, Callable[..., Any]]\"\n        @doc(\n            NDFrame.mad.__doc__,  # type: ignore[arg-type]\n            desc=\"Return the mean absolute deviation of the values \"\n            \"over the requested axis.\",\n            name1=name1,\n            name2=name2,\n            axis_descr=axis_descr,\n            see_also=\"\",\n            examples=\"\",\n        )\n        def mad(self, axis=None, skipna=True, level=None):\n            return NDFrame.mad(self, axis, skipna, level)\n\n        setattr(cls, \"mad\", mad)\n\n        @doc(\n            _num_ddof_doc,\n            desc=\"Return unbiased standard error of the mean over requested \"\n            \"axis.\\n\\nNormalized by N-1 by default. This can be changed \"\n            \"using the ddof argument\",\n            name1=name1,\n            name2=name2,\n            axis_descr=axis_descr,\n            notes=\"\",\n            examples=\"\",\n        )\n        def sem(\n            self,\n            axis=None,\n            skipna=True,\n            level=None,\n            ddof=1,\n            numeric_only=None,\n            **kwargs,\n        ):\n            return NDFrame.sem(self, axis, skipna, level, ddof, numeric_only, **kwargs)\n\n        setattr(cls, \"sem\", sem)\n\n        @doc(\n            _num_ddof_doc,\n            desc=\"Return unbiased variance over requested axis.\\n\\nNormalized by \"\n            \"N-1 by default. This can be changed using the ddof argument.\",\n            name1=name1,\n            name2=name2,\n            axis_descr=axis_descr,\n            notes=\"\",\n            examples=_var_examples,\n        )\n        def var(\n            self,\n            axis=None,\n            skipna=True,\n            level=None,\n            ddof=1,\n            numeric_only=None,\n            **kwargs,\n        ):\n            return NDFrame.var(self, axis, skipna, level, ddof, numeric_only, **kwargs)\n\n        setattr(cls, \"var\", var)\n\n        @doc(\n            _num_ddof_doc,\n            desc=\"Return sample standard deviation over requested axis.\"\n            \"\\n\\nNormalized by N-1 by default. This can be changed using the \"\n            \"ddof argument.\",\n            name1=name1,\n            name2=name2,\n            axis_descr=axis_descr,\n            notes=_std_notes,\n            examples=_std_examples,\n        )\n        def std(\n            self,\n            axis=None,\n            skipna=True,\n            level=None,\n            ddof=1,\n            numeric_only=None,\n            **kwargs,\n        ):\n            return NDFrame.std(self, axis, skipna, level, ddof, numeric_only, **kwargs)\n\n        setattr(cls, \"std\", std)\n\n        @doc(\n            _cnum_doc,\n            desc=\"minimum\",\n            name1=name1,\n            name2=name2,\n            axis_descr=axis_descr,\n            accum_func_name=\"min\",\n            examples=_cummin_examples,\n        )\n        def cummin(self, axis=None, skipna=True, *args, **kwargs):\n            return NDFrame.cummin(self, axis, skipna, *args, **kwargs)\n\n        setattr(cls, \"cummin\", cummin)\n\n        @doc(\n            _cnum_doc,\n            desc=\"maximum\",\n            name1=name1,\n            name2=name2,\n            axis_descr=axis_descr,\n            accum_func_name=\"max\",\n            examples=_cummax_examples,\n        )\n        def cummax(self, axis=None, skipna=True, *args, **kwargs):\n            return NDFrame.cummax(self, axis, skipna, *args, **kwargs)\n\n        setattr(cls, \"cummax\", cummax)\n\n        @doc(\n            _cnum_doc,\n            desc=\"sum\",\n            name1=name1,\n            name2=name2,\n            axis_descr=axis_descr,\n            accum_func_name=\"sum\",\n            examples=_cumsum_examples,\n        )\n        def cumsum(self, axis=None, skipna=True, *args, **kwargs):\n            return NDFrame.cumsum(self, axis, skipna, *args, **kwargs)\n\n        setattr(cls, \"cumsum\", cumsum)\n\n        @doc(\n            _cnum_doc,\n            desc=\"product\",\n            name1=name1,\n            name2=name2,\n            axis_descr=axis_descr,\n            accum_func_name=\"prod\",\n            examples=_cumprod_examples,\n        )\n        def cumprod(self, axis=None, skipna=True, *args, **kwargs):\n            return NDFrame.cumprod(self, axis, skipna, *args, **kwargs)\n\n        setattr(cls, \"cumprod\", cumprod)\n\n        @doc(\n            _num_doc,\n            desc=\"Return the sum of the values over the requested axis.\\n\\n\"\n            \"This is equivalent to the method ``numpy.sum``.\",\n            name1=name1,\n            name2=name2,\n            axis_descr=axis_descr,\n            min_count=_min_count_stub,\n            see_also=_stat_func_see_also,\n            examples=_sum_examples,\n        )\n        def sum(\n            self,\n            axis=None,\n            skipna=True,\n            level=None,\n            numeric_only=None,\n            min_count=0,\n            **kwargs,\n        ):\n            return NDFrame.sum(\n                self, axis, skipna, level, numeric_only, min_count, **kwargs\n            )\n\n        setattr(cls, \"sum\", sum)\n\n        @doc(\n            _num_doc,\n            desc=\"Return the product of the values over the requested axis.\",\n            name1=name1,\n            name2=name2,\n            axis_descr=axis_descr,\n            min_count=_min_count_stub,\n            see_also=_stat_func_see_also,\n            examples=_prod_examples,\n        )\n        def prod(\n            self,\n            axis=None,\n            skipna=True,\n            level=None,\n            numeric_only=None,\n            min_count=0,\n            **kwargs,\n        ):\n            return NDFrame.prod(\n                self, axis, skipna, level, numeric_only, min_count, **kwargs\n            )\n\n        setattr(cls, \"prod\", prod)\n        cls.product = prod\n\n        @doc(\n            _num_doc,\n            desc=\"Return the mean of the values over the requested axis.\",\n            name1=name1,\n            name2=name2,\n            axis_descr=axis_descr,\n            min_count=\"\",\n            see_also=\"\",\n            examples=\"\",\n        )\n        def mean(\n            self,\n            axis: int | None | lib.NoDefault = lib.no_default,\n            skipna=True,\n            level=None,\n            numeric_only=None,\n            **kwargs,\n        ):\n            return NDFrame.mean(self, axis, skipna, level, numeric_only, **kwargs)\n\n        setattr(cls, \"mean\", mean)\n\n        @doc(\n            _num_doc,\n            desc=\"Return unbiased skew over requested axis.\\n\\nNormalized by N-1.\",\n            name1=name1,\n            name2=name2,\n            axis_descr=axis_descr,\n            min_count=\"\",\n            see_also=\"\",\n            examples=\"\",\n        )\n        def skew(\n            self,\n            axis: int | None | lib.NoDefault = lib.no_default,\n            skipna=True,\n            level=None,\n            numeric_only=None,\n            **kwargs,\n        ):\n            return NDFrame.skew(self, axis, skipna, level, numeric_only, **kwargs)\n\n        setattr(cls, \"skew\", skew)\n\n        @doc(\n            _num_doc,\n            desc=\"Return unbiased kurtosis over requested axis.\\n\\n\"\n            \"Kurtosis obtained using Fisher's definition of\\n\"\n            \"kurtosis (kurtosis of normal == 0.0). Normalized \"\n            \"by N-1.\",\n            name1=name1,\n            name2=name2,\n            axis_descr=axis_descr,\n            min_count=\"\",\n            see_also=\"\",\n            examples=\"\",\n        )\n        def kurt(\n            self,\n            axis: Axis | None | lib.NoDefault = lib.no_default,\n            skipna=True,\n            level=None,\n            numeric_only=None,\n            **kwargs,\n        ):\n            return NDFrame.kurt(self, axis, skipna, level, numeric_only, **kwargs)\n\n        setattr(cls, \"kurt\", kurt)\n        cls.kurtosis = kurt\n\n        @doc(\n            _num_doc,\n            desc=\"Return the median of the values over the requested axis.\",\n            name1=name1,\n            name2=name2,\n            axis_descr=axis_descr,\n            min_count=\"\",\n            see_also=\"\",\n            examples=\"\",\n        )\n        def median(\n            self,\n            axis: int | None | lib.NoDefault = lib.no_default,\n            skipna=True,\n            level=None,\n            numeric_only=None,\n            **kwargs,\n        ):\n            return NDFrame.median(self, axis, skipna, level, numeric_only, **kwargs)\n\n        setattr(cls, \"median\", median)\n\n        # error: Untyped decorator makes function \"max\" untyped\n        @doc(  # type: ignore[misc]\n            _num_doc,\n            desc=\"Return the maximum of the values over the requested axis.\\n\\n\"\n            \"If you want the *index* of the maximum, use ``idxmax``. This is \"\n            \"the equivalent of the ``numpy.ndarray`` method ``argmax``.\",\n            name1=name1,\n            name2=name2,\n            axis_descr=axis_descr,\n            min_count=\"\",\n            see_also=_stat_func_see_also,\n            examples=_max_examples,\n        )\n        def max(\n            self,\n            axis: int | None | lib.NoDefault = lib.no_default,\n            skipna=True,\n            level=None,\n            numeric_only=None,\n            **kwargs,\n        ):\n            return NDFrame.max(self, axis, skipna, level, numeric_only, **kwargs)\n\n        setattr(cls, \"max\", max)\n\n        # error: Untyped decorator makes function \"max\" untyped\n        @doc(  # type: ignore[misc]\n            _num_doc,\n            desc=\"Return the minimum of the values over the requested axis.\\n\\n\"\n            \"If you want the *index* of the minimum, use ``idxmin``. This is \"\n            \"the equivalent of the ``numpy.ndarray`` method ``argmin``.\",\n            name1=name1,\n            name2=name2,\n            axis_descr=axis_descr,\n            min_count=\"\",\n            see_also=_stat_func_see_also,\n            examples=_min_examples,\n        )\n        def min(\n            self,\n            axis: int | None | lib.NoDefault = lib.no_default,\n            skipna=True,\n            level=None,\n            numeric_only=None,\n            **kwargs,\n        ):\n            return NDFrame.min(self, axis, skipna, level, numeric_only, **kwargs)\n\n        setattr(cls, \"min\", min)\n\n    @final\n    @doc(Rolling)\n    def rolling(\n        self,\n        window: int | timedelta | BaseOffset | BaseIndexer,\n        min_periods: int | None = None,\n        center: bool_t = False,\n        win_type: str | None = None,\n        on: str | None = None,\n        axis: Axis = 0,\n        closed: str | None = None,\n        method: str = \"single\",\n    ):\n        axis = self._get_axis_number(axis)\n\n        if win_type is not None:\n            return Window(\n                self,\n                window=window,\n                min_periods=min_periods,\n                center=center,\n                win_type=win_type,\n                on=on,\n                axis=axis,\n                closed=closed,\n                method=method,\n            )\n\n        return Rolling(\n            self,\n            window=window,\n            min_periods=min_periods,\n            center=center,\n            win_type=win_type,\n            on=on,\n            axis=axis,\n            closed=closed,\n            method=method,\n        )\n\n    @final\n    @doc(Expanding)\n    def expanding(\n        self,\n        min_periods: int = 1,\n        center: bool_t | None = None,\n        axis: Axis = 0,\n        method: str = \"single\",\n    ) -> Expanding:\n        axis = self._get_axis_number(axis)\n        if center is not None:\n            warnings.warn(\n                \"The `center` argument on `expanding` will be removed in the future.\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n        else:\n            center = False\n\n        return Expanding(\n            self, min_periods=min_periods, center=center, axis=axis, method=method\n        )\n\n    @final\n    @doc(ExponentialMovingWindow)\n    def ewm(\n        self,\n        com: float | None = None,\n        span: float | None = None,\n        halflife: float | TimedeltaConvertibleTypes | None = None,\n        alpha: float | None = None,\n        min_periods: int | None = 0,\n        adjust: bool_t = True,\n        ignore_na: bool_t = False,\n        axis: Axis = 0,\n        times: str | np.ndarray | DataFrame | Series | None = None,\n        method: str = \"single\",\n    ) -> ExponentialMovingWindow:\n        axis = self._get_axis_number(axis)\n        return ExponentialMovingWindow(\n            self,\n            com=com,\n            span=span,\n            halflife=halflife,\n            alpha=alpha,\n            min_periods=min_periods,\n            adjust=adjust,\n            ignore_na=ignore_na,\n            axis=axis,\n            times=times,\n            method=method,\n        )\n\n    # ----------------------------------------------------------------------\n    # Arithmetic Methods\n\n    @final\n    def _inplace_method(self, other, op):\n        \"\"\"\n        Wrap arithmetic method to operate inplace.\n        \"\"\"\n        result = op(self, other)\n\n        if (\n            self.ndim == 1\n            and result._indexed_same(self)\n            and is_dtype_equal(result.dtype, self.dtype)\n        ):\n            # GH#36498 this inplace op can _actually_ be inplace.\n            self._values[:] = result._values\n            return self\n\n        # Delete cacher\n        self._reset_cacher()\n\n        # this makes sure that we are aligned like the input\n        # we are updating inplace so we want to ignore is_copy\n        self._update_inplace(\n            result.reindex_like(self, copy=False), verify_is_copy=False\n        )\n        return self\n\n    def __iadd__(self, other):\n        # error: Unsupported left operand type for + (\"Type[NDFrame]\")\n        return self._inplace_method(other, type(self).__add__)  # type: ignore[operator]\n\n    def __isub__(self, other):\n        # error: Unsupported left operand type for - (\"Type[NDFrame]\")\n        return self._inplace_method(other, type(self).__sub__)  # type: ignore[operator]\n\n    def __imul__(self, other):\n        # error: Unsupported left operand type for * (\"Type[NDFrame]\")\n        return self._inplace_method(other, type(self).__mul__)  # type: ignore[operator]\n\n    def __itruediv__(self, other):\n        # error: Unsupported left operand type for / (\"Type[NDFrame]\")\n        return self._inplace_method(\n            other, type(self).__truediv__  # type: ignore[operator]\n        )\n\n    def __ifloordiv__(self, other):\n        # error: Unsupported left operand type for // (\"Type[NDFrame]\")\n        return self._inplace_method(\n            other, type(self).__floordiv__  # type: ignore[operator]\n        )\n\n    def __imod__(self, other):\n        # error: Unsupported left operand type for % (\"Type[NDFrame]\")\n        return self._inplace_method(other, type(self).__mod__)  # type: ignore[operator]\n\n    def __ipow__(self, other):\n        # error: Unsupported left operand type for ** (\"Type[NDFrame]\")\n        return self._inplace_method(other, type(self).__pow__)  # type: ignore[operator]\n\n    def __iand__(self, other):\n        # error: Unsupported left operand type for & (\"Type[NDFrame]\")\n        return self._inplace_method(other, type(self).__and__)  # type: ignore[operator]\n\n    def __ior__(self, other):\n        # error: Unsupported left operand type for | (\"Type[NDFrame]\")\n        return self._inplace_method(other, type(self).__or__)  # type: ignore[operator]\n\n    def __ixor__(self, other):\n        # error: Unsupported left operand type for ^ (\"Type[NDFrame]\")\n        return self._inplace_method(other, type(self).__xor__)  # type: ignore[operator]\n\n    # ----------------------------------------------------------------------\n    # Misc methods\n\n    @final\n    def _find_valid_index(self, *, how: str) -> Hashable | None:\n        \"\"\"\n        Retrieves the index of the first valid value.\n\n        Parameters\n        ----------\n        how : {'first', 'last'}\n            Use this parameter to change between the first or last valid index.\n\n        Returns\n        -------\n        idx_first_valid : type of index\n        \"\"\"\n        idxpos = find_valid_index(self._values, how=how)\n        if idxpos is None:\n            return None\n        return self.index[idxpos]\n\n    @final\n    @doc(position=\"first\", klass=_shared_doc_kwargs[\"klass\"])\n    def first_valid_index(self) -> Hashable | None:\n        \"\"\"\n        Return index for {position} non-NA value or None, if no non-NA value is found.\n\n        Returns\n        -------\n        scalar : type of index\n\n        Notes\n        -----\n        If all elements are non-NA/null, returns None.\n        Also returns None for empty {klass}.\n        \"\"\"\n        return self._find_valid_index(how=\"first\")\n\n    @final\n    @doc(first_valid_index, position=\"last\", klass=_shared_doc_kwargs[\"klass\"])\n    def last_valid_index(self) -> Hashable | None:\n        return self._find_valid_index(how=\"last\")\n\n\ndef _doc_params(cls):\n    \"\"\"Return a tuple of the doc params.\"\"\"\n    axis_descr = (\n        f\"{{{', '.join([f'{a} ({i})' for i, a in enumerate(cls._AXIS_ORDERS)])}}}\"\n    )\n    name = cls._constructor_sliced.__name__ if cls._AXIS_LEN > 1 else \"scalar\"\n    name2 = cls.__name__\n    return axis_descr, name, name2\n\n\n_num_doc = \"\"\"\n{desc}\n\nParameters\n----------\naxis : {axis_descr}\n    Axis for the function to be applied on.\nskipna : bool, default True\n    Exclude NA/null values when computing the result.\nlevel : int or level name, default None\n    If the axis is a MultiIndex (hierarchical), count along a\n    particular level, collapsing into a {name1}.\nnumeric_only : bool, default None\n    Include only float, int, boolean columns. If None, will attempt to use\n    everything, then use only numeric data. Not implemented for Series.\n{min_count}\\\n**kwargs\n    Additional keyword arguments to be passed to the function.\n\nReturns\n-------\n{name1} or {name2} (if level specified)\\\n{see_also}\\\n{examples}\n\"\"\"\n\n_num_ddof_doc = \"\"\"\n{desc}\n\nParameters\n----------\naxis : {axis_descr}\nskipna : bool, default True\n    Exclude NA/null values. If an entire row/column is NA, the result\n    will be NA.\nlevel : int or level name, default None\n    If the axis is a MultiIndex (hierarchical), count along a\n    particular level, collapsing into a {name1}.\nddof : int, default 1\n    Delta Degrees of Freedom. The divisor used in calculations is N - ddof,\n    where N represents the number of elements.\nnumeric_only : bool, default None\n    Include only float, int, boolean columns. If None, will attempt to use\n    everything, then use only numeric data. Not implemented for Series.\n\nReturns\n-------\n{name1} or {name2} (if level specified) \\\n{notes}\\\n{examples}\n\"\"\"\n\n_std_notes = \"\"\"\n\nNotes\n-----\nTo have the same behaviour as `numpy.std`, use `ddof=0` (instead of the\ndefault `ddof=1`)\"\"\"\n\n_std_examples = \"\"\"\n\nExamples\n--------\n>>> df = pd.DataFrame({'person_id': [0, 1, 2, 3],\n...                   'age': [21, 25, 62, 43],\n...                   'height': [1.61, 1.87, 1.49, 2.01]}\n...                  ).set_index('person_id')\n>>> df\n           age  height\nperson_id\n0           21    1.61\n1           25    1.87\n2           62    1.49\n3           43    2.01\n\nThe standard deviation of the columns can be found as follows:\n\n>>> df.std()\nage       18.786076\nheight     0.237417\n\nAlternatively, `ddof=0` can be set to normalize by N instead of N-1:\n\n>>> df.std(ddof=0)\nage       16.269219\nheight     0.205609\"\"\"\n\n_var_examples = \"\"\"\n\nExamples\n--------\n>>> df = pd.DataFrame({'person_id': [0, 1, 2, 3],\n...                   'age': [21, 25, 62, 43],\n...                   'height': [1.61, 1.87, 1.49, 2.01]}\n...                  ).set_index('person_id')\n>>> df\n           age  height\nperson_id\n0           21    1.61\n1           25    1.87\n2           62    1.49\n3           43    2.01\n\n>>> df.var()\nage       352.916667\nheight      0.056367\n\nAlternatively, ``ddof=0`` can be set to normalize by N instead of N-1:\n\n>>> df.var(ddof=0)\nage       264.687500\nheight      0.042275\"\"\"\n\n_bool_doc = \"\"\"\n{desc}\n\nParameters\n----------\naxis : {{0 or 'index', 1 or 'columns', None}}, default 0\n    Indicate which axis or axes should be reduced.\n\n    * 0 / 'index' : reduce the index, return a Series whose index is the\n      original column labels.\n    * 1 / 'columns' : reduce the columns, return a Series whose index is the\n      original index.\n    * None : reduce all axes, return a scalar.\n\nbool_only : bool, default None\n    Include only boolean columns. If None, will attempt to use everything,\n    then use only boolean data. Not implemented for Series.\nskipna : bool, default True\n    Exclude NA/null values. If the entire row/column is NA and skipna is\n    True, then the result will be {empty_value}, as for an empty row/column.\n    If skipna is False, then NA are treated as True, because these are not\n    equal to zero.\nlevel : int or level name, default None\n    If the axis is a MultiIndex (hierarchical), count along a\n    particular level, collapsing into a {name1}.\n**kwargs : any, default None\n    Additional keywords have no effect but might be accepted for\n    compatibility with NumPy.\n\nReturns\n-------\n{name1} or {name2}\n    If level is specified, then, {name2} is returned; otherwise, {name1}\n    is returned.\n\n{see_also}\n{examples}\"\"\"\n\n_all_desc = \"\"\"\\\nReturn whether all elements are True, potentially over an axis.\n\nReturns True unless there at least one element within a series or\nalong a Dataframe axis that is False or equivalent (e.g. zero or\nempty).\"\"\"\n\n_all_examples = \"\"\"\\\nExamples\n--------\n**Series**\n\n>>> pd.Series([True, True]).all()\nTrue\n>>> pd.Series([True, False]).all()\nFalse\n>>> pd.Series([], dtype=\"float64\").all()\nTrue\n>>> pd.Series([np.nan]).all()\nTrue\n>>> pd.Series([np.nan]).all(skipna=False)\nTrue\n\n**DataFrames**\n\nCreate a dataframe from a dictionary.\n\n>>> df = pd.DataFrame({'col1': [True, True], 'col2': [True, False]})\n>>> df\n   col1   col2\n0  True   True\n1  True  False\n\nDefault behaviour checks if column-wise values all return True.\n\n>>> df.all()\ncol1     True\ncol2    False\ndtype: bool\n\nSpecify ``axis='columns'`` to check if row-wise values all return True.\n\n>>> df.all(axis='columns')\n0     True\n1    False\ndtype: bool\n\nOr ``axis=None`` for whether every value is True.\n\n>>> df.all(axis=None)\nFalse\n\"\"\"\n\n_all_see_also = \"\"\"\\\nSee Also\n--------\nSeries.all : Return True if all elements are True.\nDataFrame.any : Return True if one (or more) elements are True.\n\"\"\"\n\n_cnum_doc = \"\"\"\nReturn cumulative {desc} over a DataFrame or Series axis.\n\nReturns a DataFrame or Series of the same size containing the cumulative\n{desc}.\n\nParameters\n----------\naxis : {{0 or 'index', 1 or 'columns'}}, default 0\n    The index or the name of the axis. 0 is equivalent to None or 'index'.\nskipna : bool, default True\n    Exclude NA/null values. If an entire row/column is NA, the result\n    will be NA.\n*args, **kwargs\n    Additional keywords have no effect but might be accepted for\n    compatibility with NumPy.\n\nReturns\n-------\n{name1} or {name2}\n    Return cumulative {desc} of {name1} or {name2}.\n\nSee Also\n--------\ncore.window.Expanding.{accum_func_name} : Similar functionality\n    but ignores ``NaN`` values.\n{name2}.{accum_func_name} : Return the {desc} over\n    {name2} axis.\n{name2}.cummax : Return cumulative maximum over {name2} axis.\n{name2}.cummin : Return cumulative minimum over {name2} axis.\n{name2}.cumsum : Return cumulative sum over {name2} axis.\n{name2}.cumprod : Return cumulative product over {name2} axis.\n\n{examples}\"\"\"\n\n_cummin_examples = \"\"\"\\\nExamples\n--------\n**Series**\n\n>>> s = pd.Series([2, np.nan, 5, -1, 0])\n>>> s\n0    2.0\n1    NaN\n2    5.0\n3   -1.0\n4    0.0\ndtype: float64\n\nBy default, NA values are ignored.\n\n>>> s.cummin()\n0    2.0\n1    NaN\n2    2.0\n3   -1.0\n4   -1.0\ndtype: float64\n\nTo include NA values in the operation, use ``skipna=False``\n\n>>> s.cummin(skipna=False)\n0    2.0\n1    NaN\n2    NaN\n3    NaN\n4    NaN\ndtype: float64\n\n**DataFrame**\n\n>>> df = pd.DataFrame([[2.0, 1.0],\n...                    [3.0, np.nan],\n...                    [1.0, 0.0]],\n...                    columns=list('AB'))\n>>> df\n     A    B\n0  2.0  1.0\n1  3.0  NaN\n2  1.0  0.0\n\nBy default, iterates over rows and finds the minimum\nin each column. This is equivalent to ``axis=None`` or ``axis='index'``.\n\n>>> df.cummin()\n     A    B\n0  2.0  1.0\n1  2.0  NaN\n2  1.0  0.0\n\nTo iterate over columns and find the minimum in each row,\nuse ``axis=1``\n\n>>> df.cummin(axis=1)\n     A    B\n0  2.0  1.0\n1  3.0  NaN\n2  1.0  0.0\n\"\"\"\n\n_cumsum_examples = \"\"\"\\\nExamples\n--------\n**Series**\n\n>>> s = pd.Series([2, np.nan, 5, -1, 0])\n>>> s\n0    2.0\n1    NaN\n2    5.0\n3   -1.0\n4    0.0\ndtype: float64\n\nBy default, NA values are ignored.\n\n>>> s.cumsum()\n0    2.0\n1    NaN\n2    7.0\n3    6.0\n4    6.0\ndtype: float64\n\nTo include NA values in the operation, use ``skipna=False``\n\n>>> s.cumsum(skipna=False)\n0    2.0\n1    NaN\n2    NaN\n3    NaN\n4    NaN\ndtype: float64\n\n**DataFrame**\n\n>>> df = pd.DataFrame([[2.0, 1.0],\n...                    [3.0, np.nan],\n...                    [1.0, 0.0]],\n...                    columns=list('AB'))\n>>> df\n     A    B\n0  2.0  1.0\n1  3.0  NaN\n2  1.0  0.0\n\nBy default, iterates over rows and finds the sum\nin each column. This is equivalent to ``axis=None`` or ``axis='index'``.\n\n>>> df.cumsum()\n     A    B\n0  2.0  1.0\n1  5.0  NaN\n2  6.0  1.0\n\nTo iterate over columns and find the sum in each row,\nuse ``axis=1``\n\n>>> df.cumsum(axis=1)\n     A    B\n0  2.0  3.0\n1  3.0  NaN\n2  1.0  1.0\n\"\"\"\n\n_cumprod_examples = \"\"\"\\\nExamples\n--------\n**Series**\n\n>>> s = pd.Series([2, np.nan, 5, -1, 0])\n>>> s\n0    2.0\n1    NaN\n2    5.0\n3   -1.0\n4    0.0\ndtype: float64\n\nBy default, NA values are ignored.\n\n>>> s.cumprod()\n0     2.0\n1     NaN\n2    10.0\n3   -10.0\n4    -0.0\ndtype: float64\n\nTo include NA values in the operation, use ``skipna=False``\n\n>>> s.cumprod(skipna=False)\n0    2.0\n1    NaN\n2    NaN\n3    NaN\n4    NaN\ndtype: float64\n\n**DataFrame**\n\n>>> df = pd.DataFrame([[2.0, 1.0],\n...                    [3.0, np.nan],\n...                    [1.0, 0.0]],\n...                    columns=list('AB'))\n>>> df\n     A    B\n0  2.0  1.0\n1  3.0  NaN\n2  1.0  0.0\n\nBy default, iterates over rows and finds the product\nin each column. This is equivalent to ``axis=None`` or ``axis='index'``.\n\n>>> df.cumprod()\n     A    B\n0  2.0  1.0\n1  6.0  NaN\n2  6.0  0.0\n\nTo iterate over columns and find the product in each row,\nuse ``axis=1``\n\n>>> df.cumprod(axis=1)\n     A    B\n0  2.0  2.0\n1  3.0  NaN\n2  1.0  0.0\n\"\"\"\n\n_cummax_examples = \"\"\"\\\nExamples\n--------\n**Series**\n\n>>> s = pd.Series([2, np.nan, 5, -1, 0])\n>>> s\n0    2.0\n1    NaN\n2    5.0\n3   -1.0\n4    0.0\ndtype: float64\n\nBy default, NA values are ignored.\n\n>>> s.cummax()\n0    2.0\n1    NaN\n2    5.0\n3    5.0\n4    5.0\ndtype: float64\n\nTo include NA values in the operation, use ``skipna=False``\n\n>>> s.cummax(skipna=False)\n0    2.0\n1    NaN\n2    NaN\n3    NaN\n4    NaN\ndtype: float64\n\n**DataFrame**\n\n>>> df = pd.DataFrame([[2.0, 1.0],\n...                    [3.0, np.nan],\n...                    [1.0, 0.0]],\n...                    columns=list('AB'))\n>>> df\n     A    B\n0  2.0  1.0\n1  3.0  NaN\n2  1.0  0.0\n\nBy default, iterates over rows and finds the maximum\nin each column. This is equivalent to ``axis=None`` or ``axis='index'``.\n\n>>> df.cummax()\n     A    B\n0  2.0  1.0\n1  3.0  NaN\n2  3.0  1.0\n\nTo iterate over columns and find the maximum in each row,\nuse ``axis=1``\n\n>>> df.cummax(axis=1)\n     A    B\n0  2.0  2.0\n1  3.0  NaN\n2  1.0  1.0\n\"\"\"\n\n_any_see_also = \"\"\"\\\nSee Also\n--------\nnumpy.any : Numpy version of this method.\nSeries.any : Return whether any element is True.\nSeries.all : Return whether all elements are True.\nDataFrame.any : Return whether any element is True over requested axis.\nDataFrame.all : Return whether all elements are True over requested axis.\n\"\"\"\n\n_any_desc = \"\"\"\\\nReturn whether any element is True, potentially over an axis.\n\nReturns False unless there is at least one element within a series or\nalong a Dataframe axis that is True or equivalent (e.g. non-zero or\nnon-empty).\"\"\"\n\n_any_examples = \"\"\"\\\nExamples\n--------\n**Series**\n\nFor Series input, the output is a scalar indicating whether any element\nis True.\n\n>>> pd.Series([False, False]).any()\nFalse\n>>> pd.Series([True, False]).any()\nTrue\n>>> pd.Series([], dtype=\"float64\").any()\nFalse\n>>> pd.Series([np.nan]).any()\nFalse\n>>> pd.Series([np.nan]).any(skipna=False)\nTrue\n\n**DataFrame**\n\nWhether each column contains at least one True element (the default).\n\n>>> df = pd.DataFrame({\"A\": [1, 2], \"B\": [0, 2], \"C\": [0, 0]})\n>>> df\n   A  B  C\n0  1  0  0\n1  2  2  0\n\n>>> df.any()\nA     True\nB     True\nC    False\ndtype: bool\n\nAggregating over the columns.\n\n>>> df = pd.DataFrame({\"A\": [True, False], \"B\": [1, 2]})\n>>> df\n       A  B\n0   True  1\n1  False  2\n\n>>> df.any(axis='columns')\n0    True\n1    True\ndtype: bool\n\n>>> df = pd.DataFrame({\"A\": [True, False], \"B\": [1, 0]})\n>>> df\n       A  B\n0   True  1\n1  False  0\n\n>>> df.any(axis='columns')\n0    True\n1    False\ndtype: bool\n\nAggregating over the entire DataFrame with ``axis=None``.\n\n>>> df.any(axis=None)\nTrue\n\n`any` for an empty DataFrame is an empty Series.\n\n>>> pd.DataFrame([]).any()\nSeries([], dtype: bool)\n\"\"\"\n\n_shared_docs[\n    \"stat_func_example\"\n] = \"\"\"\n\nExamples\n--------\n>>> idx = pd.MultiIndex.from_arrays([\n...     ['warm', 'warm', 'cold', 'cold'],\n...     ['dog', 'falcon', 'fish', 'spider']],\n...     names=['blooded', 'animal'])\n>>> s = pd.Series([4, 2, 0, 8], name='legs', index=idx)\n>>> s\nblooded  animal\nwarm     dog       4\n         falcon    2\ncold     fish      0\n         spider    8\nName: legs, dtype: int64\n\n>>> s.{stat_func}()\n{default_output}\"\"\"\n\n_sum_examples = _shared_docs[\"stat_func_example\"].format(\n    stat_func=\"sum\", verb=\"Sum\", default_output=14, level_output_0=6, level_output_1=8\n)\n\n_sum_examples += \"\"\"\n\nBy default, the sum of an empty or all-NA Series is ``0``.\n\n>>> pd.Series([], dtype=\"float64\").sum()  # min_count=0 is the default\n0.0\n\nThis can be controlled with the ``min_count`` parameter. For example, if\nyou'd like the sum of an empty series to be NaN, pass ``min_count=1``.\n\n>>> pd.Series([], dtype=\"float64\").sum(min_count=1)\nnan\n\nThanks to the ``skipna`` parameter, ``min_count`` handles all-NA and\nempty series identically.\n\n>>> pd.Series([np.nan]).sum()\n0.0\n\n>>> pd.Series([np.nan]).sum(min_count=1)\nnan\"\"\"\n\n_max_examples = _shared_docs[\"stat_func_example\"].format(\n    stat_func=\"max\", verb=\"Max\", default_output=8, level_output_0=4, level_output_1=8\n)\n\n_min_examples = _shared_docs[\"stat_func_example\"].format(\n    stat_func=\"min\", verb=\"Min\", default_output=0, level_output_0=2, level_output_1=0\n)\n\n_stat_func_see_also = \"\"\"\n\nSee Also\n--------\nSeries.sum : Return the sum.\nSeries.min : Return the minimum.\nSeries.max : Return the maximum.\nSeries.idxmin : Return the index of the minimum.\nSeries.idxmax : Return the index of the maximum.\nDataFrame.sum : Return the sum over the requested axis.\nDataFrame.min : Return the minimum over the requested axis.\nDataFrame.max : Return the maximum over the requested axis.\nDataFrame.idxmin : Return the index of the minimum over the requested axis.\nDataFrame.idxmax : Return the index of the maximum over the requested axis.\"\"\"\n\n_prod_examples = \"\"\"\n\nExamples\n--------\nBy default, the product of an empty or all-NA Series is ``1``\n\n>>> pd.Series([], dtype=\"float64\").prod()\n1.0\n\nThis can be controlled with the ``min_count`` parameter\n\n>>> pd.Series([], dtype=\"float64\").prod(min_count=1)\nnan\n\nThanks to the ``skipna`` parameter, ``min_count`` handles all-NA and\nempty series identically.\n\n>>> pd.Series([np.nan]).prod()\n1.0\n\n>>> pd.Series([np.nan]).prod(min_count=1)\nnan\"\"\"\n\n_min_count_stub = \"\"\"\\\nmin_count : int, default 0\n    The required number of valid values to perform the operation. If fewer than\n    ``min_count`` non-NA values are present the result will be NA.\n\"\"\"\n\n\ndef _align_as_utc(\n    left: NDFrameT, right: NDFrameT, join_index: Index | None\n) -> tuple[NDFrameT, NDFrameT]:\n    \"\"\"\n    If we are aligning timezone-aware DatetimeIndexes and the timezones\n    do not match, convert both to UTC.\n    \"\"\"\n    if is_datetime64tz_dtype(left.index.dtype):\n        if left.index.tz != right.index.tz:\n            if join_index is not None:\n                # GH#33671 ensure we don't change the index on\n                #  our original Series (NB: by default deep=False)\n                left = left.copy()\n                right = right.copy()\n                left.index = join_index\n                right.index = join_index\n\n    return left, right\n", 12173], "/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/series.py": ["\"\"\"\nData structure for 1-dimensional cross-sectional and time series data\n\"\"\"\nfrom __future__ import annotations\n\nfrom textwrap import dedent\nfrom typing import (\n    IO,\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Hashable,\n    Iterable,\n    Literal,\n    Sequence,\n    Union,\n    cast,\n    overload,\n)\nimport warnings\nimport weakref\n\nimport numpy as np\n\nfrom pandas._config import get_option\n\nfrom pandas._libs import (\n    lib,\n    properties,\n    reshape,\n    tslibs,\n)\nfrom pandas._libs.lib import no_default\nfrom pandas._typing import (\n    AggFuncType,\n    ArrayLike,\n    Axis,\n    Dtype,\n    DtypeObj,\n    FillnaOptions,\n    IndexKeyFunc,\n    SingleManager,\n    StorageOptions,\n    TimedeltaConvertibleTypes,\n    TimestampConvertibleTypes,\n    ValueKeyFunc,\n    npt,\n)\nfrom pandas.compat.numpy import function as nv\nfrom pandas.errors import InvalidIndexError\nfrom pandas.util._decorators import (\n    Appender,\n    Substitution,\n    deprecate_nonkeyword_arguments,\n    doc,\n)\nfrom pandas.util._exceptions import find_stack_level\nfrom pandas.util._validators import (\n    validate_ascending,\n    validate_bool_kwarg,\n    validate_percentile,\n)\n\nfrom pandas.core.dtypes.cast import (\n    convert_dtypes,\n    maybe_box_native,\n    maybe_cast_pointwise_result,\n)\nfrom pandas.core.dtypes.common import (\n    ensure_platform_int,\n    is_dict_like,\n    is_integer,\n    is_iterator,\n    is_list_like,\n    is_object_dtype,\n    is_scalar,\n    pandas_dtype,\n    validate_all_hashable,\n)\nfrom pandas.core.dtypes.generic import ABCDataFrame\nfrom pandas.core.dtypes.inference import is_hashable\nfrom pandas.core.dtypes.missing import (\n    isna,\n    na_value_for_dtype,\n    notna,\n    remove_na_arraylike,\n)\n\nfrom pandas.core import (\n    algorithms,\n    base,\n    missing,\n    nanops,\n    ops,\n)\nfrom pandas.core.accessor import CachedAccessor\nfrom pandas.core.apply import SeriesApply\nfrom pandas.core.arrays import ExtensionArray\nfrom pandas.core.arrays.categorical import CategoricalAccessor\nfrom pandas.core.arrays.sparse import SparseAccessor\nimport pandas.core.common as com\nfrom pandas.core.construction import (\n    create_series_with_explicit_dtype,\n    extract_array,\n    is_empty_data,\n    sanitize_array,\n)\nfrom pandas.core.generic import NDFrame\nfrom pandas.core.indexers import (\n    deprecate_ndim_indexing,\n    unpack_1tuple,\n)\nfrom pandas.core.indexes.accessors import CombinedDatetimelikeProperties\nfrom pandas.core.indexes.api import (\n    CategoricalIndex,\n    DatetimeIndex,\n    Float64Index,\n    Index,\n    MultiIndex,\n    PeriodIndex,\n    TimedeltaIndex,\n    default_index,\n    ensure_index,\n)\nimport pandas.core.indexes.base as ibase\nfrom pandas.core.indexing import (\n    check_bool_indexer,\n    check_deprecated_indexers,\n)\nfrom pandas.core.internals import (\n    SingleArrayManager,\n    SingleBlockManager,\n)\nfrom pandas.core.shared_docs import _shared_docs\nfrom pandas.core.sorting import (\n    ensure_key_mapped,\n    nargsort,\n)\nfrom pandas.core.strings import StringMethods\nfrom pandas.core.tools.datetimes import to_datetime\n\nimport pandas.io.formats.format as fmt\nfrom pandas.io.formats.info import (\n    INFO_DOCSTRING,\n    SeriesInfo,\n    series_sub_kwargs,\n)\nimport pandas.plotting\n\nif TYPE_CHECKING:\n\n    from pandas._typing import (\n        NumpySorter,\n        NumpyValueArrayLike,\n    )\n\n    from pandas.core.frame import DataFrame\n    from pandas.core.groupby.generic import SeriesGroupBy\n    from pandas.core.resample import Resampler\n\n__all__ = [\"Series\"]\n\n_shared_doc_kwargs = {\n    \"axes\": \"index\",\n    \"klass\": \"Series\",\n    \"axes_single_arg\": \"{0 or 'index'}\",\n    \"axis\": \"\"\"axis : {0 or 'index'}\n        Parameter needed for compatibility with DataFrame.\"\"\",\n    \"inplace\": \"\"\"inplace : bool, default False\n        If True, performs operation inplace and returns None.\"\"\",\n    \"unique\": \"np.ndarray\",\n    \"duplicated\": \"Series\",\n    \"optional_by\": \"\",\n    \"optional_mapper\": \"\",\n    \"optional_labels\": \"\",\n    \"optional_axis\": \"\",\n    \"replace_iloc\": \"\"\"\n    This differs from updating with ``.loc`` or ``.iloc``, which require\n    you to specify a location to update with some value.\"\"\",\n}\n\n\ndef _coerce_method(converter):\n    \"\"\"\n    Install the scalar coercion methods.\n    \"\"\"\n\n    def wrapper(self):\n        if len(self) == 1:\n            return converter(self.iloc[0])\n        raise TypeError(f\"cannot convert the series to {converter}\")\n\n    wrapper.__name__ = f\"__{converter.__name__}__\"\n    return wrapper\n\n\n# ----------------------------------------------------------------------\n# Series class\n\n\nclass Series(base.IndexOpsMixin, NDFrame):\n    \"\"\"\n    One-dimensional ndarray with axis labels (including time series).\n\n    Labels need not be unique but must be a hashable type. The object\n    supports both integer- and label-based indexing and provides a host of\n    methods for performing operations involving the index. Statistical\n    methods from ndarray have been overridden to automatically exclude\n    missing data (currently represented as NaN).\n\n    Operations between Series (+, -, /, \\\\*, \\\\*\\\\*) align values based on their\n    associated index values-- they need not be the same length. The result\n    index will be the sorted union of the two indexes.\n\n    Parameters\n    ----------\n    data : array-like, Iterable, dict, or scalar value\n        Contains data stored in Series. If data is a dict, argument order is\n        maintained.\n    index : array-like or Index (1d)\n        Values must be hashable and have the same length as `data`.\n        Non-unique index values are allowed. Will default to\n        RangeIndex (0, 1, 2, ..., n) if not provided. If data is dict-like\n        and index is None, then the keys in the data are used as the index. If the\n        index is not None, the resulting Series is reindexed with the index values.\n    dtype : str, numpy.dtype, or ExtensionDtype, optional\n        Data type for the output Series. If not specified, this will be\n        inferred from `data`.\n        See the :ref:`user guide <basics.dtypes>` for more usages.\n    name : str, optional\n        The name to give to the Series.\n    copy : bool, default False\n        Copy input data. Only affects Series or 1d ndarray input. See examples.\n\n    Examples\n    --------\n    Constructing Series from a dictionary with an Index specified\n\n    >>> d = {'a': 1, 'b': 2, 'c': 3}\n    >>> ser = pd.Series(data=d, index=['a', 'b', 'c'])\n    >>> ser\n    a   1\n    b   2\n    c   3\n    dtype: int64\n\n    The keys of the dictionary match with the Index values, hence the Index\n    values have no effect.\n\n    >>> d = {'a': 1, 'b': 2, 'c': 3}\n    >>> ser = pd.Series(data=d, index=['x', 'y', 'z'])\n    >>> ser\n    x   NaN\n    y   NaN\n    z   NaN\n    dtype: float64\n\n    Note that the Index is first build with the keys from the dictionary.\n    After this the Series is reindexed with the given Index values, hence we\n    get all NaN as a result.\n\n    Constructing Series from a list with `copy=False`.\n\n    >>> r = [1, 2]\n    >>> ser = pd.Series(r, copy=False)\n    >>> ser.iloc[0] = 999\n    >>> r\n    [1, 2]\n    >>> ser\n    0    999\n    1      2\n    dtype: int64\n\n    Due to input data type the Series has a `copy` of\n    the original data even though `copy=False`, so\n    the data is unchanged.\n\n    Constructing Series from a 1d ndarray with `copy=False`.\n\n    >>> r = np.array([1, 2])\n    >>> ser = pd.Series(r, copy=False)\n    >>> ser.iloc[0] = 999\n    >>> r\n    array([999,   2])\n    >>> ser\n    0    999\n    1      2\n    dtype: int64\n\n    Due to input data type the Series has a `view` on\n    the original data, so\n    the data is changed as well.\n    \"\"\"\n\n    _typ = \"series\"\n    _HANDLED_TYPES = (Index, ExtensionArray, np.ndarray)\n\n    _name: Hashable\n    _metadata: list[str] = [\"name\"]\n    _internal_names_set = {\"index\"} | NDFrame._internal_names_set\n    _accessors = {\"dt\", \"cat\", \"str\", \"sparse\"}\n    _hidden_attrs = (\n        base.IndexOpsMixin._hidden_attrs\n        | NDFrame._hidden_attrs\n        | frozenset([\"compress\", \"ptp\"])\n    )\n\n    # Override cache_readonly bc Series is mutable\n    # error: Incompatible types in assignment (expression has type \"property\",\n    # base class \"IndexOpsMixin\" defined the type as \"Callable[[IndexOpsMixin], bool]\")\n    hasnans = property(  # type: ignore[assignment]\n        # error: \"Callable[[IndexOpsMixin], bool]\" has no attribute \"fget\"\n        base.IndexOpsMixin.hasnans.fget,  # type: ignore[attr-defined]\n        doc=base.IndexOpsMixin.hasnans.__doc__,\n    )\n    _mgr: SingleManager\n    div: Callable[[Series, Any], Series]\n    rdiv: Callable[[Series, Any], Series]\n\n    # ----------------------------------------------------------------------\n    # Constructors\n\n    def __init__(\n        self,\n        data=None,\n        index=None,\n        dtype: Dtype | None = None,\n        name=None,\n        copy: bool = False,\n        fastpath: bool = False,\n    ):\n\n        if (\n            isinstance(data, (SingleBlockManager, SingleArrayManager))\n            and index is None\n            and dtype is None\n            and copy is False\n        ):\n            # GH#33357 called with just the SingleBlockManager\n            NDFrame.__init__(self, data)\n            if fastpath:\n                # e.g. from _box_col_values, skip validation of name\n                object.__setattr__(self, \"_name\", name)\n            else:\n                self.name = name\n            return\n\n        # we are called internally, so short-circuit\n        if fastpath:\n\n            # data is an ndarray, index is defined\n            if not isinstance(data, (SingleBlockManager, SingleArrayManager)):\n                manager = get_option(\"mode.data_manager\")\n                if manager == \"block\":\n                    data = SingleBlockManager.from_array(data, index)\n                elif manager == \"array\":\n                    data = SingleArrayManager.from_array(data, index)\n            if copy:\n                data = data.copy()\n            if index is None:\n                index = data.index\n\n        else:\n\n            name = ibase.maybe_extract_name(name, data, type(self))\n\n            if is_empty_data(data) and dtype is None:\n                # gh-17261\n                warnings.warn(\n                    \"The default dtype for empty Series will be 'object' instead \"\n                    \"of 'float64' in a future version. Specify a dtype explicitly \"\n                    \"to silence this warning.\",\n                    FutureWarning,\n                    stacklevel=find_stack_level(),\n                )\n                # uncomment the line below when removing the FutureWarning\n                # dtype = np.dtype(object)\n\n            if index is not None:\n                index = ensure_index(index)\n\n            if data is None:\n                data = {}\n            if dtype is not None:\n                dtype = self._validate_dtype(dtype)\n\n            if isinstance(data, MultiIndex):\n                raise NotImplementedError(\n                    \"initializing a Series from a MultiIndex is not supported\"\n                )\n            elif isinstance(data, Index):\n\n                if dtype is not None:\n                    # astype copies\n                    data = data.astype(dtype)\n                else:\n                    # GH#24096 we need to ensure the index remains immutable\n                    data = data._values.copy()\n                copy = False\n\n            elif isinstance(data, np.ndarray):\n                if len(data.dtype):\n                    # GH#13296 we are dealing with a compound dtype, which\n                    #  should be treated as 2D\n                    raise ValueError(\n                        \"Cannot construct a Series from an ndarray with \"\n                        \"compound dtype.  Use DataFrame instead.\"\n                    )\n            elif isinstance(data, Series):\n                if index is None:\n                    index = data.index\n                else:\n                    data = data.reindex(index, copy=copy)\n                    copy = False\n                data = data._mgr\n            elif is_dict_like(data):\n                data, index = self._init_dict(data, index, dtype)\n                dtype = None\n                copy = False\n            elif isinstance(data, (SingleBlockManager, SingleArrayManager)):\n                if index is None:\n                    index = data.index\n                elif not data.index.equals(index) or copy:\n                    # GH#19275 SingleBlockManager input should only be called\n                    # internally\n                    raise AssertionError(\n                        \"Cannot pass both SingleBlockManager \"\n                        \"`data` argument and a different \"\n                        \"`index` argument. `copy` must be False.\"\n                    )\n\n            elif isinstance(data, ExtensionArray):\n                pass\n            else:\n                data = com.maybe_iterable_to_list(data)\n\n            if index is None:\n                if not is_list_like(data):\n                    data = [data]\n                index = default_index(len(data))\n            elif is_list_like(data):\n                com.require_length_match(data, index)\n\n            # create/copy the manager\n            if isinstance(data, (SingleBlockManager, SingleArrayManager)):\n                if dtype is not None:\n                    data = data.astype(dtype=dtype, errors=\"ignore\", copy=copy)\n                elif copy:\n                    data = data.copy()\n            else:\n                data = sanitize_array(data, index, dtype, copy)\n\n                manager = get_option(\"mode.data_manager\")\n                if manager == \"block\":\n                    data = SingleBlockManager.from_array(data, index)\n                elif manager == \"array\":\n                    data = SingleArrayManager.from_array(data, index)\n\n        NDFrame.__init__(self, data)\n        self.name = name\n        self._set_axis(0, index, fastpath=True)\n\n    def _init_dict(\n        self, data, index: Index | None = None, dtype: DtypeObj | None = None\n    ):\n        \"\"\"\n        Derive the \"_mgr\" and \"index\" attributes of a new Series from a\n        dictionary input.\n\n        Parameters\n        ----------\n        data : dict or dict-like\n            Data used to populate the new Series.\n        index : Index or None, default None\n            Index for the new Series: if None, use dict keys.\n        dtype : np.dtype, ExtensionDtype, or None, default None\n            The dtype for the new Series: if None, infer from data.\n\n        Returns\n        -------\n        _data : BlockManager for the new Series\n        index : index for the new Series\n        \"\"\"\n        keys: Index | tuple\n\n        # Looking for NaN in dict doesn't work ({np.nan : 1}[float('nan')]\n        # raises KeyError), so we iterate the entire dict, and align\n        if data:\n            # GH:34717, issue was using zip to extract key and values from data.\n            # using generators in effects the performance.\n            # Below is the new way of extracting the keys and values\n\n            keys = tuple(data.keys())\n            values = list(data.values())  # Generating list of values- faster way\n        elif index is not None:\n            # fastpath for Series(data=None). Just use broadcasting a scalar\n            # instead of reindexing.\n            values = na_value_for_dtype(pandas_dtype(dtype), compat=False)\n            keys = index\n        else:\n            keys, values = (), []\n\n        # Input is now list-like, so rely on \"standard\" construction:\n\n        # TODO: passing np.float64 to not break anything yet. See GH-17261\n        s = create_series_with_explicit_dtype(\n            # error: Argument \"index\" to \"create_series_with_explicit_dtype\" has\n            # incompatible type \"Tuple[Any, ...]\"; expected \"Union[ExtensionArray,\n            # ndarray, Index, None]\"\n            values,\n            index=keys,  # type: ignore[arg-type]\n            dtype=dtype,\n            dtype_if_empty=np.float64,\n        )\n\n        # Now we just make sure the order is respected, if any\n        if data and index is not None:\n            s = s.reindex(index, copy=False)\n        return s._mgr, s.index\n\n    # ----------------------------------------------------------------------\n\n    @property\n    def _constructor(self) -> Callable[..., Series]:\n        return Series\n\n    @property\n    def _constructor_expanddim(self) -> Callable[..., DataFrame]:\n        \"\"\"\n        Used when a manipulation result has one higher dimension as the\n        original, such as Series.to_frame()\n        \"\"\"\n        from pandas.core.frame import DataFrame\n\n        return DataFrame\n\n    # types\n    @property\n    def _can_hold_na(self) -> bool:\n        return self._mgr._can_hold_na\n\n    def _set_axis(self, axis: int, labels, fastpath: bool = False) -> None:\n        \"\"\"\n        Override generic, we want to set the _typ here.\n\n        This is called from the cython code when we set the `index` attribute\n        directly, e.g. `series.index = [1, 2, 3]`.\n        \"\"\"\n        if not fastpath:\n            labels = ensure_index(labels)\n\n        if labels._is_all_dates:\n            deep_labels = labels\n            if isinstance(labels, CategoricalIndex):\n                deep_labels = labels.categories\n\n            if not isinstance(\n                deep_labels, (DatetimeIndex, PeriodIndex, TimedeltaIndex)\n            ):\n                try:\n                    labels = DatetimeIndex(labels)\n                    # need to set here because we changed the index\n                    if fastpath:\n                        self._mgr.set_axis(axis, labels)\n                except (tslibs.OutOfBoundsDatetime, ValueError):\n                    # labels may exceeds datetime bounds,\n                    # or not be a DatetimeIndex\n                    pass\n\n        if not fastpath:\n            # The ensure_index call above ensures we have an Index object\n            self._mgr.set_axis(axis, labels)\n\n    # ndarray compatibility\n    @property\n    def dtype(self) -> DtypeObj:\n        \"\"\"\n        Return the dtype object of the underlying data.\n        \"\"\"\n        return self._mgr.dtype\n\n    @property\n    def dtypes(self) -> DtypeObj:\n        \"\"\"\n        Return the dtype object of the underlying data.\n        \"\"\"\n        # DataFrame compatibility\n        return self.dtype\n\n    @property\n    def name(self) -> Hashable:\n        \"\"\"\n        Return the name of the Series.\n\n        The name of a Series becomes its index or column name if it is used\n        to form a DataFrame. It is also used whenever displaying the Series\n        using the interpreter.\n\n        Returns\n        -------\n        label (hashable object)\n            The name of the Series, also the column name if part of a DataFrame.\n\n        See Also\n        --------\n        Series.rename : Sets the Series name when given a scalar input.\n        Index.name : Corresponding Index property.\n\n        Examples\n        --------\n        The Series name can be set initially when calling the constructor.\n\n        >>> s = pd.Series([1, 2, 3], dtype=np.int64, name='Numbers')\n        >>> s\n        0    1\n        1    2\n        2    3\n        Name: Numbers, dtype: int64\n        >>> s.name = \"Integers\"\n        >>> s\n        0    1\n        1    2\n        2    3\n        Name: Integers, dtype: int64\n\n        The name of a Series within a DataFrame is its column name.\n\n        >>> df = pd.DataFrame([[1, 2], [3, 4], [5, 6]],\n        ...                   columns=[\"Odd Numbers\", \"Even Numbers\"])\n        >>> df\n           Odd Numbers  Even Numbers\n        0            1             2\n        1            3             4\n        2            5             6\n        >>> df[\"Even Numbers\"].name\n        'Even Numbers'\n        \"\"\"\n        return self._name\n\n    @name.setter\n    def name(self, value: Hashable) -> None:\n        validate_all_hashable(value, error_name=f\"{type(self).__name__}.name\")\n        object.__setattr__(self, \"_name\", value)\n\n    @property\n    def values(self):\n        \"\"\"\n        Return Series as ndarray or ndarray-like depending on the dtype.\n\n        .. warning::\n\n           We recommend using :attr:`Series.array` or\n           :meth:`Series.to_numpy`, depending on whether you need\n           a reference to the underlying data or a NumPy array.\n\n        Returns\n        -------\n        numpy.ndarray or ndarray-like\n\n        See Also\n        --------\n        Series.array : Reference to the underlying data.\n        Series.to_numpy : A NumPy array representing the underlying data.\n\n        Examples\n        --------\n        >>> pd.Series([1, 2, 3]).values\n        array([1, 2, 3])\n\n        >>> pd.Series(list('aabc')).values\n        array(['a', 'a', 'b', 'c'], dtype=object)\n\n        >>> pd.Series(list('aabc')).astype('category').values\n        ['a', 'a', 'b', 'c']\n        Categories (3, object): ['a', 'b', 'c']\n\n        Timezone aware datetime data is converted to UTC:\n\n        >>> pd.Series(pd.date_range('20130101', periods=3,\n        ...                         tz='US/Eastern')).values\n        array(['2013-01-01T05:00:00.000000000',\n               '2013-01-02T05:00:00.000000000',\n               '2013-01-03T05:00:00.000000000'], dtype='datetime64[ns]')\n        \"\"\"\n        return self._mgr.external_values()\n\n    @property\n    def _values(self):\n        \"\"\"\n        Return the internal repr of this data (defined by Block.interval_values).\n        This are the values as stored in the Block (ndarray or ExtensionArray\n        depending on the Block class), with datetime64[ns] and timedelta64[ns]\n        wrapped in ExtensionArrays to match Index._values behavior.\n\n        Differs from the public ``.values`` for certain data types, because of\n        historical backwards compatibility of the public attribute (e.g. period\n        returns object ndarray and datetimetz a datetime64[ns] ndarray for\n        ``.values`` while it returns an ExtensionArray for ``._values`` in those\n        cases).\n\n        Differs from ``.array`` in that this still returns the numpy array if\n        the Block is backed by a numpy array (except for datetime64 and\n        timedelta64 dtypes), while ``.array`` ensures to always return an\n        ExtensionArray.\n\n        Overview:\n\n        dtype       | values        | _values       | array         |\n        ----------- | ------------- | ------------- | ------------- |\n        Numeric     | ndarray       | ndarray       | PandasArray   |\n        Category    | Categorical   | Categorical   | Categorical   |\n        dt64[ns]    | ndarray[M8ns] | DatetimeArray | DatetimeArray |\n        dt64[ns tz] | ndarray[M8ns] | DatetimeArray | DatetimeArray |\n        td64[ns]    | ndarray[m8ns] | TimedeltaArray| ndarray[m8ns] |\n        Period      | ndarray[obj]  | PeriodArray   | PeriodArray   |\n        Nullable    | EA            | EA            | EA            |\n\n        \"\"\"\n        return self._mgr.internal_values()\n\n    # error: Decorated property not supported\n    @Appender(base.IndexOpsMixin.array.__doc__)  # type: ignore[misc]\n    @property\n    def array(self) -> ExtensionArray:\n        return self._mgr.array_values()\n\n    # ops\n    def ravel(self, order=\"C\"):\n        \"\"\"\n        Return the flattened underlying data as an ndarray.\n\n        Returns\n        -------\n        numpy.ndarray or ndarray-like\n            Flattened data of the Series.\n\n        See Also\n        --------\n        numpy.ndarray.ravel : Return a flattened array.\n        \"\"\"\n        return self._values.ravel(order=order)\n\n    def __len__(self) -> int:\n        \"\"\"\n        Return the length of the Series.\n        \"\"\"\n        return len(self._mgr)\n\n    def view(self, dtype: Dtype | None = None) -> Series:\n        \"\"\"\n        Create a new view of the Series.\n\n        This function will return a new Series with a view of the same\n        underlying values in memory, optionally reinterpreted with a new data\n        type. The new data type must preserve the same size in bytes as to not\n        cause index misalignment.\n\n        Parameters\n        ----------\n        dtype : data type\n            Data type object or one of their string representations.\n\n        Returns\n        -------\n        Series\n            A new Series object as a view of the same data in memory.\n\n        See Also\n        --------\n        numpy.ndarray.view : Equivalent numpy function to create a new view of\n            the same data in memory.\n\n        Notes\n        -----\n        Series are instantiated with ``dtype=float64`` by default. While\n        ``numpy.ndarray.view()`` will return a view with the same data type as\n        the original array, ``Series.view()`` (without specified dtype)\n        will try using ``float64`` and may fail if the original data type size\n        in bytes is not the same.\n\n        Examples\n        --------\n        >>> s = pd.Series([-2, -1, 0, 1, 2], dtype='int8')\n        >>> s\n        0   -2\n        1   -1\n        2    0\n        3    1\n        4    2\n        dtype: int8\n\n        The 8 bit signed integer representation of `-1` is `0b11111111`, but\n        the same bytes represent 255 if read as an 8 bit unsigned integer:\n\n        >>> us = s.view('uint8')\n        >>> us\n        0    254\n        1    255\n        2      0\n        3      1\n        4      2\n        dtype: uint8\n\n        The views share the same underlying values:\n\n        >>> us[0] = 128\n        >>> s\n        0   -128\n        1     -1\n        2      0\n        3      1\n        4      2\n        dtype: int8\n        \"\"\"\n        # self.array instead of self._values so we piggyback on PandasArray\n        #  implementation\n        res_values = self.array.view(dtype)\n        res_ser = self._constructor(res_values, index=self.index)\n        return res_ser.__finalize__(self, method=\"view\")\n\n    # ----------------------------------------------------------------------\n    # NDArray Compat\n    _HANDLED_TYPES = (Index, ExtensionArray, np.ndarray)\n\n    def __array__(self, dtype: npt.DTypeLike | None = None) -> np.ndarray:\n        \"\"\"\n        Return the values as a NumPy array.\n\n        Users should not call this directly. Rather, it is invoked by\n        :func:`numpy.array` and :func:`numpy.asarray`.\n\n        Parameters\n        ----------\n        dtype : str or numpy.dtype, optional\n            The dtype to use for the resulting NumPy array. By default,\n            the dtype is inferred from the data.\n\n        Returns\n        -------\n        numpy.ndarray\n            The values in the series converted to a :class:`numpy.ndarray`\n            with the specified `dtype`.\n\n        See Also\n        --------\n        array : Create a new array from data.\n        Series.array : Zero-copy view to the array backing the Series.\n        Series.to_numpy : Series method for similar behavior.\n\n        Examples\n        --------\n        >>> ser = pd.Series([1, 2, 3])\n        >>> np.asarray(ser)\n        array([1, 2, 3])\n\n        For timezone-aware data, the timezones may be retained with\n        ``dtype='object'``\n\n        >>> tzser = pd.Series(pd.date_range('2000', periods=2, tz=\"CET\"))\n        >>> np.asarray(tzser, dtype=\"object\")\n        array([Timestamp('2000-01-01 00:00:00+0100', tz='CET'),\n               Timestamp('2000-01-02 00:00:00+0100', tz='CET')],\n              dtype=object)\n\n        Or the values may be localized to UTC and the tzinfo discarded with\n        ``dtype='datetime64[ns]'``\n\n        >>> np.asarray(tzser, dtype=\"datetime64[ns]\")  # doctest: +ELLIPSIS\n        array(['1999-12-31T23:00:00.000000000', ...],\n              dtype='datetime64[ns]')\n        \"\"\"\n        return np.asarray(self._values, dtype)\n\n    # ----------------------------------------------------------------------\n    # Unary Methods\n\n    # coercion\n    __float__ = _coerce_method(float)\n    __long__ = _coerce_method(int)\n    __int__ = _coerce_method(int)\n\n    # ----------------------------------------------------------------------\n\n    # indexers\n    @property\n    def axes(self) -> list[Index]:\n        \"\"\"\n        Return a list of the row axis labels.\n        \"\"\"\n        return [self.index]\n\n    # ----------------------------------------------------------------------\n    # Indexing Methods\n\n    @Appender(NDFrame.take.__doc__)\n    def take(self, indices, axis=0, is_copy=None, **kwargs) -> Series:\n        if is_copy is not None:\n            warnings.warn(\n                \"is_copy is deprecated and will be removed in a future version. \"\n                \"'take' always returns a copy, so there is no need to specify this.\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n        nv.validate_take((), kwargs)\n\n        indices = ensure_platform_int(indices)\n        new_index = self.index.take(indices)\n        new_values = self._values.take(indices)\n\n        result = self._constructor(new_values, index=new_index, fastpath=True)\n        return result.__finalize__(self, method=\"take\")\n\n    def _take_with_is_copy(self, indices, axis=0) -> Series:\n        \"\"\"\n        Internal version of the `take` method that sets the `_is_copy`\n        attribute to keep track of the parent dataframe (using in indexing\n        for the SettingWithCopyWarning). For Series this does the same\n        as the public take (it never sets `_is_copy`).\n\n        See the docstring of `take` for full explanation of the parameters.\n        \"\"\"\n        return self.take(indices=indices, axis=axis)\n\n    def _ixs(self, i: int, axis: int = 0):\n        \"\"\"\n        Return the i-th value or values in the Series by location.\n\n        Parameters\n        ----------\n        i : int\n\n        Returns\n        -------\n        scalar (int) or Series (slice, sequence)\n        \"\"\"\n        return self._values[i]\n\n    def _slice(self, slobj: slice, axis: int = 0) -> Series:\n        # axis kwarg is retained for compat with NDFrame method\n        #  _slice is *always* positional\n        return self._get_values(slobj)\n\n    def __getitem__(self, key):\n        check_deprecated_indexers(key)\n        key = com.apply_if_callable(key, self)\n\n        if key is Ellipsis:\n            return self\n\n        key_is_scalar = is_scalar(key)\n        if isinstance(key, (list, tuple)):\n            key = unpack_1tuple(key)\n\n        if is_integer(key) and self.index._should_fallback_to_positional:\n            return self._values[key]\n\n        elif key_is_scalar:\n            return self._get_value(key)\n\n        if is_hashable(key):\n            # Otherwise index.get_value will raise InvalidIndexError\n            try:\n                # For labels that don't resolve as scalars like tuples and frozensets\n                result = self._get_value(key)\n\n                return result\n\n            except (KeyError, TypeError, InvalidIndexError):\n                # InvalidIndexError for e.g. generator\n                #  see test_series_getitem_corner_generator\n                if isinstance(key, tuple) and isinstance(self.index, MultiIndex):\n                    # We still have the corner case where a tuple is a key\n                    # in the first level of our MultiIndex\n                    return self._get_values_tuple(key)\n\n        if is_iterator(key):\n            key = list(key)\n\n        if com.is_bool_indexer(key):\n            key = check_bool_indexer(self.index, key)\n            key = np.asarray(key, dtype=bool)\n            return self._get_values(key)\n\n        return self._get_with(key)\n\n    def _get_with(self, key):\n        # other: fancy integer or otherwise\n        if isinstance(key, slice):\n            # _convert_slice_indexer to determine if this slice is positional\n            #  or label based, and if the latter, convert to positional\n            slobj = self.index._convert_slice_indexer(key, kind=\"getitem\")\n            return self._slice(slobj)\n        elif isinstance(key, ABCDataFrame):\n            raise TypeError(\n                \"Indexing a Series with DataFrame is not \"\n                \"supported, use the appropriate DataFrame column\"\n            )\n        elif isinstance(key, tuple):\n            return self._get_values_tuple(key)\n\n        elif not is_list_like(key):\n            # e.g. scalars that aren't recognized by lib.is_scalar, GH#32684\n            return self.loc[key]\n\n        if not isinstance(key, (list, np.ndarray, ExtensionArray, Series, Index)):\n            key = list(key)\n\n        if isinstance(key, Index):\n            key_type = key.inferred_type\n        else:\n            key_type = lib.infer_dtype(key, skipna=False)\n\n        # Note: The key_type == \"boolean\" case should be caught by the\n        #  com.is_bool_indexer check in __getitem__\n        if key_type == \"integer\":\n            # We need to decide whether to treat this as a positional indexer\n            #  (i.e. self.iloc) or label-based (i.e. self.loc)\n            if not self.index._should_fallback_to_positional:\n                return self.loc[key]\n            else:\n                return self.iloc[key]\n\n        # handle the dup indexing case GH#4246\n        return self.loc[key]\n\n    def _get_values_tuple(self, key):\n        # mpl hackaround\n        if com.any_none(*key):\n            result = self._get_values(key)\n            deprecate_ndim_indexing(result, stacklevel=find_stack_level())\n            return result\n\n        if not isinstance(self.index, MultiIndex):\n            raise KeyError(\"key of type tuple not found and not a MultiIndex\")\n\n        # If key is contained, would have returned by now\n        indexer, new_index = self.index.get_loc_level(key)\n        return self._constructor(self._values[indexer], index=new_index).__finalize__(\n            self\n        )\n\n    def _get_values(self, indexer):\n        try:\n            new_mgr = self._mgr.getitem_mgr(indexer)\n            return self._constructor(new_mgr).__finalize__(self)\n        except ValueError:\n            # mpl compat if we look up e.g. ser[:, np.newaxis];\n            #  see tests.series.timeseries.test_mpl_compat_hack\n            # the asarray is needed to avoid returning a 2D DatetimeArray\n            return np.asarray(self._values[indexer])\n\n    def _get_value(self, label, takeable: bool = False):\n        \"\"\"\n        Quickly retrieve single value at passed index label.\n\n        Parameters\n        ----------\n        label : object\n        takeable : interpret the index as indexers, default False\n\n        Returns\n        -------\n        scalar value\n        \"\"\"\n        if takeable:\n            return self._values[label]\n\n        # Similar to Index.get_value, but we do not fall back to positional\n        loc = self.index.get_loc(label)\n        return self.index._get_values_for_loc(self, loc, label)\n\n    def __setitem__(self, key, value) -> None:\n        check_deprecated_indexers(key)\n        key = com.apply_if_callable(key, self)\n        cacher_needs_updating = self._check_is_chained_assignment_possible()\n\n        if key is Ellipsis:\n            key = slice(None)\n\n        if isinstance(key, slice):\n            indexer = self.index._convert_slice_indexer(key, kind=\"getitem\")\n            return self._set_values(indexer, value)\n\n        try:\n            self._set_with_engine(key, value)\n        except (KeyError, ValueError):\n            if is_integer(key) and self.index.inferred_type != \"integer\":\n                # positional setter\n                if not self.index._should_fallback_to_positional:\n                    # GH#33469\n                    warnings.warn(\n                        \"Treating integers as positional in Series.__setitem__ \"\n                        \"with a Float64Index is deprecated. In a future version, \"\n                        \"`series[an_int] = val` will insert a new key into the \"\n                        \"Series. Use `series.iloc[an_int] = val` to treat the \"\n                        \"key as positional.\",\n                        FutureWarning,\n                        stacklevel=find_stack_level(),\n                    )\n                # this is equivalent to self._values[key] = value\n                self._mgr.setitem_inplace(key, value)\n            else:\n                # GH#12862 adding a new key to the Series\n                self.loc[key] = value\n\n        except (InvalidIndexError, TypeError) as err:\n            if isinstance(key, tuple) and not isinstance(self.index, MultiIndex):\n                # cases with MultiIndex don't get here bc they raise KeyError\n                raise KeyError(\n                    \"key of type tuple not found and not a MultiIndex\"\n                ) from err\n\n            if com.is_bool_indexer(key):\n                key = check_bool_indexer(self.index, key)\n                key = np.asarray(key, dtype=bool)\n\n                if (\n                    is_list_like(value)\n                    and len(value) != len(self)\n                    and not isinstance(value, Series)\n                    and not is_object_dtype(self.dtype)\n                ):\n                    # Series will be reindexed to have matching length inside\n                    #  _where call below\n                    # GH#44265\n                    indexer = key.nonzero()[0]\n                    self._set_values(indexer, value)\n                    return\n\n                # otherwise with listlike other we interpret series[mask] = other\n                #  as series[mask] = other[mask]\n                try:\n                    self._where(~key, value, inplace=True)\n                except InvalidIndexError:\n                    # test_where_dups\n                    self.iloc[key] = value\n                return\n\n            else:\n                self._set_with(key, value)\n\n        if cacher_needs_updating:\n            self._maybe_update_cacher()\n\n    def _set_with_engine(self, key, value) -> None:\n        loc = self.index.get_loc(key)\n\n        # this is equivalent to self._values[key] = value\n        self._mgr.setitem_inplace(loc, value)\n\n    def _set_with(self, key, value):\n        # other: fancy integer or otherwise\n        assert not isinstance(key, tuple)\n\n        if is_scalar(key):\n            key = [key]\n        elif is_iterator(key):\n            # Without this, the call to infer_dtype will consume the generator\n            key = list(key)\n\n        key_type = lib.infer_dtype(key, skipna=False)\n\n        # Note: key_type == \"boolean\" should not occur because that\n        #  should be caught by the is_bool_indexer check in __setitem__\n        if key_type == \"integer\":\n            if not self.index._should_fallback_to_positional:\n                self._set_labels(key, value)\n            else:\n                self._set_values(key, value)\n        else:\n            self.loc[key] = value\n\n    def _set_labels(self, key, value) -> None:\n        key = com.asarray_tuplesafe(key)\n        indexer: np.ndarray = self.index.get_indexer(key)\n        mask = indexer == -1\n        if mask.any():\n            raise KeyError(f\"{key[mask]} not in index\")\n        self._set_values(indexer, value)\n\n    def _set_values(self, key, value) -> None:\n        if isinstance(key, (Index, Series)):\n            key = key._values\n\n        self._mgr = self._mgr.setitem(indexer=key, value=value)\n        self._maybe_update_cacher()\n\n    def _set_value(self, label, value, takeable: bool = False):\n        \"\"\"\n        Quickly set single value at passed label.\n\n        If label is not contained, a new object is created with the label\n        placed at the end of the result index.\n\n        Parameters\n        ----------\n        label : object\n            Partial indexing with MultiIndex not allowed.\n        value : object\n            Scalar value.\n        takeable : interpret the index as indexers, default False\n        \"\"\"\n        if not takeable:\n            try:\n                loc = self.index.get_loc(label)\n            except KeyError:\n                # set using a non-recursive method\n                self.loc[label] = value\n                return\n        else:\n            loc = label\n\n        self._set_values(loc, value)\n\n    # ----------------------------------------------------------------------\n    # Lookup Caching\n\n    @property\n    def _is_cached(self) -> bool:\n        \"\"\"Return boolean indicating if self is cached or not.\"\"\"\n        return getattr(self, \"_cacher\", None) is not None\n\n    def _get_cacher(self):\n        \"\"\"return my cacher or None\"\"\"\n        cacher = getattr(self, \"_cacher\", None)\n        if cacher is not None:\n            cacher = cacher[1]()\n        return cacher\n\n    def _reset_cacher(self) -> None:\n        \"\"\"\n        Reset the cacher.\n        \"\"\"\n        if hasattr(self, \"_cacher\"):\n            # should only get here with self.ndim == 1\n            del self._cacher\n\n    def _set_as_cached(self, item, cacher) -> None:\n        \"\"\"\n        Set the _cacher attribute on the calling object with a weakref to\n        cacher.\n        \"\"\"\n        self._cacher = (item, weakref.ref(cacher))\n\n    def _clear_item_cache(self) -> None:\n        # no-op for Series\n        pass\n\n    def _check_is_chained_assignment_possible(self) -> bool:\n        \"\"\"\n        See NDFrame._check_is_chained_assignment_possible.__doc__\n        \"\"\"\n        if self._is_view and self._is_cached:\n            ref = self._get_cacher()\n            if ref is not None and ref._is_mixed_type:\n                self._check_setitem_copy(t=\"referent\", force=True)\n            return True\n        return super()._check_is_chained_assignment_possible()\n\n    def _maybe_update_cacher(\n        self, clear: bool = False, verify_is_copy: bool = True, inplace: bool = False\n    ) -> None:\n        \"\"\"\n        See NDFrame._maybe_update_cacher.__doc__\n        \"\"\"\n        cacher = getattr(self, \"_cacher\", None)\n        if cacher is not None:\n            assert self.ndim == 1\n            ref: DataFrame = cacher[1]()\n\n            # we are trying to reference a dead referent, hence\n            # a copy\n            if ref is None:\n                del self._cacher\n            elif len(self) == len(ref) and self.name in ref.columns:\n                # GH#42530 self.name must be in ref.columns\n                # to ensure column still in dataframe\n                # otherwise, either self or ref has swapped in new arrays\n                ref._maybe_cache_changed(cacher[0], self, inplace=inplace)\n            else:\n                # GH#33675 we have swapped in a new array, so parent\n                #  reference to self is now invalid\n                ref._item_cache.pop(cacher[0], None)\n\n        super()._maybe_update_cacher(\n            clear=clear, verify_is_copy=verify_is_copy, inplace=inplace\n        )\n\n    # ----------------------------------------------------------------------\n    # Unsorted\n\n    @property\n    def _is_mixed_type(self):\n        return False\n\n    def repeat(self, repeats, axis=None) -> Series:\n        \"\"\"\n        Repeat elements of a Series.\n\n        Returns a new Series where each element of the current Series\n        is repeated consecutively a given number of times.\n\n        Parameters\n        ----------\n        repeats : int or array of ints\n            The number of repetitions for each element. This should be a\n            non-negative integer. Repeating 0 times will return an empty\n            Series.\n        axis : None\n            Must be ``None``. Has no effect but is accepted for compatibility\n            with numpy.\n\n        Returns\n        -------\n        Series\n            Newly created Series with repeated elements.\n\n        See Also\n        --------\n        Index.repeat : Equivalent function for Index.\n        numpy.repeat : Similar method for :class:`numpy.ndarray`.\n\n        Examples\n        --------\n        >>> s = pd.Series(['a', 'b', 'c'])\n        >>> s\n        0    a\n        1    b\n        2    c\n        dtype: object\n        >>> s.repeat(2)\n        0    a\n        0    a\n        1    b\n        1    b\n        2    c\n        2    c\n        dtype: object\n        >>> s.repeat([1, 2, 3])\n        0    a\n        1    b\n        1    b\n        2    c\n        2    c\n        2    c\n        dtype: object\n        \"\"\"\n        nv.validate_repeat((), {\"axis\": axis})\n        new_index = self.index.repeat(repeats)\n        new_values = self._values.repeat(repeats)\n        return self._constructor(new_values, index=new_index).__finalize__(\n            self, method=\"repeat\"\n        )\n\n    @deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\", \"level\"])\n    def reset_index(self, level=None, drop=False, name=lib.no_default, inplace=False):\n        \"\"\"\n        Generate a new DataFrame or Series with the index reset.\n\n        This is useful when the index needs to be treated as a column, or\n        when the index is meaningless and needs to be reset to the default\n        before another operation.\n\n        Parameters\n        ----------\n        level : int, str, tuple, or list, default optional\n            For a Series with a MultiIndex, only remove the specified levels\n            from the index. Removes all levels by default.\n        drop : bool, default False\n            Just reset the index, without inserting it as a column in\n            the new DataFrame.\n        name : object, optional\n            The name to use for the column containing the original Series\n            values. Uses ``self.name`` by default. This argument is ignored\n            when `drop` is True.\n        inplace : bool, default False\n            Modify the Series in place (do not create a new object).\n\n        Returns\n        -------\n        Series or DataFrame or None\n            When `drop` is False (the default), a DataFrame is returned.\n            The newly created columns will come first in the DataFrame,\n            followed by the original Series values.\n            When `drop` is True, a `Series` is returned.\n            In either case, if ``inplace=True``, no value is returned.\n\n        See Also\n        --------\n        DataFrame.reset_index: Analogous function for DataFrame.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3, 4], name='foo',\n        ...               index=pd.Index(['a', 'b', 'c', 'd'], name='idx'))\n\n        Generate a DataFrame with default index.\n\n        >>> s.reset_index()\n          idx  foo\n        0   a    1\n        1   b    2\n        2   c    3\n        3   d    4\n\n        To specify the name of the new column use `name`.\n\n        >>> s.reset_index(name='values')\n          idx  values\n        0   a       1\n        1   b       2\n        2   c       3\n        3   d       4\n\n        To generate a new Series with the default set `drop` to True.\n\n        >>> s.reset_index(drop=True)\n        0    1\n        1    2\n        2    3\n        3    4\n        Name: foo, dtype: int64\n\n        To update the Series in place, without generating a new one\n        set `inplace` to True. Note that it also requires ``drop=True``.\n\n        >>> s.reset_index(inplace=True, drop=True)\n        >>> s\n        0    1\n        1    2\n        2    3\n        3    4\n        Name: foo, dtype: int64\n\n        The `level` parameter is interesting for Series with a multi-level\n        index.\n\n        >>> arrays = [np.array(['bar', 'bar', 'baz', 'baz']),\n        ...           np.array(['one', 'two', 'one', 'two'])]\n        >>> s2 = pd.Series(\n        ...     range(4), name='foo',\n        ...     index=pd.MultiIndex.from_arrays(arrays,\n        ...                                     names=['a', 'b']))\n\n        To remove a specific level from the Index, use `level`.\n\n        >>> s2.reset_index(level='a')\n               a  foo\n        b\n        one  bar    0\n        two  bar    1\n        one  baz    2\n        two  baz    3\n\n        If `level` is not set, all levels are removed from the Index.\n\n        >>> s2.reset_index()\n             a    b  foo\n        0  bar  one    0\n        1  bar  two    1\n        2  baz  one    2\n        3  baz  two    3\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        if drop:\n            new_index = default_index(len(self))\n            if level is not None:\n                if not isinstance(level, (tuple, list)):\n                    level = [level]\n                level = [self.index._get_level_number(lev) for lev in level]\n                if len(level) < self.index.nlevels:\n                    new_index = self.index.droplevel(level)\n\n            if inplace:\n                self.index = new_index\n            else:\n                return self._constructor(\n                    self._values.copy(), index=new_index\n                ).__finalize__(self, method=\"reset_index\")\n        elif inplace:\n            raise TypeError(\n                \"Cannot reset_index inplace on a Series to create a DataFrame\"\n            )\n        else:\n            if name is lib.no_default:\n                # For backwards compatibility, keep columns as [0] instead of\n                #  [None] when self.name is None\n                if self.name is None:\n                    name = 0\n                else:\n                    name = self.name\n\n            df = self.to_frame(name)\n            return df.reset_index(level=level, drop=drop)\n\n    # ----------------------------------------------------------------------\n    # Rendering Methods\n\n    def __repr__(self) -> str:\n        \"\"\"\n        Return a string representation for a particular Series.\n        \"\"\"\n        repr_params = fmt.get_series_repr_params()\n        return self.to_string(**repr_params)\n\n    def to_string(\n        self,\n        buf=None,\n        na_rep=\"NaN\",\n        float_format=None,\n        header=True,\n        index=True,\n        length=False,\n        dtype=False,\n        name=False,\n        max_rows=None,\n        min_rows=None,\n    ):\n        \"\"\"\n        Render a string representation of the Series.\n\n        Parameters\n        ----------\n        buf : StringIO-like, optional\n            Buffer to write to.\n        na_rep : str, optional\n            String representation of NaN to use, default 'NaN'.\n        float_format : one-parameter function, optional\n            Formatter function to apply to columns' elements if they are\n            floats, default None.\n        header : bool, default True\n            Add the Series header (index name).\n        index : bool, optional\n            Add index (row) labels, default True.\n        length : bool, default False\n            Add the Series length.\n        dtype : bool, default False\n            Add the Series dtype.\n        name : bool, default False\n            Add the Series name if not None.\n        max_rows : int, optional\n            Maximum number of rows to show before truncating. If None, show\n            all.\n        min_rows : int, optional\n            The number of rows to display in a truncated repr (when number\n            of rows is above `max_rows`).\n\n        Returns\n        -------\n        str or None\n            String representation of Series if ``buf=None``, otherwise None.\n        \"\"\"\n        formatter = fmt.SeriesFormatter(\n            self,\n            name=name,\n            length=length,\n            header=header,\n            index=index,\n            dtype=dtype,\n            na_rep=na_rep,\n            float_format=float_format,\n            min_rows=min_rows,\n            max_rows=max_rows,\n        )\n        result = formatter.to_string()\n\n        # catch contract violations\n        if not isinstance(result, str):\n            raise AssertionError(\n                \"result must be of type str, type \"\n                f\"of result is {repr(type(result).__name__)}\"\n            )\n\n        if buf is None:\n            return result\n        else:\n            try:\n                buf.write(result)\n            except AttributeError:\n                with open(buf, \"w\") as f:\n                    f.write(result)\n\n    @doc(\n        klass=_shared_doc_kwargs[\"klass\"],\n        storage_options=_shared_docs[\"storage_options\"],\n        examples=dedent(\n            \"\"\"Examples\n            --------\n            >>> s = pd.Series([\"elk\", \"pig\", \"dog\", \"quetzal\"], name=\"animal\")\n            >>> print(s.to_markdown())\n            |    | animal   |\n            |---:|:---------|\n            |  0 | elk      |\n            |  1 | pig      |\n            |  2 | dog      |\n            |  3 | quetzal  |\n\n            Output markdown with a tabulate option.\n\n            >>> print(s.to_markdown(tablefmt=\"grid\"))\n            +----+----------+\n            |    | animal   |\n            +====+==========+\n            |  0 | elk      |\n            +----+----------+\n            |  1 | pig      |\n            +----+----------+\n            |  2 | dog      |\n            +----+----------+\n            |  3 | quetzal  |\n            +----+----------+\"\"\"\n        ),\n    )\n    def to_markdown(\n        self,\n        buf: IO[str] | None = None,\n        mode: str = \"wt\",\n        index: bool = True,\n        storage_options: StorageOptions = None,\n        **kwargs,\n    ) -> str | None:\n        \"\"\"\n        Print {klass} in Markdown-friendly format.\n\n        .. versionadded:: 1.0.0\n\n        Parameters\n        ----------\n        buf : str, Path or StringIO-like, optional, default None\n            Buffer to write to. If None, the output is returned as a string.\n        mode : str, optional\n            Mode in which file is opened, \"wt\" by default.\n        index : bool, optional, default True\n            Add index (row) labels.\n\n            .. versionadded:: 1.1.0\n        {storage_options}\n\n            .. versionadded:: 1.2.0\n\n        **kwargs\n            These parameters will be passed to `tabulate \\\n                <https://pypi.org/project/tabulate>`_.\n\n        Returns\n        -------\n        str\n            {klass} in Markdown-friendly format.\n\n        Notes\n        -----\n        Requires the `tabulate <https://pypi.org/project/tabulate>`_ package.\n\n        {examples}\n        \"\"\"\n        return self.to_frame().to_markdown(\n            buf, mode, index, storage_options=storage_options, **kwargs\n        )\n\n    # ----------------------------------------------------------------------\n\n    def items(self) -> Iterable[tuple[Hashable, Any]]:\n        \"\"\"\n        Lazily iterate over (index, value) tuples.\n\n        This method returns an iterable tuple (index, value). This is\n        convenient if you want to create a lazy iterator.\n\n        Returns\n        -------\n        iterable\n            Iterable of tuples containing the (index, value) pairs from a\n            Series.\n\n        See Also\n        --------\n        DataFrame.items : Iterate over (column name, Series) pairs.\n        DataFrame.iterrows : Iterate over DataFrame rows as (index, Series) pairs.\n\n        Examples\n        --------\n        >>> s = pd.Series(['A', 'B', 'C'])\n        >>> for index, value in s.items():\n        ...     print(f\"Index : {index}, Value : {value}\")\n        Index : 0, Value : A\n        Index : 1, Value : B\n        Index : 2, Value : C\n        \"\"\"\n        return zip(iter(self.index), iter(self))\n\n    @Appender(items.__doc__)\n    def iteritems(self) -> Iterable[tuple[Hashable, Any]]:\n        return self.items()\n\n    # ----------------------------------------------------------------------\n    # Misc public methods\n\n    def keys(self) -> Index:\n        \"\"\"\n        Return alias for index.\n\n        Returns\n        -------\n        Index\n            Index of the Series.\n        \"\"\"\n        return self.index\n\n    def to_dict(self, into=dict):\n        \"\"\"\n        Convert Series to {label -> value} dict or dict-like object.\n\n        Parameters\n        ----------\n        into : class, default dict\n            The collections.abc.Mapping subclass to use as the return\n            object. Can be the actual class or an empty\n            instance of the mapping type you want.  If you want a\n            collections.defaultdict, you must pass it initialized.\n\n        Returns\n        -------\n        collections.abc.Mapping\n            Key-value representation of Series.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3, 4])\n        >>> s.to_dict()\n        {0: 1, 1: 2, 2: 3, 3: 4}\n        >>> from collections import OrderedDict, defaultdict\n        >>> s.to_dict(OrderedDict)\n        OrderedDict([(0, 1), (1, 2), (2, 3), (3, 4)])\n        >>> dd = defaultdict(list)\n        >>> s.to_dict(dd)\n        defaultdict(<class 'list'>, {0: 1, 1: 2, 2: 3, 3: 4})\n        \"\"\"\n        # GH16122\n        into_c = com.standardize_mapping(into)\n        return into_c((k, maybe_box_native(v)) for k, v in self.items())\n\n    def to_frame(self, name: Hashable = lib.no_default) -> DataFrame:\n        \"\"\"\n        Convert Series to DataFrame.\n\n        Parameters\n        ----------\n        name : object, optional\n            The passed name should substitute for the series name (if it has\n            one).\n\n        Returns\n        -------\n        DataFrame\n            DataFrame representation of Series.\n\n        Examples\n        --------\n        >>> s = pd.Series([\"a\", \"b\", \"c\"],\n        ...               name=\"vals\")\n        >>> s.to_frame()\n          vals\n        0    a\n        1    b\n        2    c\n        \"\"\"\n        if name is None:\n            warnings.warn(\n                \"Explicitly passing `name=None` currently preserves the Series' name \"\n                \"or uses a default name of 0. This behaviour is deprecated, and in \"\n                \"the future `None` will be used as the name of the resulting \"\n                \"DataFrame column.\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n            name = lib.no_default\n\n        columns: Index\n        if name is lib.no_default:\n            name = self.name\n            if name is None:\n                # default to [0], same as we would get with DataFrame(self)\n                columns = default_index(1)\n            else:\n                columns = Index([name])\n        else:\n            columns = Index([name])\n\n        mgr = self._mgr.to_2d_mgr(columns)\n        return self._constructor_expanddim(mgr)\n\n    def _set_name(self, name, inplace=False) -> Series:\n        \"\"\"\n        Set the Series name.\n\n        Parameters\n        ----------\n        name : str\n        inplace : bool\n            Whether to modify `self` directly or return a copy.\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        ser = self if inplace else self.copy()\n        ser.name = name\n        return ser\n\n    @Appender(\n        \"\"\"\nExamples\n--------\n>>> ser = pd.Series([390., 350., 30., 20.],\n...                 index=['Falcon', 'Falcon', 'Parrot', 'Parrot'], name=\"Max Speed\")\n>>> ser\nFalcon    390.0\nFalcon    350.0\nParrot     30.0\nParrot     20.0\nName: Max Speed, dtype: float64\n>>> ser.groupby([\"a\", \"b\", \"a\", \"b\"]).mean()\na    210.0\nb    185.0\nName: Max Speed, dtype: float64\n>>> ser.groupby(level=0).mean()\nFalcon    370.0\nParrot     25.0\nName: Max Speed, dtype: float64\n>>> ser.groupby(ser > 100).mean()\nMax Speed\nFalse     25.0\nTrue     370.0\nName: Max Speed, dtype: float64\n\n**Grouping by Indexes**\n\nWe can groupby different levels of a hierarchical index\nusing the `level` parameter:\n\n>>> arrays = [['Falcon', 'Falcon', 'Parrot', 'Parrot'],\n...           ['Captive', 'Wild', 'Captive', 'Wild']]\n>>> index = pd.MultiIndex.from_arrays(arrays, names=('Animal', 'Type'))\n>>> ser = pd.Series([390., 350., 30., 20.], index=index, name=\"Max Speed\")\n>>> ser\nAnimal  Type\nFalcon  Captive    390.0\n        Wild       350.0\nParrot  Captive     30.0\n        Wild        20.0\nName: Max Speed, dtype: float64\n>>> ser.groupby(level=0).mean()\nAnimal\nFalcon    370.0\nParrot     25.0\nName: Max Speed, dtype: float64\n>>> ser.groupby(level=\"Type\").mean()\nType\nCaptive    210.0\nWild       185.0\nName: Max Speed, dtype: float64\n\nWe can also choose to include `NA` in group keys or not by defining\n`dropna` parameter, the default setting is `True`.\n\n>>> ser = pd.Series([1, 2, 3, 3], index=[\"a\", 'a', 'b', np.nan])\n>>> ser.groupby(level=0).sum()\na    3\nb    3\ndtype: int64\n\n>>> ser.groupby(level=0, dropna=False).sum()\na    3\nb    3\nNaN  3\ndtype: int64\n\n>>> arrays = ['Falcon', 'Falcon', 'Parrot', 'Parrot']\n>>> ser = pd.Series([390., 350., 30., 20.], index=arrays, name=\"Max Speed\")\n>>> ser.groupby([\"a\", \"b\", \"a\", np.nan]).mean()\na    210.0\nb    350.0\nName: Max Speed, dtype: float64\n\n>>> ser.groupby([\"a\", \"b\", \"a\", np.nan], dropna=False).mean()\na    210.0\nb    350.0\nNaN   20.0\nName: Max Speed, dtype: float64\n\"\"\"\n    )\n    @Appender(_shared_docs[\"groupby\"] % _shared_doc_kwargs)\n    def groupby(\n        self,\n        by=None,\n        axis=0,\n        level=None,\n        as_index: bool = True,\n        sort: bool = True,\n        group_keys: bool = True,\n        squeeze: bool | lib.NoDefault = no_default,\n        observed: bool = False,\n        dropna: bool = True,\n    ) -> SeriesGroupBy:\n        from pandas.core.groupby.generic import SeriesGroupBy\n\n        if squeeze is not no_default:\n            warnings.warn(\n                (\n                    \"The `squeeze` parameter is deprecated and \"\n                    \"will be removed in a future version.\"\n                ),\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n        else:\n            squeeze = False\n\n        if level is None and by is None:\n            raise TypeError(\"You have to supply one of 'by' and 'level'\")\n        axis = self._get_axis_number(axis)\n\n        # error: Argument \"squeeze\" to \"SeriesGroupBy\" has incompatible type\n        # \"Union[bool, NoDefault]\"; expected \"bool\"\n        return SeriesGroupBy(\n            obj=self,\n            keys=by,\n            axis=axis,\n            level=level,\n            as_index=as_index,\n            sort=sort,\n            group_keys=group_keys,\n            squeeze=squeeze,  # type: ignore[arg-type]\n            observed=observed,\n            dropna=dropna,\n        )\n\n    # ----------------------------------------------------------------------\n    # Statistics, overridden ndarray methods\n\n    # TODO: integrate bottleneck\n\n    def count(self, level=None):\n        \"\"\"\n        Return number of non-NA/null observations in the Series.\n\n        Parameters\n        ----------\n        level : int or level name, default None\n            If the axis is a MultiIndex (hierarchical), count along a\n            particular level, collapsing into a smaller Series.\n\n        Returns\n        -------\n        int or Series (if level specified)\n            Number of non-null values in the Series.\n\n        See Also\n        --------\n        DataFrame.count : Count non-NA cells for each column or row.\n\n        Examples\n        --------\n        >>> s = pd.Series([0.0, 1.0, np.nan])\n        >>> s.count()\n        2\n        \"\"\"\n        if level is None:\n            return notna(self._values).sum().astype(\"int64\")\n        else:\n            warnings.warn(\n                \"Using the level keyword in DataFrame and Series aggregations is \"\n                \"deprecated and will be removed in a future version. Use groupby \"\n                \"instead. ser.count(level=1) should use ser.groupby(level=1).count().\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n            if not isinstance(self.index, MultiIndex):\n                raise ValueError(\"Series.count level is only valid with a MultiIndex\")\n\n        index = self.index\n        assert isinstance(index, MultiIndex)  # for mypy\n\n        if isinstance(level, str):\n            level = index._get_level_number(level)\n\n        lev = index.levels[level]\n        level_codes = np.array(index.codes[level], subok=False, copy=True)\n\n        mask = level_codes == -1\n        if mask.any():\n            level_codes[mask] = cnt = len(lev)\n            lev = lev.insert(cnt, lev._na_value)\n\n        obs = level_codes[notna(self._values)]\n        # Argument \"minlength\" to \"bincount\" has incompatible type \"Optional[int]\";\n        # expected \"SupportsIndex\"  [arg-type]\n        out = np.bincount(obs, minlength=len(lev) or None)  # type: ignore[arg-type]\n        return self._constructor(out, index=lev, dtype=\"int64\").__finalize__(\n            self, method=\"count\"\n        )\n\n    def mode(self, dropna: bool = True) -> Series:\n        \"\"\"\n        Return the mode(s) of the Series.\n\n        The mode is the value that appears most often. There can be multiple modes.\n\n        Always returns Series even if only one value is returned.\n\n        Parameters\n        ----------\n        dropna : bool, default True\n            Don't consider counts of NaN/NaT.\n\n        Returns\n        -------\n        Series\n            Modes of the Series in sorted order.\n        \"\"\"\n        # TODO: Add option for bins like value_counts()\n        values = self._values\n        if isinstance(values, np.ndarray):\n            res_values = algorithms.mode(values, dropna=dropna)\n        else:\n            res_values = values._mode(dropna=dropna)\n\n        # Ensure index is type stable (should always use int index)\n        return self._constructor(\n            res_values, index=range(len(res_values)), name=self.name\n        )\n\n    def unique(self) -> ArrayLike:\n        \"\"\"\n        Return unique values of Series object.\n\n        Uniques are returned in order of appearance. Hash table-based unique,\n        therefore does NOT sort.\n\n        Returns\n        -------\n        ndarray or ExtensionArray\n            The unique values returned as a NumPy array. See Notes.\n\n        See Also\n        --------\n        unique : Top-level unique method for any 1-d array-like object.\n        Index.unique : Return Index with unique values from an Index object.\n\n        Notes\n        -----\n        Returns the unique values as a NumPy array. In case of an\n        extension-array backed Series, a new\n        :class:`~api.extensions.ExtensionArray` of that type with just\n        the unique values is returned. This includes\n\n            * Categorical\n            * Period\n            * Datetime with Timezone\n            * Interval\n            * Sparse\n            * IntegerNA\n\n        See Examples section.\n\n        Examples\n        --------\n        >>> pd.Series([2, 1, 3, 3], name='A').unique()\n        array([2, 1, 3])\n\n        >>> pd.Series([pd.Timestamp('2016-01-01') for _ in range(3)]).unique()\n        array(['2016-01-01T00:00:00.000000000'], dtype='datetime64[ns]')\n\n        >>> pd.Series([pd.Timestamp('2016-01-01', tz='US/Eastern')\n        ...            for _ in range(3)]).unique()\n        <DatetimeArray>\n        ['2016-01-01 00:00:00-05:00']\n        Length: 1, dtype: datetime64[ns, US/Eastern]\n\n        An Categorical will return categories in the order of\n        appearance and with the same dtype.\n\n        >>> pd.Series(pd.Categorical(list('baabc'))).unique()\n        ['b', 'a', 'c']\n        Categories (3, object): ['a', 'b', 'c']\n        >>> pd.Series(pd.Categorical(list('baabc'), categories=list('abc'),\n        ...                          ordered=True)).unique()\n        ['b', 'a', 'c']\n        Categories (3, object): ['a' < 'b' < 'c']\n        \"\"\"\n        return super().unique()\n\n    @overload\n    def drop_duplicates(self, keep=..., inplace: Literal[False] = ...) -> Series:\n        ...\n\n    @overload\n    def drop_duplicates(self, keep, inplace: Literal[True]) -> None:\n        ...\n\n    @overload\n    def drop_duplicates(self, *, inplace: Literal[True]) -> None:\n        ...\n\n    @overload\n    def drop_duplicates(self, keep=..., inplace: bool = ...) -> Series | None:\n        ...\n\n    @deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\"])\n    def drop_duplicates(self, keep=\"first\", inplace=False) -> Series | None:\n        \"\"\"\n        Return Series with duplicate values removed.\n\n        Parameters\n        ----------\n        keep : {'first', 'last', ``False``}, default 'first'\n            Method to handle dropping duplicates:\n\n            - 'first' : Drop duplicates except for the first occurrence.\n            - 'last' : Drop duplicates except for the last occurrence.\n            - ``False`` : Drop all duplicates.\n\n        inplace : bool, default ``False``\n            If ``True``, performs operation inplace and returns None.\n\n        Returns\n        -------\n        Series or None\n            Series with duplicates dropped or None if ``inplace=True``.\n\n        See Also\n        --------\n        Index.drop_duplicates : Equivalent method on Index.\n        DataFrame.drop_duplicates : Equivalent method on DataFrame.\n        Series.duplicated : Related method on Series, indicating duplicate\n            Series values.\n\n        Examples\n        --------\n        Generate a Series with duplicated entries.\n\n        >>> s = pd.Series(['lama', 'cow', 'lama', 'beetle', 'lama', 'hippo'],\n        ...               name='animal')\n        >>> s\n        0      lama\n        1       cow\n        2      lama\n        3    beetle\n        4      lama\n        5     hippo\n        Name: animal, dtype: object\n\n        With the 'keep' parameter, the selection behaviour of duplicated values\n        can be changed. The value 'first' keeps the first occurrence for each\n        set of duplicated entries. The default value of keep is 'first'.\n\n        >>> s.drop_duplicates()\n        0      lama\n        1       cow\n        3    beetle\n        5     hippo\n        Name: animal, dtype: object\n\n        The value 'last' for parameter 'keep' keeps the last occurrence for\n        each set of duplicated entries.\n\n        >>> s.drop_duplicates(keep='last')\n        1       cow\n        3    beetle\n        4      lama\n        5     hippo\n        Name: animal, dtype: object\n\n        The value ``False`` for parameter 'keep' discards all sets of\n        duplicated entries. Setting the value of 'inplace' to ``True`` performs\n        the operation inplace and returns ``None``.\n\n        >>> s.drop_duplicates(keep=False, inplace=True)\n        >>> s\n        1       cow\n        3    beetle\n        5     hippo\n        Name: animal, dtype: object\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        result = super().drop_duplicates(keep=keep)\n        if inplace:\n            self._update_inplace(result)\n            return None\n        else:\n            return result\n\n    def duplicated(self, keep=\"first\") -> Series:\n        \"\"\"\n        Indicate duplicate Series values.\n\n        Duplicated values are indicated as ``True`` values in the resulting\n        Series. Either all duplicates, all except the first or all except the\n        last occurrence of duplicates can be indicated.\n\n        Parameters\n        ----------\n        keep : {'first', 'last', False}, default 'first'\n            Method to handle dropping duplicates:\n\n            - 'first' : Mark duplicates as ``True`` except for the first\n              occurrence.\n            - 'last' : Mark duplicates as ``True`` except for the last\n              occurrence.\n            - ``False`` : Mark all duplicates as ``True``.\n\n        Returns\n        -------\n        Series[bool]\n            Series indicating whether each value has occurred in the\n            preceding values.\n\n        See Also\n        --------\n        Index.duplicated : Equivalent method on pandas.Index.\n        DataFrame.duplicated : Equivalent method on pandas.DataFrame.\n        Series.drop_duplicates : Remove duplicate values from Series.\n\n        Examples\n        --------\n        By default, for each set of duplicated values, the first occurrence is\n        set on False and all others on True:\n\n        >>> animals = pd.Series(['lama', 'cow', 'lama', 'beetle', 'lama'])\n        >>> animals.duplicated()\n        0    False\n        1    False\n        2     True\n        3    False\n        4     True\n        dtype: bool\n\n        which is equivalent to\n\n        >>> animals.duplicated(keep='first')\n        0    False\n        1    False\n        2     True\n        3    False\n        4     True\n        dtype: bool\n\n        By using 'last', the last occurrence of each set of duplicated values\n        is set on False and all others on True:\n\n        >>> animals.duplicated(keep='last')\n        0     True\n        1    False\n        2     True\n        3    False\n        4    False\n        dtype: bool\n\n        By setting keep on ``False``, all duplicates are True:\n\n        >>> animals.duplicated(keep=False)\n        0     True\n        1    False\n        2     True\n        3    False\n        4     True\n        dtype: bool\n        \"\"\"\n        res = self._duplicated(keep=keep)\n        result = self._constructor(res, index=self.index)\n        return result.__finalize__(self, method=\"duplicated\")\n\n    def idxmin(self, axis=0, skipna=True, *args, **kwargs):\n        \"\"\"\n        Return the row label of the minimum value.\n\n        If multiple values equal the minimum, the first row label with that\n        value is returned.\n\n        Parameters\n        ----------\n        axis : int, default 0\n            For compatibility with DataFrame.idxmin. Redundant for application\n            on Series.\n        skipna : bool, default True\n            Exclude NA/null values. If the entire Series is NA, the result\n            will be NA.\n        *args, **kwargs\n            Additional arguments and keywords have no effect but might be\n            accepted for compatibility with NumPy.\n\n        Returns\n        -------\n        Index\n            Label of the minimum value.\n\n        Raises\n        ------\n        ValueError\n            If the Series is empty.\n\n        See Also\n        --------\n        numpy.argmin : Return indices of the minimum values\n            along the given axis.\n        DataFrame.idxmin : Return index of first occurrence of minimum\n            over requested axis.\n        Series.idxmax : Return index *label* of the first occurrence\n            of maximum of values.\n\n        Notes\n        -----\n        This method is the Series version of ``ndarray.argmin``. This method\n        returns the label of the minimum, while ``ndarray.argmin`` returns\n        the position. To get the position, use ``series.values.argmin()``.\n\n        Examples\n        --------\n        >>> s = pd.Series(data=[1, None, 4, 1],\n        ...               index=['A', 'B', 'C', 'D'])\n        >>> s\n        A    1.0\n        B    NaN\n        C    4.0\n        D    1.0\n        dtype: float64\n\n        >>> s.idxmin()\n        'A'\n\n        If `skipna` is False and there is an NA value in the data,\n        the function returns ``nan``.\n\n        >>> s.idxmin(skipna=False)\n        nan\n        \"\"\"\n        i = self.argmin(axis, skipna, *args, **kwargs)\n        if i == -1:\n            return np.nan\n        return self.index[i]\n\n    def idxmax(self, axis=0, skipna=True, *args, **kwargs):\n        \"\"\"\n        Return the row label of the maximum value.\n\n        If multiple values equal the maximum, the first row label with that\n        value is returned.\n\n        Parameters\n        ----------\n        axis : int, default 0\n            For compatibility with DataFrame.idxmax. Redundant for application\n            on Series.\n        skipna : bool, default True\n            Exclude NA/null values. If the entire Series is NA, the result\n            will be NA.\n        *args, **kwargs\n            Additional arguments and keywords have no effect but might be\n            accepted for compatibility with NumPy.\n\n        Returns\n        -------\n        Index\n            Label of the maximum value.\n\n        Raises\n        ------\n        ValueError\n            If the Series is empty.\n\n        See Also\n        --------\n        numpy.argmax : Return indices of the maximum values\n            along the given axis.\n        DataFrame.idxmax : Return index of first occurrence of maximum\n            over requested axis.\n        Series.idxmin : Return index *label* of the first occurrence\n            of minimum of values.\n\n        Notes\n        -----\n        This method is the Series version of ``ndarray.argmax``. This method\n        returns the label of the maximum, while ``ndarray.argmax`` returns\n        the position. To get the position, use ``series.values.argmax()``.\n\n        Examples\n        --------\n        >>> s = pd.Series(data=[1, None, 4, 3, 4],\n        ...               index=['A', 'B', 'C', 'D', 'E'])\n        >>> s\n        A    1.0\n        B    NaN\n        C    4.0\n        D    3.0\n        E    4.0\n        dtype: float64\n\n        >>> s.idxmax()\n        'C'\n\n        If `skipna` is False and there is an NA value in the data,\n        the function returns ``nan``.\n\n        >>> s.idxmax(skipna=False)\n        nan\n        \"\"\"\n        i = self.argmax(axis, skipna, *args, **kwargs)\n        if i == -1:\n            return np.nan\n        return self.index[i]\n\n    def round(self, decimals=0, *args, **kwargs) -> Series:\n        \"\"\"\n        Round each value in a Series to the given number of decimals.\n\n        Parameters\n        ----------\n        decimals : int, default 0\n            Number of decimal places to round to. If decimals is negative,\n            it specifies the number of positions to the left of the decimal point.\n        *args, **kwargs\n            Additional arguments and keywords have no effect but might be\n            accepted for compatibility with NumPy.\n\n        Returns\n        -------\n        Series\n            Rounded values of the Series.\n\n        See Also\n        --------\n        numpy.around : Round values of an np.array.\n        DataFrame.round : Round values of a DataFrame.\n\n        Examples\n        --------\n        >>> s = pd.Series([0.1, 1.3, 2.7])\n        >>> s.round()\n        0    0.0\n        1    1.0\n        2    3.0\n        dtype: float64\n        \"\"\"\n        nv.validate_round(args, kwargs)\n        result = self._values.round(decimals)\n        result = self._constructor(result, index=self.index).__finalize__(\n            self, method=\"round\"\n        )\n\n        return result\n\n    def quantile(self, q=0.5, interpolation=\"linear\"):\n        \"\"\"\n        Return value at the given quantile.\n\n        Parameters\n        ----------\n        q : float or array-like, default 0.5 (50% quantile)\n            The quantile(s) to compute, which can lie in range: 0 <= q <= 1.\n        interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n            This optional parameter specifies the interpolation method to use,\n            when the desired quantile lies between two data points `i` and `j`:\n\n                * linear: `i + (j - i) * fraction`, where `fraction` is the\n                  fractional part of the index surrounded by `i` and `j`.\n                * lower: `i`.\n                * higher: `j`.\n                * nearest: `i` or `j` whichever is nearest.\n                * midpoint: (`i` + `j`) / 2.\n\n        Returns\n        -------\n        float or Series\n            If ``q`` is an array, a Series will be returned where the\n            index is ``q`` and the values are the quantiles, otherwise\n            a float will be returned.\n\n        See Also\n        --------\n        core.window.Rolling.quantile : Calculate the rolling quantile.\n        numpy.percentile : Returns the q-th percentile(s) of the array elements.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3, 4])\n        >>> s.quantile(.5)\n        2.5\n        >>> s.quantile([.25, .5, .75])\n        0.25    1.75\n        0.50    2.50\n        0.75    3.25\n        dtype: float64\n        \"\"\"\n        validate_percentile(q)\n\n        # We dispatch to DataFrame so that core.internals only has to worry\n        #  about 2D cases.\n        df = self.to_frame()\n\n        result = df.quantile(q=q, interpolation=interpolation, numeric_only=False)\n        if result.ndim == 2:\n            result = result.iloc[:, 0]\n\n        if is_list_like(q):\n            result.name = self.name\n            return self._constructor(result, index=Float64Index(q), name=self.name)\n        else:\n            # scalar\n            return result.iloc[0]\n\n    def corr(self, other, method=\"pearson\", min_periods=None) -> float:\n        \"\"\"\n        Compute correlation with `other` Series, excluding missing values.\n\n        Parameters\n        ----------\n        other : Series\n            Series with which to compute the correlation.\n        method : {'pearson', 'kendall', 'spearman'} or callable\n            Method used to compute correlation:\n\n            - pearson : Standard correlation coefficient\n            - kendall : Kendall Tau correlation coefficient\n            - spearman : Spearman rank correlation\n            - callable: Callable with input two 1d ndarrays and returning a float.\n\n            .. warning::\n                Note that the returned matrix from corr will have 1 along the\n                diagonals and will be symmetric regardless of the callable's\n                behavior.\n        min_periods : int, optional\n            Minimum number of observations needed to have a valid result.\n\n        Returns\n        -------\n        float\n            Correlation with other.\n\n        See Also\n        --------\n        DataFrame.corr : Compute pairwise correlation between columns.\n        DataFrame.corrwith : Compute pairwise correlation with another\n            DataFrame or Series.\n\n        Examples\n        --------\n        >>> def histogram_intersection(a, b):\n        ...     v = np.minimum(a, b).sum().round(decimals=1)\n        ...     return v\n        >>> s1 = pd.Series([.2, .0, .6, .2])\n        >>> s2 = pd.Series([.3, .6, .0, .1])\n        >>> s1.corr(s2, method=histogram_intersection)\n        0.3\n        \"\"\"\n        this, other = self.align(other, join=\"inner\", copy=False)\n        if len(this) == 0:\n            return np.nan\n\n        if method in [\"pearson\", \"spearman\", \"kendall\"] or callable(method):\n            return nanops.nancorr(\n                this.values, other.values, method=method, min_periods=min_periods\n            )\n\n        raise ValueError(\n            \"method must be either 'pearson', \"\n            \"'spearman', 'kendall', or a callable, \"\n            f\"'{method}' was supplied\"\n        )\n\n    def cov(\n        self,\n        other: Series,\n        min_periods: int | None = None,\n        ddof: int | None = 1,\n    ) -> float:\n        \"\"\"\n        Compute covariance with Series, excluding missing values.\n\n        Parameters\n        ----------\n        other : Series\n            Series with which to compute the covariance.\n        min_periods : int, optional\n            Minimum number of observations needed to have a valid result.\n        ddof : int, default 1\n            Delta degrees of freedom.  The divisor used in calculations\n            is ``N - ddof``, where ``N`` represents the number of elements.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        float\n            Covariance between Series and other normalized by N-1\n            (unbiased estimator).\n\n        See Also\n        --------\n        DataFrame.cov : Compute pairwise covariance of columns.\n\n        Examples\n        --------\n        >>> s1 = pd.Series([0.90010907, 0.13484424, 0.62036035])\n        >>> s2 = pd.Series([0.12528585, 0.26962463, 0.51111198])\n        >>> s1.cov(s2)\n        -0.01685762652715874\n        \"\"\"\n        this, other = self.align(other, join=\"inner\", copy=False)\n        if len(this) == 0:\n            return np.nan\n        return nanops.nancov(\n            this.values, other.values, min_periods=min_periods, ddof=ddof\n        )\n\n    @doc(\n        klass=\"Series\",\n        extra_params=\"\",\n        other_klass=\"DataFrame\",\n        examples=dedent(\n            \"\"\"\n        Difference with previous row\n\n        >>> s = pd.Series([1, 1, 2, 3, 5, 8])\n        >>> s.diff()\n        0    NaN\n        1    0.0\n        2    1.0\n        3    1.0\n        4    2.0\n        5    3.0\n        dtype: float64\n\n        Difference with 3rd previous row\n\n        >>> s.diff(periods=3)\n        0    NaN\n        1    NaN\n        2    NaN\n        3    2.0\n        4    4.0\n        5    6.0\n        dtype: float64\n\n        Difference with following row\n\n        >>> s.diff(periods=-1)\n        0    0.0\n        1   -1.0\n        2   -1.0\n        3   -2.0\n        4   -3.0\n        5    NaN\n        dtype: float64\n\n        Overflow in input dtype\n\n        >>> s = pd.Series([1, 0], dtype=np.uint8)\n        >>> s.diff()\n        0      NaN\n        1    255.0\n        dtype: float64\"\"\"\n        ),\n    )\n    def diff(self, periods: int = 1) -> Series:\n        \"\"\"\n        First discrete difference of element.\n\n        Calculates the difference of a {klass} element compared with another\n        element in the {klass} (default is element in previous row).\n\n        Parameters\n        ----------\n        periods : int, default 1\n            Periods to shift for calculating difference, accepts negative\n            values.\n        {extra_params}\n        Returns\n        -------\n        {klass}\n            First differences of the Series.\n\n        See Also\n        --------\n        {klass}.pct_change: Percent change over given number of periods.\n        {klass}.shift: Shift index by desired number of periods with an\n            optional time freq.\n        {other_klass}.diff: First discrete difference of object.\n\n        Notes\n        -----\n        For boolean dtypes, this uses :meth:`operator.xor` rather than\n        :meth:`operator.sub`.\n        The result is calculated according to current dtype in {klass},\n        however dtype of the result is always float64.\n\n        Examples\n        --------\n        {examples}\n        \"\"\"\n        result = algorithms.diff(self._values, periods)\n        return self._constructor(result, index=self.index).__finalize__(\n            self, method=\"diff\"\n        )\n\n    def autocorr(self, lag=1) -> float:\n        \"\"\"\n        Compute the lag-N autocorrelation.\n\n        This method computes the Pearson correlation between\n        the Series and its shifted self.\n\n        Parameters\n        ----------\n        lag : int, default 1\n            Number of lags to apply before performing autocorrelation.\n\n        Returns\n        -------\n        float\n            The Pearson correlation between self and self.shift(lag).\n\n        See Also\n        --------\n        Series.corr : Compute the correlation between two Series.\n        Series.shift : Shift index by desired number of periods.\n        DataFrame.corr : Compute pairwise correlation of columns.\n        DataFrame.corrwith : Compute pairwise correlation between rows or\n            columns of two DataFrame objects.\n\n        Notes\n        -----\n        If the Pearson correlation is not well defined return 'NaN'.\n\n        Examples\n        --------\n        >>> s = pd.Series([0.25, 0.5, 0.2, -0.05])\n        >>> s.autocorr()  # doctest: +ELLIPSIS\n        0.10355...\n        >>> s.autocorr(lag=2)  # doctest: +ELLIPSIS\n        -0.99999...\n\n        If the Pearson correlation is not well defined, then 'NaN' is returned.\n\n        >>> s = pd.Series([1, 0, 0, 0])\n        >>> s.autocorr()\n        nan\n        \"\"\"\n        return self.corr(self.shift(lag))\n\n    def dot(self, other):\n        \"\"\"\n        Compute the dot product between the Series and the columns of other.\n\n        This method computes the dot product between the Series and another\n        one, or the Series and each columns of a DataFrame, or the Series and\n        each columns of an array.\n\n        It can also be called using `self @ other` in Python >= 3.5.\n\n        Parameters\n        ----------\n        other : Series, DataFrame or array-like\n            The other object to compute the dot product with its columns.\n\n        Returns\n        -------\n        scalar, Series or numpy.ndarray\n            Return the dot product of the Series and other if other is a\n            Series, the Series of the dot product of Series and each rows of\n            other if other is a DataFrame or a numpy.ndarray between the Series\n            and each columns of the numpy array.\n\n        See Also\n        --------\n        DataFrame.dot: Compute the matrix product with the DataFrame.\n        Series.mul: Multiplication of series and other, element-wise.\n\n        Notes\n        -----\n        The Series and other has to share the same index if other is a Series\n        or a DataFrame.\n\n        Examples\n        --------\n        >>> s = pd.Series([0, 1, 2, 3])\n        >>> other = pd.Series([-1, 2, -3, 4])\n        >>> s.dot(other)\n        8\n        >>> s @ other\n        8\n        >>> df = pd.DataFrame([[0, 1], [-2, 3], [4, -5], [6, 7]])\n        >>> s.dot(df)\n        0    24\n        1    14\n        dtype: int64\n        >>> arr = np.array([[0, 1], [-2, 3], [4, -5], [6, 7]])\n        >>> s.dot(arr)\n        array([24, 14])\n        \"\"\"\n        if isinstance(other, (Series, ABCDataFrame)):\n            common = self.index.union(other.index)\n            if len(common) > len(self.index) or len(common) > len(other.index):\n                raise ValueError(\"matrices are not aligned\")\n\n            left = self.reindex(index=common, copy=False)\n            right = other.reindex(index=common, copy=False)\n            lvals = left.values\n            rvals = right.values\n        else:\n            lvals = self.values\n            rvals = np.asarray(other)\n            if lvals.shape[0] != rvals.shape[0]:\n                raise Exception(\n                    f\"Dot product shape mismatch, {lvals.shape} vs {rvals.shape}\"\n                )\n\n        if isinstance(other, ABCDataFrame):\n            return self._constructor(\n                np.dot(lvals, rvals), index=other.columns\n            ).__finalize__(self, method=\"dot\")\n        elif isinstance(other, Series):\n            return np.dot(lvals, rvals)\n        elif isinstance(rvals, np.ndarray):\n            return np.dot(lvals, rvals)\n        else:  # pragma: no cover\n            raise TypeError(f\"unsupported type: {type(other)}\")\n\n    def __matmul__(self, other):\n        \"\"\"\n        Matrix multiplication using binary `@` operator in Python>=3.5.\n        \"\"\"\n        return self.dot(other)\n\n    def __rmatmul__(self, other):\n        \"\"\"\n        Matrix multiplication using binary `@` operator in Python>=3.5.\n        \"\"\"\n        return self.dot(np.transpose(other))\n\n    @doc(base.IndexOpsMixin.searchsorted, klass=\"Series\")\n    # Signature of \"searchsorted\" incompatible with supertype \"IndexOpsMixin\"\n    def searchsorted(  # type: ignore[override]\n        self,\n        value: NumpyValueArrayLike | ExtensionArray,\n        side: Literal[\"left\", \"right\"] = \"left\",\n        sorter: NumpySorter = None,\n    ) -> npt.NDArray[np.intp] | np.intp:\n        return base.IndexOpsMixin.searchsorted(self, value, side=side, sorter=sorter)\n\n    # -------------------------------------------------------------------\n    # Combination\n\n    def append(\n        self, to_append, ignore_index: bool = False, verify_integrity: bool = False\n    ):\n        \"\"\"\n        Concatenate two or more Series.\n\n        .. deprecated:: 1.4.0\n            Use :func:`concat` instead. For further details see\n            :ref:`whatsnew_140.deprecations.frame_series_append`\n\n        Parameters\n        ----------\n        to_append : Series or list/tuple of Series\n            Series to append with self.\n        ignore_index : bool, default False\n            If True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n        verify_integrity : bool, default False\n            If True, raise Exception on creating index with duplicates.\n\n        Returns\n        -------\n        Series\n            Concatenated Series.\n\n        See Also\n        --------\n        concat : General function to concatenate DataFrame or Series objects.\n\n        Notes\n        -----\n        Iteratively appending to a Series can be more computationally intensive\n        than a single concatenate. A better solution is to append values to a\n        list and then concatenate the list with the original Series all at\n        once.\n\n        Examples\n        --------\n        >>> s1 = pd.Series([1, 2, 3])\n        >>> s2 = pd.Series([4, 5, 6])\n        >>> s3 = pd.Series([4, 5, 6], index=[3, 4, 5])\n        >>> s1.append(s2)\n        0    1\n        1    2\n        2    3\n        0    4\n        1    5\n        2    6\n        dtype: int64\n\n        >>> s1.append(s3)\n        0    1\n        1    2\n        2    3\n        3    4\n        4    5\n        5    6\n        dtype: int64\n\n        With `ignore_index` set to True:\n\n        >>> s1.append(s2, ignore_index=True)\n        0    1\n        1    2\n        2    3\n        3    4\n        4    5\n        5    6\n        dtype: int64\n\n        With `verify_integrity` set to True:\n\n        >>> s1.append(s2, verify_integrity=True)\n        Traceback (most recent call last):\n        ...\n        ValueError: Indexes have overlapping values: [0, 1, 2]\n        \"\"\"\n        warnings.warn(\n            \"The series.append method is deprecated \"\n            \"and will be removed from pandas in a future version. \"\n            \"Use pandas.concat instead.\",\n            FutureWarning,\n            stacklevel=find_stack_level(),\n        )\n\n        return self._append(to_append, ignore_index, verify_integrity)\n\n    def _append(\n        self, to_append, ignore_index: bool = False, verify_integrity: bool = False\n    ):\n        from pandas.core.reshape.concat import concat\n\n        if isinstance(to_append, (list, tuple)):\n            to_concat = [self]\n            to_concat.extend(to_append)\n        else:\n            to_concat = [self, to_append]\n        if any(isinstance(x, (ABCDataFrame,)) for x in to_concat[1:]):\n            msg = \"to_append should be a Series or list/tuple of Series, got DataFrame\"\n            raise TypeError(msg)\n        return concat(\n            to_concat, ignore_index=ignore_index, verify_integrity=verify_integrity\n        )\n\n    def _binop(self, other: Series, func, level=None, fill_value=None):\n        \"\"\"\n        Perform generic binary operation with optional fill value.\n\n        Parameters\n        ----------\n        other : Series\n        func : binary operator\n        fill_value : float or object\n            Value to substitute for NA/null values. If both Series are NA in a\n            location, the result will be NA regardless of the passed fill value.\n        level : int or level name, default None\n            Broadcast across a level, matching Index values on the\n            passed MultiIndex level.\n\n        Returns\n        -------\n        Series\n        \"\"\"\n        if not isinstance(other, Series):\n            raise AssertionError(\"Other operand must be Series\")\n\n        this = self\n\n        if not self.index.equals(other.index):\n            this, other = self.align(other, level=level, join=\"outer\", copy=False)\n\n        this_vals, other_vals = ops.fill_binop(this._values, other._values, fill_value)\n\n        with np.errstate(all=\"ignore\"):\n            result = func(this_vals, other_vals)\n\n        name = ops.get_op_result_name(self, other)\n        return this._construct_result(result, name)\n\n    def _construct_result(\n        self, result: ArrayLike | tuple[ArrayLike, ArrayLike], name: Hashable\n    ) -> Series | tuple[Series, Series]:\n        \"\"\"\n        Construct an appropriately-labelled Series from the result of an op.\n\n        Parameters\n        ----------\n        result : ndarray or ExtensionArray\n        name : Label\n\n        Returns\n        -------\n        Series\n            In the case of __divmod__ or __rdivmod__, a 2-tuple of Series.\n        \"\"\"\n        if isinstance(result, tuple):\n            # produced by divmod or rdivmod\n\n            res1 = self._construct_result(result[0], name=name)\n            res2 = self._construct_result(result[1], name=name)\n\n            # GH#33427 assertions to keep mypy happy\n            assert isinstance(res1, Series)\n            assert isinstance(res2, Series)\n            return (res1, res2)\n\n        # We do not pass dtype to ensure that the Series constructor\n        #  does inference in the case where `result` has object-dtype.\n        out = self._constructor(result, index=self.index)\n        out = out.__finalize__(self)\n\n        # Set the result's name after __finalize__ is called because __finalize__\n        #  would set it back to self.name\n        out.name = name\n        return out\n\n    @doc(\n        _shared_docs[\"compare\"],\n        \"\"\"\nReturns\n-------\nSeries or DataFrame\n    If axis is 0 or 'index' the result will be a Series.\n    The resulting index will be a MultiIndex with 'self' and 'other'\n    stacked alternately at the inner level.\n\n    If axis is 1 or 'columns' the result will be a DataFrame.\n    It will have two columns namely 'self' and 'other'.\n\nSee Also\n--------\nDataFrame.compare : Compare with another DataFrame and show differences.\n\nNotes\n-----\nMatching NaNs will not appear as a difference.\n\nExamples\n--------\n>>> s1 = pd.Series([\"a\", \"b\", \"c\", \"d\", \"e\"])\n>>> s2 = pd.Series([\"a\", \"a\", \"c\", \"b\", \"e\"])\n\nAlign the differences on columns\n\n>>> s1.compare(s2)\n  self other\n1    b     a\n3    d     b\n\nStack the differences on indices\n\n>>> s1.compare(s2, align_axis=0)\n1  self     b\n   other    a\n3  self     d\n   other    b\ndtype: object\n\nKeep all original rows\n\n>>> s1.compare(s2, keep_shape=True)\n  self other\n0  NaN   NaN\n1    b     a\n2  NaN   NaN\n3    d     b\n4  NaN   NaN\n\nKeep all original rows and also all original values\n\n>>> s1.compare(s2, keep_shape=True, keep_equal=True)\n  self other\n0    a     a\n1    b     a\n2    c     c\n3    d     b\n4    e     e\n\"\"\",\n        klass=_shared_doc_kwargs[\"klass\"],\n    )\n    def compare(\n        self,\n        other: Series,\n        align_axis: Axis = 1,\n        keep_shape: bool = False,\n        keep_equal: bool = False,\n    ) -> DataFrame | Series:\n        return super().compare(\n            other=other,\n            align_axis=align_axis,\n            keep_shape=keep_shape,\n            keep_equal=keep_equal,\n        )\n\n    def combine(self, other, func, fill_value=None) -> Series:\n        \"\"\"\n        Combine the Series with a Series or scalar according to `func`.\n\n        Combine the Series and `other` using `func` to perform elementwise\n        selection for combined Series.\n        `fill_value` is assumed when value is missing at some index\n        from one of the two objects being combined.\n\n        Parameters\n        ----------\n        other : Series or scalar\n            The value(s) to be combined with the `Series`.\n        func : function\n            Function that takes two scalars as inputs and returns an element.\n        fill_value : scalar, optional\n            The value to assume when an index is missing from\n            one Series or the other. The default specifies to use the\n            appropriate NaN value for the underlying dtype of the Series.\n\n        Returns\n        -------\n        Series\n            The result of combining the Series with the other object.\n\n        See Also\n        --------\n        Series.combine_first : Combine Series values, choosing the calling\n            Series' values first.\n\n        Examples\n        --------\n        Consider 2 Datasets ``s1`` and ``s2`` containing\n        highest clocked speeds of different birds.\n\n        >>> s1 = pd.Series({'falcon': 330.0, 'eagle': 160.0})\n        >>> s1\n        falcon    330.0\n        eagle     160.0\n        dtype: float64\n        >>> s2 = pd.Series({'falcon': 345.0, 'eagle': 200.0, 'duck': 30.0})\n        >>> s2\n        falcon    345.0\n        eagle     200.0\n        duck       30.0\n        dtype: float64\n\n        Now, to combine the two datasets and view the highest speeds\n        of the birds across the two datasets\n\n        >>> s1.combine(s2, max)\n        duck        NaN\n        eagle     200.0\n        falcon    345.0\n        dtype: float64\n\n        In the previous example, the resulting value for duck is missing,\n        because the maximum of a NaN and a float is a NaN.\n        So, in the example, we set ``fill_value=0``,\n        so the maximum value returned will be the value from some dataset.\n\n        >>> s1.combine(s2, max, fill_value=0)\n        duck       30.0\n        eagle     200.0\n        falcon    345.0\n        dtype: float64\n        \"\"\"\n        if fill_value is None:\n            fill_value = na_value_for_dtype(self.dtype, compat=False)\n\n        if isinstance(other, Series):\n            # If other is a Series, result is based on union of Series,\n            # so do this element by element\n            new_index = self.index.union(other.index)\n            new_name = ops.get_op_result_name(self, other)\n            new_values = np.empty(len(new_index), dtype=object)\n            for i, idx in enumerate(new_index):\n                lv = self.get(idx, fill_value)\n                rv = other.get(idx, fill_value)\n                with np.errstate(all=\"ignore\"):\n                    new_values[i] = func(lv, rv)\n        else:\n            # Assume that other is a scalar, so apply the function for\n            # each element in the Series\n            new_index = self.index\n            new_values = np.empty(len(new_index), dtype=object)\n            with np.errstate(all=\"ignore\"):\n                new_values[:] = [func(lv, other) for lv in self._values]\n            new_name = self.name\n\n        # try_float=False is to match agg_series\n        npvalues = lib.maybe_convert_objects(new_values, try_float=False)\n        res_values = maybe_cast_pointwise_result(npvalues, self.dtype, same_dtype=False)\n        return self._constructor(res_values, index=new_index, name=new_name)\n\n    def combine_first(self, other) -> Series:\n        \"\"\"\n        Update null elements with value in the same location in 'other'.\n\n        Combine two Series objects by filling null values in one Series with\n        non-null values from the other Series. Result index will be the union\n        of the two indexes.\n\n        Parameters\n        ----------\n        other : Series\n            The value(s) to be used for filling null values.\n\n        Returns\n        -------\n        Series\n            The result of combining the provided Series with the other object.\n\n        See Also\n        --------\n        Series.combine : Perform element-wise operation on two Series\n            using a given function.\n\n        Examples\n        --------\n        >>> s1 = pd.Series([1, np.nan])\n        >>> s2 = pd.Series([3, 4, 5])\n        >>> s1.combine_first(s2)\n        0    1.0\n        1    4.0\n        2    5.0\n        dtype: float64\n\n        Null values still persist if the location of that null value\n        does not exist in `other`\n\n        >>> s1 = pd.Series({'falcon': np.nan, 'eagle': 160.0})\n        >>> s2 = pd.Series({'eagle': 200.0, 'duck': 30.0})\n        >>> s1.combine_first(s2)\n        duck       30.0\n        eagle     160.0\n        falcon      NaN\n        dtype: float64\n        \"\"\"\n        new_index = self.index.union(other.index)\n        this = self.reindex(new_index, copy=False)\n        other = other.reindex(new_index, copy=False)\n        if this.dtype.kind == \"M\" and other.dtype.kind != \"M\":\n            other = to_datetime(other)\n\n        return this.where(notna(this), other)\n\n    def update(self, other) -> None:\n        \"\"\"\n        Modify Series in place using values from passed Series.\n\n        Uses non-NA values from passed Series to make updates. Aligns\n        on index.\n\n        Parameters\n        ----------\n        other : Series, or object coercible into Series\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.update(pd.Series([4, 5, 6]))\n        >>> s\n        0    4\n        1    5\n        2    6\n        dtype: int64\n\n        >>> s = pd.Series(['a', 'b', 'c'])\n        >>> s.update(pd.Series(['d', 'e'], index=[0, 2]))\n        >>> s\n        0    d\n        1    b\n        2    e\n        dtype: object\n\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.update(pd.Series([4, 5, 6, 7, 8]))\n        >>> s\n        0    4\n        1    5\n        2    6\n        dtype: int64\n\n        If ``other`` contains NaNs the corresponding values are not updated\n        in the original Series.\n\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.update(pd.Series([4, np.nan, 6]))\n        >>> s\n        0    4\n        1    2\n        2    6\n        dtype: int64\n\n        ``other`` can also be a non-Series object type\n        that is coercible into a Series\n\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.update([4, np.nan, 6])\n        >>> s\n        0    4\n        1    2\n        2    6\n        dtype: int64\n\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.update({1: 9})\n        >>> s\n        0    1\n        1    9\n        2    3\n        dtype: int64\n        \"\"\"\n\n        if not isinstance(other, Series):\n            other = Series(other)\n\n        other = other.reindex_like(self)\n        mask = notna(other)\n\n        self._mgr = self._mgr.putmask(mask=mask, new=other)\n        self._maybe_update_cacher()\n\n    # ----------------------------------------------------------------------\n    # Reindexing, sorting\n\n    @deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\"])\n    def sort_values(\n        self,\n        axis=0,\n        ascending: bool | int | Sequence[bool | int] = True,\n        inplace: bool = False,\n        kind: str = \"quicksort\",\n        na_position: str = \"last\",\n        ignore_index: bool = False,\n        key: ValueKeyFunc = None,\n    ):\n        \"\"\"\n        Sort by the values.\n\n        Sort a Series in ascending or descending order by some\n        criterion.\n\n        Parameters\n        ----------\n        axis : {0 or 'index'}, default 0\n            Axis to direct sorting. The value 'index' is accepted for\n            compatibility with DataFrame.sort_values.\n        ascending : bool or list of bools, default True\n            If True, sort values in ascending order, otherwise descending.\n        inplace : bool, default False\n            If True, perform operation in-place.\n        kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, default 'quicksort'\n            Choice of sorting algorithm. See also :func:`numpy.sort` for more\n            information. 'mergesort' and 'stable' are the only stable  algorithms.\n        na_position : {'first' or 'last'}, default 'last'\n            Argument 'first' puts NaNs at the beginning, 'last' puts NaNs at\n            the end.\n        ignore_index : bool, default False\n            If True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\n            .. versionadded:: 1.0.0\n\n        key : callable, optional\n            If not None, apply the key function to the series values\n            before sorting. This is similar to the `key` argument in the\n            builtin :meth:`sorted` function, with the notable difference that\n            this `key` function should be *vectorized*. It should expect a\n            ``Series`` and return an array-like.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        Series or None\n            Series ordered by values or None if ``inplace=True``.\n\n        See Also\n        --------\n        Series.sort_index : Sort by the Series indices.\n        DataFrame.sort_values : Sort DataFrame by the values along either axis.\n        DataFrame.sort_index : Sort DataFrame by indices.\n\n        Examples\n        --------\n        >>> s = pd.Series([np.nan, 1, 3, 10, 5])\n        >>> s\n        0     NaN\n        1     1.0\n        2     3.0\n        3     10.0\n        4     5.0\n        dtype: float64\n\n        Sort values ascending order (default behaviour)\n\n        >>> s.sort_values(ascending=True)\n        1     1.0\n        2     3.0\n        4     5.0\n        3    10.0\n        0     NaN\n        dtype: float64\n\n        Sort values descending order\n\n        >>> s.sort_values(ascending=False)\n        3    10.0\n        4     5.0\n        2     3.0\n        1     1.0\n        0     NaN\n        dtype: float64\n\n        Sort values inplace\n\n        >>> s.sort_values(ascending=False, inplace=True)\n        >>> s\n        3    10.0\n        4     5.0\n        2     3.0\n        1     1.0\n        0     NaN\n        dtype: float64\n\n        Sort values putting NAs first\n\n        >>> s.sort_values(na_position='first')\n        0     NaN\n        1     1.0\n        2     3.0\n        4     5.0\n        3    10.0\n        dtype: float64\n\n        Sort a series of strings\n\n        >>> s = pd.Series(['z', 'b', 'd', 'a', 'c'])\n        >>> s\n        0    z\n        1    b\n        2    d\n        3    a\n        4    c\n        dtype: object\n\n        >>> s.sort_values()\n        3    a\n        1    b\n        4    c\n        2    d\n        0    z\n        dtype: object\n\n        Sort using a key function. Your `key` function will be\n        given the ``Series`` of values and should return an array-like.\n\n        >>> s = pd.Series(['a', 'B', 'c', 'D', 'e'])\n        >>> s.sort_values()\n        1    B\n        3    D\n        0    a\n        2    c\n        4    e\n        dtype: object\n        >>> s.sort_values(key=lambda x: x.str.lower())\n        0    a\n        1    B\n        2    c\n        3    D\n        4    e\n        dtype: object\n\n        NumPy ufuncs work well here. For example, we can\n        sort by the ``sin`` of the value\n\n        >>> s = pd.Series([-4, -2, 0, 2, 4])\n        >>> s.sort_values(key=np.sin)\n        1   -2\n        4    4\n        2    0\n        0   -4\n        3    2\n        dtype: int64\n\n        More complicated user-defined functions can be used,\n        as long as they expect a Series and return an array-like\n\n        >>> s.sort_values(key=lambda x: (np.tan(x.cumsum())))\n        0   -4\n        3    2\n        4    4\n        1   -2\n        2    0\n        dtype: int64\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        # Validate the axis parameter\n        self._get_axis_number(axis)\n\n        # GH 5856/5853\n        if inplace and self._is_cached:\n            raise ValueError(\n                \"This Series is a view of some other array, to \"\n                \"sort in-place you must create a copy\"\n            )\n\n        if is_list_like(ascending):\n            ascending = cast(Sequence[Union[bool, int]], ascending)\n            if len(ascending) != 1:\n                raise ValueError(\n                    f\"Length of ascending ({len(ascending)}) must be 1 for Series\"\n                )\n            ascending = ascending[0]\n\n        ascending = validate_ascending(ascending)\n\n        if na_position not in [\"first\", \"last\"]:\n            raise ValueError(f\"invalid na_position: {na_position}\")\n\n        # GH 35922. Make sorting stable by leveraging nargsort\n        values_to_sort = ensure_key_mapped(self, key)._values if key else self._values\n        sorted_index = nargsort(values_to_sort, kind, bool(ascending), na_position)\n\n        result = self._constructor(\n            self._values[sorted_index], index=self.index[sorted_index]\n        )\n\n        if ignore_index:\n            result.index = default_index(len(sorted_index))\n\n        if inplace:\n            self._update_inplace(result)\n        else:\n            return result.__finalize__(self, method=\"sort_values\")\n\n    @deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\"])\n    def sort_index(\n        self,\n        axis=0,\n        level=None,\n        ascending: bool | int | Sequence[bool | int] = True,\n        inplace: bool = False,\n        kind: str = \"quicksort\",\n        na_position: str = \"last\",\n        sort_remaining: bool = True,\n        ignore_index: bool = False,\n        key: IndexKeyFunc = None,\n    ):\n        \"\"\"\n        Sort Series by index labels.\n\n        Returns a new Series sorted by label if `inplace` argument is\n        ``False``, otherwise updates the original series and returns None.\n\n        Parameters\n        ----------\n        axis : int, default 0\n            Axis to direct sorting. This can only be 0 for Series.\n        level : int, optional\n            If not None, sort on values in specified index level(s).\n        ascending : bool or list-like of bools, default True\n            Sort ascending vs. descending. When the index is a MultiIndex the\n            sort direction can be controlled for each level individually.\n        inplace : bool, default False\n            If True, perform operation in-place.\n        kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, default 'quicksort'\n            Choice of sorting algorithm. See also :func:`numpy.sort` for more\n            information. 'mergesort' and 'stable' are the only stable algorithms. For\n            DataFrames, this option is only applied when sorting on a single\n            column or label.\n        na_position : {'first', 'last'}, default 'last'\n            If 'first' puts NaNs at the beginning, 'last' puts NaNs at the end.\n            Not implemented for MultiIndex.\n        sort_remaining : bool, default True\n            If True and sorting by level and index is multilevel, sort by other\n            levels too (in order) after sorting by specified level.\n        ignore_index : bool, default False\n            If True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\n            .. versionadded:: 1.0.0\n\n        key : callable, optional\n            If not None, apply the key function to the index values\n            before sorting. This is similar to the `key` argument in the\n            builtin :meth:`sorted` function, with the notable difference that\n            this `key` function should be *vectorized*. It should expect an\n            ``Index`` and return an ``Index`` of the same shape.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        Series or None\n            The original Series sorted by the labels or None if ``inplace=True``.\n\n        See Also\n        --------\n        DataFrame.sort_index: Sort DataFrame by the index.\n        DataFrame.sort_values: Sort DataFrame by the value.\n        Series.sort_values : Sort Series by the value.\n\n        Examples\n        --------\n        >>> s = pd.Series(['a', 'b', 'c', 'd'], index=[3, 2, 1, 4])\n        >>> s.sort_index()\n        1    c\n        2    b\n        3    a\n        4    d\n        dtype: object\n\n        Sort Descending\n\n        >>> s.sort_index(ascending=False)\n        4    d\n        3    a\n        2    b\n        1    c\n        dtype: object\n\n        Sort Inplace\n\n        >>> s.sort_index(inplace=True)\n        >>> s\n        1    c\n        2    b\n        3    a\n        4    d\n        dtype: object\n\n        By default NaNs are put at the end, but use `na_position` to place\n        them at the beginning\n\n        >>> s = pd.Series(['a', 'b', 'c', 'd'], index=[3, 2, 1, np.nan])\n        >>> s.sort_index(na_position='first')\n        NaN     d\n         1.0    c\n         2.0    b\n         3.0    a\n        dtype: object\n\n        Specify index level to sort\n\n        >>> arrays = [np.array(['qux', 'qux', 'foo', 'foo',\n        ...                     'baz', 'baz', 'bar', 'bar']),\n        ...           np.array(['two', 'one', 'two', 'one',\n        ...                     'two', 'one', 'two', 'one'])]\n        >>> s = pd.Series([1, 2, 3, 4, 5, 6, 7, 8], index=arrays)\n        >>> s.sort_index(level=1)\n        bar  one    8\n        baz  one    6\n        foo  one    4\n        qux  one    2\n        bar  two    7\n        baz  two    5\n        foo  two    3\n        qux  two    1\n        dtype: int64\n\n        Does not sort by remaining levels when sorting by levels\n\n        >>> s.sort_index(level=1, sort_remaining=False)\n        qux  one    2\n        foo  one    4\n        baz  one    6\n        bar  one    8\n        qux  two    1\n        foo  two    3\n        baz  two    5\n        bar  two    7\n        dtype: int64\n\n        Apply a key function before sorting\n\n        >>> s = pd.Series([1, 2, 3, 4], index=['A', 'b', 'C', 'd'])\n        >>> s.sort_index(key=lambda x : x.str.lower())\n        A    1\n        b    2\n        C    3\n        d    4\n        dtype: int64\n        \"\"\"\n\n        return super().sort_index(\n            axis,\n            level,\n            ascending,\n            inplace,\n            kind,\n            na_position,\n            sort_remaining,\n            ignore_index,\n            key,\n        )\n\n    def argsort(self, axis=0, kind=\"quicksort\", order=None) -> Series:\n        \"\"\"\n        Return the integer indices that would sort the Series values.\n\n        Override ndarray.argsort. Argsorts the value, omitting NA/null values,\n        and places the result in the same locations as the non-NA values.\n\n        Parameters\n        ----------\n        axis : {0 or \"index\"}\n            Has no effect but is accepted for compatibility with numpy.\n        kind : {'mergesort', 'quicksort', 'heapsort', 'stable'}, default 'quicksort'\n            Choice of sorting algorithm. See :func:`numpy.sort` for more\n            information. 'mergesort' and 'stable' are the only stable algorithms.\n        order : None\n            Has no effect but is accepted for compatibility with numpy.\n\n        Returns\n        -------\n        Series[np.intp]\n            Positions of values within the sort order with -1 indicating\n            nan values.\n\n        See Also\n        --------\n        numpy.ndarray.argsort : Returns the indices that would sort this array.\n        \"\"\"\n        values = self._values\n        mask = isna(values)\n\n        if mask.any():\n            result = np.full(len(self), -1, dtype=np.intp)\n            notmask = ~mask\n            result[notmask] = np.argsort(values[notmask], kind=kind)\n        else:\n            result = np.argsort(values, kind=kind)\n\n        res = self._constructor(result, index=self.index, name=self.name, dtype=np.intp)\n        return res.__finalize__(self, method=\"argsort\")\n\n    def nlargest(self, n=5, keep=\"first\") -> Series:\n        \"\"\"\n        Return the largest `n` elements.\n\n        Parameters\n        ----------\n        n : int, default 5\n            Return this many descending sorted values.\n        keep : {'first', 'last', 'all'}, default 'first'\n            When there are duplicate values that cannot all fit in a\n            Series of `n` elements:\n\n            - ``first`` : return the first `n` occurrences in order\n              of appearance.\n            - ``last`` : return the last `n` occurrences in reverse\n              order of appearance.\n            - ``all`` : keep all occurrences. This can result in a Series of\n              size larger than `n`.\n\n        Returns\n        -------\n        Series\n            The `n` largest values in the Series, sorted in decreasing order.\n\n        See Also\n        --------\n        Series.nsmallest: Get the `n` smallest elements.\n        Series.sort_values: Sort Series by values.\n        Series.head: Return the first `n` rows.\n\n        Notes\n        -----\n        Faster than ``.sort_values(ascending=False).head(n)`` for small `n`\n        relative to the size of the ``Series`` object.\n\n        Examples\n        --------\n        >>> countries_population = {\"Italy\": 59000000, \"France\": 65000000,\n        ...                         \"Malta\": 434000, \"Maldives\": 434000,\n        ...                         \"Brunei\": 434000, \"Iceland\": 337000,\n        ...                         \"Nauru\": 11300, \"Tuvalu\": 11300,\n        ...                         \"Anguilla\": 11300, \"Montserrat\": 5200}\n        >>> s = pd.Series(countries_population)\n        >>> s\n        Italy       59000000\n        France      65000000\n        Malta         434000\n        Maldives      434000\n        Brunei        434000\n        Iceland       337000\n        Nauru          11300\n        Tuvalu         11300\n        Anguilla       11300\n        Montserrat      5200\n        dtype: int64\n\n        The `n` largest elements where ``n=5`` by default.\n\n        >>> s.nlargest()\n        France      65000000\n        Italy       59000000\n        Malta         434000\n        Maldives      434000\n        Brunei        434000\n        dtype: int64\n\n        The `n` largest elements where ``n=3``. Default `keep` value is 'first'\n        so Malta will be kept.\n\n        >>> s.nlargest(3)\n        France    65000000\n        Italy     59000000\n        Malta       434000\n        dtype: int64\n\n        The `n` largest elements where ``n=3`` and keeping the last duplicates.\n        Brunei will be kept since it is the last with value 434000 based on\n        the index order.\n\n        >>> s.nlargest(3, keep='last')\n        France      65000000\n        Italy       59000000\n        Brunei        434000\n        dtype: int64\n\n        The `n` largest elements where ``n=3`` with all duplicates kept. Note\n        that the returned Series has five elements due to the three duplicates.\n\n        >>> s.nlargest(3, keep='all')\n        France      65000000\n        Italy       59000000\n        Malta         434000\n        Maldives      434000\n        Brunei        434000\n        dtype: int64\n        \"\"\"\n        return algorithms.SelectNSeries(self, n=n, keep=keep).nlargest()\n\n    def nsmallest(self, n: int = 5, keep: str = \"first\") -> Series:\n        \"\"\"\n        Return the smallest `n` elements.\n\n        Parameters\n        ----------\n        n : int, default 5\n            Return this many ascending sorted values.\n        keep : {'first', 'last', 'all'}, default 'first'\n            When there are duplicate values that cannot all fit in a\n            Series of `n` elements:\n\n            - ``first`` : return the first `n` occurrences in order\n              of appearance.\n            - ``last`` : return the last `n` occurrences in reverse\n              order of appearance.\n            - ``all`` : keep all occurrences. This can result in a Series of\n              size larger than `n`.\n\n        Returns\n        -------\n        Series\n            The `n` smallest values in the Series, sorted in increasing order.\n\n        See Also\n        --------\n        Series.nlargest: Get the `n` largest elements.\n        Series.sort_values: Sort Series by values.\n        Series.head: Return the first `n` rows.\n\n        Notes\n        -----\n        Faster than ``.sort_values().head(n)`` for small `n` relative to\n        the size of the ``Series`` object.\n\n        Examples\n        --------\n        >>> countries_population = {\"Italy\": 59000000, \"France\": 65000000,\n        ...                         \"Brunei\": 434000, \"Malta\": 434000,\n        ...                         \"Maldives\": 434000, \"Iceland\": 337000,\n        ...                         \"Nauru\": 11300, \"Tuvalu\": 11300,\n        ...                         \"Anguilla\": 11300, \"Montserrat\": 5200}\n        >>> s = pd.Series(countries_population)\n        >>> s\n        Italy       59000000\n        France      65000000\n        Brunei        434000\n        Malta         434000\n        Maldives      434000\n        Iceland       337000\n        Nauru          11300\n        Tuvalu         11300\n        Anguilla       11300\n        Montserrat      5200\n        dtype: int64\n\n        The `n` smallest elements where ``n=5`` by default.\n\n        >>> s.nsmallest()\n        Montserrat    5200\n        Nauru        11300\n        Tuvalu       11300\n        Anguilla     11300\n        Iceland     337000\n        dtype: int64\n\n        The `n` smallest elements where ``n=3``. Default `keep` value is\n        'first' so Nauru and Tuvalu will be kept.\n\n        >>> s.nsmallest(3)\n        Montserrat   5200\n        Nauru       11300\n        Tuvalu      11300\n        dtype: int64\n\n        The `n` smallest elements where ``n=3`` and keeping the last\n        duplicates. Anguilla and Tuvalu will be kept since they are the last\n        with value 11300 based on the index order.\n\n        >>> s.nsmallest(3, keep='last')\n        Montserrat   5200\n        Anguilla    11300\n        Tuvalu      11300\n        dtype: int64\n\n        The `n` smallest elements where ``n=3`` with all duplicates kept. Note\n        that the returned Series has four elements due to the three duplicates.\n\n        >>> s.nsmallest(3, keep='all')\n        Montserrat   5200\n        Nauru       11300\n        Tuvalu      11300\n        Anguilla    11300\n        dtype: int64\n        \"\"\"\n        return algorithms.SelectNSeries(self, n=n, keep=keep).nsmallest()\n\n    @doc(\n        klass=_shared_doc_kwargs[\"klass\"],\n        extra_params=dedent(\n            \"\"\"copy : bool, default True\n            Whether to copy underlying data.\"\"\"\n        ),\n        examples=dedent(\n            \"\"\"\\\n        Examples\n        --------\n        >>> s = pd.Series(\n        ...     [\"A\", \"B\", \"A\", \"C\"],\n        ...     index=[\n        ...         [\"Final exam\", \"Final exam\", \"Coursework\", \"Coursework\"],\n        ...         [\"History\", \"Geography\", \"History\", \"Geography\"],\n        ...         [\"January\", \"February\", \"March\", \"April\"],\n        ...     ],\n        ... )\n        >>> s\n        Final exam  History     January      A\n                    Geography   February     B\n        Coursework  History     March        A\n                    Geography   April        C\n        dtype: object\n\n        In the following example, we will swap the levels of the indices.\n        Here, we will swap the levels column-wise, but levels can be swapped row-wise\n        in a similar manner. Note that column-wise is the default behaviour.\n        By not supplying any arguments for i and j, we swap the last and second to\n        last indices.\n\n        >>> s.swaplevel()\n        Final exam  January     History         A\n                    February    Geography       B\n        Coursework  March       History         A\n                    April       Geography       C\n        dtype: object\n\n        By supplying one argument, we can choose which index to swap the last\n        index with. We can for example swap the first index with the last one as\n        follows.\n\n        >>> s.swaplevel(0)\n        January     History     Final exam      A\n        February    Geography   Final exam      B\n        March       History     Coursework      A\n        April       Geography   Coursework      C\n        dtype: object\n\n        We can also define explicitly which indices we want to swap by supplying values\n        for both i and j. Here, we for example swap the first and second indices.\n\n        >>> s.swaplevel(0, 1)\n        History     Final exam  January         A\n        Geography   Final exam  February        B\n        History     Coursework  March           A\n        Geography   Coursework  April           C\n        dtype: object\"\"\"\n        ),\n    )\n    def swaplevel(self, i=-2, j=-1, copy=True) -> Series:\n        \"\"\"\n        Swap levels i and j in a :class:`MultiIndex`.\n\n        Default is to swap the two innermost levels of the index.\n\n        Parameters\n        ----------\n        i, j : int or str\n            Levels of the indices to be swapped. Can pass level name as string.\n        {extra_params}\n\n        Returns\n        -------\n        {klass}\n            {klass} with levels swapped in MultiIndex.\n\n        {examples}\n        \"\"\"\n        assert isinstance(self.index, MultiIndex)\n        new_index = self.index.swaplevel(i, j)\n        return self._constructor(self._values, index=new_index, copy=copy).__finalize__(\n            self, method=\"swaplevel\"\n        )\n\n    def reorder_levels(self, order) -> Series:\n        \"\"\"\n        Rearrange index levels using input order.\n\n        May not drop or duplicate levels.\n\n        Parameters\n        ----------\n        order : list of int representing new level order\n            Reference level by number or key.\n\n        Returns\n        -------\n        type of caller (new object)\n        \"\"\"\n        if not isinstance(self.index, MultiIndex):  # pragma: no cover\n            raise Exception(\"Can only reorder levels on a hierarchical axis.\")\n\n        result = self.copy()\n        assert isinstance(result.index, MultiIndex)\n        result.index = result.index.reorder_levels(order)\n        return result\n\n    def explode(self, ignore_index: bool = False) -> Series:\n        \"\"\"\n        Transform each element of a list-like to a row.\n\n        .. versionadded:: 0.25.0\n\n        Parameters\n        ----------\n        ignore_index : bool, default False\n            If True, the resulting index will be labeled 0, 1, \u2026, n - 1.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        Series\n            Exploded lists to rows; index will be duplicated for these rows.\n\n        See Also\n        --------\n        Series.str.split : Split string values on specified separator.\n        Series.unstack : Unstack, a.k.a. pivot, Series with MultiIndex\n            to produce DataFrame.\n        DataFrame.melt : Unpivot a DataFrame from wide format to long format.\n        DataFrame.explode : Explode a DataFrame from list-like\n            columns to long format.\n\n        Notes\n        -----\n        This routine will explode list-likes including lists, tuples, sets,\n        Series, and np.ndarray. The result dtype of the subset rows will\n        be object. Scalars will be returned unchanged, and empty list-likes will\n        result in a np.nan for that row. In addition, the ordering of elements in\n        the output will be non-deterministic when exploding sets.\n\n        Reference :ref:`the user guide <reshaping.explode>` for more examples.\n\n        Examples\n        --------\n        >>> s = pd.Series([[1, 2, 3], 'foo', [], [3, 4]])\n        >>> s\n        0    [1, 2, 3]\n        1          foo\n        2           []\n        3       [3, 4]\n        dtype: object\n\n        >>> s.explode()\n        0      1\n        0      2\n        0      3\n        1    foo\n        2    NaN\n        3      3\n        3      4\n        dtype: object\n        \"\"\"\n        if not len(self) or not is_object_dtype(self):\n            result = self.copy()\n            return result.reset_index(drop=True) if ignore_index else result\n\n        values, counts = reshape.explode(np.asarray(self._values))\n\n        if ignore_index:\n            index = default_index(len(values))\n        else:\n            index = self.index.repeat(counts)\n\n        return self._constructor(values, index=index, name=self.name)\n\n    def unstack(self, level=-1, fill_value=None) -> DataFrame:\n        \"\"\"\n        Unstack, also known as pivot, Series with MultiIndex to produce DataFrame.\n\n        Parameters\n        ----------\n        level : int, str, or list of these, default last level\n            Level(s) to unstack, can pass level name.\n        fill_value : scalar value, default None\n            Value to use when replacing NaN values.\n\n        Returns\n        -------\n        DataFrame\n            Unstacked Series.\n\n        Notes\n        -----\n        Reference :ref:`the user guide <reshaping.stacking>` for more examples.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3, 4],\n        ...               index=pd.MultiIndex.from_product([['one', 'two'],\n        ...                                                 ['a', 'b']]))\n        >>> s\n        one  a    1\n             b    2\n        two  a    3\n             b    4\n        dtype: int64\n\n        >>> s.unstack(level=-1)\n             a  b\n        one  1  2\n        two  3  4\n\n        >>> s.unstack(level=0)\n           one  two\n        a    1    3\n        b    2    4\n        \"\"\"\n        from pandas.core.reshape.reshape import unstack\n\n        return unstack(self, level, fill_value)\n\n    # ----------------------------------------------------------------------\n    # function application\n\n    def map(self, arg, na_action=None) -> Series:\n        \"\"\"\n        Map values of Series according to an input mapping or function.\n\n        Used for substituting each value in a Series with another value,\n        that may be derived from a function, a ``dict`` or\n        a :class:`Series`.\n\n        Parameters\n        ----------\n        arg : function, collections.abc.Mapping subclass or Series\n            Mapping correspondence.\n        na_action : {None, 'ignore'}, default None\n            If 'ignore', propagate NaN values, without passing them to the\n            mapping correspondence.\n\n        Returns\n        -------\n        Series\n            Same index as caller.\n\n        See Also\n        --------\n        Series.apply : For applying more complex functions on a Series.\n        DataFrame.apply : Apply a function row-/column-wise.\n        DataFrame.applymap : Apply a function elementwise on a whole DataFrame.\n\n        Notes\n        -----\n        When ``arg`` is a dictionary, values in Series that are not in the\n        dictionary (as keys) are converted to ``NaN``. However, if the\n        dictionary is a ``dict`` subclass that defines ``__missing__`` (i.e.\n        provides a method for default values), then this default is used\n        rather than ``NaN``.\n\n        Examples\n        --------\n        >>> s = pd.Series(['cat', 'dog', np.nan, 'rabbit'])\n        >>> s\n        0      cat\n        1      dog\n        2      NaN\n        3   rabbit\n        dtype: object\n\n        ``map`` accepts a ``dict`` or a ``Series``. Values that are not found\n        in the ``dict`` are converted to ``NaN``, unless the dict has a default\n        value (e.g. ``defaultdict``):\n\n        >>> s.map({'cat': 'kitten', 'dog': 'puppy'})\n        0   kitten\n        1    puppy\n        2      NaN\n        3      NaN\n        dtype: object\n\n        It also accepts a function:\n\n        >>> s.map('I am a {}'.format)\n        0       I am a cat\n        1       I am a dog\n        2       I am a nan\n        3    I am a rabbit\n        dtype: object\n\n        To avoid applying the function to missing values (and keep them as\n        ``NaN``) ``na_action='ignore'`` can be used:\n\n        >>> s.map('I am a {}'.format, na_action='ignore')\n        0     I am a cat\n        1     I am a dog\n        2            NaN\n        3  I am a rabbit\n        dtype: object\n        \"\"\"\n        new_values = self._map_values(arg, na_action=na_action)\n        return self._constructor(new_values, index=self.index).__finalize__(\n            self, method=\"map\"\n        )\n\n    def _gotitem(self, key, ndim, subset=None) -> Series:\n        \"\"\"\n        Sub-classes to define. Return a sliced object.\n\n        Parameters\n        ----------\n        key : string / list of selections\n        ndim : {1, 2}\n            Requested ndim of result.\n        subset : object, default None\n            Subset to act on.\n        \"\"\"\n        return self\n\n    _agg_see_also_doc = dedent(\n        \"\"\"\n    See Also\n    --------\n    Series.apply : Invoke function on a Series.\n    Series.transform : Transform function producing a Series with like indexes.\n    \"\"\"\n    )\n\n    _agg_examples_doc = dedent(\n        \"\"\"\n    Examples\n    --------\n    >>> s = pd.Series([1, 2, 3, 4])\n    >>> s\n    0    1\n    1    2\n    2    3\n    3    4\n    dtype: int64\n\n    >>> s.agg('min')\n    1\n\n    >>> s.agg(['min', 'max'])\n    min   1\n    max   4\n    dtype: int64\n    \"\"\"\n    )\n\n    @doc(\n        _shared_docs[\"aggregate\"],\n        klass=_shared_doc_kwargs[\"klass\"],\n        axis=_shared_doc_kwargs[\"axis\"],\n        see_also=_agg_see_also_doc,\n        examples=_agg_examples_doc,\n    )\n    def aggregate(self, func=None, axis=0, *args, **kwargs):\n        # Validate the axis parameter\n        self._get_axis_number(axis)\n\n        # if func is None, will switch to user-provided \"named aggregation\" kwargs\n        if func is None:\n            func = dict(kwargs.items())\n\n        op = SeriesApply(self, func, convert_dtype=False, args=args, kwargs=kwargs)\n        result = op.agg()\n        return result\n\n    agg = aggregate\n\n    @doc(\n        _shared_docs[\"transform\"],\n        klass=_shared_doc_kwargs[\"klass\"],\n        axis=_shared_doc_kwargs[\"axis\"],\n    )\n    def transform(\n        self, func: AggFuncType, axis: Axis = 0, *args, **kwargs\n    ) -> DataFrame | Series:\n        # Validate axis argument\n        self._get_axis_number(axis)\n        result = SeriesApply(\n            self, func=func, convert_dtype=True, args=args, kwargs=kwargs\n        ).transform()\n        return result\n\n    def apply(\n        self,\n        func: AggFuncType,\n        convert_dtype: bool = True,\n        args: tuple[Any, ...] = (),\n        **kwargs,\n    ) -> DataFrame | Series:\n        \"\"\"\n        Invoke function on values of Series.\n\n        Can be ufunc (a NumPy function that applies to the entire Series)\n        or a Python function that only works on single values.\n\n        Parameters\n        ----------\n        func : function\n            Python function or NumPy ufunc to apply.\n        convert_dtype : bool, default True\n            Try to find better dtype for elementwise function results. If\n            False, leave as dtype=object. Note that the dtype is always\n            preserved for some extension array dtypes, such as Categorical.\n        args : tuple\n            Positional arguments passed to func after the series value.\n        **kwargs\n            Additional keyword arguments passed to func.\n\n        Returns\n        -------\n        Series or DataFrame\n            If func returns a Series object the result will be a DataFrame.\n\n        See Also\n        --------\n        Series.map: For element-wise operations.\n        Series.agg: Only perform aggregating type operations.\n        Series.transform: Only perform transforming type operations.\n\n        Notes\n        -----\n        Functions that mutate the passed object can produce unexpected\n        behavior or errors and are not supported. See :ref:`gotchas.udf-mutation`\n        for more details.\n\n        Examples\n        --------\n        Create a series with typical summer temperatures for each city.\n\n        >>> s = pd.Series([20, 21, 12],\n        ...               index=['London', 'New York', 'Helsinki'])\n        >>> s\n        London      20\n        New York    21\n        Helsinki    12\n        dtype: int64\n\n        Square the values by defining a function and passing it as an\n        argument to ``apply()``.\n\n        >>> def square(x):\n        ...     return x ** 2\n        >>> s.apply(square)\n        London      400\n        New York    441\n        Helsinki    144\n        dtype: int64\n\n        Square the values by passing an anonymous function as an\n        argument to ``apply()``.\n\n        >>> s.apply(lambda x: x ** 2)\n        London      400\n        New York    441\n        Helsinki    144\n        dtype: int64\n\n        Define a custom function that needs additional positional\n        arguments and pass these additional arguments using the\n        ``args`` keyword.\n\n        >>> def subtract_custom_value(x, custom_value):\n        ...     return x - custom_value\n\n        >>> s.apply(subtract_custom_value, args=(5,))\n        London      15\n        New York    16\n        Helsinki     7\n        dtype: int64\n\n        Define a custom function that takes keyword arguments\n        and pass these arguments to ``apply``.\n\n        >>> def add_custom_values(x, **kwargs):\n        ...     for month in kwargs:\n        ...         x += kwargs[month]\n        ...     return x\n\n        >>> s.apply(add_custom_values, june=30, july=20, august=25)\n        London      95\n        New York    96\n        Helsinki    87\n        dtype: int64\n\n        Use a function from the Numpy library.\n\n        >>> s.apply(np.log)\n        London      2.995732\n        New York    3.044522\n        Helsinki    2.484907\n        dtype: float64\n        \"\"\"\n        return SeriesApply(self, func, convert_dtype, args, kwargs).apply()\n\n    def _reduce(\n        self,\n        op,\n        name: str,\n        *,\n        axis=0,\n        skipna=True,\n        numeric_only=None,\n        filter_type=None,\n        **kwds,\n    ):\n        \"\"\"\n        Perform a reduction operation.\n\n        If we have an ndarray as a value, then simply perform the operation,\n        otherwise delegate to the object.\n        \"\"\"\n        delegate = self._values\n\n        if axis is not None:\n            self._get_axis_number(axis)\n\n        if isinstance(delegate, ExtensionArray):\n            # dispatch to ExtensionArray interface\n            return delegate._reduce(name, skipna=skipna, **kwds)\n\n        else:\n            # dispatch to numpy arrays\n            if numeric_only:\n                kwd_name = \"numeric_only\"\n                if name in [\"any\", \"all\"]:\n                    kwd_name = \"bool_only\"\n                raise NotImplementedError(\n                    f\"Series.{name} does not implement {kwd_name}.\"\n                )\n            with np.errstate(all=\"ignore\"):\n                return op(delegate, skipna=skipna, **kwds)\n\n    def _reindex_indexer(\n        self, new_index: Index | None, indexer: npt.NDArray[np.intp] | None, copy: bool\n    ) -> Series:\n        # Note: new_index is None iff indexer is None\n        # if not None, indexer is np.intp\n        if indexer is None:\n            if copy:\n                return self.copy()\n            return self\n\n        new_values = algorithms.take_nd(\n            self._values, indexer, allow_fill=True, fill_value=None\n        )\n        return self._constructor(new_values, index=new_index)\n\n    def _needs_reindex_multi(self, axes, method, level) -> bool:\n        \"\"\"\n        Check if we do need a multi reindex; this is for compat with\n        higher dims.\n        \"\"\"\n        return False\n\n    # error: Cannot determine type of 'align'\n    @doc(\n        NDFrame.align,  # type: ignore[has-type]\n        klass=_shared_doc_kwargs[\"klass\"],\n        axes_single_arg=_shared_doc_kwargs[\"axes_single_arg\"],\n    )\n    def align(\n        self,\n        other,\n        join=\"outer\",\n        axis=None,\n        level=None,\n        copy=True,\n        fill_value=None,\n        method=None,\n        limit=None,\n        fill_axis=0,\n        broadcast_axis=None,\n    ):\n        return super().align(\n            other,\n            join=join,\n            axis=axis,\n            level=level,\n            copy=copy,\n            fill_value=fill_value,\n            method=method,\n            limit=limit,\n            fill_axis=fill_axis,\n            broadcast_axis=broadcast_axis,\n        )\n\n    def rename(\n        self,\n        index=None,\n        *,\n        axis=None,\n        copy=True,\n        inplace=False,\n        level=None,\n        errors=\"ignore\",\n    ) -> Series | None:\n        \"\"\"\n        Alter Series index labels or name.\n\n        Function / dict values must be unique (1-to-1). Labels not contained in\n        a dict / Series will be left as-is. Extra labels listed don't throw an\n        error.\n\n        Alternatively, change ``Series.name`` with a scalar value.\n\n        See the :ref:`user guide <basics.rename>` for more.\n\n        Parameters\n        ----------\n        axis : {0 or \"index\"}\n            Unused. Accepted for compatibility with DataFrame method only.\n        index : scalar, hashable sequence, dict-like or function, optional\n            Functions or dict-like are transformations to apply to\n            the index.\n            Scalar or hashable sequence-like will alter the ``Series.name``\n            attribute.\n\n        **kwargs\n            Additional keyword arguments passed to the function. Only the\n            \"inplace\" keyword is used.\n\n        Returns\n        -------\n        Series or None\n            Series with index labels or name altered or None if ``inplace=True``.\n\n        See Also\n        --------\n        DataFrame.rename : Corresponding DataFrame method.\n        Series.rename_axis : Set the name of the axis.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3])\n        >>> s\n        0    1\n        1    2\n        2    3\n        dtype: int64\n        >>> s.rename(\"my_name\")  # scalar, changes Series.name\n        0    1\n        1    2\n        2    3\n        Name: my_name, dtype: int64\n        >>> s.rename(lambda x: x ** 2)  # function, changes labels\n        0    1\n        1    2\n        4    3\n        dtype: int64\n        >>> s.rename({1: 3, 2: 5})  # mapping, changes labels\n        0    1\n        3    2\n        5    3\n        dtype: int64\n        \"\"\"\n        if axis is not None:\n            # Make sure we raise if an invalid 'axis' is passed.\n            axis = self._get_axis_number(axis)\n\n        if callable(index) or is_dict_like(index):\n            return super()._rename(\n                index, copy=copy, inplace=inplace, level=level, errors=errors\n            )\n        else:\n            return self._set_name(index, inplace=inplace)\n\n    @overload\n    def set_axis(\n        self, labels, axis: Axis = ..., inplace: Literal[False] = ...\n    ) -> Series:\n        ...\n\n    @overload\n    def set_axis(self, labels, axis: Axis, inplace: Literal[True]) -> None:\n        ...\n\n    @overload\n    def set_axis(self, labels, *, inplace: Literal[True]) -> None:\n        ...\n\n    @overload\n    def set_axis(self, labels, axis: Axis = ..., inplace: bool = ...) -> Series | None:\n        ...\n\n    @deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\", \"labels\"])\n    @Appender(\n        \"\"\"\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3])\n        >>> s\n        0    1\n        1    2\n        2    3\n        dtype: int64\n\n        >>> s.set_axis(['a', 'b', 'c'], axis=0)\n        a    1\n        b    2\n        c    3\n        dtype: int64\n    \"\"\"\n    )\n    @Substitution(\n        **_shared_doc_kwargs,\n        extended_summary_sub=\"\",\n        axis_description_sub=\"\",\n        see_also_sub=\"\",\n    )\n    @Appender(NDFrame.set_axis.__doc__)\n    def set_axis(self, labels, axis: Axis = 0, inplace: bool = False):\n        return super().set_axis(labels, axis=axis, inplace=inplace)\n\n    # error: Cannot determine type of 'reindex'\n    @doc(\n        NDFrame.reindex,  # type: ignore[has-type]\n        klass=_shared_doc_kwargs[\"klass\"],\n        axes=_shared_doc_kwargs[\"axes\"],\n        optional_labels=_shared_doc_kwargs[\"optional_labels\"],\n        optional_axis=_shared_doc_kwargs[\"optional_axis\"],\n    )\n    def reindex(self, *args, **kwargs) -> Series:\n        if len(args) > 1:\n            raise TypeError(\"Only one positional argument ('index') is allowed\")\n        if args:\n            (index,) = args\n            if \"index\" in kwargs:\n                raise TypeError(\n                    \"'index' passed as both positional and keyword argument\"\n                )\n            kwargs.update({\"index\": index})\n        return super().reindex(**kwargs)\n\n    @deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\", \"labels\"])\n    def drop(\n        self,\n        labels=None,\n        axis=0,\n        index=None,\n        columns=None,\n        level=None,\n        inplace=False,\n        errors=\"raise\",\n    ) -> Series:\n        \"\"\"\n        Return Series with specified index labels removed.\n\n        Remove elements of a Series based on specifying the index labels.\n        When using a multi-index, labels on different levels can be removed\n        by specifying the level.\n\n        Parameters\n        ----------\n        labels : single label or list-like\n            Index labels to drop.\n        axis : 0, default 0\n            Redundant for application on Series.\n        index : single label or list-like\n            Redundant for application on Series, but 'index' can be used instead\n            of 'labels'.\n        columns : single label or list-like\n            No change is made to the Series; use 'index' or 'labels' instead.\n        level : int or level name, optional\n            For MultiIndex, level for which the labels will be removed.\n        inplace : bool, default False\n            If True, do operation inplace and return None.\n        errors : {'ignore', 'raise'}, default 'raise'\n            If 'ignore', suppress error and only existing labels are dropped.\n\n        Returns\n        -------\n        Series or None\n            Series with specified index labels removed or None if ``inplace=True``.\n\n        Raises\n        ------\n        KeyError\n            If none of the labels are found in the index.\n\n        See Also\n        --------\n        Series.reindex : Return only specified index labels of Series.\n        Series.dropna : Return series without null values.\n        Series.drop_duplicates : Return Series with duplicate values removed.\n        DataFrame.drop : Drop specified labels from rows or columns.\n\n        Examples\n        --------\n        >>> s = pd.Series(data=np.arange(3), index=['A', 'B', 'C'])\n        >>> s\n        A  0\n        B  1\n        C  2\n        dtype: int64\n\n        Drop labels B en C\n\n        >>> s.drop(labels=['B', 'C'])\n        A  0\n        dtype: int64\n\n        Drop 2nd level label in MultiIndex Series\n\n        >>> midx = pd.MultiIndex(levels=[['lama', 'cow', 'falcon'],\n        ...                              ['speed', 'weight', 'length']],\n        ...                      codes=[[0, 0, 0, 1, 1, 1, 2, 2, 2],\n        ...                             [0, 1, 2, 0, 1, 2, 0, 1, 2]])\n        >>> s = pd.Series([45, 200, 1.2, 30, 250, 1.5, 320, 1, 0.3],\n        ...               index=midx)\n        >>> s\n        lama    speed      45.0\n                weight    200.0\n                length      1.2\n        cow     speed      30.0\n                weight    250.0\n                length      1.5\n        falcon  speed     320.0\n                weight      1.0\n                length      0.3\n        dtype: float64\n\n        >>> s.drop(labels='weight', level=1)\n        lama    speed      45.0\n                length      1.2\n        cow     speed      30.0\n                length      1.5\n        falcon  speed     320.0\n                length      0.3\n        dtype: float64\n        \"\"\"\n        return super().drop(\n            labels=labels,\n            axis=axis,\n            index=index,\n            columns=columns,\n            level=level,\n            inplace=inplace,\n            errors=errors,\n        )\n\n    @overload\n    def fillna(\n        self,\n        value=...,\n        method: FillnaOptions | None = ...,\n        axis: Axis | None = ...,\n        inplace: Literal[False] = ...,\n        limit=...,\n        downcast=...,\n    ) -> Series:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        value,\n        method: FillnaOptions | None,\n        axis: Axis | None,\n        inplace: Literal[True],\n        limit=...,\n        downcast=...,\n    ) -> None:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        *,\n        inplace: Literal[True],\n        limit=...,\n        downcast=...,\n    ) -> None:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        value,\n        *,\n        inplace: Literal[True],\n        limit=...,\n        downcast=...,\n    ) -> None:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        *,\n        method: FillnaOptions | None,\n        inplace: Literal[True],\n        limit=...,\n        downcast=...,\n    ) -> None:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        *,\n        axis: Axis | None,\n        inplace: Literal[True],\n        limit=...,\n        downcast=...,\n    ) -> None:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        *,\n        method: FillnaOptions | None,\n        axis: Axis | None,\n        inplace: Literal[True],\n        limit=...,\n        downcast=...,\n    ) -> None:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        value,\n        *,\n        axis: Axis | None,\n        inplace: Literal[True],\n        limit=...,\n        downcast=...,\n    ) -> None:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        value,\n        method: FillnaOptions | None,\n        *,\n        inplace: Literal[True],\n        limit=...,\n        downcast=...,\n    ) -> None:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        value=...,\n        method: FillnaOptions | None = ...,\n        axis: Axis | None = ...,\n        inplace: bool = ...,\n        limit=...,\n        downcast=...,\n    ) -> Series | None:\n        ...\n\n    # error: Cannot determine type of 'fillna'\n    @deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\", \"value\"])\n    @doc(NDFrame.fillna, **_shared_doc_kwargs)  # type: ignore[has-type]\n    def fillna(\n        self,\n        value: object | ArrayLike | None = None,\n        method: FillnaOptions | None = None,\n        axis=None,\n        inplace=False,\n        limit=None,\n        downcast=None,\n    ) -> Series | None:\n        return super().fillna(\n            value=value,\n            method=method,\n            axis=axis,\n            inplace=inplace,\n            limit=limit,\n            downcast=downcast,\n        )\n\n    def pop(self, item: Hashable) -> Any:\n        \"\"\"\n        Return item and drops from series. Raise KeyError if not found.\n\n        Parameters\n        ----------\n        item : label\n            Index of the element that needs to be removed.\n\n        Returns\n        -------\n        Value that is popped from series.\n\n        Examples\n        --------\n        >>> ser = pd.Series([1,2,3])\n\n        >>> ser.pop(0)\n        1\n\n        >>> ser\n        1    2\n        2    3\n        dtype: int64\n        \"\"\"\n        return super().pop(item=item)\n\n    # error: Cannot determine type of 'replace'\n    @doc(\n        NDFrame.replace,  # type: ignore[has-type]\n        klass=_shared_doc_kwargs[\"klass\"],\n        inplace=_shared_doc_kwargs[\"inplace\"],\n        replace_iloc=_shared_doc_kwargs[\"replace_iloc\"],\n    )\n    def replace(\n        self,\n        to_replace=None,\n        value=lib.no_default,\n        inplace=False,\n        limit=None,\n        regex=False,\n        method: str | lib.NoDefault = lib.no_default,\n    ):\n        return super().replace(\n            to_replace=to_replace,\n            value=value,\n            inplace=inplace,\n            limit=limit,\n            regex=regex,\n            method=method,\n        )\n\n    @doc(INFO_DOCSTRING, **series_sub_kwargs)\n    def info(\n        self,\n        verbose: bool | None = None,\n        buf: IO[str] | None = None,\n        max_cols: int | None = None,\n        memory_usage: bool | str | None = None,\n        show_counts: bool = True,\n    ) -> None:\n        return SeriesInfo(self, memory_usage).render(\n            buf=buf,\n            max_cols=max_cols,\n            verbose=verbose,\n            show_counts=show_counts,\n        )\n\n    def _replace_single(self, to_replace, method: str, inplace: bool, limit):\n        \"\"\"\n        Replaces values in a Series using the fill method specified when no\n        replacement value is given in the replace method\n        \"\"\"\n\n        result = self if inplace else self.copy()\n\n        values = result._values\n        mask = missing.mask_missing(values, to_replace)\n\n        if isinstance(values, ExtensionArray):\n            # dispatch to the EA's _pad_mask_inplace method\n            values._fill_mask_inplace(method, limit, mask)\n        else:\n            fill_f = missing.get_fill_func(method)\n            values, _ = fill_f(values, limit=limit, mask=mask)\n\n        if inplace:\n            return\n        return result\n\n    # error: Cannot determine type of 'shift'\n    @doc(NDFrame.shift, klass=_shared_doc_kwargs[\"klass\"])  # type: ignore[has-type]\n    def shift(self, periods=1, freq=None, axis=0, fill_value=None) -> Series:\n        return super().shift(\n            periods=periods, freq=freq, axis=axis, fill_value=fill_value\n        )\n\n    def memory_usage(self, index: bool = True, deep: bool = False) -> int:\n        \"\"\"\n        Return the memory usage of the Series.\n\n        The memory usage can optionally include the contribution of\n        the index and of elements of `object` dtype.\n\n        Parameters\n        ----------\n        index : bool, default True\n            Specifies whether to include the memory usage of the Series index.\n        deep : bool, default False\n            If True, introspect the data deeply by interrogating\n            `object` dtypes for system-level memory consumption, and include\n            it in the returned value.\n\n        Returns\n        -------\n        int\n            Bytes of memory consumed.\n\n        See Also\n        --------\n        numpy.ndarray.nbytes : Total bytes consumed by the elements of the\n            array.\n        DataFrame.memory_usage : Bytes consumed by a DataFrame.\n\n        Examples\n        --------\n        >>> s = pd.Series(range(3))\n        >>> s.memory_usage()\n        152\n\n        Not including the index gives the size of the rest of the data, which\n        is necessarily smaller:\n\n        >>> s.memory_usage(index=False)\n        24\n\n        The memory footprint of `object` values is ignored by default:\n\n        >>> s = pd.Series([\"a\", \"b\"])\n        >>> s.values\n        array(['a', 'b'], dtype=object)\n        >>> s.memory_usage()\n        144\n        >>> s.memory_usage(deep=True)\n        244\n        \"\"\"\n        v = self._memory_usage(deep=deep)\n        if index:\n            v += self.index.memory_usage(deep=deep)\n        return v\n\n    def isin(self, values) -> Series:\n        \"\"\"\n        Whether elements in Series are contained in `values`.\n\n        Return a boolean Series showing whether each element in the Series\n        matches an element in the passed sequence of `values` exactly.\n\n        Parameters\n        ----------\n        values : set or list-like\n            The sequence of values to test. Passing in a single string will\n            raise a ``TypeError``. Instead, turn a single string into a\n            list of one element.\n\n        Returns\n        -------\n        Series\n            Series of booleans indicating if each element is in values.\n\n        Raises\n        ------\n        TypeError\n          * If `values` is a string\n\n        See Also\n        --------\n        DataFrame.isin : Equivalent method on DataFrame.\n\n        Examples\n        --------\n        >>> s = pd.Series(['lama', 'cow', 'lama', 'beetle', 'lama',\n        ...                'hippo'], name='animal')\n        >>> s.isin(['cow', 'lama'])\n        0     True\n        1     True\n        2     True\n        3    False\n        4     True\n        5    False\n        Name: animal, dtype: bool\n\n        To invert the boolean values, use the ``~`` operator:\n\n        >>> ~s.isin(['cow', 'lama'])\n        0    False\n        1    False\n        2    False\n        3     True\n        4    False\n        5     True\n        Name: animal, dtype: bool\n\n        Passing a single string as ``s.isin('lama')`` will raise an error. Use\n        a list of one element instead:\n\n        >>> s.isin(['lama'])\n        0     True\n        1    False\n        2     True\n        3    False\n        4     True\n        5    False\n        Name: animal, dtype: bool\n\n        Strings and integers are distinct and are therefore not comparable:\n\n        >>> pd.Series([1]).isin(['1'])\n        0    False\n        dtype: bool\n        >>> pd.Series([1.1]).isin(['1.1'])\n        0    False\n        dtype: bool\n        \"\"\"\n        result = algorithms.isin(self._values, values)\n        return self._constructor(result, index=self.index).__finalize__(\n            self, method=\"isin\"\n        )\n\n    def between(self, left, right, inclusive=\"both\") -> Series:\n        \"\"\"\n        Return boolean Series equivalent to left <= series <= right.\n\n        This function returns a boolean vector containing `True` wherever the\n        corresponding Series element is between the boundary values `left` and\n        `right`. NA values are treated as `False`.\n\n        Parameters\n        ----------\n        left : scalar or list-like\n            Left boundary.\n        right : scalar or list-like\n            Right boundary.\n        inclusive : {\"both\", \"neither\", \"left\", \"right\"}\n            Include boundaries. Whether to set each bound as closed or open.\n\n            .. versionchanged:: 1.3.0\n\n        Returns\n        -------\n        Series\n            Series representing whether each element is between left and\n            right (inclusive).\n\n        See Also\n        --------\n        Series.gt : Greater than of series and other.\n        Series.lt : Less than of series and other.\n\n        Notes\n        -----\n        This function is equivalent to ``(left <= ser) & (ser <= right)``\n\n        Examples\n        --------\n        >>> s = pd.Series([2, 0, 4, 8, np.nan])\n\n        Boundary values are included by default:\n\n        >>> s.between(1, 4)\n        0     True\n        1    False\n        2     True\n        3    False\n        4    False\n        dtype: bool\n\n        With `inclusive` set to ``\"neither\"`` boundary values are excluded:\n\n        >>> s.between(1, 4, inclusive=\"neither\")\n        0     True\n        1    False\n        2    False\n        3    False\n        4    False\n        dtype: bool\n\n        `left` and `right` can be any scalar value:\n\n        >>> s = pd.Series(['Alice', 'Bob', 'Carol', 'Eve'])\n        >>> s.between('Anna', 'Daniel')\n        0    False\n        1     True\n        2     True\n        3    False\n        dtype: bool\n        \"\"\"\n        if inclusive is True or inclusive is False:\n            warnings.warn(\n                \"Boolean inputs to the `inclusive` argument are deprecated in \"\n                \"favour of `both` or `neither`.\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n            if inclusive:\n                inclusive = \"both\"\n            else:\n                inclusive = \"neither\"\n        if inclusive == \"both\":\n            lmask = self >= left\n            rmask = self <= right\n        elif inclusive == \"left\":\n            lmask = self >= left\n            rmask = self < right\n        elif inclusive == \"right\":\n            lmask = self > left\n            rmask = self <= right\n        elif inclusive == \"neither\":\n            lmask = self > left\n            rmask = self < right\n        else:\n            raise ValueError(\n                \"Inclusive has to be either string of 'both',\"\n                \"'left', 'right', or 'neither'.\"\n            )\n\n        return lmask & rmask\n\n    # ----------------------------------------------------------------------\n    # Convert to types that support pd.NA\n\n    def _convert_dtypes(\n        self,\n        infer_objects: bool = True,\n        convert_string: bool = True,\n        convert_integer: bool = True,\n        convert_boolean: bool = True,\n        convert_floating: bool = True,\n    ) -> Series:\n        input_series = self\n        if infer_objects:\n            input_series = input_series.infer_objects()\n            if is_object_dtype(input_series):\n                input_series = input_series.copy()\n\n        if convert_string or convert_integer or convert_boolean or convert_floating:\n            inferred_dtype = convert_dtypes(\n                input_series._values,\n                convert_string,\n                convert_integer,\n                convert_boolean,\n                convert_floating,\n            )\n            result = input_series.astype(inferred_dtype)\n        else:\n            result = input_series.copy()\n        return result\n\n    # error: Cannot determine type of 'isna'\n    @doc(NDFrame.isna, klass=_shared_doc_kwargs[\"klass\"])  # type: ignore[has-type]\n    def isna(self) -> Series:\n        return NDFrame.isna(self)\n\n    # error: Cannot determine type of 'isna'\n    @doc(NDFrame.isna, klass=_shared_doc_kwargs[\"klass\"])  # type: ignore[has-type]\n    def isnull(self) -> Series:\n        \"\"\"\n        Series.isnull is an alias for Series.isna.\n        \"\"\"\n        return super().isnull()\n\n    # error: Cannot determine type of 'notna'\n    @doc(NDFrame.notna, klass=_shared_doc_kwargs[\"klass\"])  # type: ignore[has-type]\n    def notna(self) -> Series:\n        return super().notna()\n\n    # error: Cannot determine type of 'notna'\n    @doc(NDFrame.notna, klass=_shared_doc_kwargs[\"klass\"])  # type: ignore[has-type]\n    def notnull(self) -> Series:\n        \"\"\"\n        Series.notnull is an alias for Series.notna.\n        \"\"\"\n        return super().notnull()\n\n    @deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\"])\n    def dropna(self, axis=0, inplace=False, how=None):\n        \"\"\"\n        Return a new Series with missing values removed.\n\n        See the :ref:`User Guide <missing_data>` for more on which values are\n        considered missing, and how to work with missing data.\n\n        Parameters\n        ----------\n        axis : {0 or 'index'}, default 0\n            There is only one axis to drop values from.\n        inplace : bool, default False\n            If True, do operation inplace and return None.\n        how : str, optional\n            Not in use. Kept for compatibility.\n\n        Returns\n        -------\n        Series or None\n            Series with NA entries dropped from it or None if ``inplace=True``.\n\n        See Also\n        --------\n        Series.isna: Indicate missing values.\n        Series.notna : Indicate existing (non-missing) values.\n        Series.fillna : Replace missing values.\n        DataFrame.dropna : Drop rows or columns which contain NA values.\n        Index.dropna : Drop missing indices.\n\n        Examples\n        --------\n        >>> ser = pd.Series([1., 2., np.nan])\n        >>> ser\n        0    1.0\n        1    2.0\n        2    NaN\n        dtype: float64\n\n        Drop NA values from a Series.\n\n        >>> ser.dropna()\n        0    1.0\n        1    2.0\n        dtype: float64\n\n        Keep the Series with valid entries in the same variable.\n\n        >>> ser.dropna(inplace=True)\n        >>> ser\n        0    1.0\n        1    2.0\n        dtype: float64\n\n        Empty strings are not considered NA values. ``None`` is considered an\n        NA value.\n\n        >>> ser = pd.Series([np.NaN, 2, pd.NaT, '', None, 'I stay'])\n        >>> ser\n        0       NaN\n        1         2\n        2       NaT\n        3\n        4      None\n        5    I stay\n        dtype: object\n        >>> ser.dropna()\n        1         2\n        3\n        5    I stay\n        dtype: object\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        # Validate the axis parameter\n        self._get_axis_number(axis or 0)\n\n        if self._can_hold_na:\n            result = remove_na_arraylike(self)\n            if inplace:\n                self._update_inplace(result)\n            else:\n                return result\n        else:\n            if inplace:\n                # do nothing\n                pass\n            else:\n                return self.copy()\n\n    # ----------------------------------------------------------------------\n    # Time series-oriented methods\n\n    # error: Cannot determine type of 'asfreq'\n    @doc(NDFrame.asfreq, **_shared_doc_kwargs)  # type: ignore[has-type]\n    def asfreq(\n        self,\n        freq,\n        method=None,\n        how: str | None = None,\n        normalize: bool = False,\n        fill_value=None,\n    ) -> Series:\n        return super().asfreq(\n            freq=freq,\n            method=method,\n            how=how,\n            normalize=normalize,\n            fill_value=fill_value,\n        )\n\n    # error: Cannot determine type of 'resample'\n    @doc(NDFrame.resample, **_shared_doc_kwargs)  # type: ignore[has-type]\n    def resample(\n        self,\n        rule,\n        axis=0,\n        closed: str | None = None,\n        label: str | None = None,\n        convention: str = \"start\",\n        kind: str | None = None,\n        loffset=None,\n        base: int | None = None,\n        on=None,\n        level=None,\n        origin: str | TimestampConvertibleTypes = \"start_day\",\n        offset: TimedeltaConvertibleTypes | None = None,\n    ) -> Resampler:\n        return super().resample(\n            rule=rule,\n            axis=axis,\n            closed=closed,\n            label=label,\n            convention=convention,\n            kind=kind,\n            loffset=loffset,\n            base=base,\n            on=on,\n            level=level,\n            origin=origin,\n            offset=offset,\n        )\n\n    def to_timestamp(self, freq=None, how=\"start\", copy=True) -> Series:\n        \"\"\"\n        Cast to DatetimeIndex of Timestamps, at *beginning* of period.\n\n        Parameters\n        ----------\n        freq : str, default frequency of PeriodIndex\n            Desired frequency.\n        how : {'s', 'e', 'start', 'end'}\n            Convention for converting period to timestamp; start of period\n            vs. end.\n        copy : bool, default True\n            Whether or not to return a copy.\n\n        Returns\n        -------\n        Series with DatetimeIndex\n        \"\"\"\n        new_values = self._values\n        if copy:\n            new_values = new_values.copy()\n\n        if not isinstance(self.index, PeriodIndex):\n            raise TypeError(f\"unsupported Type {type(self.index).__name__}\")\n        new_index = self.index.to_timestamp(freq=freq, how=how)\n        return self._constructor(new_values, index=new_index).__finalize__(\n            self, method=\"to_timestamp\"\n        )\n\n    def to_period(self, freq=None, copy=True) -> Series:\n        \"\"\"\n        Convert Series from DatetimeIndex to PeriodIndex.\n\n        Parameters\n        ----------\n        freq : str, default None\n            Frequency associated with the PeriodIndex.\n        copy : bool, default True\n            Whether or not to return a copy.\n\n        Returns\n        -------\n        Series\n            Series with index converted to PeriodIndex.\n        \"\"\"\n        new_values = self._values\n        if copy:\n            new_values = new_values.copy()\n\n        if not isinstance(self.index, DatetimeIndex):\n            raise TypeError(f\"unsupported Type {type(self.index).__name__}\")\n        new_index = self.index.to_period(freq=freq)\n        return self._constructor(new_values, index=new_index).__finalize__(\n            self, method=\"to_period\"\n        )\n\n    @deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\"])\n    def ffill(\n        self: Series,\n        axis: None | Axis = None,\n        inplace: bool = False,\n        limit: None | int = None,\n        downcast=None,\n    ) -> Series | None:\n        return super().ffill(axis, inplace, limit, downcast)\n\n    @deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\"])\n    def bfill(\n        self: Series,\n        axis: None | Axis = None,\n        inplace: bool = False,\n        limit: None | int = None,\n        downcast=None,\n    ) -> Series | None:\n        return super().bfill(axis, inplace, limit, downcast)\n\n    @deprecate_nonkeyword_arguments(\n        version=None, allowed_args=[\"self\", \"lower\", \"upper\"]\n    )\n    def clip(\n        self: Series,\n        lower=None,\n        upper=None,\n        axis: Axis | None = None,\n        inplace: bool = False,\n        *args,\n        **kwargs,\n    ) -> Series | None:\n        return super().clip(lower, upper, axis, inplace, *args, **kwargs)\n\n    @deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\", \"method\"])\n    def interpolate(\n        self: Series,\n        method: str = \"linear\",\n        axis: Axis = 0,\n        limit: int | None = None,\n        inplace: bool = False,\n        limit_direction: str | None = None,\n        limit_area: str | None = None,\n        downcast: str | None = None,\n        **kwargs,\n    ) -> Series | None:\n        return super().interpolate(\n            method,\n            axis,\n            limit,\n            inplace,\n            limit_direction,\n            limit_area,\n            downcast,\n            **kwargs,\n        )\n\n    @deprecate_nonkeyword_arguments(\n        version=None, allowed_args=[\"self\", \"cond\", \"other\"]\n    )\n    def where(\n        self,\n        cond,\n        other=lib.no_default,\n        inplace=False,\n        axis=None,\n        level=None,\n        errors=lib.no_default,\n        try_cast=lib.no_default,\n    ):\n        return super().where(cond, other, inplace, axis, level, errors, try_cast)\n\n    @deprecate_nonkeyword_arguments(\n        version=None, allowed_args=[\"self\", \"cond\", \"other\"]\n    )\n    def mask(\n        self,\n        cond,\n        other=np.nan,\n        inplace=False,\n        axis=None,\n        level=None,\n        errors=lib.no_default,\n        try_cast=lib.no_default,\n    ):\n        return super().mask(cond, other, inplace, axis, level, errors, try_cast)\n\n    # ----------------------------------------------------------------------\n    # Add index\n    _AXIS_ORDERS = [\"index\"]\n    _AXIS_LEN = len(_AXIS_ORDERS)\n    _info_axis_number = 0\n    _info_axis_name = \"index\"\n\n    index: Index = properties.AxisProperty(\n        axis=0, doc=\"The index (axis labels) of the Series.\"\n    )\n\n    # ----------------------------------------------------------------------\n    # Accessor Methods\n    # ----------------------------------------------------------------------\n    str = CachedAccessor(\"str\", StringMethods)\n    dt = CachedAccessor(\"dt\", CombinedDatetimelikeProperties)\n    cat = CachedAccessor(\"cat\", CategoricalAccessor)\n    plot = CachedAccessor(\"plot\", pandas.plotting.PlotAccessor)\n    sparse = CachedAccessor(\"sparse\", SparseAccessor)\n\n    # ----------------------------------------------------------------------\n    # Add plotting methods to Series\n    hist = pandas.plotting.hist_series\n\n    # ----------------------------------------------------------------------\n    # Template-Based Arithmetic/Comparison Methods\n\n    def _cmp_method(self, other, op):\n        res_name = ops.get_op_result_name(self, other)\n\n        if isinstance(other, Series) and not self._indexed_same(other):\n            raise ValueError(\"Can only compare identically-labeled Series objects\")\n\n        lvalues = self._values\n        rvalues = extract_array(other, extract_numpy=True, extract_range=True)\n\n        with np.errstate(all=\"ignore\"):\n            res_values = ops.comparison_op(lvalues, rvalues, op)\n\n        return self._construct_result(res_values, name=res_name)\n\n    def _logical_method(self, other, op):\n        res_name = ops.get_op_result_name(self, other)\n        self, other = ops.align_method_SERIES(self, other, align_asobject=True)\n\n        lvalues = self._values\n        rvalues = extract_array(other, extract_numpy=True, extract_range=True)\n\n        res_values = ops.logical_op(lvalues, rvalues, op)\n        return self._construct_result(res_values, name=res_name)\n\n    def _arith_method(self, other, op):\n        self, other = ops.align_method_SERIES(self, other)\n        return base.IndexOpsMixin._arith_method(self, other, op)\n\n\nSeries._add_numeric_operations()\n\n# Add arithmetic!\nops.add_flex_arithmetic_methods(Series)\n", 5645], "/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/indexes/numeric.py": ["from __future__ import annotations\n\nfrom typing import (\n    Callable,\n    Hashable,\n)\nimport warnings\n\nimport numpy as np\n\nfrom pandas._libs import (\n    index as libindex,\n    lib,\n)\nfrom pandas._typing import (\n    Dtype,\n    DtypeObj,\n    npt,\n)\nfrom pandas.util._decorators import (\n    cache_readonly,\n    doc,\n)\nfrom pandas.util._exceptions import find_stack_level\n\nfrom pandas.core.dtypes.cast import astype_nansafe\nfrom pandas.core.dtypes.common import (\n    is_dtype_equal,\n    is_extension_array_dtype,\n    is_float,\n    is_float_dtype,\n    is_integer_dtype,\n    is_numeric_dtype,\n    is_scalar,\n    is_signed_integer_dtype,\n    is_unsigned_integer_dtype,\n    needs_i8_conversion,\n    pandas_dtype,\n)\nfrom pandas.core.dtypes.generic import ABCSeries\n\nfrom pandas.core.indexes.base import (\n    Index,\n    maybe_extract_name,\n)\n\n\nclass NumericIndex(Index):\n    \"\"\"\n    Immutable sequence used for indexing and alignment. The basic object\n    storing axis labels for all pandas objects. NumericIndex is a special case\n    of `Index` with purely numpy int/uint/float labels.\n\n    .. versionadded:: 1.4.0\n\n    Parameters\n    ----------\n    data : array-like (1-dimensional)\n    dtype : NumPy dtype (default: None)\n    copy : bool\n        Make a copy of input ndarray.\n    name : object\n        Name to be stored in the index.\n\n    Attributes\n    ----------\n    None\n\n    Methods\n    ----------\n    None\n\n    See Also\n    --------\n    Index : The base pandas Index type.\n    Int64Index : Index of purely int64 labels (deprecated).\n    UInt64Index : Index of purely uint64 labels (deprecated).\n    Float64Index : Index of  purely float64 labels (deprecated).\n\n    Notes\n    -----\n    An NumericIndex instance can **only** contain numpy int64/32/16/8, uint64/32/16/8 or\n    float64/32/16 dtype. In particular, ``NumericIndex`` *can not* hold Pandas numeric\n    dtypes (:class:`Int64Dtype`, :class:`Int32Dtype` etc.).\n    \"\"\"\n\n    _typ = \"numericindex\"\n    _values: np.ndarray\n    _default_dtype: np.dtype | None = None\n    _dtype_validation_metadata: tuple[Callable[..., bool], str] = (\n        is_numeric_dtype,\n        \"numeric type\",\n    )\n    _is_numeric_dtype = True\n    _can_hold_strings = False\n    _is_backward_compat_public_numeric_index: bool = True\n\n    # error: Signature of \"_can_hold_na\" incompatible with supertype \"Index\"\n    @cache_readonly\n    def _can_hold_na(self) -> bool:  # type: ignore[override]\n        if is_float_dtype(self.dtype):\n            return True\n        else:\n            return False\n\n    _engine_types: dict[np.dtype, type[libindex.IndexEngine]] = {\n        np.dtype(np.int8): libindex.Int8Engine,\n        np.dtype(np.int16): libindex.Int16Engine,\n        np.dtype(np.int32): libindex.Int32Engine,\n        np.dtype(np.int64): libindex.Int64Engine,\n        np.dtype(np.uint8): libindex.UInt8Engine,\n        np.dtype(np.uint16): libindex.UInt16Engine,\n        np.dtype(np.uint32): libindex.UInt32Engine,\n        np.dtype(np.uint64): libindex.UInt64Engine,\n        np.dtype(np.float32): libindex.Float32Engine,\n        np.dtype(np.float64): libindex.Float64Engine,\n    }\n\n    @property\n    def _engine_type(self):\n        # error: Invalid index type \"Union[dtype[Any], ExtensionDtype]\" for\n        # \"Dict[dtype[Any], Type[IndexEngine]]\"; expected type \"dtype[Any]\"\n        return self._engine_types[self.dtype]  # type: ignore[index]\n\n    @cache_readonly\n    def inferred_type(self) -> str:\n        return {\n            \"i\": \"integer\",\n            \"u\": \"integer\",\n            \"f\": \"floating\",\n        }[self.dtype.kind]\n\n    def __new__(cls, data=None, dtype: Dtype | None = None, copy=False, name=None):\n        name = maybe_extract_name(name, data, cls)\n\n        subarr = cls._ensure_array(data, dtype, copy)\n        return cls._simple_new(subarr, name=name)\n\n    @classmethod\n    def _ensure_array(cls, data, dtype, copy: bool):\n        \"\"\"\n        Ensure we have a valid array to pass to _simple_new.\n        \"\"\"\n        cls._validate_dtype(dtype)\n\n        if not isinstance(data, (np.ndarray, Index)):\n            # Coerce to ndarray if not already ndarray or Index\n            if is_scalar(data):\n                raise cls._scalar_data_error(data)\n\n            # other iterable of some kind\n            if not isinstance(data, (ABCSeries, list, tuple)):\n                data = list(data)\n\n            orig = data\n            data = np.asarray(data, dtype=dtype)\n            if dtype is None and data.dtype.kind == \"f\":\n                if cls is UInt64Index and (data >= 0).all():\n                    # https://github.com/numpy/numpy/issues/19146\n                    data = np.asarray(orig, dtype=np.uint64)\n\n        if issubclass(data.dtype.type, str):\n            cls._string_data_error(data)\n\n        dtype = cls._ensure_dtype(dtype)\n\n        if copy or not is_dtype_equal(data.dtype, dtype):\n            # TODO: the try/except below is because it's difficult to predict the error\n            # and/or error message from different combinations of data and dtype.\n            # Efforts to avoid this try/except welcome.\n            # See https://github.com/pandas-dev/pandas/pull/41153#discussion_r676206222\n            try:\n                subarr = np.array(data, dtype=dtype, copy=copy)\n                cls._validate_dtype(subarr.dtype)\n            except (TypeError, ValueError):\n                raise ValueError(f\"data is not compatible with {cls.__name__}\")\n            cls._assert_safe_casting(data, subarr)\n        else:\n            subarr = data\n\n        if subarr.ndim > 1:\n            # GH#13601, GH#20285, GH#27125\n            raise ValueError(\"Index data must be 1-dimensional\")\n\n        subarr = np.asarray(subarr)\n        return subarr\n\n    @classmethod\n    def _validate_dtype(cls, dtype: Dtype | None) -> None:\n        if dtype is None:\n            return\n\n        validation_func, expected = cls._dtype_validation_metadata\n        if not validation_func(dtype):\n            raise ValueError(\n                f\"Incorrect `dtype` passed: expected {expected}, received {dtype}\"\n            )\n\n    @classmethod\n    def _ensure_dtype(cls, dtype: Dtype | None) -> np.dtype | None:\n        \"\"\"\n        Ensure int64 dtype for Int64Index etc. but allow int32 etc. for NumericIndex.\n\n        Assumes dtype has already been validated.\n        \"\"\"\n        if dtype is None:\n            return cls._default_dtype\n\n        dtype = pandas_dtype(dtype)\n        assert isinstance(dtype, np.dtype)\n\n        if cls._is_backward_compat_public_numeric_index:\n            # dtype for NumericIndex\n            return dtype\n        else:\n            # dtype for Int64Index, UInt64Index etc. Needed for backwards compat.\n            return cls._default_dtype\n\n    def __contains__(self, key) -> bool:\n        \"\"\"\n        Check if key is a float and has a decimal. If it has, return False.\n        \"\"\"\n        if not is_integer_dtype(self.dtype):\n            return super().__contains__(key)\n\n        hash(key)\n        try:\n            if is_float(key) and int(key) != key:\n                # otherwise the `key in self._engine` check casts e.g. 1.1 -> 1\n                return False\n            return key in self._engine\n        except (OverflowError, TypeError, ValueError):\n            return False\n\n    @doc(Index.astype)\n    def astype(self, dtype, copy: bool = True):\n        dtype = pandas_dtype(dtype)\n        if is_float_dtype(self.dtype):\n            if needs_i8_conversion(dtype):\n                raise TypeError(\n                    f\"Cannot convert Float64Index to dtype {dtype}; integer \"\n                    \"values are required for conversion\"\n                )\n            elif is_integer_dtype(dtype) and not is_extension_array_dtype(dtype):\n                # TODO(ExtensionIndex); this can change once we have an EA Index type\n                # GH 13149\n                arr = astype_nansafe(self._values, dtype=dtype)\n                if isinstance(self, Float64Index):\n                    return Int64Index(arr, name=self.name)\n                else:\n                    return NumericIndex(arr, name=self.name, dtype=dtype)\n        elif self._is_backward_compat_public_numeric_index:\n            # this block is needed so e.g. NumericIndex[int8].astype(\"int32\") returns\n            # NumericIndex[int32] and not Int64Index with dtype int64.\n            # When Int64Index etc. are removed from the code base, removed this also.\n            if not is_extension_array_dtype(dtype) and is_numeric_dtype(dtype):\n                return self._constructor(self, dtype=dtype, copy=copy)\n\n        return super().astype(dtype, copy=copy)\n\n    # ----------------------------------------------------------------\n    # Indexing Methods\n\n    # error: Decorated property not supported\n    @cache_readonly  # type: ignore[misc]\n    @doc(Index._should_fallback_to_positional)\n    def _should_fallback_to_positional(self) -> bool:\n        return False\n\n    @doc(Index._convert_slice_indexer)\n    def _convert_slice_indexer(self, key: slice, kind: str):\n        if is_float_dtype(self.dtype):\n            assert kind in [\"loc\", \"getitem\"]\n\n            # We always treat __getitem__ slicing as label-based\n            # translate to locations\n            return self.slice_indexer(key.start, key.stop, key.step)\n\n        return super()._convert_slice_indexer(key, kind=kind)\n\n    @doc(Index._maybe_cast_slice_bound)\n    def _maybe_cast_slice_bound(self, label, side: str, kind=lib.no_default):\n        assert kind in [\"loc\", \"getitem\", None, lib.no_default]\n        self._deprecated_arg(kind, \"kind\", \"_maybe_cast_slice_bound\")\n\n        # we will try to coerce to integers\n        return self._maybe_cast_indexer(label)\n\n    # ----------------------------------------------------------------\n\n    @doc(Index._shallow_copy)\n    def _shallow_copy(self, values, name: Hashable = lib.no_default):\n        if not self._can_hold_na and values.dtype.kind == \"f\":\n            name = self._name if name is lib.no_default else name\n            # Ensure we are not returning an Int64Index with float data:\n            return Float64Index._simple_new(values, name=name)\n        return super()._shallow_copy(values=values, name=name)\n\n    def _convert_tolerance(self, tolerance, target):\n        tolerance = super()._convert_tolerance(tolerance, target)\n\n        if not np.issubdtype(tolerance.dtype, np.number):\n            if tolerance.ndim > 0:\n                raise ValueError(\n                    f\"tolerance argument for {type(self).__name__} must contain \"\n                    \"numeric elements if it is list type\"\n                )\n            else:\n                raise ValueError(\n                    f\"tolerance argument for {type(self).__name__} must be numeric \"\n                    f\"if it is a scalar: {repr(tolerance)}\"\n                )\n        return tolerance\n\n    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:\n        # If we ever have BoolIndex or ComplexIndex, this may need to be tightened\n        return is_numeric_dtype(dtype)\n\n    @classmethod\n    def _assert_safe_casting(cls, data: np.ndarray, subarr: np.ndarray) -> None:\n        \"\"\"\n        Ensure incoming data can be represented with matching signed-ness.\n\n        Needed if the process of casting data from some accepted dtype to the internal\n        dtype(s) bears the risk of truncation (e.g. float to int).\n        \"\"\"\n        if is_integer_dtype(subarr.dtype):\n            if not np.array_equal(data, subarr):\n                raise TypeError(\"Unsafe NumPy casting, you must explicitly cast\")\n\n    @property\n    def _is_all_dates(self) -> bool:\n        \"\"\"\n        Checks that all the labels are datetime objects.\n        \"\"\"\n        return False\n\n    def _format_native_types(\n        self, *, na_rep=\"\", float_format=None, decimal=\".\", quoting=None, **kwargs\n    ):\n        from pandas.io.formats.format import FloatArrayFormatter\n\n        if is_float_dtype(self.dtype):\n            formatter = FloatArrayFormatter(\n                self._values,\n                na_rep=na_rep,\n                float_format=float_format,\n                decimal=decimal,\n                quoting=quoting,\n                fixed_width=False,\n            )\n            return formatter.get_result_as_array()\n\n        return super()._format_native_types(\n            na_rep=na_rep,\n            float_format=float_format,\n            decimal=decimal,\n            quoting=quoting,\n            **kwargs,\n        )\n\n\n_num_index_shared_docs = {}\n\n\n_num_index_shared_docs[\n    \"class_descr\"\n] = \"\"\"\n    Immutable sequence used for indexing and alignment. The basic object\n    storing axis labels for all pandas objects. %(klass)s is a special case\n    of `Index` with purely %(ltype)s labels. %(extra)s.\n\n    .. deprecated:: 1.4.0\n        In pandas v2.0 %(klass)s will be removed and :class:`NumericIndex` used instead.\n        %(klass)s will remain fully functional for the duration of pandas 1.x.\n\n    Parameters\n    ----------\n    data : array-like (1-dimensional)\n    dtype : NumPy dtype (default: %(dtype)s)\n    copy : bool\n        Make a copy of input ndarray.\n    name : object\n        Name to be stored in the index.\n\n    Attributes\n    ----------\n    None\n\n    Methods\n    ----------\n    None\n\n    See Also\n    --------\n    Index : The base pandas Index type.\n    NumericIndex : Index of numpy int/uint/float data.\n\n    Notes\n    -----\n    An Index instance can **only** contain hashable objects.\n\"\"\"\n\n\nclass IntegerIndex(NumericIndex):\n    \"\"\"\n    This is an abstract class for Int64Index, UInt64Index.\n    \"\"\"\n\n    _is_backward_compat_public_numeric_index: bool = False\n\n    @property\n    def asi8(self) -> npt.NDArray[np.int64]:\n        # do not cache or you'll create a memory leak\n        warnings.warn(\n            \"Index.asi8 is deprecated and will be removed in a future version.\",\n            FutureWarning,\n            stacklevel=find_stack_level(),\n        )\n        return self._values.view(self._default_dtype)\n\n    def _validate_fill_value(self, value):\n        # e.g. np.array([1.0]) we want np.array([1], dtype=self.dtype)\n        #  see TestSetitemFloatNDarrayIntoIntegerSeries\n        super()._validate_fill_value(value)\n        if hasattr(value, \"dtype\") and is_float_dtype(value.dtype):\n            converted = value.astype(self.dtype)\n            if (converted == value).all():\n                # See also: can_hold_element\n                return converted\n            raise TypeError\n        return value\n\n\nclass Int64Index(IntegerIndex):\n    _index_descr_args = {\n        \"klass\": \"Int64Index\",\n        \"ltype\": \"integer\",\n        \"dtype\": \"int64\",\n        \"extra\": \"\",\n    }\n    __doc__ = _num_index_shared_docs[\"class_descr\"] % _index_descr_args\n\n    _typ = \"int64index\"\n    _engine_type = libindex.Int64Engine\n    _default_dtype = np.dtype(np.int64)\n    _dtype_validation_metadata = (is_signed_integer_dtype, \"signed integer\")\n\n\nclass UInt64Index(IntegerIndex):\n    _index_descr_args = {\n        \"klass\": \"UInt64Index\",\n        \"ltype\": \"unsigned integer\",\n        \"dtype\": \"uint64\",\n        \"extra\": \"\",\n    }\n    __doc__ = _num_index_shared_docs[\"class_descr\"] % _index_descr_args\n\n    _typ = \"uint64index\"\n    _engine_type = libindex.UInt64Engine\n    _default_dtype = np.dtype(np.uint64)\n    _dtype_validation_metadata = (is_unsigned_integer_dtype, \"unsigned integer\")\n\n    def _validate_fill_value(self, value):\n        # e.g. np.array([1]) we want np.array([1], dtype=np.uint64)\n        #  see test_where_uin64\n        super()._validate_fill_value(value)\n        if hasattr(value, \"dtype\") and is_signed_integer_dtype(value.dtype):\n            if (value >= 0).all():\n                return value.astype(self.dtype)\n            raise TypeError\n        return value\n\n\nclass Float64Index(NumericIndex):\n    _index_descr_args = {\n        \"klass\": \"Float64Index\",\n        \"dtype\": \"float64\",\n        \"ltype\": \"float\",\n        \"extra\": \"\",\n    }\n    __doc__ = _num_index_shared_docs[\"class_descr\"] % _index_descr_args\n\n    _typ = \"float64index\"\n    _engine_type = libindex.Float64Engine\n    _default_dtype = np.dtype(np.float64)\n    _dtype_validation_metadata = (is_float_dtype, \"float\")\n    _is_backward_compat_public_numeric_index: bool = False\n", 488]}, "functions": {"_check (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/generic.py:43)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/generic.py", 43], "is_hashable (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/inference.py:321)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/inference.py", 321], "maybe_extract_name (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/indexes/base.py:7082)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/indexes/base.py", 7082], "is_empty_data (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/construction.py:802)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/construction.py", 802], "<genexpr> (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/inference.py:288)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/inference.py", 288], "is_dict_like (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/inference.py:262)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/inference.py", 262], "__instancecheck__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/abc.py:117)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/abc.py", 117], "_check_methods (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/_collections_abc.py:78)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/_collections_abc.py", 78], "__subclasshook__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/_collections_abc.py:381)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/_collections_abc.py", 381], "__subclasscheck__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/abc.py:121)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/abc.py", 121], "cast (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/typing.py:1736)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/typing.py", 1736], "maybe_iterable_to_list (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/common.py:287)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/common.py", 287], "_reset_identity (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/indexes/base.py:834)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/indexes/base.py", 834], "_simple_new (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/indexes/range.py:167)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/indexes/range.py", 167], "default_index (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/indexes/api.py:322)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/indexes/api.py", 322], "extract_array (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/construction.py:379)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/construction.py", 379], "construct_1d_object_array_from_listlike (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/cast.py:1960)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/cast.py", 1960], "copyto (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/numpy/core/multiarray.py:1071)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/numpy/core/multiarray.py", 1071], "full (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/numpy/core/numeric.py:289)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/numpy/core/numeric.py", 289], "maybe_convert_platform (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/cast.py:115)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/cast.py", 115], "maybe_infer_to_datetimelike (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/cast.py:1466)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/cast.py", 1466], "_maybe_repeat (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/construction.py:684)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/construction.py", 684], "_sanitize_ndim (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/construction.py:627)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/construction.py", 627], "_sanitize_str_dtypes (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/construction.py:664)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/construction.py", 664], "sanitize_array (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/construction.py:470)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/construction.py", 470], "_select_options (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:571)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py", 571], "_get_deprecated_option (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:603)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py", 603], "_warn_if_deprecated (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:642)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py", 642], "_translate_key (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:630)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py", 630], "_get_single_key (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:109)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py", 109], "_get_root (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:589)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py", 589], "_get_option (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:127)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py", 127], "__call__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py:255)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/_config/config.py", 255], "__len__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/indexes/range.py:909)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/indexes/range.py", 909], "is_1d_only_ea_dtype (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/common.py:1416)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/common.py", 1416], "check_ndim (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/internals/blocks.py:2055)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/internals/blocks.py", 2055], "get_block_type (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/internals/blocks.py:1989)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/internals/blocks.py", 1989], "ensure_wrapped_if_datetimelike (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/construction.py:438)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/construction.py", 438], "maybe_coerce_values (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/internals/blocks.py:1960)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/internals/blocks.py", 1960], "new_block (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/internals/blocks.py:2041)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/internals/blocks.py", 2041], "__init__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/internals/managers.py:1700)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/internals/managers.py", 1700], "from_array (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/internals/managers.py:1731)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/internals/managers.py", 1731], "__init__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/flags.py:47)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/flags.py", 47], "__init__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/generic.py:239)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/generic.py", 239], "__getattr__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/generic.py:5561)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/generic.py", 5561], "name (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/series.py:590)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/series.py", 590], "<genexpr> (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/common.py:1740)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/common.py", 1740], "validate_all_hashable (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/common.py:1721)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/dtypes/common.py", 1721], "name (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/series.py:640)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/series.py", 640], "__setattr__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/generic.py:5577)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/generic.py", 5577], "_is_all_dates (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/indexes/numeric.py:331)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/indexes/numeric.py", 331], "_set_axis (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/series.py:542)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/series.py", 542], "__init__ (/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/series.py:323)": ["/home/tankerma/miniconda/envs/grav_inv2/lib/python3.10/site-packages/pandas/core/series.py", 323]}}}